Core guard system Dynamo detects when compiled code needs recompiled due changes program state Guards conditions must remain true previously-compiled code valid reuse This module provides infrastructure creating managing checking guards including - Guard creation composition - Guard state management invalidation - Guard checking failure handling - Utilities guard optimization debugging - Integration Dynamo s compilation caching The guard system critical Dynamo s ability efficiently reuse compiled code while maintaining correctness detecting when recompilation necessary due changes program state tensor properties control flow __future__ annotations ast builtins collections dataclasses enum functools importlib inspect io logging math pickle sys textwrap traceback types warnings weakref contextlib contextmanager copy deepcopy inspect currentframe typing Any NoReturn Optional TYPE_CHECKING Union try typing LiteralString except ImportError typing_extensions LiteralString typing_extensions TypeAliasType TypeVar weakref ReferenceType torch torch overrides torch utils _device torch _C _dynamo eval_frame code_framelocals_names torch _C _dynamo guards check_obj_id check_type_id ClosureGuardAccessor CodeGuardAccessor dict_version DictGetItemGuardAccessor DictGuardManager FuncDefaultsGuardAccessor FuncKwDefaultsGuardAccessor GetAttrGuardAccessor GetGenericDictGuardAccessor GuardAccessor GuardDebugInfo GuardManager install_no_tensor_aliasing_guard install_object_aliasing_guard install_storage_overlapping_guard install_symbolic_shape_guard LeafGuard profile_guard_manager RelationalGuard RootGuardManager TupleGetItemGuardAccessor TypeDictGuardAccessor TypeGuardAccessor TypeMROGuardAccessor torch _dynamo source get_global_source_name get_local_source_name IndexedSource is_from_flatten_script_object_source is_from_local_source is_from_optimizer_source is_from_skip_guard_source is_from_unspecialized_builtin_nn_module_source TensorProperty TensorPropertySource torch _dynamo utils CompileEventLogger get_metrics_context torch _guards CompileContext CompileId DuplicateInputs Guard GuardBuilderBase GuardEnvExpr GuardSource Source StorageOverlap torch _inductor utils IndentedBuffer torch _logging structured torch _utils_internal justknobs_check torch fx experimental symbolic_shapes _CppShapeGuardsHelper _ShapeGuardsHelper EqualityConstraint is_symbolic SYMPY_INTERP torch utils _pytree pytree torch utils _ordered_set OrderedSet torch utils _traceback format_frame report_compile_source_on_error torch utils weak TensorWeakRef config convert_frame exc eval_frame set_guard_error_hook source AttrProxySource AttrSource CallFunctionNoArgsSource CallMethodItemSource ChainedSource ClosureSource CodeSource ConstantSource ConstDictKeySource CurrentStreamSource DataclassFieldsSource DefaultsSource DictGetItemSource DictSubclassGetItemSource DynamicScalarSource FlattenScriptObjectSource FloatTensorSource FSDPNNModuleSource GenericAttrSource GetItemSource GlobalSource GlobalStateSource GlobalWeakRefSource GradSource ListGetItemSource LocalSource NamedTupleFieldsSource NNModuleSource NonSerializableSetGetItemSource NumpyTensorSource OptimizerSource ScriptObjectQualifiedNameSource ShapeEnvSource SubclassAttrListSource TorchFunctionModeStackSource TorchSource TupleIteratorGetItemSource TypeDictSource TypeMROSource TypeSource UnspecializedBuiltinNNModuleSource UnspecializedNNModuleSource UnspecializedParamBufferSource WeakRefCallSource types noqa F CacheEntry DynamoFrameType ExtraState GuardedCode GuardFail GuardFilterEntry GuardFn utils builtin_dict_keys common_constant_types dataclass_fields dict_keys get_current_stream get_custom_getattr get_torch_function_mode_stack get_torch_function_mode_stack_at guard_failures istype key_is_id key_to_id normalize_range_iter orig_code_map tensor_always_has_static_shape tuple_iterator_getitem tuple_iterator_len unpatched_nn_module_getattr verify_guard_fn_signature TYPE_CHECKING collections abc Callable guard_manager_testing_hook_fn Optional Callable Any Any Any Any = None try numpy np except ModuleNotFoundError np = None type ignore assignment TYPE_CHECKING collections abc Generator KeysView Sequence sympy Symbol torch _C DispatchKeySet torch _dynamo output_graph OutputGraphCommon OutputGraphGuardsState T = TypeVar T log = logging getLogger __name__ guards_log = torch _logging getArtifactLogger __name__ guards recompiles_log = torch _logging getArtifactLogger __name__ recompiles recompiles_verbose_log = torch _logging getArtifactLogger __name__ recompiles_verbose verbose_guards_log = torch _logging getArtifactLogger __name__ verbose_guards dunder_attrs_assumed_constants = __defaults__ __kwdefaults__ __code__ __closure__ __annotations__ __func__ __mro__ get_framelocals_idx code types CodeType var_name str - int Refer index frame s localsplus directly NOTE name order code object doesn t change NOTE we need find LAST matching index because = contains duplicate names case cells name can both local cell will take up slots frame s localsplus The correct behavior refer cell which has higher index framelocals_names_reversed = code_framelocals_names_reversed_cached code framelocals_idx = len framelocals_names_reversed - framelocals_names_reversed index var_name - framelocals_idx IndentedBufferWithPrefix IndentedBuffer prefix - str &#124; _indent tabwidth writeline line str skip_prefix bool = False - None type ignore override skip_prefix super writeline line super writeline +- + line GuardManagerWrapper A helper contains root guard manager An instance stored Dynamo cache entry so cache entry can access RootGuardManager stored root attribute directly call check_nopybind C++ __init__ root Optional RootGuardManager = None - None root None root = RootGuardManager root = root diff_guard_root Optional RootGuardManager = None closure_vars Optional dict str Any = None args Optional list str = None code_parts list str = verbose_code_parts Optional list str = None global_scope Optional dict str Any = None guard_fail_fn Optional Callable GuardFail None = None cache_entry Optional CacheEntry = None extra_state Optional ExtraState = None id_matched_objs dict str ReferenceType object = no_tensor_aliasing_sources list str = printed_relational_guards set RelationalGuard = set diff_guard_sources OrderedSet str = OrderedSet contextmanager _preserve_printed_relational_guards - Generator None None None printed_relational_guards = set try yield finally printed_relational_guards = set TODO clarify what fn attributes guard manager has get right things here collect_diff_guard_sources - OrderedSet str At time finalize we have only marked guard managers TENSOR_MATCH guards diff guard managers So we do tree traversal collect all nodes tree branches lead tensor guards After recompilation some guard managers will have fail_count so we collect them well Later we accumulate diff guard sources all guard managers visit_dict_manager node DictGuardManager - bool is_diff_guard_node = node get_source diff_guard_sources node fail_count _idx key_mgr val_mgr sorted node get_key_value_managers items is_diff_guard_node &#124; = visit key_mgr &#124; visit val_mgr is_diff_guard_node diff_guard_sources add node get_source is_diff_guard_node visit_manager node GuardManager - bool assert isinstance node DictGuardManager is_diff_guard_node = node get_source diff_guard_sources node fail_count child_mgr node get_child_managers is_diff_guard_node &#124; = visit child_mgr is_diff_guard_node diff_guard_sources add node get_source is_diff_guard_node visit node GuardManager - bool node None False isinstance node DictGuardManager visit_dict_manager node visit_manager node visit root diff_guard_sources finalize - None config use_recursive_dict_tags_for_guards justknobs_check pytorch compiler use_recursive_dict_tags_for_guards find_tag_safe_roots prepare_diff_guard_manager prepare_diff_guard_manager - None collect_diff_guard_sources populate_diff_guard_manager find_tag_safe_roots - None Identify ` ` tag safe nodes ` ` ` ` tag safe roots ` ` within guard tree ----------------------------------------------------------------------- tag safe node ----------------------------------------------------------------------- A tag safe node ` ` GuardManager ` ` whose guarded value satisfies one following conditions Immutable value - The value intrinsically immutable according ` ` is_immutable_object ` ` Tensors considered immutable To ensure symbolic guards run we also check GuardManager has no accessors Nested tag safe dictionary - The value ` ` dict ` ` whose keys values all tag safe nodes checked recursively Such dictionaries allow entire nested structures skipped once their identity tag matches Pure ` ` nn Module ` ` - The value ` ` nn Module ` ` whose sole accessor ` ` GetGenericDictGuardAccessor ` ` — i e only exposes its ` ` __dict__ ` ` nothing could mutate between runs For every tag safe node verifying identity tag just top-level dictionary enough guarantee entire subtree unchanged enabling fast-path guard check ----------------------------------------------------------------------- tag safe root ----------------------------------------------------------------------- A ` ` tag safe root ` ` tag safe node whose parent tag safe These boundary nodes mark points where guard evaluation can safely prune traversal tag-safe root s dictionary tag matches entire subtree beneath skipped One strong requirement tag safe root guarded object support weakref Refer more details Recursive dict tag matching note In short we need save weakref object first invocation check still valid later iterations apply recursive dict tag optimizations ` dict ` objects do NOT support weakref Therefore now we only mark nn module related guard managers tag safe roots Algorithm --------- The search runs post-order traversal Visit leaves classify them tag safe Propagate tag-safety upward parent dictionary becomes tag safe only all its children already tag-safe Propagate tag-safe-rootness upward whole subtree tag safe current node becomes new tag safe root otherwise propagate subtree tag safe roots Collect every tag safe node inspecting parent tags label subset tag safe roots check_tag_safety node GuardManager accepted_accessors tuple type GuardAccessor - bool accessors = node get_accessors child_mgrs = node get_child_managers all isinstance accessor accepted_accessors mgr is_tag_safe accessor mgr zip accessors child_mgrs visit_dict_manager node DictGuardManager - list GuardManager Just recurse through key value dict managers check all them tag safe nodes assert issubclass node get_type_of_guarded_value dict tag_safe_roots = is_subtree_tag_safe = True Recurse get tag safe roots subtree _idx key_mgr val_mgr sorted node get_key_value_managers items key_mgr None visit key_mgr val_mgr None tag_safe_roots extend visit val_mgr key_mgr val_mgr node get_key_value_managers values key_mgr is_subtree_tag_safe = key_mgr is_tag_safe val_mgr is_subtree_tag_safe = val_mgr is_tag_safe is_subtree_tag_safe node mark_tag_safe tag_safe_roots visit_manager node GuardManager - list GuardManager assert isinstance node DictGuardManager Collect subtree tag safe roots tag_safe_roots = child_mgr node get_child_managers tag_safe_roots extend visit child_mgr node is_guarded_value_immutable If node guards tensor mark tag safe only there no accessors Presence accessors means presence symbolic shape guards issubclass node get_type_of_guarded_value torch Tensor node has_no_accessors node has_object_aliasing_guard node mark_tag_safe node mark_tag_safe issubclass node get_type_of_guarded_value dict accessors = node get_accessors child_mgrs = node get_child_managers is_subtree_tag_safe = all isinstance accessor DictGetItemGuardAccessor mgr is_tag_safe accessor mgr zip accessors child_mgrs is_subtree_tag_safe node mark_tag_safe issubclass node get_type_of_guarded_value torch nn Module is_subtree_tag_safe = check_tag_safety node GetGenericDictGuardAccessor TypeGuardAccessor is_subtree_tag_safe node mark_tag_safe Return current node tag safe root discarding subtree tag safe roots node node get_type_of_guarded_value types FunctionType types MethodType staticmethod classmethod config assume_dunder_attributes_remain_unchanged Assumption callers will reassignthe attributes func __code__ func __closure__ func __defaults__ func __kwdefaults__ Mutating objects those attributes point fine rebinding attribute itself Example ─ allowed foo __defaults__ bar = forbidden foo __defaults__ = is_subtree_tag_safe = check_tag_safety node CodeGuardAccessor ClosureGuardAccessor FuncDefaultsGuardAccessor FuncKwDefaultsGuardAccessor GetAttrGuardAccessor accessor node get_accessors isinstance accessor GetAttrGuardAccessor is_subtree_tag_safe = accessor get_attr_name dunder_attrs_assumed_constants is_subtree_tag_safe node mark_tag_safe issubclass node get_type_of_guarded_value types CellType is_subtree_tag_safe = check_tag_safety node GetAttrGuardAccessor is_subtree_tag_safe = all isinstance accessor GetAttrGuardAccessor accessor get_attr_name == cell_contents accessor node get_accessors is_subtree_tag_safe node mark_tag_safe issubclass node get_type_of_guarded_value tuple node get_source endswith dunder_attrs_assumed_constants config assume_dunder_attributes_remain_unchanged We trust tuples obtained function s __closure__ __defaults__ Any other tuple-valued attribute can silently replaced — example foo bar = original foo bar = rebinding our dict-tag optimisation won t see Therefore only tuples __closure__ __defaults__ participate recursive-dict-tag optimization all others ignored is_subtree_tag_safe = check_tag_safety node TupleGetItemGuardAccessor is_subtree_tag_safe node mark_tag_safe issubclass node get_type_of_guarded_value type is_subtree_tag_safe = check_tag_safety node TypeDictGuardAccessor TypeMROGuardAccessor is_subtree_tag_safe node mark_tag_safe tag_safe_roots visit node GuardManager - list GuardManager node None isinstance node DictGuardManager visit_dict_manager node visit_manager node tag_safe_roots = visit root node tag_safe_roots issubclass node get_type_of_guarded_value torch nn Module node mark_tag_safe_root populate_diff_guard_manager - None diff_guard_root = clone_with_chosen_sources diff_guard_sources Ensure C++ side points updated diff guard manager When new GuardManagerWrapper created does have cache_entry attribute so relies CacheEntry constructor set diff_guard_root C++ But once saved Dynamo cache C++ side adds cache_entry attribute On recompiles cache_entry visible so we update C++ side point update guard manager cache_entry cache_entry update_diff_guard_root_manager clone_with_chosen_sources chosen_sources OrderedSet str - RootGuardManager filter_fn node_mgr GuardManager - bool node_mgr get_source chosen_sources root clone_manager filter_fn get_guard_lines guard LeafGuard - list str guard_name = guard __class__ __name__ parts = guard verbose_code_parts parts = guard_name + + part part parts parts get_manager_line guard_manager GuardManager accessor_str Optional str = None - str source = guard_manager get_source t = guard_manager __class__ __name__ s = t + source= + source accessor_str s += + accessor_str s += f type= guard_manager get_type_of_guarded_value s += f tag_safe= guard_manager is_tag_safe guard_manager is_tag_safe_root s construct_dict_manager_string mgr DictGuardManager body IndentedBufferWithPrefix - None idx key_mgr val_mgr sorted mgr get_key_value_managers items body writeline f KeyValueManager pair index= idx body indent key_mgr body writeline f KeyManager get_manager_line key_mgr construct_manager_string key_mgr body val_mgr body writeline f ValueManager get_manager_line val_mgr construct_manager_string val_mgr body construct_manager_string mgr GuardManager body IndentedBufferWithPrefix - None body indent guard mgr get_leaf_guards isinstance guard RelationalGuard guard printed_relational_guards printed_relational_guards add guard pyrefly ignore bad-argument-type body writelines get_guard_lines guard body writelines guard __class__ __name__ body writelines get_guard_lines guard This works both DictGuardManager SubclassedDictGuardManager isinstance mgr DictGuardManager construct_dict_manager_string mgr body General case GuardManager RootGuardManager accessor child_mgr zip mgr get_accessors mgr get_child_managers body writeline get_manager_line child_mgr f accessed_by= accessor repr construct_manager_string child_mgr body __str__ - str _preserve_printed_relational_guards body = IndentedBufferWithPrefix body tabwidth = body writeline skip_prefix=True body writeline TREE_GUARD_MANAGER skip_prefix=True body writeline RootGuardManager construct_manager_string root body hasattr root get_epilogue_lambda_guards guard root get_epilogue_lambda_guards body writelines get_guard_lines guard body getvalue check x Any - bool Only needed debugging purposes root check x check_verbose x Any - GuardDebugInfo Only needed debugging purposes root check_verbose x populate_code_parts_for_debugging - None This should called when guard manager fully populated relational_guards_seen = set get_code_parts leaf_guard LeafGuard - list str code_parts = verbose_code_part leaf_guard verbose_code_parts code_part = verbose_code_part split rstrip code_parts append code_part code_parts visit mgr GuardManager - None nonlocal relational_guards_seen guard mgr get_leaf_guards isinstance guard RelationalGuard guard relational_guards_seen pyrefly ignore bad-argument-type code_parts extend get_code_parts guard relational_guards_seen add guard code_parts extend get_code_parts guard child_mgr mgr get_child_managers visit child_mgr visit root from_numpy Any - torch Tensor If numpy array piggy back e g tensor guards check type Re-enable torch function since we disable leaf guards we need properly construct tensor default device set torch overrides _enable_torch_function pyrefly ignore missing-attribute torch as_tensor isinstance np generic np ndarray For user stack printing functools cache uninteresting_files - set str torch _dynamo external_utils torch _dynamo polyfills mods = torch _dynamo external_utils torch _dynamo polyfills torch _dynamo polyfills loader POLYFILLED_MODULES pyrefly ignore bad-argument-type mods extend POLYFILLED_MODULES inspect getfile m m mods _CLOSURE_VARS Optional dict str object = None _get_closure_vars - dict str object global _CLOSURE_VARS _CLOSURE_VARS None _CLOSURE_VARS = ___check_type_id check_type_id ___check_obj_id check_obj_id ___odict_getitem collections OrderedDict __getitem__ ___key_to_id key_to_id ___dict_version dict_version ___dict_contains lambda b dict __contains__ b ___tuple_iterator_len tuple_iterator_len ___normalize_range_iter normalize_range_iter ___tuple_iterator_getitem tuple_iterator_getitem ___dataclass_fields dataclass_fields ___namedtuple_fields lambda x x _fields ___get_torch_function_mode_stack_at get_torch_function_mode_stack_at ___get_current_stream get_current_stream __math_isnan math isnan __numpy_isnan None np None np isnan inf float inf __load_module importlib import_module utils_device torch utils _device device torch device ___from_numpy from_numpy ___as_tensor torch _as_tensor_fullprec torch torch inspect inspect _CLOSURE_VARS _ast_unparse node ast AST - str ast unparse node replace \n strip_function_call = torch _C _dynamo strip_function_call get_verbose_code_part code_part str guard Optional Guard - str extra = guard None guard user_stack fs reversed guard user_stack fs filename uninteresting_files extra = f format_frame fs line=True len extra For fx graphs line can very long case torch stack ops where many inputs set None after operation This increases size guards log file In such cases do print line contents extra = f format_frame fs break guard stack summary = guard stack summary len summary extra = f format_frame summary - extra = unknown f code_part extra get_verbose_code_parts code_parts Union str list str guard Optional Guard recompile_hint Optional str = None - list str isinstance code_parts list code_parts = code_parts verbose_code_parts = get_verbose_code_part code_part guard code_part code_parts recompile_hint verbose_code_parts = f part HINT recompile_hint part verbose_code_parts verbose_code_parts convert_int_to_concrete_values dim Any - Optional int dim None None is_symbolic dim dim assert isinstance dim torch SymInt dim node maybe_as_int convert_to_concrete_values size_or_stride list Any - list Optional int convert_int_to_concrete_values dim dim size_or_stride get_tensor_guard_code_part value torch Tensor name str sizes list Optional int strides list Optional int pytype type dispatch_keys DispatchKeySet - str dispatch_key = dispatch_keys &#124; torch _C _dispatch_tls_local_include_set - torch _C _dispatch_tls_local_exclude_set dtype = value dtype device_index = value device index requires_grad = value requires_grad guard_str = f check_tensor name pytype __qualname__ dispatch_key dtype f device= device_index requires_grad= requires_grad size= sizes stride= strides guard_str get_key_index dct dict Any Any key Any - int Ensure we call dict keys value keys which can call overridden keys method In C++ guards we relied PyDict_Next traverse dictionary which uses internal data structure does call overridden keys method list builtin_dict_keys dct index key get_key_index_source source Any index Any - str f list dict keys source index raise_local_type_error obj Any - NoReturn raise TypeError f Type type obj object obj cannot saved + into torch compile package since s defined local scope + Please define global scope top level module should_optimize_getattr_on_nn_module value Any - bool If inline_inbuilt_nn_modules flag True Dynamo has already traced through __getattr__ therefore always safe optimize getattr nn modules isinstance value torch nn Module config inline_inbuilt_nn_modules get_custom_getattr value unpatched_nn_module_getattr dataclasses dataclass frozen=True NNModuleAttrAccessorInfo Represents where attr name present nn module attribute access Tells attribute can accessed via __dict__ present_in_generic_dict bool = False Either actual name _parameters _buffers _modules l _key Optional str = None Actual parameter buffer submodule name l _key Optional str = None getitem_on_dict_manager source Union DictGetItemSource DictSubclassGetItemSource base_guard_manager DictGuardManager base_example_value Any example_value Any guard_manager_enum GuardManagerType - GuardManager base_source_name = source base name isinstance source index ConstDictKeySource index = source index index assert isinstance base_example_value dict index = get_key_index base_example_value source index key_source = get_key_index_source base_source_name index Ensure we call dict keys value keys which can call overridden keys method In C++ guards we relied PyDict_Next traverse dictionary which uses internal data structure does call overridden keys method key_example_value = list builtin_dict_keys base_example_value index isinstance key_example_value int str value_source = f base_source_name key_example_value r value_source = f base_source_name key_source isinstance source index ConstDictKeySource We have insert key manager guard here TODO - source debug string probably wrong here base_guard_manager get_key_manager index=index source=key_source example_value=source index guard_manager_enum=GuardManagerType GUARD_MANAGER add_equals_match_guard source index f key_source == key_example_value r base_guard_manager get_value_manager index=index source=value_source example_value=example_value guard_manager_enum=guard_manager_enum match_on_id_for_tensor guard Guard - bool source = guard originating_source For numpy tensors always use TENSOR_MATCH because __from_numpy leads new tensor every time therefore id differs isinstance source NumpyTensorSource False guard is_specialized_nn_module True source is_dict_key isinstance source GradSource The ready eval generated code possibly multiple parts guard plus original guard object created provenance dataclasses dataclass GuardCodeList code_list list str guard Guard GuardManagerType enum Enum GUARD_MANAGER = DICT_GUARD_MANAGER = functools cache code_framelocals_names_reversed_cached code types CodeType - list str list reversed code_framelocals_names code GuardBuilder GuardBuilderBase __init__ f_code types CodeType id_ref Callable object str int source_ref Callable Source str lookup_weakrefs Callable object Optional weakref ref object local_scope dict str object global_scope dict str object guard_manager GuardManagerWrapper check_fn_manager CheckFunctionManager save_guards bool = False runtime_global_scope Optional dict str object = None source_get_cache Optional dict str Any = None - None f_code = f_code id_ref = id_ref source_ref = source_ref lookup_weakrefs = lookup_weakrefs scope dict str dict str object = L local_scope G global_scope runtime_global_scope = runtime_global_scope global_scope source_get_cache = source_get_cache scope __builtins__ = builtins __dict__ copy name package_module torch package package_importer _package_imported_modules items name = name replace _ replace _ replace _dot_ Write package module into scope so we can scope __builtins__ name = package_module Write demangled name scope so we can use scope name = package_module guard_manager = guard_manager argnames list str = Code python expression strings generated each guard code list GuardCodeList = shape_env_code only used builder used shape env code This exists only because we need make sure shape env guards get run after tensor match guards since tensor match guards make sure we actually have tensors shape_env_code list GuardCodeList = Collect guard managers debug info insert no tensor aliasing guards no_tensor_aliasing_names list str = no_tensor_aliasing_guard_managers list GuardManager = check_fn_manager CheckFunctionManager = check_fn_manager guard_tree_values dict int Any = save_guards = save_guards Collect ids dicts which need key order guarding source_name sufficient because nn modules we can have different sources access same object - _module param same param key_order_guarded_dict_ids = set assert check_fn_manager output_graph None source check_fn_manager output_graph guard_on_key_order dict_obj = get source name save_guards source_get_cache source name = dict_obj key_order_guarded_dict_ids add id dict_obj Keep track weak references objects ID_MATCH guard This info stored alongside optimized_code guard_manager used limit number cache entries same ID_MATCH d object id_matched_objs dict str ReferenceType object = Save guard managers avoid repeatedly traversing sources _cached_guard_managers dict str GuardManager = _cached_duplicate_input_guards set tuple str str = set object_aliasing_guard_codes list tuple str str = guard_nn_modules = config guard_nn_modules justknobs_check pytorch compiler guard_nn_modules already_added_code_parts OrderedSet str = OrderedSet guard_on_dict_keys_and_ignore_order example_value dict Any Any guard Guard - None dict_mgr = get_guard_manager guard isinstance dict_mgr DictGuardManager raise NotImplementedError Not expecting DictGuardManager Seems like Dynamo incorrectly f added dict tx output guard_on_key_order guard name Iterate over dicts install dict_getitem_manager dict_source = guard originating_source name Ensure we call dict keys value keys which can call overridden keys method In C++ guards we relied PyDict_Next traverse dictionary which uses internal data structure does call overridden keys method key builtin_dict_keys example_value value = example_value key value_source = DictGetItemSource guard originating_source index=key guard_manager_enum = get_guard_manager_type value_source example_value dict_mgr dict_getitem_manager key=key source=f dict_source key r example_value=value guard_manager_enum=guard_manager_enum guard_on_dict_keys_and_order value dict Any Any guard Guard - None Add key managers DictGuardManager Then add either ID_MATCH EQUALS_MATCH guard key dict_mgr = get_guard_manager guard isinstance dict_mgr DictGuardManager raise NotImplementedError Expecting DictGuardManager Seems like Dynamo forgot f set right guard manager enum guard name assert isinstance dict_mgr DictGuardManager Ensure we call dict keys value keys which can call overridden keys method In C++ guards we relied PyDict_Next traverse dictionary which uses internal data structure does call overridden keys method idx key enumerate builtin_dict_keys value key_source = get_key_index_source guard name idx key_manager = dict_mgr get_key_manager index=idx source=key_source example_value=key guard_manager_enum=GuardManagerType GUARD_MANAGER key_is_id key Install ID_MATCH guard id_val = id_ref key key_source key_manager add_id_match_guard id_val get_verbose_code_parts f __check_obj_id key_source id_val guard Install EQUALS_MATCH guard key_manager add_equals_match_guard key get_verbose_code_parts f key_source == key r guard staticmethod _get_generic_dict_manager_example_value example_value Any - Optional Any due bug introduced https github com python cpython pull reported https github com python cpython issues fixed https github com python cpython pull we cannot take advantage __dict__ versions speed up guard checks config issue_ _ _ _warning sys version_info = sys version_info warnings warn Guards may run slower Python Consider upgrading Python + RuntimeWarning None example_value getattr_on_nn_module source AttrSource base_guard_manager GuardManager base_example_value Any example_value Any base_source_name str source_name str guard_manager_enum GuardManagerType - GuardManager This tries avoid calling expensive nn module custom getattr method checking attribute accessible via __dict__ For attributes accessible via __dict__ like descriptors we fallback PyObject_GetAttr There two cases we optimize attributes present directly __dict__ e g training parameters buffers modules - they can accessed via _parameters _buffers _modules keys __dict__ For example mod linear can accessed mod __dict__ _parameters linear The most common expensive case nn module guards type mod submod submod submod training We avoid python getattr nn modules going through __dict__ getitem_on_dict_mgr mgr GuardManager key Any source_name str base_example_value Any example_value Any guard_manager_enum GuardManagerType - GuardManager isinstance mgr DictGuardManager Case where user code relies key order e g named_parameters index = get_key_index base_example_value key Install key manager add equals match guard key_source = f list dict keys source_name index r mgr get_key_manager index=index source=key_source example_value=key guard_manager_enum=GuardManagerType GUARD_MANAGER add_equals_match_guard key f key_source == key r Install value manager mgr get_value_manager index=index source=source_name example_value=example_value guard_manager_enum=guard_manager_enum mgr dict_getitem_manager key=key source=source_name example_value=example_value guard_manager_enum=guard_manager_enum attr_name = source member mod_dict = base_example_value __dict__ all_class_attribute_names set str = set x inspect getmro base_example_value __class__ all_class_attribute_names update x __dict__ keys accessor_info = NNModuleAttrAccessorInfo False None None attr_name mod_dict accessor_info = NNModuleAttrAccessorInfo True attr_name None _parameters mod_dict attr_name mod_dict _parameters accessor_info = NNModuleAttrAccessorInfo True _parameters attr_name _buffers mod_dict attr_name mod_dict _buffers accessor_info = NNModuleAttrAccessorInfo True _buffers attr_name attr_name all_class_attribute_names _modules mod_dict attr_name mod_dict _modules Check test_attr_precedence test - instance attributes always take precedence unless its nn Module accessor_info = NNModuleAttrAccessorInfo True _modules attr_name accessor_info present_in_generic_dict The attribute can accessed __getattribute__ call so rely PyObject_GetAttr base_guard_manager getattr_manager attr=source member source=source_name example_value=example_value guard_manager_enum=guard_manager_enum assert accessor_info l _key l _key = accessor_info l _key l _key = accessor_info l _key Set source strings debug info mod_dict_source = f base_source_name __dict__ l _source_name = l _source_name = None l _value = l _value = None l _guard_manager_enum = l _guard_manager_enum = None l _key l _source = AttrSource source base l _key l _source_name = l _source name l _value = mod_dict l _key do guard key order _parameters etc unless user code actually needs key order e g calling named_parameters l _guard_manager_enum = get_guard_manager_type l _source l _value l _source_name = source_name l _value = example_value l _guard_manager_enum = get_guard_manager_type source example_value l _source_name = source_name l _value = example_value l _guard_manager_enum = get_guard_manager_type source example_value Get __dict__ accessor No need guard dict key order so use base Guard Manager mod_generic_dict_manager = base_guard_manager get_generic_dict_manager source=mod_dict_source example_value=self _get_generic_dict_manager_example_value mod_dict guard_manager_enum=GuardManagerType GUARD_MANAGER l _mgr = getitem_on_dict_mgr mgr=mod_generic_dict_manager key=l _key source_name=l _source_name base_example_value=mod_dict example_value=l _value guard_manager_enum=l _guard_manager_enum l _key assert l _source_name None l _guard_manager_enum None getitem_on_dict_mgr mgr=l _mgr key=l _key source_name=l _source_name base_example_value=l _value example_value=l _value guard_manager_enum=l _guard_manager_enum l _mgr requires_key_order_guarding source Source - bool source_name = source name source_name == False obj_id = id get source_name obj_id key_order_guarded_dict_ids get_guard_manager_type source Source example_value Optional Union KeysView Any set Any frozenset Any dict Any Any - GuardManagerType guard_manager_enum = GuardManagerType GUARD_MANAGER requires_key_order_guarding source Fix condition isinstance example_value dict_keys guard_manager_enum = GuardManagerType DICT_GUARD_MANAGER isinstance example_value set frozenset we don t need guard key order set frozenset above will true these types set implemented using dict Dynamo guard_manager_enum = GuardManagerType GUARD_MANAGER assert isinstance example_value dict guard_manager_enum = GuardManagerType DICT_GUARD_MANAGER guard_manager_enum manager_guards_on_keys mgr_enum GuardManagerType - bool mgr_enum == GuardManagerType DICT_GUARD_MANAGER get_global_guard_manager - GuardManager guard_manager root globals_dict_manager f_globals=self runtime_global_scope source= G example_value=self scope G guard_manager_enum=GuardManagerType GUARD_MANAGER get_guard_manager_from_source source Source - GuardManager root_guard_manager = guard_manager root example_value = None source_name = source name source_name = source_name _cached_guard_managers _cached_guard_managers source_name source_name = example_value = get source_name guard_tree_values id example_value = example_value guard_manager_enum = get_guard_manager_type source example_value Get base manager related information base_source_name = None base_example_value = None base_guard_manager = None base_guard_manager_enum = GuardManagerType GUARD_MANAGER isinstance source ChainedSource base_source_name = source base name base_example_value = get base_source_name base_guard_manager = get_guard_manager_from_source source base base_guard_manager_enum = get_guard_manager_type source base base_example_value Use istype instead isinstance check exact type source istype source LocalSource framelocals_idx = get_framelocals_idx f_code source local_name out = root_guard_manager framelocals_manager key= source local_name framelocals_idx source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source GlobalSource Global manager accepts dict DictGuardManager because globals dict big we typically guard very selected items globals out = get_global_guard_manager dict_getitem_manager key=source global_name source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source GlobalWeakRefSource out = get_global_guard_manager global_weakref_manager global_name=source global_name source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source GlobalStateSource Don t do anything here We guard global state completely C++ So just root mgr root_guard_manager istype source ShapeEnvSource root_guard_manager istype source TypeSource assert base_guard_manager make mypy happy out = base_guard_manager type_manager source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source TypeDictSource assert base_guard_manager make mypy happy out = base_guard_manager type_dict_manager source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source TypeMROSource assert base_guard_manager make mypy happy out = base_guard_manager type_mro_manager source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source OptimizerSource NNModuleSource UnspecializedNNModuleSource UnspecializedBuiltinNNModuleSource FSDPNNModuleSource assert base_guard_manager make mypy happy out = base_guard_manager istype source TorchSource out = root_guard_manager lambda_manager python_lambda=lambda _ torch source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source TorchFunctionModeStackSource out = root_guard_manager lambda_manager python_lambda=lambda _ get_torch_function_mode_stack_at source _get_index source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source CurrentStreamSource out = root_guard_manager lambda_manager python_lambda=lambda _ get_current_stream source device source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source GradSource assert base_guard_manager make mypy happy out = base_guard_manager grad_manager source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source GenericAttrSource assert base_guard_manager make mypy happy out = base_guard_manager generic_getattr_manager attr=source member source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source AttrSource UnspecializedParamBufferSource assert base_guard_manager make mypy happy assert isinstance source AttrSource should_optimize_getattr_on_nn_module base_example_value assert base_source_name out = getattr_on_nn_module source base_guard_manager base_example_value example_value base_source_name source_name guard_manager_enum out = base_guard_manager getattr_manager attr=source member source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source DictGetItemSource DictSubclassGetItemSource assert base_guard_manager make mypy happy assert isinstance base_example_value dict collections OrderedDict assert isinstance source DictGetItemSource DictSubclassGetItemSource isinstance base_guard_manager DictGuardManager assert manager_guards_on_keys base_guard_manager_enum out = getitem_on_dict_manager source base_guard_manager base_example_value example_value guard_manager_enum isinstance source index ConstDictKeySource raise RuntimeError Expecting clean index here Likely Dynamo forgot mark dict guard_on_key_order out = base_guard_manager dict_getitem_manager key=source index source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source TensorPropertySource out = getattr base_guard_manager f tensor_property_ source prop name lower _manager idx=source idx source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source IndexedSource assert base_guard_manager make mypy happy out = base_guard_manager indexed_manager idx=source idx source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source ListGetItemSource assert base_guard_manager make mypy happy out = base_guard_manager list_getitem_manager key=source index source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source GetItemSource assert base_guard_manager make mypy happy assert isinstance base_example_value dict collections OrderedDict Use DictGetItemSource isinstance base_example_value list source index_is_slice out = base_guard_manager list_getitem_manager key=source index source=source_name example_value=example_value guard_manager_enum=guard_manager_enum isinstance base_example_value tuple source index_is_slice out = base_guard_manager tuple_getitem_manager key=source index source=source_name example_value=example_value guard_manager_enum=guard_manager_enum index = source index source index_is_slice index = source unpack_slice out = base_guard_manager getitem_manager key=index source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source DefaultsSource assert base_guard_manager make mypy happy assert base_source_name assert callable base_example_value source is_kw out = base_guard_manager func_defaults_manager source=base_source_name example_value=base_example_value __defaults__ guard_manager_enum=GuardManagerType GUARD_MANAGER getitem_manager key=source idx_key source=source_name example_value=example_value guard_manager_enum=guard_manager_enum kwdefauts dict so use DictGuardManager kwdefaults = base_example_value __kwdefaults__ assert base_source_name None kw_source = base_source_name + __kwdefaults__ kwdefaults dict No need guard dict order dict_mgr = base_guard_manager func_kwdefaults_manager source=kw_source example_value=kwdefaults guard_manager_enum=GuardManagerType GUARD_MANAGER assert isinstance dict_mgr DictGuardManager out = dict_mgr dict_getitem_manager key=source idx_key source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source NumpyTensorSource assert base_guard_manager make mypy happy out = base_guard_manager lambda_manager python_lambda=from_numpy source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source SubclassAttrListSource assert base_guard_manager make mypy happy out = base_guard_manager lambda_manager python_lambda=lambda x x __tensor_flatten__ source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source FlattenScriptObjectSource assert base_guard_manager make mypy happy out = base_guard_manager lambda_manager python_lambda=lambda x x __obj_flatten__ source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source ScriptObjectQualifiedNameSource assert base_guard_manager make mypy happy out = base_guard_manager lambda_manager python_lambda=lambda x x _type qualified_name source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source AttrProxySource assert base_guard_manager make mypy happy out = base_guard_manager lambda_manager python_lambda=lambda x x get_base source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source CallMethodItemSource assert base_guard_manager make mypy happy out = base_guard_manager lambda_manager python_lambda=lambda x x item source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source FloatTensorSource assert base_guard_manager make mypy happy out = base_guard_manager lambda_manager python_lambda=lambda x torch _as_tensor_fullprec x source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source TupleIteratorGetItemSource assert base_guard_manager make mypy happy out = base_guard_manager tuple_iterator_getitem_manager index=source index source=source_name example_value=example_value guard_manager_enum=guard_manager_enum isinstance source ConstDictKeySource isinstance base_guard_manager DictGuardManager raise AssertionError ConstDictKeySource can only work DictGuardManager out = base_guard_manager get_key_manager index=source index source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source NonSerializableSetGetItemSource assert base_guard_manager out = base_guard_manager set_getitem_manager index=source index source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source WeakRefCallSource assert base_guard_manager make mypy happy out = base_guard_manager weakref_call_manager source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source CallFunctionNoArgsSource assert base_guard_manager make mypy happy out = base_guard_manager call_function_no_args_manager source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source DataclassFieldsSource assert base_guard_manager out = base_guard_manager lambda_manager python_lambda=lambda x dataclass_fields x source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source NamedTupleFieldsSource assert base_guard_manager out = base_guard_manager lambda_manager python_lambda=lambda x x _fields source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source CodeSource assert base_guard_manager make mypy happy out = base_guard_manager code_manager source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source ClosureSource assert base_guard_manager make mypy happy out = base_guard_manager closure_manager source=source_name example_value=example_value guard_manager_enum=guard_manager_enum istype source DynamicScalarSource assert base_guard_manager out = base_guard_manager lambda_manager python_lambda=lambda x int x source=source_name example_value=example_value guard_manager_enum=guard_manager_enum raise AssertionError f missing guard manager builder source - source name _cached_guard_managers source name = out out get_guard_manager guard Guard - GuardManager get_guard_manager_from_source guard originating_source add_python_lambda_leaf_guard_to_root code_parts list str verbose_code_parts list str closure_vars Optional dict str object = None is_epilogue bool = True - None closure_vars None closure_vars = _get_closure_vars Adds lambda leaf guard root guard manager It wraps code_parts function object which then passed leaf guard make_guard_fn_args = join closure_vars keys _guard_body pycode = build_guard_function code_parts make_guard_fn_args out dict str Any = globals_for_guard_fn = G scope G guards_log debug Python shape guard function \n s pycode exec pycode globals_for_guard_fn out guard_fn = out ___make_guard_fn closure_vars values is_epilogue Epilogue guards run after all other guards have finished If epilogue guards contain getattr getitem access one other guards would fail preventing epilogue guards run guard_manager root add_epilogue_lambda_guard guard_fn verbose_code_parts guard_manager root add_lambda_guard guard_fn verbose_code_parts Warning use care This lets you access what current value value you guarding You probably don t want actually durably save value though because s specific frame Instead you should reading out some property like its type which what you permanently install into guard code get name str closure_vars Optional dict str Any = None - Any source_get_cache name source_get_cache source_get_cache name closure_vars None closure_vars = _get_closure_vars ret = eval name scope closure_vars save_guards __closure__ name source_get_cache name = ret ret Registers usage source name referenced string stored Guard being guarded upon It s important call before generating some code makes use guard because without call we won t actually bind variable you reference actual guard closure oops arg_ref guard Union str Guard - str name str isinstance guard str name = guard name = guard name base = strip_function_call name base argnames is_valid = torch _C _dynamo is_valid_var_name base is_valid is_valid == log warning invalid var name s guard argnames append base name _guard_on_attribute guard Guard attr_name str guard_fn Callable GuardBuilderBase Guard Any - None attr_name == __code__ attr_source = CodeSource guard originating_source attr_source = AttrSource guard originating_source attr_name type ignore assignment Copy stack info new_guard = Guard attr_source guard_fn stack=guard stack user_stack=guard user_stack new_guard create Note order guards file matters since we sort guards same object lineno HASATTR guard Guard - None source = guard originating_source isinstance source NNModuleSource source = source base isinstance source CodeSource No need guard function has __code__ attribute assert isinstance source AttrSource f invalid source guard name base_source = source base base = base_source name attr = source member ref = arg_ref base val = hasattr get base attr code = None val code = f hasattr ref attr r code = f hasattr ref attr r code already_added_code_parts _set_guard_export_info guard code provided_guarded_object=self get base base_manager = get_guard_manager_from_source base_source val Just install getattr manager GetAttrGuardAccessor itself acts hasattr guard example_value = get source name base_example_value = get base guard_manager_enum = get_guard_manager_type source example_value base value nn Module check we can speedup guard going through __dict__ attrs should_optimize_getattr_on_nn_module base_example_value getattr_on_nn_module source base_manager base_example_value example_value base source name guard_manager_enum base_manager getattr_manager attr=attr source=guard name example_value=example_value guard_manager_enum=guard_manager_enum base_manager add_no_hasattr_guard attr get_verbose_code_parts code guard already_added_code_parts add code NOT_PRESENT_IN_GENERIC_DICT guard Guard attr Optional Any = None - None assert attr None ref = arg_ref guard val = get guard name base_manager = get_guard_manager guard code = f ___dict_contains attr r ref __dict__ code already_added_code_parts mod_dict_source = f guard name __dict__ mod_generic_dict_manager = base_manager get_generic_dict_manager source=mod_dict_source example_value=self _get_generic_dict_manager_example_value val __dict__ guard_manager_enum=GuardManagerType GUARD_MANAGER mod_generic_dict_manager add_dict_contains_guard False attr get_verbose_code_parts code guard already_added_code_parts add code TYPE_MATCH guard Guard - None ___check_type_id same ` id type x == y ` value = get guard name isinstance value torch _subclasses FakeTensor value pytype t = value pytype t = type value t __qualname__ = t __name__ Type match guards must local scope raised serialize_guards guard _unserializable = True obj_id = id_ref t f type guard name code = f ___check_type_id arg_ref guard obj_id _set_guard_export_info guard code get_guard_manager guard add_type_match_guard obj_id get_verbose_code_parts code guard DICT_VERSION guard Guard - None ___check_dict_version same ` dict_version x == y ` ref = arg_ref guard val = get guard name version = dict_version get guard name code = f ___dict_version ref == version _set_guard_export_info guard code TODO anijain - Delete when DictGuardManager uses tags dicts get_guard_manager guard add_dict_version_guard val get_verbose_code_parts code guard DICT_CONTAINS guard Guard key str invert bool - None dict_ref = arg_ref guard maybe_not = invert code = f maybe_not ___dict_contains key r dict_ref code already_added_code_parts _set_guard_export_info guard code get_guard_manager guard add_dict_contains_guard invert key get_verbose_code_parts code guard already_added_code_parts add code SET_CONTAINS guard Guard key Any invert bool - None set_ref = arg_ref guard item = key contains = invert install_dict_contains_guard inverts contains code = f set __contains__ set_ref item r code already_added_code_parts _set_guard_export_info guard code get_guard_manager guard add_set_contains_guard contains item get_verbose_code_parts code guard already_added_code_parts add code BOOL_MATCH guard Guard - None checks val == True val == False ref = arg_ref guard val = get guard name assert istype val bool code = f ref == val r _set_guard_export_info guard code val get_guard_manager guard add_true_match_guard get_verbose_code_parts code guard get_guard_manager guard add_false_match_guard get_verbose_code_parts code guard NONE_MATCH guard Guard - None checks ` val None ` ref = arg_ref guard val = get guard name assert val None code = f ref None _set_guard_export_info guard code get_guard_manager guard add_none_match_guard get_verbose_code_parts code guard ID_MATCH guard Guard recompile_hint Optional str = None - None TODO - Run CI following uncommented find remaining places val = get guard name inspect isclass val raise AssertionError f guard name use CLASS_MATCH guard inspect ismodule val raise AssertionError f guard name module use MODULE_MATCH guard id_match_unchecked guard recompile_hint id_match_unchecked guard Guard recompile_hint Optional str = None - None ___check_obj_id same ` id x == y ` isinstance guard originating_source TypeSource optional optimization produce cleaner faster guard code TYPE_MATCH Guard guard originating_source base GuardBuilder TYPE_MATCH type ignore arg-type ref = arg_ref guard val = get guard name id_val = id_ref val guard name code = f ___check_obj_id ref id_val _set_guard_export_info guard code provided_func_name= ID_MATCH get_guard_manager guard add_id_match_guard id_val get_verbose_code_parts code guard recompile_hint Keep track ID_MATCH d objects This will used modify cache size logic isinstance guard originating_source LocalSource TODO anijain - This currently restricted nn Module objects because many other ID_MATCH d objects fail - like DeviceMesh Increase scope ID_MATCH d objects isinstance val torch nn Module local_name = guard originating_source local_name weak_id = lookup_weakrefs val weak_id None id_matched_objs local_name = weak_id NOT_NONE_MATCH guard Guard value Optional Any = None - None ref = arg_ref guard val = get guard name assert isinstance val torch Tensor code = f ref None _set_guard_export_info guard code get_guard_manager guard add_not_none_guard get_verbose_code_parts code guard DISPATCH_KEY_SET_MATCH guard Guard - None ref = arg_ref guard val = get guard name assert isinstance val torch _C DispatchKeySet code_parts = f ref raw_repr == val r raw_repr get_guard_manager guard add_dispatch_key_set_guard val get_verbose_code_parts code_parts guard DUAL_LEVEL guard Guard - None Invalidate dual level current dual level different than one fx graph assert check_fn_manager output_graph None dual_level = check_fn_manager output_graph dual_level code = f torch autograd forward_ad _current_level == dual_level _set_guard_export_info guard code guard_manager root add_dual_level_match_guard dual_level get_verbose_code_parts code guard FUNCTORCH_STACK_MATCH guard Guard - None Invalidate functorch code current level different than one when FX graph generated assert check_fn_manager output_graph None cis = check_fn_manager output_graph functorch_layers states = ci get_state ci cis code = f torch _functorch pyfunctorch compare_functorch_state states _set_guard_export_info guard code TODO anijain - Consider moving guard C++ compare_fn = torch _functorch pyfunctorch compare_functorch_state fn x Any - bool compare_fn states guard_manager root add_lambda_guard fn get_verbose_code_parts code guard AUTOGRAD_SAVED_TENSORS_HOOKS guard Guard - None get_hooks = torch _functorch _aot_autograd utils top_saved_tensors_hooks are_inline_hooks = torch _functorch _aot_autograd utils saved_tensors_hooks_are_inlineable hooks_ids_fn hooks tuple Callable torch Tensor Any Callable Any torch Tensor - Optional tuple int are_inline_hooks hooks None pack_hook unpack_hook = hooks tuple map id hooks guard_hooks_ids = hooks_ids_fn get_hooks code = f torch _functorch aot_autograd utils top_saved_tensors_hooks ids == guard_hooks_ids _set_guard_export_info guard code fn x Any - bool guard_hooks_ids == hooks_ids_fn get_hooks guard_manager root add_lambda_guard fn get_verbose_code_parts code guard TENSOR_SUBCLASS_METADATA_MATCH guard Guard - None value = get guard name original_metadata = deepcopy get guard name __tensor_flatten__ hasattr value __metadata_guard__ verify_guard_fn_signature value metadata_checker x Any - bool value __metadata_guard__ original_metadata x __tensor_flatten__ metadata_checker x Any - bool x __tensor_flatten__ == original_metadata global_name = f ___check_metadata_ id metadata_checker _c CompileContext current_compile_id get_guard_manager guard add_lambda_guard metadata_checker get_verbose_code_parts global_name guard EQUALS_MATCH guard Guard recompile_hint Optional str = None - None ref = arg_ref guard val = get guard name np np_types tuple type Any = np int np int np int np int np uint np uint np uint np uint np float np float np float np_types = ok_mutable_types = list set ok_types = tuple common_constant_types &#124; type tuple frozenset slice range dict_keys torch Size torch Stream torch cuda streams Stream np_types ok_mutable_types torch distributed is_available torch distributed device_mesh DeviceMesh torch distributed tensor placement_types _StridedShard Partial Replicate Shard ok_types = ok_types + Shard Replicate Partial DeviceMesh _StridedShard torch export dynamic_shapes _IntWrapper ok_types = ok_types + _IntWrapper torch utils _pytree pytree assert isinstance val ok_types pytree is_constant_class type val f Unexpected type type val Special case nan because float nan == float nan evaluates False istype val float math isnan val code = f type ref float __math_isnan ref _set_guard_export_info guard code get_guard_manager guard add_float_is_nan_guard get_verbose_code_parts code guard Python math library doesn t support complex nan so we need use numpy pyrefly ignore missing-attribute istype val complex np isnan val code = f type ref complex __numpy_isnan ref _set_guard_export_info guard code get_guard_manager guard add_complex_is_nan_guard get_verbose_code_parts code guard Construct debug string put into c++ equals match guard code = f ref == val r istype val ok_mutable_types C++ guards perform pointer equality check speedup guards assumption object immutable For few corner cases like sets lists we make deepcopy purposefully fail pointer equality check val = deepcopy val verbose_code_parts = get_verbose_code_parts code guard recompile_hint verbose_code_parts = f part HINT recompile_hint part verbose_code_parts get_guard_manager guard add_equals_match_guard val verbose_code_parts _set_guard_export_info guard code CONSTANT_MATCH guard Guard - None val = get guard name istype val bool BOOL_MATCH guard val None NONE_MATCH guard istype val types CodeType ID_MATCH guard EQUALS_MATCH guard NN_MODULE guard Guard - None don t support serialization because uses unsupported ID_MATCH ID_MATCH guard inline-inbuilt-nn-modules-candidate val = get guard name hasattr val training assert istype val training bool guard_nn_modules If guard_nn_modules true we will guard right set guards _guard_on_attribute guard training GuardBuilder CONSTANT_MATCH type ignore arg-type exc unimplemented_v gb_type= Attempted guard uninitialized nn Module context= explanation= Attempted setup NN_MODULE guard uninitialized f nn Module subclass ` type val ` hints= Ensure ` nn Module ` subclass instance has called ` super __init__ ` FUNCTION_MATCH guard Guard - None things like torch add user defined functions don t support serialization because uses unsupported ID_MATCH ID_MATCH guard CLASS_MATCH guard Guard - None Equals ID_MATCH classes - better readability than directly calling ID_MATCH val = get guard name inspect isclass val raise AssertionError f guard name CLASS_MATCH used id_match_unchecked guard MODULE_MATCH guard Guard - None Equals ID_MATCH modules - better readability than directly calling ID_MATCH val = get guard name inspect ismodule val raise AssertionError f guard name module MODULE_MATCH used id_match_unchecked guard CLOSURE_MATCH guard Guard - None matches closure __code__ id don t support serialization because uses unsupported FUNCTION_MATCH val = get guard name Strictly only want user-defined functions type val types FunctionType hasattr val __code__ _guard_on_attribute guard __code__ GuardBuilder HASATTR type ignore arg-type _guard_on_attribute guard __code__ GuardBuilder CONSTANT_MATCH type ignore arg-type FUNCTION_MATCH guard BUILTIN_MATCH guard Guard - None save_guards Record which builtin variables used pruning later isinstance guard originating_source DictGetItemSource check_fn_manager used_builtin_vars add guard originating_source index id_match_unchecked guard SEQUENCE_LENGTH guard Guard - None This guard used check length PySequence objects like list tuple collections deque etc ref = arg_ref guard value = get guard name isinstance value dict C++ DICT_LENGTH checks type TYPE_MATCH guard code = len value == code append f ref code append f len ref == len value _set_guard_export_info guard code isinstance value dict get_guard_manager guard add_dict_length_check_guard len value get_verbose_code_parts code guard get_guard_manager guard add_length_check_guard len value get_verbose_code_parts code guard TUPLE_ITERATOR_LEN guard Guard - None ref = arg_ref guard value = get guard name t = type value code = code append f ___tuple_iterator_len ref == tuple_iterator_len value _set_guard_export_info guard code t = type value obj_id = id_ref t f type guard name get_guard_manager guard add_tuple_iterator_length_guard tuple_iterator_len value obj_id get_verbose_code_parts code guard RANGE_ITERATOR_MATCH guard Guard - None ref = arg_ref guard value = get guard name t = type value code = normalized_range_iter = normalize_range_iter value code append f ___normalize_range_iter ref == normalized_range_iter _set_guard_export_info guard code t = type value obj_id = id_ref t f type guard name start stop step = normalized_range_iter get_guard_manager guard add_range_iterator_match_guard start stop step obj_id get_verbose_code_parts code guard TODO voz Deduplicate w AOTAutograd dupe input guards DUPLICATE_INPUT guard Guard source_b Source - None save_guards name = get_local_source_name source_b check_fn_manager additional_used_local_vars add name name = get_global_source_name source_b check_fn_manager additional_used_global_vars add name ref_a = arg_ref guard ref_b = arg_ref source_b name is_from_optimizer_source guard originating_source is_from_optimizer_source source_b Check guard has been inserted already key = ref_a ref_b key _cached_duplicate_input_guards _cached_duplicate_input_guards add ref_a ref_b _cached_duplicate_input_guards add ref_b ref_a code = f ref_b ref_a _set_guard_export_info guard code config use_lamba_guard_for_object_aliasing Save code part so we can install lambda guard end Read Note - On Lambda guarding object aliasing - get more information code_part = code verbose_code_part = get_verbose_code_parts code_part guard object_aliasing_guard_codes append code_part verbose_code_part install_object_aliasing_guard get_guard_manager guard get_guard_manager_from_source source_b get_verbose_code_parts code guard WEAKREF_ALIVE guard Guard - None code = f arg_ref guard None _set_guard_export_info guard code get_guard_manager guard add_not_none_guard get_verbose_code_parts code guard MAPPING_KEYS_CHECK guard Guard - None Guard key order types MappingProxyType object ref = arg_ref guard value = get guard name code = code append f list ref keys == list value keys _set_guard_export_info guard code get_guard_manager guard add_mapping_keys_guard value code DICT_KEYS_MATCH guard Guard - None Insert guard check keys dict same ref = arg_ref guard value = get guard name value torch utils _pytree SUPPORTED_NODES For SUPPORTED_NODES we can guard dictionary version PEP DICT_VERSION guard SEQUENCE_LENGTH guard code = Ensure we call dict keys value keys which can call overridden keys method In C++ guards we relied PyDict_Next traverse dictionary which uses internal data structure does call overridden keys method code append f list dict keys ref == list builtin_dict_keys value r _set_guard_export_info guard code requires_key_order_guarding guard originating_source guard_on_dict_keys_and_order value guard guard_on_dict_keys_and_ignore_order value guard EMPTY_NN_MODULE_HOOKS_DICT guard Guard - None Special guard skip guards empty hooks This controlled skip_nnmodule_hook_guards config skip_nnmodule_hook_guards This unsafe you add remove hook nn module variable SEQUENCE_LENGTH guard GRAD_MODE guard Guard - None pass we always guard via GlobalStateGuard DETERMINISTIC_ALGORITHMS guard Guard - None pass we always guard via GlobalStateGuard TORCH_FUNCTION_STATE guard Guard - None pass we always guard via GlobalStateGuard FSDP_TRAINING_STATE guard Guard - None pass we always guard via GlobalStateGuard DEFAULT_DEVICE guard Guard - None Guard CURRENT_DEVICE per torch utils _device assert guard source GuardSource GLOBAL assert check_fn_manager output_graph None code = f utils_device CURRENT_DEVICE == check_fn_manager output_graph current_device r _set_guard_export_info guard code get_guard_manager guard add_default_device_guard get_verbose_code_parts code guard SHAPE_ENV guard Guard - None torch _dynamo output_graph OutputGraphCommon assert guard name == output_graph = check_fn_manager output_graph assert output_graph None check_fn_manager shape_code_parts None shape_code_parts = check_fn_manager shape_code_parts python_code_parts = shape_code_parts python_code_parts verbose_code_parts = shape_code_parts verbose_code_parts shape_code_parts cpp_code_parts None cpp_code_parts = shape_code_parts cpp_code_parts python_fallback = shape_code_parts python_fallback Let s handle ShapeEnv guards To do we will resolve shape variables sources tracked_fakes This must happen after tensor checks NB output_graph can None debug_nops tests assert isinstance output_graph OutputGraphCommon assert output_graph shape_env None fs = output_graph shape_env tracked_fakes input_contexts = symbolic_context fs get_sources t_id int dim int - list Source Looks up base sources mapped tensor id uses them create sources corresponding tensor dimension TensorPropertySource source TensorProperty SIZE dim pyrefly ignore missing-attribute source output_graph tracked_fakes_id_to_source t_id output_graph export_constraints names dict str tuple int int = source_pairs list tuple Source Source = derived_equalities list type ignore type-arg tuple Source Union Source Symbol Callable = phantom_symbols dict str Symbol = relaxed_sources set Source = set constraint output_graph export_constraints type ignore attr-defined constraint t_id output_graph tracked_fakes_id_to_source torch export dynamic_shapes _process_equalities constraint get_sources output_graph shape_env names source_pairs derived_equalities phantom_symbols relaxed_sources log warning Untracked tensor used export constraints equalities_inputs = EqualityConstraint source_pairs=source_pairs derived_equalities=derived_equalities phantom_symbols=list phantom_symbols values relaxed_sources=relaxed_sources warn_only=False equalities_inputs = None _get_code_parts langs tuple str - list _ShapeGuardsHelper pyrefly ignore missing-attribute output_graph shape_env produce_guards_verbose fake fs type ignore misc source fs input_contexts=input_contexts type ignore arg-type equalities_inputs=equalities_inputs source_ref=self source_ref Export keeps static pyrefly ignore missing-attribute ignore_static= output_graph export langs=langs config enable_cpp_symbolic_shape_guards try For exporting we need python code parts python_code_parts verbose_code_parts cpp_code_parts = _get_code_parts python verbose_python cpp type ignore assignment python_fallback = False except OverflowError Cannot use int _t python_fallback = True python_code_parts verbose_code_parts = _get_code_parts python verbose_python python_fallback = True python_code_parts verbose_code_parts = _get_code_parts python verbose_python When exporting we may work shape constraints some more postprocessing so don t freeze yet output_graph export output_graph shape_env freeze save_guards For SHAPE_ENV we want skip serializing entire ShapeEnv so instead we directly serialize generated code here maybe_cpp_code_parts = locals get cpp_code_parts assert maybe_cpp_code_parts None isinstance maybe_cpp_code_parts _CppShapeGuardsHelper maybe_shape_env_sources = maybe_cpp_code_parts None list maybe_cpp_code_parts source_to_symbol keys check_fn_manager shape_code_parts = ShapeCodeParts python_code_parts=python_code_parts verbose_code_parts=verbose_code_parts cpp_code_parts=maybe_cpp_code_parts python_fallback=python_fallback shape_env_sources=maybe_shape_env_sources code python_code_parts exprs _set_guard_export_info guard code Make ShapeEnv guards available testing compile_context = CompileContext try_get compile_context shape_env_guards extend verbose_code_parts exprs int_source_to_symbol = float_source_to_symbol = python_fallback assert cpp_code_parts type ignore possibly-undefined code_parts source_to_symbol = pyrefly ignore unbound-name cpp_code_parts exprs pyrefly ignore unbound-name missing-attribute cpp_code_parts source_to_symbol code_parts source symbol source_to_symbol items isinstance source ConstantSource python_fallback = True example_value = get source name closure_vars= SYMPY_INTERP _get_closure_vars isinstance example_value int int_source_to_symbol append source symbol isinstance example_value float float_source_to_symbol append source symbol SymInts SymFloats go through python guard we only support int _t double C++ guards now python_fallback = True python_fallback ctypes torch _inductor codecache CppCodeCache assert cpp_code_parts type ignore possibly-undefined code_parts source_to_symbol = pyrefly ignore unbound-name cpp_code_parts exprs pyrefly ignore unbound-name missing-attribute cpp_code_parts source_to_symbol source_to_symbol = dict int_source_to_symbol + float_source_to_symbol try guard_managers = get_guard_manager_from_source IndexedSource source i i source enumerate source_to_symbol int_symbols_str = join f symbol = int_values i i _ symbol enumerate int_source_to_symbol float_symbols_str = join f symbol = float_values i i _ symbol enumerate float_source_to_symbol int_symbols_str int_symbols_str = f int _t int_symbols_str float_symbols_str float_symbols_str = f double float_symbols_str func_str = textwrap dedent f #include algorithm #include cstdint #include cmath #include c util generic_math h #if defined _MSC_VER define EXTERN_DLL_EXPORT extern C __declspec dllexport #else define EXTERN_DLL_EXPORT extern C #endif EXTERN_DLL_EXPORT int _t guard int _t int_values double float_values int_symbols_str float_symbols_str join code_parts guards_log debug C++ shape guard function s s func_str verbose_code_parts exprs clib = CppCodeCache load func_str cguard = ctypes cast clib guard ctypes c_void_p value assert cguard except torch _inductor exc InvalidCxxCompiler No valid C++ compiler compile shape guard pass install_symbolic_shape_guard guard_managers len int_source_to_symbol len float_source_to_symbol cguard clib verbose_code_parts exprs Install all symbolic guards one python lambda guard These run very end RootGuardManager via epilogue guards TODO anijain williamwen - Consider moving C++ python_code_parts exprs add_python_lambda_leaf_guard_to_root python_code_parts exprs verbose_code_parts exprs closure_vars= SYMPY_INTERP _get_closure_vars TENSOR_MATCH guard Guard value Optional Any = None - None config _unsafe_skip_fsdp_module_guards guard is_fsdp_module For tensors part Dynamo extracted Fx graph module ID_MATCH suffices Once we turn inline_inbuilt_nn_modules these will lifted inputs have TENSOR_MATCH guard match_on_id_for_tensor guard ID_MATCH guard isinstance value TensorWeakRef value = value value = value value None get guard name pytype = type value dispatch_keys = torch _C _dispatch_keys value isinstance value torch _subclasses FakeTensor value pytype None pytype = value pytype value dispatch_keys None dispatch_keys = value dispatch_keys assert isinstance value torch Tensor config log_compilation_metrics isinstance value torch nn Parameter metrics_context = get_metrics_context metrics_context in_progress metrics_context increment param_numel value numel metrics_context increment param_bytes value nbytes metrics_context increment param_count tensor_name = arg_ref guard Note - On Export Tensor Guards In eager mode tensor guards evaluated through C++ guards cpp see Note - On Eager Tensor Guards more info In export mode we instead maintain parallel logic between C++ python here exception checking dispatch key - idea dispatch key entirely runtime notion would make no sense keep exported graph Now idea okay paraphrase ezyang mental model sufficient now although entirely true For example suppose one input tensors had negative dispatch key You should end up graph specialized tensors have negative dispatch key If you allow Tensor does NOT have bit set you will accidentally run negated Now negative key only shows up complex numbers most likely exported target doesn t support feature all point stands some tensor state only shows up dispatch key TODO voz Either populate dispatch_key check into guards error users passing unsupported subset keys during export The list tensor fields calls we care about can found ` terms ` below TODO voz We missing storage offset all our tensor guards code list str = assert check_fn_manager output_graph None check_fn_manager output_graph export TYPE_MATCH guard terms = dtype device requires_grad ndimension term terms real_value = get tensor_name + + term istype real_value torch device torch dtype copy pasted EQUALS_MATCH code append f str tensor_name term == str real_value r code append f tensor_name term == real_value guard_manager = get_guard_manager guard skip_no_tensor_aliasing_guards_on_parameters bring unsoundness If you compile function two different parameters later you pass same tensor two different outputs aliasing Dynamo will detect But we deliberately take soundness hit because usecase quite rare there substantial reduction guard overhead For numpy tensors since those ephemeral we don t have insert aliasing guards them config skip_no_tensor_aliasing_guards_on_parameters istype value torch nn Parameter is_from_unspecialized_builtin_nn_module_source guard originating_source isinstance guard originating_source NumpyTensorSource Keep track all tensor guard managers insert NoAliasing check end no_tensor_aliasing_names append tensor_name no_tensor_aliasing_guard_managers append guard_manager output_graph = check_fn_manager output_graph metadata = output_graph input_source_to_sizes_strides guard originating_source size = convert_to_concrete_values metadata size stride = convert_to_concrete_values metadata stride verbose_code_parts = get_verbose_code_parts get_tensor_guard_code_part value tensor_name size stride pytype dispatch_keys guard guard_manager add_tensor_match_guard value size type ignore arg-type stride type ignore arg-type tensor_name verbose_code_parts pytype dispatch_keys We consider TENSOR_MATCH guard important enough included diff guard manager default isinstance value torch nn Parameter guard_manager diff_guard_sources add guard name A frame valid reuse dynamic dimensions new user-requested dynamic dimensions subset old already compiled dynamic dimensions It s little non-obvious why you d want particular already compiled frame matches all guards why just use why force recompile We force two reasons - The user required us compile new dynamic dimension we should ignore serve up old specialized frame Listen user - In fact we obligated raise error we fail make requested dimension dynamic If we don t recompile we can t tell dimension can actually made dynamic If new dynamic dims subset old we already know we can make them dynamic since we made them dynamic old This slightly unsound because maybe your input size s s s so you can do dynamic you say dynamic dims you can t you only do because now second s specialized But we re entirely sure good idea anyway lol you want try removing logic my guest -- ezyang assert guard source None static _reason = tensor_always_has_static_shape value is_tensor=True tensor_source=guard originating_source static hasattr value _dynamo_dynamic_indices dynamic_indices = value _dynamo_dynamic_indices code_part = f tensor_name _dynamo_dynamic_indices issubset dynamic_indices hasattr tensor_name _dynamo_dynamic_indices True noqa B code append code_part get_guard_manager guard add_dynamic_indices_guard dynamic_indices get_verbose_code_parts code_part guard In case us having any dynamic dimension indices we compiled frame no chance raising specific tensor - any inputs more dynamic user directives specified must recompiled code_part = f hasattr tensor_name _dynamo_dynamic_indices == False code append code_part get_guard_manager guard add_no_hasattr_guard _dynamo_dynamic_indices get_verbose_code_parts code_part guard len code _set_guard_export_info guard code A util case export adds data onto guards _set_guard_export_info guard Guard code_list list str provided_guarded_object Optional Any = None provided_func_name Optional str = None - None WARNING It important cur_frame caller do NOT stay current frame because they will keep things live longer than they should See TestMisc test_release_module_memory cur_frame = currentframe assert cur_frame None caller = cur_frame f_back del cur_frame assert caller None func_name = provided_func_name caller f_code co_name del caller We use func_name export so might well get nice defensive check out assert func_name __class__ __dict__ f _produce_guard_code must called inside GuardedCode Called func_name Not all guards have names some can installed globally see asserts HAS_GRAD provided_guarded_object None name = guard name guarded_object = None name get name guarded_object = provided_guarded_object guarded_object_type = weakref ref type guarded_object guarded_object None None obj_ref = None Not necessary have weakref Enum type there bug makes hasattr guarded_object __class__ __weakref__ True supports_weakref = getattr guarded_object __class__ __weakrefoffset__ = See D why we checking tuple supports_weakref isinstance guarded_object enum Enum tuple weakref ProxyTypes obj_ref = weakref ref guarded_object guard set_export_info func_name guarded_object_type code_list obj_ref Common Sub-Expression Elimination Python expressions There steps pass Count frequency each sub-expression i e inner node AST tree Replace those occur more than once fresh variable v v will defined preface list output argument NodeTransformer NB use ast unparse while visiting nodes makes pass quadratic depth tree NB pass creates new variable each AST node repeated more than USE_THRESHOLD e g b c d used times b c b also used times So there will new variable each them PyExprCSEPass Maximum number times given expression can used without being replaced fresh variable USE_THRESHOLD = Ad-Hoc AST nodes pass focuses ALLOWED_NODE_TYPES = ast Attribute ast Call ast Subscript dataclasses dataclass Config expr_count dict str int expr_to_name dict str str ExprCounter ast NodeVisitor __init__ config PyExprCSEPass Config - None _config = config visit node ast AST - None isinstance node PyExprCSEPass ALLOWED_NODE_TYPES _config expr_count _ast_unparse node += super visit node Replacer ast NodeTransformer __init__ config PyExprCSEPass Config gen_name Callable str - None super __init__ _config = config _gen_name = gen_name preface list str = visit node ast AST - Any isinstance node PyExprCSEPass ALLOWED_NODE_TYPES expr = _ast_unparse node Replacement only occurs given expression used more than once _config expr_count expr PyExprCSEPass USE_THRESHOLD expr _config expr_to_name Parent visit called so we CSE inner expressions first The resulting expression used right-hand-side variable assignment i e we CSE-ing children before parents Indexing still uses old node since s what counted NodeVisitor node_ = super visit node expr_ = _ast_unparse node_ var_name = _gen_name preface append f var_name = expr_ _config expr_to_name expr = var_name var_name = _config expr_to_name expr ast Name var_name ast Load super visit node __init__ - None _counter = _config = Config expr_count=collections defaultdict lambda expr_to_name= _new_var prefix str = _var - str name = f prefix _counter _counter += name count exprs list str - None counter = ExprCounter _config e exprs try counter visit ast parse e except SyntaxError ex log exception Failed visit expr line s \n s ex lineno e raise replace expr str - tuple list str str replacer = Replacer _config _new_var new_node = replacer visit ast parse expr replacer preface _ast_unparse new_node must_add_nn_module_guards guard Guard - bool For config guard_nn_modules=False we can skip all guards originate inside nn module except few categories Guard defaults isinstance guard originating_source DefaultsSource Guard using dict tags config flag set config guard_nn_modules_using_dict_tags guard create_fn GuardBuilder NN_MODULE DeletedGuardManagerWrapper GuardManagerWrapper __init__ reason str - None super __init__ invalidation_reason = reason populate_diff_guard_manager - None diff_guard_root = None dataclasses dataclass ShapeCodeParts python_code_parts _ShapeGuardsHelper verbose_code_parts _ShapeGuardsHelper cpp_code_parts Optional _CppShapeGuardsHelper python_fallback bool shape_env_sources list Source dataclasses dataclass GuardsState output_graph OutputGraphGuardsState shape_code_parts Optional ShapeCodeParts source_get_cache Optional dict str Any = None _Missing __init__ reason Optional str = None - None _reason = reason __repr__ - str f _Missing _reason __str__ - str f _Missing _reason Sometimes _Missing object used callable functools partial so we add dummy __call__ here bypass TypeError partial __call__ args Any kwargs Any - Any _Missing functools cache _get_unsupported_types - tuple type We only do ID_MATCH C objects which already banned guards serialization ret tuple type = types CodeType torch _C Stream weakref ReferenceType try ret += torch _C _distributed_c d ProcessGroup except AttributeError pass ret GuardsStatePickler pickle Pickler __init__ guard_tree_values dict int Any empty_values dict int Any missing_values dict int Any args Any kwargs Any - None super __init__ args kwargs fake_mode = torch _subclasses FakeTensorMode tensor_converter = torch _subclasses fake_tensor FakeTensorConverter guard_tree_values = guard_tree_values empty_values = empty_values missing_values = missing_values classmethod _unpickle_module cls state Any - torch nn Module mod = torch nn Module mod __setstate__ state mod classmethod _unpickle_tensor cls meta_tensor torch Tensor device torch device pytype type dispatch_keys_raw int grad torch Tensor - torch Tensor fake_mode = torch _subclasses FakeTensorMode tensor_converter = torch _subclasses fake_tensor FakeTensorConverter ret = tensor_converter from_meta_and_device fake_mode meta_tensor device pytype torch _C DispatchKeySet from_raw_repr dispatch_keys_raw ret grad = grad ret classmethod _unpickle_traceable_wrapper_subclass cls meta_tensor torch Tensor device torch device pytype type dispatch_keys_raw int ctx Any inner_data list tuple str Callable Any tuple Any - torch Tensor Unpickle inner tensor components These could also subclass instances inner_tensors = attr unpickle_func unpickle_func_args inner_data inner_tensors attr = unpickle_func unpickle_func_args outer_size outer_stride = meta_tensor shape meta_tensor stride out = type meta_tensor __tensor_unflatten__ type ignore attr-defined inner_tensors ctx outer_size outer_stride out pytype = pytype out dispatch_keys = torch _C DispatchKeySet from_raw_repr dispatch_keys_raw out classmethod _unpickle_python_module cls alias str - types ModuleType importlib import_module alias classmethod _unpickle_dispatch_key_set cls raw_repr int - torch _C DispatchKeySet torch _C DispatchKeySet from_raw_repr raw_repr classmethod _unpickle_functorch_interpreter cls json bytes - torch _C _functorch CInterpreter torch _C _functorch CInterpreter deserialize json classmethod _unpickle_mapping_proxy cls d dict Any Any - types MappingProxyType Any Any types MappingProxyType d classmethod _unpickle_dict_keys cls elems list Any - Any dict fromkeys elems keys classmethod _unpickle_fsdp_module_type cls original_type type torch nn Module - type torch nn Module torch distributed fsdp _fully_shard _fully_shard get_cls_to_fsdp_cls original_type classmethod _unpickle_ddp_module cls state dict str Any - torch nn parallel DistributedDataParallel ty = torch nn parallel DistributedDataParallel ddp = ty __new__ ty torch nn Module __setstate__ ddp state ddp classmethod _unpickle_c_op cls name str - Any getattr torch ops _C name classmethod _unpickle_bound_method cls func Any base Any - Any types MethodType func base classmethod _unpickle_cell cls val Any - Any _ - Any val assert _ __closure__ None _ __closure__ pyrefly ignore bad-override reducer_override obj Any - Union tuple Callable Any tuple Any Any sympy id obj empty_values type obj __new__ type obj id obj missing_values _Missing missing values isinstance obj torch Tensor obj device type = meta torch utils _python_dispatch is_traceable_wrapper_subclass id obj guard_tree_values _Missing tensor guard tree is_traceable_wrapper_subclass obj inner_data list tuples inner attr name unpickle func tuple func inputs This supports traceable wrapper subclass inner tensors inner_data = attrs ctx = obj __tensor_flatten__ recursively call inner tensor components attr attrs inner = getattr obj attr isinstance inner torch Tensor guard_tree_values id inner = inner func args_tuple = reducer_override inner inner_data append attr func args_tuple type _unpickle_traceable_wrapper_subclass torch empty_like obj device= meta obj device type obj torch _C _dispatch_keys obj raw_repr ctx inner_data type _unpickle_tensor torch empty_like obj device= meta requires_grad=obj requires_grad obj device type obj torch _C _dispatch_keys obj raw_repr obj grad isinstance obj torch nn Module id obj guard_tree_values _Missing module guard tree DDP module special case because tries restore unneeded data custom __setstate__ We cannot skip ddp module because often toplevel module isinstance obj torch nn parallel DistributedDataParallel type _unpickle_ddp_module obj __getstate__ type obj __qualname__ == type obj __name__ NotImplemented obj __class__ __getstate__ == torch nn Module __getstate__ type _unpickle_module obj __getstate__ inspect ismodule obj type _unpickle_python_module obj __name__ isinstance obj torch _C DispatchKeySet type _unpickle_dispatch_key_set obj raw_repr isinstance obj torch _C _functorch CInterpreter type _unpickle_functorch_interpreter obj serialize inspect isclass obj issubclass obj sympy Function hasattr obj _torch_handler_name assert hasattr obj _torch_unpickler obj _torch_unpickler obj _torch_handler_name isinstance obj torch SymInt raise RuntimeError f Cannot serialize SymInt obj node obj node isinstance obj types MappingProxyType type _unpickle_mapping_proxy obj copy isinstance obj torch _dynamo utils dict_keys type _unpickle_dict_keys list obj isinstance obj torch _ops OpOverloadPacket obj _qualified_op_name startswith _C type _unpickle_c_op obj __name__ obj __class__ __module__ == builtins obj __class__ __name__ == PyCapsule Skipping PyCapsule since there isn t much guarded about them _Missing capsule isinstance obj _get_unsupported_types _Missing unsupported inspect isfunction obj obj __code__ co_flags inspect CO_NESTED _Missing nested function obj __module__ sys modules f = sys modules obj __module__ name obj __qualname__ split f = getattr f name None type ignore assignment f obj _Missing fqn mismatch inspect ismethod obj func = obj __func__ method_self = obj __self__ inner_func = getattr method_self func __name__ inspect ismethod inner_func inner_func = inner_func __func__ func inner_func type _unpickle_bound_method func method_self isinstance obj type lambda x lambda x __closure__ type ignore index noqa PLC type _unpickle_cell obj cell_contents hasattr torch distributed distributed_c d isinstance obj torch distributed distributed_c d Work id obj guard_tree_values _Missing distributed_c d Work type obj __qualname__ = type obj __name__ raise torch _dynamo exc PackageError f Type type obj object obj cannot saved + into torch compile package since s defined local scope + Please define global scope top level module inspect isclass obj hasattr torch distributed fsdp issubclass obj torch distributed fsdp _fully_shard FSDPModule obj torch distributed fsdp _fully_shard FSDPModule original_type = obj __mro__ assert issubclass original_type torch nn Module assert original_type torch distributed fsdp _fully_shard _fully_shard get_cls_to_fsdp_cls type _unpickle_fsdp_module_type original_type NotImplemented pickle_guards_state state GuardsState guard_tree_values dict int Any - bytes buf = io BytesIO empty_values = missing_values = leaves = pytree tree_leaves state output_graph local_scope leaf leaves inspect ismethod leaf hasattr leaf __self__ base = leaf __self__ id base guard_tree_values try type base __new__ type base empty_values id base = base except noqa E B pass id leaf guard_tree_values TODO See we have lift branch first one Prune more objects pytree hierarchy missing_values id leaf = leaf pickler = GuardsStatePickler guard_tree_values empty_values missing_values buf try pickler dump state except AttributeError e raise torch _dynamo exc PackageError str e e buf getvalue NB Naively you d expect only function produces callable constitutes guard However there some delicate handling invalidating check function when locals globals get invalidated so there s some extra state we have hold manager CheckFunctionManager __init__ f_code types CodeType output_graph OutputGraphCommon cache_entry Optional CacheEntry = None guard_fail_fn Optional Callable GuardFail None = None guard_filter_fn Optional Callable list GuardFilterEntry list bool = None shape_code_parts Optional ShapeCodeParts = None runtime_global_scope Optional dict str Any = None save_guards bool = False strict_error bool = False source_get_cache Optional dict str Any = None guards = output_graph guards output_graph None _weakrefs dict int ReferenceType object = existing_diff_guard_sources = update_diff_guard_managers_for_existing_cache_entries cache_entry output_graph Optional OutputGraphCommon = output_graph assert output_graph None Only used serialization shape_code_parts = shape_code_parts NB Until we trace device contexts we need use stack recorded beginning tracing case set default device call made graph torch_function_mode_stack = output_graph torch_function_mode_stack output_graph None used_builtin_vars OrderedSet str = OrderedSet additional_used_local_vars OrderedSet str = OrderedSet additional_used_global_vars OrderedSet str = OrderedSet runtime_global_scope = runtime_global_scope justknobs_check pytorch compiler guard_nn_modules log warning guard_nn_modules turned off using justknobs killswitch TODO Be more explicit about behavior users torch _dynamo config caching_precompile _guard_filter_fn = guard_filter_fn lambda gs True g gs guard_filter_fn guards list GuardFilterEntry - list bool ret = keep g zip _guard_filter_fn guards guards keep ret append False g guard_type ID_MATCH CLOSURE_MATCH WEAKREF_ALIVE DICT_VERSION ID_MATCH g derived_guard_types DICT_VERSION g derived_guard_types log warning s guard s dropped caching_precompile=True g guard_type g orig_guard name ret append False ret append True ret sorted_guards = sorted guards key=Guard sort_key guard_filter_fn If we re filtering guards we need build extra time first because filtering depends builder guard_manager results builder guard_manager = build_guards sorted_guards existing_diff_guard_sources f_code output_graph False source_get_cache=source_get_cache make_guard_filter_entry guard Guard - GuardFilterEntry MISSING = object name = strip_local_scope guard name name == has_value = False value = MISSING try Guard evaluation expected fail when we guard things like hasattr x foo In cases like we don t have well defined value because such thing doesn t exist value = builder get guard name has_value = True except noqa B E value = MISSING has_value = False is_global = get_global_source_name guard originating_source None GuardFilterEntry name=name has_value=has_value value=value guard_type=guard create_fn_name derived_guard_types= tuple guard guard_types guard guard_types is_global=is_global orig_guard=guard filter_results = guard_filter_fn make_guard_filter_entry guard guard sorted_guards assert len filter_results == len sorted_guards assert all type x bool x filter_results sorted_guards = guard i guard enumerate sorted_guards filter_results i Redo guards because filtering relies results last guard builder builder guard_manager = build_guards sorted_guards existing_diff_guard_sources f_code output_graph save_guards source_get_cache=source_get_cache guard_manager = guard_manager compile_check_fn builder sorted_guards guard_fail_fn Keep track weak references objects ID_MATCH guard This info stored alongside optimized_code guard_manager used limit number cache entries same ID_MATCH d object TODO anijain - Currently information stored attr guard_manager itself avoid changing CacheEntry data structure eval_frame c In future we should probably replace guard_manager queryable data structure such information already present some form guard_manager id_matched_objs = builder id_matched_objs guards_log debug s guard_manager guard_manager id_matched_objs = builder id_matched_objs Check guard returns True False means we will always recompile TODO anijain ydwu - Skipping export because following test python -s test dynamo test_export py -k test_export_with_symbool_inputs latency = output_graph skip_guards_check output_graph export guard_manager check output_graph local_scope reasons = get_guard_fail_reason_helper guard_manager output_graph local_scope CompileContext current_compile_id raise AssertionError Guard failed same frame created This bug - please create issue f Guard fail reason reasons guard_manager_testing_hook_fn None guard_manager_testing_hook_fn guard_manager output_graph local_scope builder NB developers n_iters chosen prevent excessive increase compile time We first do cache flush measure guard latency more accurately This cache flush expensive Note - If you working guard optimization might good idea increase number more stability during development latency = profile_guard_manager guard_manager root output_graph local_scope guards_log debug Guard eval latency = s us f latency f Note We use ` increment_toplevel ` instead ` compilation_metric ` here This because scenarios where ` torch _dynamo reset ` invoked same frame ID compile ID may reused during new compilation cycle This behavior causes issues ` compilation_metric ` expects metric field empty Ideally we would overwrite existing entry such cases we currently lack API support overwriting metrics However since these situations rare typically impractical account we simply increment toplevel instead CompileEventLogger increment_toplevel guard_latency_us int latency guards_state Optional bytes = None save_guards torch _dynamo output_graph OutputGraphCommon assert isinstance output_graph OutputGraphCommon try guards_state = serialize_guards builder sorted_guards output_graph except exc PackageError e torch _dynamo config strict_precompile strict_error raise e output_graph bypass_package f Guard evaluation failed str e traceback=traceback format_exc split \n TODO don t do string rep do something more structured here torch _logging trace_structured dynamo_cpp_guards_str payload_fn=lambda f guard_manager \nGuard latency = latency f us NB - We have very careful cleaning up here Because invalidate function we can create weakref finalizer keeps ` ` alive very long Sometimes mistake we can run invalidate type object check id_ref method Python can leak design preventing us calling finalizer In case ` ` will alive even though cache entry will deleted check invalidate method which can cause memory leak e g setting output_graph = None can keep hold nn_modules _weakrefs clear output_graph = None UNSUPPORTED_SERIALIZATION_GUARD_TYPES tuple LiteralString = DICT_VERSION NN_MODULE ID_MATCH FUNCTION_MATCH CLASS_MATCH MODULE_MATCH CLOSURE_MATCH WEAKREF_ALIVE serialize_guards builder GuardBuilder sorted_guards list Guard output_graph OutputGraphCommon - bytes We check whether our list guards serializable here guard sorted_guards guard_type = guard create_fn_name derived_guard_types = tuple guard guard_types guard guard_types BUILTIN_MATCH calls TYPE_MATCH sometimes so we need check both chance guard unserializable guard_type TYPE_MATCH BUILTIN_MATCH guard _unserializable Only call builder get again we know we re going throw obj = builder get guard name raise_local_type_error obj guard_type CheckFunctionManager UNSUPPORTED_SERIALIZATION_GUARD_TYPES raise torch _dynamo exc PackageError f guard_type guard cannot serialized failed = next i i derived_guard_types i CheckFunctionManager UNSUPPORTED_SERIALIZATION_GUARD_TYPES None Just raise first failed guard name raise torch _dynamo exc PackageError f failed guard cannot serialized builtins_dict_name = output_graph name_of_builtins_dict_key_in_fglobals used_global_vars = set used_local_vars = set prune_variable source Source - None name = get_global_source_name source assert isinstance name str Leave out builtins dict key we will special handle later because guarded code rarely use entire builtin dict common case name = builtins_dict_name used_global_vars add name name = get_local_source_name source assert isinstance name str used_local_vars add name output_graph_guards_state = output_graph dump_guards_state Only serialize global variables actually used guards guard sorted_guards isinstance guard originating_source ShapeEnvSource assert shape_code_parts source shape_code_parts shape_env_sources prune_variable source prune_variable guard originating_source source output_graph guard_on_key_order prune_variable source normalize_create_fn x Callable None - Callable None isinstance x functools partial _ref x Any - Any isinstance x TensorWeakRef weakref ref x x new_args = tuple _ref x args new_keywords = k _ref v k v x keywords items functools partial x func new_args new_keywords x global_scope_state = k v k v output_graph_guards_state global_scope items k used_global_vars k additional_used_global_vars global_scope_state builtins_dict_name = k v k v output_graph_guards_state global_scope builtins_dict_name items type ignore attr-defined k used_builtin_vars output_graph_guards_state = dataclasses replace output_graph_guards_state local_scope= k v k v output_graph_guards_state local_scope items k used_local_vars k additional_used_local_vars global_scope=global_scope_state _guards=torch _guards GuardsSet dataclasses replace guard obj_weakref=None guarded_class_weakref=None create_fn=normalize_create_fn guard create_fn guard sorted_guards input_source_to_sizes_strides=pytree tree_map convert_int_to_concrete_values output_graph_guards_state input_source_to_sizes_strides skip_guards_check=True guards_state = GuardsState output_graph=output_graph_guards_state shape_code_parts=self shape_code_parts source_get_cache=builder source_get_cache pickle_guards_state guards_state builder guard_tree_values build_guards sorted_guards list Guard existing_diff_guard_sources OrderedSet str f_code types CodeType output_graph OutputGraphGuardsState save_guards bool source_get_cache Optional dict str Any = None - tuple GuardBuilder GuardManagerWrapper guard_manager = GuardManagerWrapper guard_manager diff_guard_sources = existing_diff_guard_sources w_builder = None source_ref source Source - str guard_source = source guard_source guard_source GuardSource CONSTANT No need track constants source name assert w_builder r_builder = w_builder assert r_builder None r_builder arg_ref source name builder = GuardBuilder f_code id_ref source_ref lookup_weakrefs output_graph local_scope output_graph global_scope guard_manager save_guards runtime_global_scope=self runtime_global_scope source_get_cache=source_get_cache Break retain cycle See test_release_scope_memory cleanup_builder weak_b weakref ref GuardBuilder - None b = weak_b b b scope = None type ignore assignment Break retain cycle See test_release_input_memory w_builder = weakref ref builder cleanup_builder guard_on_nn_modules = config guard_nn_modules justknobs_check pytorch compiler guard_nn_modules guard sorted_guards guard_on_nn_modules guard is_specialized_nn_module Default func args must guarded TODO we could make use DefaultsSource offer guard is_defaults API __defaults__ guard name __kwdefaults__ guard name config skip_nnmodule_hook_guards hooks guard name continue guard create builder builder guard_manager compile_check_fn builder GuardBuilder guards_out list Guard guard_fail_fn Optional Callable GuardFail None - None see parallel handling ___implicit _eval_frame c largs = builder argnames largs += ___kwargs_ignored guards_log debug GUARDS code_parts = verbose_code_parts = structured_guard_fns list Callable dict str Any = assert torch_function_mode_stack None torch_function_mode_stack_check_fn = make_torch_function_mode_stack_guard torch_function_mode_stack Add compile id info guard manager debugging purpose guard_manager root attach_compile_id str CompileContext current_compile_id Insert global_state guard assert output_graph None global_state = output_graph global_state_guard guard_manager root add_global_state_guard global_state ___check_global_state guard_manager root add_torch_function_mode_stack_guard torch_function_mode_stack ___check_torch_function_mode_stack Clear references torch_function modes held list torch_function_mode_stack = None add_code_part code_part str guard Optional Guard log_only bool = False - None verbose_code_part = get_verbose_code_part code_part guard guards_log debug s verbose_code_part structured_guard_fns append lambda code code_part stack structured from_traceback guard stack summary guard guard stack None user_stack structured from_traceback guard user_stack guard guard user_stack None verbose_guards_log isEnabledFor logging DEBUG maybe_stack = maybe_user_stack = guard None guard stack maybe_stack = f \nStack \n join guard stack format guard user_stack maybe_user_stack = f \nUser stack \n join guard user_stack format verbose_guards_log debug Guard s s s code_part maybe_stack maybe_user_stack log_only code_parts append code_part verbose_code_parts append verbose_code_part seen = set gcl builder code code gcl code_list code seen If Cpp guard manager enabled we don t need add code_parts add_code_part code gcl guard True seen add code no_tensor_aliasing_names = builder no_tensor_aliasing_names check_tensors_fn = None check_tensors_verbose_fn = None len no_tensor_aliasing_names Install tensor aliasing guard TENSOR_MATCH guards already installed cpp guard manager install_no_tensor_aliasing_guard builder no_tensor_aliasing_guard_managers no_tensor_aliasing_names check_no_aliasing + join no_tensor_aliasing_names + Note - On Lambda guarding object aliasing We previously installed object-aliasing guards relational guards undermined recursive-dict guard optimization placing aliasing guard leaf prevented parent dict node qualifying recursive-dict guard root Because aliasing guards rare we now emit them epilogue guards via small Python lambda This repeats access Python — adding bit work — overhead outweighed gains enabling recursive-dict guard optimization config use_lamba_guard_for_object_aliasing builder object_aliasing_guard_codes aliasing_code_parts aliasing_verbose_code_parts = map list zip builder object_aliasing_guard_codes builder add_python_lambda_leaf_guard_to_root aliasing_code_parts aliasing_verbose_code_parts aotautograd_guards list GuardEnvExpr = output_graph aotautograd_guards output_graph TODO anijain - There duplicate logic Dynamo find aliased input tensors So most probably we don t need here Revisit guard aotautograd_guards isinstance guard DuplicateInputs source_a = guard input_source_a source_b = guard input_source_b code_part = f source_a name source_b name install_object_aliasing_guard builder get_guard_manager_from_source source_a builder get_guard_manager_from_source source_b code_part add_code_part code_part None True isinstance guard StorageOverlap overlapping_guard_managers = builder get_guard_manager_from_source s s guard overlapping_sources non_overlapping_guard_managers = builder get_guard_manager_from_source s s guard non_overlapping_sources code_part = check_overlapping f overlapping= join s name s guard overlapping_sources f non_overlapping= join s name s guard non_overlapping_sources install_storage_overlapping_guard overlapping_guard_managers non_overlapping_guard_managers code_part add_code_part code_part None True raise RuntimeError f Unknown GuardEnvExpr guard TODO guard here actually just top level SHAPE_ENV which useless Get ShapeEnv pass more provenance gcl builder shape_env_code code gcl code_list Shape env guards already added CPP guard manager SHAPE_ENV implementation add_code_part code gcl guard True OK all done generating guards structured_guard_fns torch _logging trace_structured dynamo_guards payload_fn=lambda f f structured_guard_fns convert_frame initial_global_state None we should only hit case NopTests global_state = convert_frame GlobalStateGuard closure_vars = ___check_tensors check_tensors_fn ___check_tensors_verbose check_tensors_verbose_fn ___check_global_state global_state check ___check_torch_function_mode_stack torch_function_mode_stack_check_fn SYMPY_INTERP _get_closure_vars guard_manager finalize globals_for_guard_fn = G builder scope G Guard manager construction complete Ensure we did miss insert guard cpp guard manager assert len code_parts == guard_manager closure_vars = closure_vars guard_manager args = largs guard_manager populate_code_parts_for_debugging guard_manager verbose_code_parts = verbose_code_parts Grab only G preserve G because guards access G guard_manager global_scope = globals_for_guard_fn guard_manager guard_fail_fn = guard_fail_fn will populated non-owning reference CacheEntry ExtraState when CacheEntry constructed guard_manager cache_entry = None guard_manager extra_state = None guard_manager no_tensor_aliasing_sources = no_tensor_aliasing_names invalidate obj_str str - None Some tests reveal CheckFunctionManager has no attribute guard_manager case should any concern This case doesn t seem easy repro hasattr guard_manager isinstance guard_manager DeletedGuardManagerWrapper cache_entry = guard_manager cache_entry None extra_state = guard_manager extra_state None assert isinstance cache_entry CacheEntry assert isinstance extra_state ExtraState reason = f Cache line invalidated because obj_str got deallocated deleted_guard_manager = DeletedGuardManagerWrapper reason extra_state invalidate cache_entry deleted_guard_manager guard_manager = deleted_guard_manager id_ref obj object obj_str str - int add weakref id try id obj _weakrefs We will clear _weakrefs dict end __init__ function which will delete callbacks well Therefore we using finalizer which kept alive _weakrefs id obj = weakref ref obj weakref finalize obj functools partial invalidate obj_str=obj_str except TypeError pass cannot weakref bool object id obj lookup_weakrefs obj object - Optional weakref ref object Lookup _weakrefs created id_ref function ID_MATCH d objects id obj _weakrefs _weakrefs id obj None build_guard_function code_parts list str closure_args str - tuple str str torch _inductor utils IndentedBuffer csepass = PyExprCSEPass try csepass count code_parts replace expr str - tuple list str str csepass replace expr except RecursionError If we hit recursion limits during CSE analysis fall back no-op replace function This can happen extremely complex guard expressions replace expr str - tuple list str str expr Generate inner body guard function i e if-chain guard expressions guard_body = IndentedBuffer expr code_parts preface expr = replace expr guard_body writelines preface guard_body writeline f expr guard_body indent guard_body writeline False Wrap inner body into actual guard function guard = IndentedBuffer guard writeline guard L guard indent guard splice guard_body guard writeline True Wrap whole guard function into another function closure variables make_guard_fn = IndentedBuffer make_guard_fn writeline f ___make_guard_fn closure_args make_guard_fn indent make_guard_fn splice guard make_guard_fn writeline guard guard_body getvalue make_guard_fn getvalue is_recompiles_enabled - bool torch _logging _internal log_state is_artifact_enabled recompiles is_recompiles_verbose_enabled - bool torch _logging _internal log_state is_artifact_enabled recompiles_verbose will only used cpp guards disabled make_torch_function_mode_stack_guard initial_stack list torch overrides TorchFunctionMode - Callable bool types = type x x initial_stack check_torch_function_mode_stack - bool cur_stack = get_torch_function_mode_stack len cur_stack = len types False ty mode zip types cur_stack ty type mode False True check_torch_function_mode_stack Scope = TypeAliasType Scope dict str object recompilation_reason_for_no_tensor_aliasing_guard guard_manager GuardManagerWrapper scope Scope - list str assert guard_manager global_scope None global_scope = dict guard_manager global_scope ids_to_source = collections defaultdict list tensor_source guard_manager no_tensor_aliasing_sources global_scope __compile_source__ = tensor_source tensor_id = id eval tensor_source global_scope scope ids_to_source tensor_id append tensor_source duplicate_tensors = f ids_to_source key key ids_to_source len ids_to_source key reason = join duplicate_tensors f Duplicate tensors found reason strip_local_scope s str - str Replace occurrences L just inner content Handles both single double quotes This generate user friendly recompilation messages re pattern = r L\ \s \ \ \s \ re sub pattern r \ s get_guard_fail_reason_helper guard_manager GuardManagerWrapper f_locals dict str object compile_id Optional CompileId - str Return reason why ` guard_manager ` failed Updates ` guard_failures ` generated reason Only first failed check guard_manager reported assert guard_manager global_scope None assert guard_manager closure_vars None scope = L f_locals G guard_manager global_scope G scope update guard_manager closure_vars reasons list str = no_tensor_aliasing_check_failed = False verbose_code_parts list str = guard_debug_info = guard_manager check_verbose f_locals For test_export_with_map_cond check_verbose fail even without C++ guard manager We need fix issue remove comment assert guard_debug_info result guard_debug_info result verbose_code_parts = guard_debug_info verbose_code_parts verbose_code_parts either actual reason e g case TENSOR_MATCH could list verbose_code_part we passed leaf guard construction time If its list we walk through list find guard failed This very important symbolic shape guards which currently installed lambda guard can encompass long list code_parts len verbose_code_parts == Duplicate tensor found verbose_code_parts no_tensor_aliasing_check_failed = True reasons = verbose_code_parts verbose_code_parts = no_tensor_aliasing_check_failed reasons = recompilation_reason_for_no_tensor_aliasing_guard guard_manager scope part verbose_code_parts global_scope = dict guard_manager global_scope global_scope __compile_source__ = part report_compile_source_on_error try fail_reason = eval part global_scope scope except Exception is_recompiles_verbose_enabled continue raise Only ___check_tensors knows how fancy fail reason everything we just report code failed isinstance fail_reason bool fail_reason fail_reason = part isinstance fail_reason str reasons append fail_reason is_recompiles_verbose_enabled break reason_str = f compile_id + join reasons strip_local_scope reason_str get_guard_fail_reason guard_manager GuardManagerWrapper code types CodeType f_locals dict str object compile_id CompileId skip_logging bool = False - str isinstance guard_manager DeletedGuardManagerWrapper f compile_id guard_manager invalidation_reason reason_str = get_guard_fail_reason_helper guard_manager f_locals compile_id skip_logging reason_str guard_failures orig_code_map code append reason_str try guard_manager guard_fail_fn None guard_manager guard_fail_fn GuardFail reason_str unknown reason orig_code_map code except Exception log exception Failure guard_fail_fn callback - raising here will cause NULL Error guard eval reason_str get_and_maybe_log_recompilation_reasons cache_entry Optional CacheEntry frame DynamoFrameType skip_logging bool = False - list str Return list guard failure reasons using cache_entry Logs recompilation reason ` recompiles ` logging enabled Raises RecompileError ` config error_on_recompile ` enabled reasons = while cache_entry None reason = get_guard_fail_reason cache_entry guard_manager cache_entry code frame f_locals cache_entry compile_id skip_logging reason reasons append reason cache_entry = cache_entry next code = frame f_code skip_logging reasons least one recompiles recompiles_verbose enabled do_recompiles_log = is_recompiles_enabled is_recompiles_verbose_enabled do_recompiles_log config error_on_recompile is_recompiles_verbose_enabled failures = \n\n join f guard i failures \n + textwrap indent reason - i reason enumerate reasons failures = textwrap indent \n join reasons - guard_failure_details = f triggered following guard failure s \n failures message = f Recompiling function code co_name code co_filename code co_firstlineno \n f textwrap indent guard_failure_details do_recompiles_log is_recompiles_verbose_enabled recompiles_verbose_log debug message recompiles_log debug message config error_on_recompile raise exc RecompileError message torch _logging trace_structured artifact metadata_fn=lambda name recompile_reasons encoding json payload_fn=lambda reasons reasons update_diff_guard_managers_for_existing_cache_entries cache_entry Optional CacheEntry - OrderedSet str first_cache_entry = cache_entry On first pass go through cache entries accumulate diff guard sources Different guard managers can fail different sources So we collect all them first acc_diff_guard_sources OrderedSet str = OrderedSet while cache_entry None acc_diff_guard_sources update cache_entry guard_manager collect_diff_guard_sources cache_entry = cache_entry next type ignore assignment On second pass set diff_guard_sources each cache line accumulated value And re-populate diff guard manager cache_entry = first_cache_entry while cache_entry None cache_entry guard_manager diff_guard_sources = acc_diff_guard_sources cache_entry guard_manager populate_diff_guard_manager cache_entry = cache_entry next type ignore assignment accumulated sources set up new cache line acc_diff_guard_sources guard_error_hook guard_manager GuardFn code types CodeType f_locals dict str object index int last bool - None print f ERROR RUNNING GUARDS code co_name code co_filename code co_firstlineno print lambda + join guard_manager args + print and\n join guard_manager code_parts print guard_manager local_scope = L f_locals guard_manager closure_vars guard guard_manager code_parts try eval guard guard_manager global_scope local_scope except noqa B E print f Malformed guard \n guard set_guard_error_hook guard_error_hook unique seq Sequence T - Generator T None None seen = set x seq x seen yield x seen add x make_dupe_guard obj_source Source dupe_source Source - Optional functools partial Any Note - we may end up situation where we invoke something like fn x y fn x x Prior addition tracking all relevant objects we would handle just fine eagerly re-entering VB rewrapping inputs correctly creating graphargs placeholders However tracking inputs duplicate inputs aliased relationships may end up getting erased here - In fn x x example call above look like graph single input In order ensure we do reuse fn x x fn x y we create duplicate input guard Note - we may have source fine just means we had object safe have leave unsourced - like local list created discharged entirely within local scope dupe_source dupe_source = obj_source ser_source_is_local = is_from_local_source dupe_source source_is_local = is_from_local_source obj_source is_from_flatten_script_object_source dupe_source is_from_flatten_script_object_source obj_source raise exc UnsafeScriptObjectError f obj_source name aliasing dupe_source name This supported f Please do clone corresponding input Note - both must local global we will run afoul lack merging how we currently reconcile guards builder scopes compile_check_fn This technically means we miss guard here so maybe we should do refactor before we land TODO voz Combine local global guard builders ser_source_is_local == source_is_local Note - little aggressive - these being duplicate input does always matter However should always sound guard add here functools partial GuardBuilder DUPLICATE_INPUT source_b=dupe_source None install_guard guards Guard skip int = - None Add dynamo guards current tracing context Args guards guard s add skip number stack frames ignore debug stack trace torch _guards TracingContext collect_debug_stack = guards_log isEnabledFor logging DEBUG verbose_guards_log isEnabledFor logging DEBUG add = TracingContext get guards_context dynamo_guards add guard guards assert isinstance guard Guard is_from_skip_guard_source guard originating_source continue add guard collect_debug_stack=collect_debug_stack skip=skip +