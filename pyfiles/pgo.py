Profile Guided Optimization PGO implementation Dynamo This module provides functionality caching managing code state profiles guide optimization decisions Dynamo It implements both local remote caching mechanisms storing profile information across runs handles profile merging across distributed ranks manages lifecycle profile data during compilation The profiles track dynamic vs static properties tensors help Dynamo make better specialization decisions __future__ annotations base copy dataclasses enum functools logging os pickle re zlib collections defaultdict typing Optional TYPE_CHECKING TypeVar Union typing_extensions override Self torch _dynamo config torch _utils_internal torch compiler config torch distributed dist torch _dynamo utils CompileEventLogger dynamo_timed set_feature_use warn_once torch _environment is_fbcode torch _logging _internal trace_structured_artifact torch compiler _cache CacheArtifact CacheArtifactFactory CacheArtifactManager torch utils _ordered_set OrderedSet TYPE_CHECKING types torch _dynamo symbolic_convert InstructionTranslator torch _inductor remote_cache JsonDataTy RemoteCache ReservedWorkflowIdUserError ValueError pass log = logging getLogger __name__ LOCK_TIMEOUT = How does memory representation work Concretely module responsible holding GLOBAL state representing state holds no other copies permitted So we retire frame_state entirely store here This should reset when Dynamo reset We never GC information similar how filesystem doesn t get cleaned up except tmp cleaner so expectation information relatively cheap we don t mind leaking How exactly did we design cache key Here some questions - JOB_ID Do we have unique identifier training run such stays same we re running same code changes we re running something different - RANK Are we sharing cache across ranks does each rank get individual cache We choose require job_id PGO cache This prevent situations where unrelated invocations PyTorch unpredictably cause changes each other s behavior With job_id least you know there some state associated State dict might another way tell run related You can opt-in YOLO everything aliases everything passing shared job_id all your invocations We choose NOT share PGO cache across ranks With no RANK_SHARING there never contention between runs so we can leisurely update bundle information we need Because we grouped job_id we can have single consolidated bundle everything maybe worry about O n^ IO we updated every compile -- let s just instrument Can even take filelock extra safety expect no contention expect ns overhead uncontended filelock If we did share ranks everyone storming modify same cache files We can do having folks atomic write CAS-store then having readers do on-the-fly merging can implemented remote using prefix iteration As optional optimization one rank can elected handling bundling post facto ideally done async after quiescence without compiler collective need wait everyone finish writing their bits Not sure how you can avoid listdir because some rank shows up some new entries we need pull them ASAP unless you want delay bundling But compiler collectives fill similar niche compilers chat each other so rank has collected everything So elect rank only write bundle Don t even need CAS-store atomic write just one rank writing updating bundles The point use compiler collectives share profiles across ranks use PGO cache persist profiles per rank across attempts No need have one mechanism do everything functools cache _hash_containing_file filepath str - str file does exists we consider filepath hash os path exists filepath filepath open filepath rb file content = file read crc _value = zlib crc content hash = format crc _value xFFFFFFFF x hash dataclasses dataclass frozen=True CodeId filename str firstlineno int name str When job restart code can copied different path than previous attempt In case filename will have different value we do want consider those differences Instead we hash content file use identifier file filename kept object give readable information pointer actual file local code state will refer first seen file path file_hash str Exclude file name __eq__ other object - bool isinstance other CodeId False file_hash == other file_hash firstlineno == other firstlineno name == other name Ensure two CodeIds same then they have same hash excluding filename __hash__ - int hash file_hash name firstlineno __str__ - str f hash file_hash filename firstlineno name staticmethod make code types CodeType - CodeId CodeId code co_filename code co_firstlineno code co_name _hash_containing_file code co_filename dataclasses dataclass CodeState automatic_dynamic defaultdict str FrameStateSizeEntry = dataclasses field pyrefly ignore unbound-name default_factory=lambda defaultdict FrameStateSizeEntry _INIT_CODE_STATE Optional defaultdict CodeId CodeState = None _CODE_STATE Optional defaultdict CodeId CodeState = None _LOGGED_DYNAMIC_ALLOWLIST bool = False _KNOWN_DYNAMIC_SOURCES set str = set dataclasses dataclass frozen=True InferStride Denotes quantity stride dim size dim which what stride would next physical dimension results contiguous layout For example given size = stride = we can replace stride = InferStride because InferStride = stride size = = Indirecting representation way important join operation strides we join we don t want None None which would get eventually symbolized into s s notice relationship between s s broken If we instead rewrite expressions InferStride so we have InferStride InferStride we now join None InferStride will result s s desired dim int _T = TypeVar _T AutoUnset enum Enum The identity element our semilattice generic don t know element always subsumed when we get more information token = auto_unset = AutoUnset token AutoDynamic enum Enum The top element our bounded semilattice whenever you merge any other element you always get again token = auto_dynamic = AutoDynamic token dataclasses dataclass FrameStateSizeEntry scalar Union int AutoDynamic AutoUnset = dataclasses field default=auto_unset NB We don t have cases where we have known dimensionality we know NOTHING about individual sizes size Union AutoDynamic AutoUnset tuple Union int AutoDynamic = dataclasses field default=auto_unset stride Union AutoDynamic AutoUnset tuple Union int AutoDynamic InferStride = dataclasses field default=auto_unset render - str Special cases render_single s Union int AutoDynamic AutoUnset InferStride - str s auto_dynamic s auto_unset This basically shouldn t happen debugging auto unset isinstance s InferStride f S s dim str s render_tuple ss tuple Union int AutoDynamic InferStride - str + join render_single s s ss + Common cases size auto_dynamic stride auto_dynamic scalar auto_dynamic fully dynamic scalar tensor f scalar scalar scalar auto_dynamic isinstance size tuple isinstance stride tuple f tensor size= render_tuple size stride= render_tuple stride Fallback f unusual repr __post_init__ - None assert isinstance scalar torch SymInt scalar isinstance size tuple s size assert isinstance s torch SymInt s isinstance stride tuple s stride assert isinstance s torch SymInt s is_size_dynamic dim int - bool size auto_dynamic True size auto_unset False size dim auto_dynamic is_stride_dynamic dim int - bool At moment dynamic strides bit buggy Good test case here ` PYTORCH_TEST_WITH_DYNAMO= python test test_autograd py TestAutograd test_gradcheck_jacobian_mismatch ` This statement preserves historical behavior which we ONLY make strides dynamic size exactly static everywhere We could potentially relax general we should very careful about when infer dynamic strides Actually existing algorithm already somewhat problematic Suppose tensor sometimes f other times f specifically dim physically transposed If we infer strides should DYNAMIC DYNAMIC But silly we really should have just guarded dim order isinstance size tuple all type s int s size False stride auto_dynamic True stride auto_unset False stride dim auto_dynamic staticmethod _munge_symint xs tuple int - tuple Union AutoDynamic int tuple auto_dynamic isinstance x torch SymInt x x xs classmethod make_scalar cls x int - FrameStateSizeEntry FrameStateSizeEntry scalar=x size=auto_dynamic stride=auto_dynamic classmethod make_tensor cls size tuple int stride tuple int - FrameStateSizeEntry FrameStateSizeEntry scalar=auto_dynamic size=cls _munge_symint size stride=cls _munge_symint stride classmethod make_size cls size tuple int - FrameStateSizeEntry FrameStateSizeEntry scalar=auto_unset size=cls _munge_symint size stride=auto_unset staticmethod _merge_atom x _T y _T - Union AutoDynamic _T x auto_unset y y auto_unset x x auto_dynamic y auto_dynamic x = y auto_dynamic x classmethod _merge_atom_tup cls xs Union AutoDynamic AutoUnset tuple _T ys Union AutoDynamic AutoUnset tuple _T - Union AutoDynamic AutoUnset tuple Union AutoDynamic _T xs auto_unset ys ys auto_unset xs xs auto_dynamic ys auto_dynamic auto_dynamic len xs = len ys auto_dynamic tuple cls _merge_atom x y x y zip xs ys __ior__ other Self - Self scalar = _merge_atom scalar other scalar size = _merge_atom_tup size other size stride = _merge_atom_tup stride other stride update_automatic_dynamic tx InstructionTranslator name str entry FrameStateSizeEntry is_unspecialized_nn_module bool = False - FrameStateSizeEntry code_id = CodeId make tx f_code frame_state = get_code_state code_id torch _dynamo config automatic_dynamic_shapes is_update = name frame_state automatic_dynamic mut_entry = frame_state automatic_dynamic name old_entry = copy copy mut_entry mut_entry &#124; = entry Do some logs damn I spend more code logging than I do actually doing updates lol is_update old_entry scalar = mut_entry scalar log debug automatic dynamic int s val s = s name entry scalar old_entry scalar CompileEventLogger instant automatic_dynamic name name dim_changed scalar reason scalar change cached str old_entry scalar new str entry scalar is_unspecialized_nn_module log info s converted symbolic integer It attribute user defined nn module If you wish keep static you can mark nn module ` torch _dynamo mark_static ` name log_tup tup_name str short_reason str long_reason str i Optional int = None - None entry_tup = getattr entry tup_name i None getattr entry tup_name i old_entry_tup = getattr old_entry tup_name i None getattr old_entry tup_name i log debug automatic dynamic s s s s = s tup_name name short_reason NB We used only report len here dim mismatch entry_tup old_entry_tup CompileEventLogger instant automatic_dynamic name name dim_changed all i None i reason long_reason cached str old_entry_tup new str entry_tup is_update old_entry size = mut_entry size isinstance old_entry size tuple isinstance entry size tuple len old_entry size = len entry size log_tup size dim dimensionality change i range len entry size old_entry size i = entry size i log_tup size f size i size change i log_tup size other other is_update old_entry stride = mut_entry stride isinstance old_entry stride tuple isinstance entry stride tuple len old_entry stride = len entry stride log_tup stride dim dimensionality change i range len entry stride old_entry stride i = entry stride i log_tup stride f stride i stride change i log_tup stride other other old_entry = frame_state automatic_dynamic name log debug automatic dynamic off overwriting int s val s - s name old_entry scalar entry scalar frame_state automatic_dynamic name = entry mut_entry = entry mut_entry process_automatic_dynamic tx InstructionTranslator name str entry FrameStateSizeEntry is_unspecialized_nn_module bool = False - FrameStateSizeEntry st = tx distributed_state None update_automatic_dynamic tx name entry is_unspecialized_nn_module=is_unspecialized_nn_module st all_states None Preflight always pretend s static The point here we want get through preflight quickly static will run faster The preexisting frame state will get applied anyway after we do compiler collectives TODO I m sure we should just bong entire pgo state here kind depends we re going have other things talk compiler collective Also PGO state we ve already inferred something automatic dynamic will have lost actual input sizes which might useful debugging purposes e g observing specialization Bonging entire PGO state here would let us delete logic here compiler collective would just directly update_automatic_dynamic st local_state automatic_dynamic name = entry entry Apply updates NB all_states includes local state too res = None sub_state st all_states name sub_state automatic_dynamic res = update_automatic_dynamic tx name sub_state automatic_dynamic name is_unspecialized_nn_module=is_unspecialized_nn_module assert res None res format_cache_key key str - str NB We always use global rank keys even though they overkill local only cache rank = None dist is_available dist is_initialized rank = dist get_rank tag = torch compiler config cache_key_tag f key rank tag get_cache_key - Optional str TODO info versions these logs log only once torch compiler config force_disable_caches warn_once dynamo_pgo force disabled torch compiler config force_disable_caches None NB We namespace cache keys so only user-specified job id can alias each other r = torch compiler config job_id None r startswith mast raise ReservedWorkflowIdUserError torch compiler config job_id prefix mast reserved automatically generated job id associated specific MAST job name version format_cache_key r name_version = torch _utils_internal get_mast_job_name_version None mast_job_name mast_job_version = name_version format_cache_key f mast mast_job_name mast_job_version None get_extra_cache_key sticky_key str - Optional str torch compiler config force_disable_caches warn_once dynamo_pgo force disabled torch compiler config force_disable_caches None format_cache_key sticky_key This solely controls local PGO code_state_path cache_key str - Optional str torch _dynamo config automatic_dynamic_local_pgo log debug automatic_dynamic_local_pgo enabled None torch _inductor runtime runtime_utils cache_dir code_state_key = re sub r \\ &#124; _ f code_state_ cache_key pkl os path join cache_dir dynamo code_state_key should_use_remote_dynamo_pgo_cache - bool torch compiler config force_disable_caches False r = torch _dynamo config automatic_dynamic_remote_pgo None r is_fbcode False torch _utils_internal is_fb_unit_test False try torch _inductor fb remote_cache REMOTE_CACHE_VERSION except ModuleNotFoundError False REMOTE_CACHE_VERSION = torch _utils_internal justknobs_getval_int pytorch remote_cache dynamo_pgo_version get_remote_cache - Optional RemoteCache JsonDataTy torch _inductor remote_cache create_cache should_use_remote_dynamo_pgo_cache None create_cache dynamo-pgo is_fbcode FbRemoteDynamoPGOCache RemoteDynamoPGOCache _collect_dynamic_sources code_state CodeState - OrderedSet str dynamic_sources OrderedSet str = OrderedSet src fs code_state automatic_dynamic items dynamic = False isinstance fs size tuple dynamic = auto_dynamic fs size type ignore operator fs scalar == auto_dynamic dynamic = True dynamic dynamic_sources add src dynamic_sources _collect_missing_sources all_sources OrderedSet str - OrderedSet str torch _dynamo variables builder is_dynamic_source global _KNOWN_DYNAMIC_SOURCES missing_sources OrderedSet str = OrderedSet src all_sources src _KNOWN_DYNAMIC_SOURCES continue is_dynamic_source src _KNOWN_DYNAMIC_SOURCES add src continue missing_sources add src missing_sources log_frame_dynamic_whitelist f_code types CodeType - None global _KNOWN_DYNAMIC_SOURCES code_id = CodeId make f_code frame_state = get_code_state code_id all_dynamic_sources = _collect_dynamic_sources frame_state frame_whitelist = join all_dynamic_sources missing_whitelist = join _collect_missing_sources all_dynamic_sources frame_whitelist dynamo_timed name = pgo dynamic_whitelist log_pt _compile_event=True CompileEventLogger pt _compile name recompile_dynamic_whitelist=frame_whitelist missing_dynamic_whitelist=missing_whitelist _log_size_mismatch_recompile - None global _LOGGED_DYNAMIC_ALLOWLIST _LOGGED_DYNAMIC_ALLOWLIST torch _utils_internal add_mlhub_insight category= dynamic_shapes_analysis insight= Dynamic shape recompilation detected insight_description= PGO detected recompilation due dynamic shapes \ Please follow instruction action link reduce \ recompilation overhead add mlhub insight only once per rank _LOGGED_DYNAMIC_ALLOWLIST = True render_code_state cs defaultdict CodeId CodeState - str code_state_str = \n join f k \n + \n join f src fs render src fs v automatic_dynamic items k v cs items dynamic_sources OrderedSet str = OrderedSet state cs values dynamic_sources update _collect_dynamic_sources state dynamic_sources code_state_str += \n\nPGO detected recompilation due dynamic shapes To reduce shape recompilations compiling dynamically start f set environment variable TORCH_COMPILE_DYNAMIC_SOURCES= join dynamic_sources code_state_str CacheArtifactFactory register PGOCacheArtifact CacheArtifact override populate_cache - None meta = write_local_impl _rewrite_cache_key_for_mega_cache key content assert meta None override staticmethod type - str pgo staticmethod _rewrite_cache_key_for_mega_cache original_key str - str The PGO cache artifact key MAST job contains job name version When we want use cache artifact different MAST job we need update key use new MAST job s name version original_key startswith mast original_key overridden then dont change original_key new_key = get_cache_key None new_key original_key hit key str ty str - defaultdict CodeId CodeState global _INIT_CODE_STATE assert isinstance _CODE_STATE defaultdict log info get_code_state s hit s d entries key ty len _CODE_STATE trace_structured_artifact f get_ ty _code_state string lambda render_code_state _CODE_STATE type ignore arg-type set_feature_use pgo True _INIT_CODE_STATE = copy deepcopy _CODE_STATE _CODE_STATE get_local_code_state cache_key str - Optional defaultdict CodeId CodeState global _CODE_STATE path = code_state_path cache_key path None os path exists path dynamo_timed name = pgo get_local_code_state log_pt _compile_event=True CompileEventLogger pt _compile name cache_key=cache_key Read lock necessary we always write atomically write actual location open path rb f try content = f read _CODE_STATE = pickle loads content CompileEventLogger pt _compile name cache_size_bytes=f tell except Exception log warning get_code_state failed while reading s path exc_info=True CacheArtifactManager record_artifact PGOCacheArtifact type cache_key content hit path local None lookup_remote_cache_entry remote_cache RemoteCache JsonDataTy cache_key str event_name Optional str = None - Optional defaultdict CodeId CodeState code_state = None try cache_data = remote_cache get cache_key except Exception log warning get_code_state failed remote read s cache_key exc_info=True cache_data None try assert isinstance cache_data dict data = cache_data data assert isinstance data str payload = base b decode data event_name None CompileEventLogger pt _compile event_name cache_size_bytes=len payload code_state = pickle loads payload except Exception log warning get_code_state failed parsing remote result s cache_key exc_info=True CacheArtifactManager record_artifact PGOCacheArtifact type cache_key payload log info get_code_state remote miss s cache_key code_state get_remote_code_state cache_key str - Optional defaultdict CodeId CodeState global _CODE_STATE remote_cache = get_remote_cache remote_cache None dynamo_timed name = pgo get_remote_code_state log_pt _compile_event=True dynamo_compile_column_us= pgo_get_remote_code_state_time_us CompileEventLogger pt _compile name cache_key=cache_key code_state = lookup_remote_cache_entry remote_cache cache_key name code_state None _CODE_STATE = code_state hit cache_key remote None get_extra_remote_code_state cache_key str - None Reads additional PGO profile given cache key merges default PGO profile global _CODE_STATE assert _CODE_STATE None remote_cache = get_remote_cache remote_cache None dynamo_timed name = pgo get_extra_remote_code_state log_pt _compile_event=True dynamo_compile_column_us= pgo_get_remote_code_state_time_us CompileEventLogger pt _compile name cache_key=cache_key code_state = lookup_remote_cache_entry remote_cache cache_key log info get_extra_code_state s hit d entries cache_key len code_state code_state None code_state None assert _CODE_STATE _CODE_STATE = code_state log tlparse trace_structured_artifact get_extra_remote_code_state string lambda render_code_state code_state get_code_state - defaultdict CodeId CodeState global _CODE_STATE _INIT_CODE_STATE _CODE_STATE None _CODE_STATE Initialize even we don t look up profile _CODE_STATE = defaultdict CodeState cache_key = get_cache_key cache_key None _CODE_STATE Attempt local local_code_state = get_local_code_state cache_key Attempt remote local_code_state None get_remote_code_state cache_key Attempt additional remote neither local default remote succeeded _CODE_STATE sticky_read = torch compiler config pgo_extra_read_key None extra_read_key = get_extra_cache_key sticky_read extra_read_key None get_extra_remote_code_state extra_read_key log info get_code_state using default assert _CODE_STATE None _CODE_STATE put_code_state - None _CODE_STATE None log info put_code_state never initialized will write _CODE_STATE == _INIT_CODE_STATE log info put_code_state no change skipping cache_key = get_cache_key cache_key None log info put_code_state no cache key skipping put_local_code_state cache_key put_remote_code_state cache_key sticky_write = torch compiler config pgo_extra_write_key None extra_write_key = get_extra_cache_key sticky_write extra_write_key None put_remote_code_state extra_write_key write_local_impl cache_key str pickled_code bytes - Optional tuple str int path = code_state_path cache_key path None None If user isn t misusing our API we should have exclusive access directory But s too hard tmp_path = path + tmp lock_path = path + lock We mostly don t need lock tmp file could clobbered TODO use safe tempfile create eliminate lock torch utils _filelock FileLock os makedirs os path dirname path exist_ok=True FileLock lock_path timeout=LOCK_TIMEOUT open tmp_path wb f f write pickled_code size = f tell os replace tmp_path path path size put_local_code_state cache_key str - None dynamo_timed name = pgo put_local_code_state log_pt _compile_event=True CompileEventLogger pt _compile name cache_key=cache_key assert _CODE_STATE None pickled_code = pickle dumps _CODE_STATE CacheArtifactManager record_artifact PGOCacheArtifact type cache_key pickled_code meta = write_local_impl cache_key pickled_code meta None log info put_code_state local cache disabled path size = meta CompileEventLogger pt _compile name cache_size_bytes=size log info put_code_state wrote local s d entries path len _CODE_STATE trace_structured_artifact put_local_code_state string lambda render_code_state _CODE_STATE put_remote_code_state cache_key str extra_code_state bool = False - None event_name = put_remote_code_state extra_code_state put_extra_remote_code_state dynamo_timed name = f pgo event_name log_pt _compile_event=True dynamo_compile_column_us= pgo_put_remote_code_state_time_us CompileEventLogger pt _compile name cache_key=cache_key assert _CODE_STATE None remote_cache = get_remote_cache remote_cache None log info s remote cache disabled event_name content = pickle dumps _CODE_STATE CompileEventLogger pt _compile name cache_size_bytes=len content cache_data JsonDataTy = data base b encode content decode ascii remote_cache put cache_key cache_data log info s wrote remote s d entries event_name cache_key len _CODE_STATE TODO don t log multiple times trace_structured_artifact event_name string lambda render_code_state _CODE_STATE NB does NOT reset cached code state disk reset_code_state - None global _CODE_STATE _INIT_CODE_STATE _LOGGED_DYNAMIC_ALLOWLIST _CODE_STATE = None _INIT_CODE_STATE = None _LOGGED_DYNAMIC_ALLOWLIST = False