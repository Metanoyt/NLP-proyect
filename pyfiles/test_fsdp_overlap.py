Owner s oncall distributed sys time unittest statistics mean unittest mock patch torch torch nn nn torch distributed dist Event torch distributed fsdp FullyShardedDataParallel FSDP torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest torch testing _internal common_utils get_cycles_per_ms run_tests TEST_HPU TEST_WITH_DEV_DBG_ASAN TEST_XPU xfailIf dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit device_type = acc type acc = torch accelerator current_accelerator cpu Layer nn Module __init__ compute_cycles has_params bool super __init__ sleep_cycles = compute_cycles optional_param = None has_params optional_param = nn Parameter torch rand forward x Get events e = Event enable_timing=True e = Event enable_timing=True Record fake forward compute time e record sleep_cycles torch cuda is_available torch cuda _sleep sleep_cycles optional_param None x = x + optional_param force param part graph e record x get_time recorded duration e elapsed_time e _create_model compute_cycles has_params bool Use ` limit_all_gathers=False ` since timing being tested relies CPU running ahead GPU model = FSDP nn Sequential FSDP Layer compute_cycles has_params limit_all_gathers=False FSDP Layer compute_cycles has_params limit_all_gathers=False FSDP Layer compute_cycles has_params limit_all_gathers=False FSDP Layer compute_cycles has_params limit_all_gathers=False limit_all_gathers=False device_type model Min __init__ - None data = add new_data len data data append new_data data = sorted data new_data data - data - = new_data avg mean data TestForwardOverlapWorldSizeOne FSDPTest property world_size _dist_train rank = rank world_size = world_size Save original torch distributed all_gather_into_tensor function since we will patch include artificial delay orig_all_gather = torch distributed all_gather_into_tensor run compute_cycles all_gather_cycles has_params = all_gather_cycles model = _create_model compute_cycles has_params Get input sets input s requires_grad True because we have fake compute forward pass batch = torch rand device_type batch requires_grad = True Run one dummy iteration trigger execution order validation all-gathers out = model batch out backward model zero_grad set_to_none=True We run iterations only collect timing data minimal data points because nondeterministic system events can disturb timing cpu_iter = Min cpu_wait = Min gpu_compute = Min gpu_total = Min _ range Get two events measuring overall time e = Event enable_timing=True e = Event enable_timing=True cpu_start = time process_time all_gather_called = False _delayed_all_gather args kwargs nonlocal all_gather_called all_gather_called = True torch cuda is_available torch cuda _sleep all_gather_cycles assert orig_all_gather orig_all_gather args kwargs forward pass Even though both e e compute stream since compute depends all_gather e -e includes all_gather time e record patch torch distributed all_gather_into_tensor _delayed_all_gather out = model batch has_params world_size assertTrue all_gather_called assertFalse all_gather_called e record backward pass out backward model zero_grad set_to_none=True cpu_iter_time = time process_time - cpu_start wait gpu out item cpu_wait_for_gpu_time = time process_time - cpu_start - cpu_iter_time get sum compute time times = mod model modules isinstance mod Layer continue times append mod get_time get gpu compute + all_gather time overall_gpu_time = e elapsed_time e cpu_iter add cpu_iter_time cpu_wait add cpu_wait_for_gpu_time gpu_compute add sum times gpu_total add overall_gpu_time del model cpu_iter cpu_iter avg cpu_wait cpu_wait avg gpu_compute gpu_compute avg gpu_total gpu_total avg sleep_cycles = int get_cycles_per_ms e = run no compute no all-gather e = run sleep_cycles no compute only all-gather e = run sleep_cycles only compute no all-gather e = run sleep_cycles sleep_cycles both compute all-gather debug_string = f \nrank rank \n e e \n e e \n e e \n e e print debug_string Check cpu gpu timing CPU should run ahead GPU Therefore cpu-gpu wait should long except when there no real work GPU If assertions fail below we likely have cpu-gpu wait forward backward pass e cpu_iter may short cpu may take some time queue both compute all-gather short = e cpu_iter e cpu_iter e cpu_iter e cpu_wait long = e cpu_wait e cpu_wait world_size == short append e cpu_wait all gather should happening long append e cpu_wait all gather should happen prolong cpu-gpu wait s short l long X longer safe margin since GPU work timing around X more CPU assertTrue s l Check GPU timing short = e gpu_compute e gpu_total e gpu_compute long = e gpu_compute e gpu_total e gpu_compute e gpu_total world_size == short append e gpu_total all gather should happening long append e gpu_total all gather should happen prolong cpu-gpu wait s short l long X longer safe margin since time around X longer when there work GPU vs no work assertTrue s l Check GPU overlapping when there all-gather world_size compute_only = e gpu_compute all_gather_only = e gpu_total both = e gpu_total assertTrue compute_only + all_gather_only both unittest skipIf TEST_HPU HPU doesn t has HW sleep API support skipping xfailIf TEST_XPU https github com intel torch-xpu-ops issues skip_if_lt_x_gpu test_forward_overlap _dist_train TestForwardOverlapWorldSizeTwo TestForwardOverlapWorldSizeOne property world_size devices = cuda hpu xpu instantiate_device_type_tests TestForwardOverlapWorldSizeOne globals only_for=devices allow_xpu=True __name__ == __main__ run_tests