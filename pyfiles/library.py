mypy allow-untyped-defs contextlib functools inspect re sys traceback weakref collections abc Callable Sequence typing Any Optional overload TYPE_CHECKING TypeVar Union typing_extensions deprecated ParamSpec torch torch _library _library torch _library custom_ops _cast _maybe_get_opdef custom_op CustomOpDef device_types_t torch _library infer_schema infer_schema noqa F torch _library triton triton_op wrap_triton torch _ops OpOverload torch types _dtype __all__ = Library impl define fallthrough_kernel impl_abstract register_autocast register_fake register_torch_dispatch register_vmap get_ctx get_kernel custom_op triton_op wrap_triton infer_schema _T = TypeVar _T _P = ParamSpec _P Set containing combination namespace operator DispatchKey which new kernel has been registered The keys set form ` namespace + + op_name + + dispatch_key ` This set maintained ensure two libraries don t try override exact same functionality avoid libraries calling into kernels intended called _impls set str = set _defs set str = set prim reserved TorchScript interpreter _reserved_namespaces = prim fallthrough_kernel A dummy function pass ` ` Library impl ` ` order register fallthrough raise NotImplementedError fallthrough_kernel should never called Library A create libraries can used register new operators override operators existing libraries Python A user can optionally pass dispatch keyname they only want register kernels corresponding only one specific dispatch key To create library override operators existing library name ns set kind IMPL To create new library name ns register new operators set kind DEF To create fragment possibly existing library register operators bypass limitation there only one library given namespace set kind FRAGMENT Args ns library name kind DEF IMPL FRAGMENT dispatch_key PyTorch dispatch key default __init__ ns kind dispatch_key= torch fx operator_schemas _SCHEMA_TO_SIGNATURE_CACHE kind IMPL DEF FRAGMENT raise ValueError Unsupported kind kind ns _reserved_namespaces kind == DEF kind == FRAGMENT raise ValueError ns reserved namespace Please try creating library another name frame = traceback extract_stack limit= filename lineno = frame filename frame lineno m Optional Any = torch _C _dispatch_library kind ns dispatch_key filename lineno ns = ns _op_defs set str = set _op_impls set str = set _registration_handles list torch _library utils RegistrationHandle = kind = kind dispatch_key = dispatch_key Use finalizer setup destructor instead __del__ Python __del__ can lead weird things globals locals may already gone when __del__ actually gets called finalizers help situation because lets us capture references keeps them alive weakref finalize _del_library _impls _op_impls _defs _op_defs _registration_handles m _SCHEMA_TO_SIGNATURE_CACHE __repr__ f Library kind= kind ns= ns dispatch_key= dispatch_key define schema alias_analysis= tags= r Defines new operator its semantics ns namespace Args schema function schema define new operator alias_analysis optional Indicates aliasing properties operator arguments can inferred schema default behavior CONSERVATIVE tags Tag &#124; Sequence Tag one more torch Tag apply operator Tagging operator changes operator s behavior under various PyTorch subsystems please read docs torch Tag carefully before applying Returns name operator inferred schema Example my_lib = Library mylib DEF my_lib define sum Tensor - Tensor This added because we also want disallow PURE_FUNCTION alias analysis which valid AliasAnalysis type C++ alias_analysis FROM_SCHEMA CONSERVATIVE raise RuntimeError f Invalid alias_analysis type alias_analysis assert m None isinstance tags torch Tag tags = tags name = schema split packet_name = name split name name has_preexisting_packet = hasattr torch ops ns hasattr getattr torch ops ns packet_name result = m define schema alias_analysis tuple tags name = schema split qualname = ns + + name If OpOverloadPacket exists already then means we re adding new OpOverload Refresh packet include new OpOverload has_preexisting_packet ns = getattr torch ops ns packet = getattr ns packet_name torch _ops _refresh_packet packet _op_defs add qualname _defs add qualname result _register_fake op_name fn _stacklevel= allow_override=False r Registers fake impl operator defined library source = torch _library utils get_source _stacklevel + frame = sys _getframe _stacklevel caller_module = inspect getmodule frame Can none you call register_fake somewhere there isn t module e g __main__ caller_module_name = None caller_module None caller_module __name__ TODO rzou We re gonna need stage change torchvision since torchvision github first caller_module_name None caller_module_name startswith torchvision caller_module_name = None qualname = f ns op_name entry = torch _library simple_registry singleton find qualname caller_module_name None func_to_register = _check_pystubs_once fn qualname caller_module_name func_to_register = fn handle = entry fake_impl register func_to_register source lib=self allow_override=allow_override _registration_handles append handle _register_torch_dispatch_rule op_name torch_dispatch_class fn r Registers torch_dispatch rule given operator torch_dispatch_class This allows open registration specify behavior between operator torch_dispatch_class without needing modify torch_dispatch_class operator directly The torch_dispatch_class either Tensor subclass ` __torch_dispatch__ ` TorchDispatchMode If Tensor subclass we expect fn have following signature cls func OpOverload types Tuple type args kwargs - Any If TorchDispatchMode we expect fn have following signature mode func OpOverload types Tuple type args kwargs - Any qualname = f ns op_name entry = torch _library simple_registry singleton find qualname handle = entry torch_dispatch_rules register torch_dispatch_class fn _registration_handles append handle _impl_with_aoti_compile op_name dispatch_key= r Register operator use AOTI-compiled implementation Args op_name operator name along overload OpOverload object dispatch_key dispatch key input function should registered By default uses dispatch key library created Example my_lib = Library aten IMPL my_lib _impl_with_aoti_compile div Tensor CPU dispatch_key == dispatch_key = dispatch_key pyrefly ignore bad-argument-type assert torch DispatchKeySet dispatch_key has torch _C DispatchKey Dense isinstance op_name str name = op_name isinstance op_name OpOverload name = op_name _schema name overload_name = op_name _schema overload_name overload_name = name = name + + overload_name raise RuntimeError _impl_with_aoti_compile should passed either name OpOverload object first argument key = ns + + name split - + + dispatch_key key _impls TODO future add more info about where existing function registered info today already returned C++ warning when _impl_with_aoti_compile called we error out before raise RuntimeError This allowed since there s already kernel registered python overriding s behavior dispatch key namespace format name split - dispatch_key ns assert m None impl_fn Callable = m impl_with_aoti_compile impl_fn ns name split - dispatch_key _impls add key _op_impls add key impl op_name fn dispatch_key= with_keyset=False allow_override=False r Registers function implementation operator defined library Args op_name operator name along overload OpOverload object fn function s operator implementation input dispatch key func ` ~fallthrough_kernel ` register fallthrough dispatch_key dispatch key input function should registered By default uses dispatch key library created with_keyset flag controlling current dispatcher call keyset should passed first argument attr ` fn ` when calling This should used create appropriate keyset redispatch calls allow_override Flag controlling we want override existing registered kernel implementation This default off will error you re trying register kernel dispatch key kernel already registered Example my_lib = Library aten IMPL div_cpu other other my_lib impl div Tensor div_cpu CPU callable fn raise TypeError f Input function required callable found type type fn dispatch_key == dispatch_key = dispatch_key isinstance op_name str name = op_name isinstance op_name OpOverload name = op_name _schema name overload_name = op_name _schema overload_name overload_name = name = name + + overload_name raise RuntimeError impl should passed either name OpOverload object first argument key = ns + + name split - + + dispatch_key allow_override key _impls TODO future add more info about where existing function registered info today already returned C++ warning when impl called we error out before raise RuntimeError This allowed since there s already kernel registered python overriding s behavior dispatch key namespace format name split - dispatch_key ns dispatch_key == Meta dispatcher_op_name = name dispatcher_op_name dispatcher_op_name = f ns dispatcher_op_name Internally we shouldn t registering meta kernels any operators have CompositeImplicitAutograd kernels Instead we should letting those decompositions run writing meta kernels only base operators torch _C _dispatch_has_kernel_for_dispatch_key dispatcher_op_name CompositeImplicitAutograd raise RuntimeError f We should register meta kernel directly operator name because has CompositeImplicitAutograd kernel core Instead we should let operator decompose ensure we have meta kernels base ops decomposes into assert m None m impl name dispatch_key dispatch_key = CompositeImplicitAutograd fn with_keyset _impls add key _op_impls add key fallback fn dispatch_key= with_keyset=False r Registers function implementation fallback given key This function only works library global namespace _ Args fn function used fallback given dispatch key func ` ~fallthrough_kernel ` register fallthrough dispatch_key dispatch key input function should registered By default uses dispatch key library created with_keyset flag controlling current dispatcher call keyset should passed first argument attr ` fn ` when calling This should used create appropriate keyset redispatch calls Example my_lib = Library _ IMPL fallback_kernel op args kwargs Handle all autocast ops generically my_lib fallback fallback_kernel Autocast dispatch_key == dispatch_key = dispatch_key ns = _ raise RuntimeError f Fallback can only registered using library fragment global namespace _ ns assert dispatch_key = assert m None m fallback dispatch_key fn with_keyset _destroy m None m reset m = None handle _registration_handles handle destroy _registration_handles clear global _impls _impls -= _op_impls name _op_defs Delete cached torch ops ns foo registered Otherwise accessing leads segfault It s possible we only registered overload Library another library owns alive overload That s OK - next time torch ops ns foo gets called ll recomputed point right collection overloads ns name_with_overload = name split name = name_with_overload split hasattr torch ops ns continue namespace = getattr torch ops ns hasattr namespace name continue delattr namespace name namespace _dir remove name _del_library captured_impls op_impls captured_defs op_defs registration_handles m schema_to_signature_cache op_def op_defs name = op_def overload_name = op_def name overload_name = op_def split name overload_name schema_to_signature_cache del schema_to_signature_cache name overload_name captured_impls -= op_impls captured_defs -= op_defs handle registration_handles handle destroy m None m reset contextlib contextmanager _scoped_library args kwargs try lib = Library args kwargs yield lib finally lib _destroy _keep_alive list Library = NAMELESS_SCHEMA = re compile r \ \ - functools singledispatch define qualname schema lib=None tags= r Defines new operator In PyTorch defining op short operator two step-process - we need define op providing operator name schema - we need implement behavior how operator interacts various PyTorch subsystems like CPU CUDA Tensors Autograd etc This entrypoint defines custom operator first step you must then perform second step calling various ` ` impl_ ` ` APIs like func ` torch library impl ` func ` torch library register_fake ` Args qualname str The qualified name operator Should string looks like namespace name e g aten sin Operators PyTorch need namespace avoid name collisions given operator may only created once If you writing Python library we recommend namespace name your top-level module schema str The schema operator E g Tensor x - Tensor op accepts one Tensor returns one Tensor It does contain operator name passed ` ` qualname ` ` lib Optional Library If provided lifetime operator will tied lifetime Library object tags Tag &#124; Sequence Tag one more torch Tag apply operator Tagging operator changes operator s behavior under various PyTorch subsystems please read docs torch Tag carefully before applying Example torch numpy np Define operator torch library define mylib sin Tensor x - Tensor Add implementations operator torch library impl mylib sin cpu f x torch from_numpy np sin x numpy Call new operator torch ops x = torch randn y = torch ops mylib sin x assert torch allclose y x sin isinstance qualname str raise ValueError f define qualname schema expected qualname f instance str got type qualname namespace name = torch _library utils parse_namespace qualname lib None lib = Library namespace FRAGMENT _keep_alive append lib NAMELESS_SCHEMA fullmatch schema raise ValueError f define qualname schema expected schema f look like e g Tensor x - Tensor f got schema lib define name + schema alias_analysis= tags=tags define register _ lib Library schema alias_analysis= The old torch library define We re keeping around BC reasons wrap f name = lib define schema alias_analysis lib impl name f f wrap overload impl qualname str types Union str Sequence str func None = None lib Optional Library = None - Callable Callable object None overload impl qualname str types Union str Sequence str func Callable object lib Optional Library = None - None Deprecated BC API overload impl lib Library name str dispatch_key str = - Callable Callable _P _T Callable _P _T functools singledispatch impl qualname str types Union str Sequence str func Optional Callable _P _T = None lib Optional Library = None - object Register implementation device type operator You may pass default ` ` types ` ` register implementation default implementation ALL device types Please only use implementation truly supports all device types example true composition built-in PyTorch operators This API may used decorator You can use nested decorators API provided they function placed inside API see Example Some valid types cpu cuda xla mps ipu xpu Args qualname str Should string looks like namespace operator_name types str &#124; Sequence str The device types register impl lib Optional Library If provided lifetime registration will tied lifetime Library object Examples torch numpy np Example Register function Define operator torch library define mylib mysin Tensor x - Tensor Add implementations cpu device torch library impl mylib mysin cpu f x torch from_numpy np sin x numpy x = torch randn y = torch ops mylib mysin x assert torch allclose y x sin Example Register function decorator custom_decorator func wrapper args kwargs func args kwargs + wrapper Define operator torch library define mylib sin_plus_one Tensor x - Tensor Add implementations operator torch library impl mylib sin_plus_one cpu custom_decorator f x torch from_numpy np sin x numpy Call new operator torch ops x = torch randn y = torch ops mylib sin_plus_one x y = torch sin x + assert torch allclose y y _impl qualname types func lib=lib disable_dynamo=False TYPE_CHECKING impl register _ lib Library name str dispatch_key str = - Callable Callable _P _T Callable _P _T Legacy torch library impl API Kept around BC wrap f Callable _P _T - Callable _P _T lib impl name f dispatch_key f wrap overload _impl qualname str types Union str Sequence str func None = None lib Optional Library = None disable_dynamo bool = False - Callable Callable object None overload _impl qualname str types Union str Sequence str func Callable object lib Optional Library = None disable_dynamo bool = False - None _impl qualname str types Union str Sequence str func Optional Callable object = None lib Optional Library = None disable_dynamo bool = False - Optional Callable Callable object None See impl isinstance types str types = types keys = set typ types is_dispatch_key = torch _C _parse_dispatch_key typ is_dispatch_key We also support passing DispatchKey impl Please prefer using higher-level torch library APIs only pass DispatchKey torch library impl caution even better don t use option file issue GitHub what you need We don t advertise users because very easy shoot yourself foot keys add typ keys add _device_type_to_key typ register_ func Callable object - None namespace _ = torch _library utils parse_namespace qualname lib None use_lib = Library namespace FRAGMENT _keep_alive append use_lib use_lib = lib disable_dynamo torch _disable_dynamo func_no_dynamo args kwargs func args kwargs key keys use_lib impl qualname func_no_dynamo key key keys use_lib impl qualname func key func None register_ register_ func None _device_type_to_key device_type str - str device_type == default This technically correct because although all device_type DispatchKeys included CompositeExplicitAutograd everything CompositeExplicitAutograd associated device_type I don t really care much about difference CompositeExplicitAutograd torch _C _dispatch_key_for_device device_type deprecated ` torch library impl_abstract ` renamed ` torch library register_fake ` Please use instead we will remove ` torch library impl_abstract ` future version PyTorch category=FutureWarning impl_abstract qualname func=None lib=None _stacklevel= r This API renamed func ` torch library register_fake ` PyTorch Please use instead func None _stacklevel = _stacklevel + register_fake qualname func lib=lib _stacklevel=_stacklevel _op_identifier = Union str torch _ops OpOverload torch _library custom_ops CustomOpDef register_kernel op _op_identifier device_types device_types_t func Optional Callable = None lib Optional Library = None Register implementation device type operator Some valid device_types cpu cuda xla mps ipu xpu This API may used decorator Args op str &#124; OpOverload The operator register impl device_types None &#124; str &#124; Sequence str The device_types register impl If None we will register all device types -- please only use option your implementation truly device-type-agnostic func Callable The function register implementation given device types lib Optional Library If provided lifetime registration Examples xdoctest +REQUIRES env TORCH_DOCTEST_CUDA torch torch Tensor torch library custom_op numpy np Create custom op works cpu custom_op mylib numpy_sin mutates_args= device_types= cpu numpy_sin x Tensor - Tensor x_np = x numpy y_np = np sin x_np torch from_numpy y_np Add implementations cuda device torch library register_kernel mylib numpy_sin cuda _ x x_np = x cpu numpy y_np = np sin x_np torch from_numpy y_np device=x device x_cpu = torch randn x_cuda = x_cpu cuda assert torch allclose numpy_sin x_cpu x_cpu sin assert torch allclose numpy_sin x_cuda x_cuda sin isinstance op str torch _ops OpOverload torch _library custom_ops CustomOpDef raise ValueError f register_kernel op got unexpected type op type op isinstance op torch _ops OpOverload op = op _name opdef = _maybe_get_opdef op opdef None opdef register_kernel device_types func assert isinstance op str device_types None device_types = CompositeExplicitAutograd _impl op device_types func lib=lib disable_dynamo=True register_autocast op _op_identifier device_type str cast_inputs _dtype lib Optional Library = None r Register autocast dispatch rule custom op Valid ` device_type ` include cpu cuda Args op str &#124; OpOverload The operator register autocast dispatch rule device_type str Device type use cuda cpu The type same ` type ` attribute ` torch device ` Thus you may obtain device type tensor using ` Tensor device type ` cast_inputs ` torch dtype ` When custom op runs autocast-enabled region casts incoming floating-point Tensors target dtype non-floating-point Tensors affected then executes custom op autocast disabled lib Optional Library If provided lifetime registration Examples xdoctest +REQUIRES env TORCH_DOCTEST_CUDA torch torch Tensor torch library custom_op Create custom op works cuda torch library custom_op mylib my_sin mutates_args= my_sin x Tensor - Tensor torch sin x Register autocast dispatch rule cuda device torch library register_autocast mylib my_sin cuda torch float x = torch randn dtype=torch float device= cuda torch autocast cuda dtype=torch float y = torch ops mylib my_sin x assert y dtype == torch float isinstance op str torch _ops OpOverload torch _library custom_ops CustomOpDef raise ValueError f register_autocast op got unexpected type op type op device_type cpu cuda raise ValueError f Unknown device type device_type isinstance op torch _ops OpOverload op = op _name opdef = _maybe_get_opdef op opdef None opdef register_autocast device_type cast_inputs assert isinstance op str qualname = op _op = torch _library utils lookup_op qualname namespace opname = torch _library utils parse_namespace qualname lib None lib = Library namespace FRAGMENT _keep_alive append lib _maybe_override_py_impl op torch _ops OpOverload dispatch_key inner kernel op has_kernel_for_dispatch_key dispatch_key op py_kernels pop dispatch_key op py_impl dispatch_key kernel inner _maybe_override_py_impl _op torch _C DispatchKey AutocastCPU _maybe_override_py_impl _op torch _C DispatchKey AutocastCUDA _autocast_py_impl args kwargs assert len kwargs == Custom ops do support kwargs yet autocast_keyset = torch _C DispatchKeySet torch _C DispatchKey AutocastCPU &#124; torch _C DispatchKeySet torch _C DispatchKey AutocastCUDA torch _C _ExcludeDispatchKeyGuard autocast_keyset _op _cast args device_type cast_inputs kernel _ args kwargs assert len kwargs == Custom ops do support kwargs yet _autocast_py_impl args kwargs device_type == cuda lib impl opname kernel AutocastCUDA with_keyset=True device_type cpu lib impl opname kernel AutocastCPU with_keyset=True register_fake op _op_identifier func Optional Callable = None lib Optional Library = None _stacklevel int = allow_override bool = False r Register FakeTensor implementation fake impl operator Also sometimes known meta kernel abstract impl An FakeTensor implementation specifies behavior operator Tensors carry no data FakeTensor Given some input Tensors certain properties sizes strides storage_offset device specifies what properties output Tensors The FakeTensor implementation has same signature operator It run both FakeTensors meta tensors To write FakeTensor implementation assume all Tensor inputs operator regular CPU CUDA Meta tensors they do have storage you trying regular CPU CUDA Meta tensor s output The FakeTensor implementation must consist only PyTorch operations may directly access storage data any input intermediate Tensors This API may used decorator see examples For detailed guide custom ops please see https pytorch org tutorials advanced custom_ops_landing_page html Args op_name Operator name along overload OpOverload object func Fake tensor implementation lib Optional Library Library register fake tensor allow_override Flag controlling we want override existing registered fake impl This default off will error you re trying register fake impl operator already has fake impl This also only applies custom operator created via torch library custom_op overriding existing fake impl already allowed Examples torch numpy np torch Tensor Example operator without data-dependent output shape torch library custom_op mylib custom_linear mutates_args= custom_linear x Tensor weight Tensor bias Tensor - Tensor raise NotImplementedError Implementation goes here torch library register_fake mylib custom_linear _ x weight bias assert x dim == assert weight dim == assert bias dim == assert x shape == weight shape assert weight shape == bias shape assert x device == weight device x weight t + bias torch _subclasses fake_tensor FakeTensorMode x = torch randn w = torch randn b = torch randn y = torch ops mylib custom_linear x w b assert y shape == Example operator data-dependent output shape torch library custom_op mylib custom_nonzero mutates_args= custom_nonzero x Tensor - Tensor x_np = x numpy force=True res = np stack np nonzero x_np axis= torch tensor res device=x device torch library register_fake mylib custom_nonzero _ x Number nonzero-elements data-dependent Since we cannot peek data fake impl we use ctx object construct new symint represents data-dependent size ctx = torch library get_ctx nnz = ctx new_dynamic_size shape = nnz x dim result = x new_empty shape dtype=torch int result torch fx experimental proxy_tensor make_fx x = torch tensor trace = make_fx torch ops mylib custom_nonzero tracing_mode= symbolic x trace print_readable assert torch allclose trace x torch ops mylib custom_nonzero x isinstance op str torch _ops OpOverload torch _library custom_ops CustomOpDef raise ValueError f register_fake op got unexpected type op type op isinstance op torch _ops OpOverload op = op _name opdef = _maybe_get_opdef op opdef None func None opdef register_fake opdef register_fake func assert isinstance op str stacklevel = _stacklevel register func namespace op_name = torch _library utils parse_namespace op lib None use_lib = Library namespace FRAGMENT _keep_alive append use_lib use_lib = lib use_lib _register_fake op_name func _stacklevel=stacklevel + allow_override=allow_override func func None register stacklevel += register func register_autograd op _op_identifier backward Callable setup_context Optional Callable = None lib=None - None r Register backward formula custom op In order operator work autograd you need register backward formula You must tell us how compute gradients during backward pass providing us backward function If you need any values forward compute gradients you can use ` setup_context ` save values backward ` ` backward ` ` runs during backward pass It accepts ` ` ctx grads ` ` - ` ` grads ` ` one more gradients The number gradients matches number outputs operator The ` ` ctx ` ` object ` same ctx object context_method_mixins ` _ used ` torch autograd Function ` The semantics ` ` backward_fn ` ` same meth ` torch autograd Function backward ` ` ` setup_context ctx inputs output ` ` runs during forward pass Please save quantities needed backward onto ` ` ctx ` ` object via either meth ` torch autograd function FunctionCtx save_for_backward ` assigning them attributes ` ` ctx ` ` If your custom op has kwarg-only arguments we expect signature ` ` setup_context ` ` ` ` setup_context ctx inputs keyword_only_inputs output ` ` Both ` ` setup_context_fn ` ` ` ` backward_fn ` ` must traceable That they may directly access meth ` torch Tensor data_ptr ` they must depend mutate global state If you need non-traceable backward you can make separate custom_op you call inside ` ` backward_fn ` ` If you need different autograd behavior different devices then we recommend creating two different custom operators one each device needs different behavior switching between them runtime Examples torch numpy np torch Tensor torch library custom_op mylib numpy_sin mutates_args= numpy_sin x Tensor - Tensor x_np = x cpu numpy y_np = np sin x_np torch from_numpy y_np device=x device setup_context ctx inputs output - Tensor x = inputs ctx save_for_backward x backward ctx grad x = ctx saved_tensors grad x cos torch library register_autograd mylib numpy_sin backward setup_context=setup_context x = torch randn requires_grad=True y = numpy_sin x grad_x = torch autograd grad y x torch ones_like y assert torch allclose grad_x x cos Example keyword-only arg torch library custom_op mylib numpy_mul mutates_args= numpy_mul x Tensor val float - Tensor x_np = x cpu numpy y_np = x_np val torch from_numpy y_np device=x device setup_context ctx inputs keyword_only_inputs output - Tensor ctx val = keyword_only_inputs val backward ctx grad grad ctx val torch library register_autograd mylib numpy_mul backward setup_context=setup_context x = torch randn requires_grad=True y = numpy_mul x val= grad_x = torch autograd grad y x torch ones_like y assert torch allclose grad_x torch full_like x isinstance op str torch _ops OpOverload torch _library custom_ops CustomOpDef raise ValueError f register_autograd op got unexpected type op type op isinstance op torch _ops OpOverload op = op _name opdef = _maybe_get_opdef op opdef None opdef register_autograd backward setup_context=setup_context assert isinstance op str qualname = op op = torch _library utils lookup_op qualname schema = op _schema _library utils is_functional_schema schema raise RuntimeError f Cannot register autograd formula non-functional operator f op schema schema Please create f functional operator register autograd formula _library utils has_kwarg_only_tensors schema raise NotImplementedError f register_autograd kwarg-only Tensor args In original f definition op please make your tensors kwarg-only f Got schema info = _library autograd Info backward setup_context autograd_kernel = _library autograd make_autograd_impl op info namespace opname = torch _library utils parse_namespace qualname lib None lib = Library namespace FRAGMENT _keep_alive append lib lib impl opname autograd_kernel Autograd with_keyset=True register_torch_dispatch op _op_identifier torch_dispatch_class Any func Optional Callable = None lib Optional Library = None r Registers torch_dispatch rule given operator ` ` torch_dispatch_class ` ` This allows open registration specify behavior between operator ` ` torch_dispatch_class ` ` without needing modify ` ` torch_dispatch_class ` ` operator directly The ` ` torch_dispatch_class ` ` either Tensor subclass ` ` __torch_dispatch__ ` ` TorchDispatchMode If Tensor subclass we expect ` ` func ` ` have following signature ` ` cls func OpOverload types Tuple type args kwargs - Any ` ` If TorchDispatchMode we expect ` ` func ` ` have following signature ` ` mode func OpOverload types Tuple type args kwargs - Any ` ` ` ` args ` ` ` ` kwargs ` ` will have been normalized same way they ` ` __torch_dispatch__ ` ` see ref ` torch-dispatch-calling-convention ` Examples torch torch library custom_op mylib foo mutates_args= foo x torch Tensor - torch Tensor x clone MyMode torch utils _python_dispatch TorchDispatchMode __torch_dispatch__ func types args= kwargs=None func args kwargs torch library register_torch_dispatch mylib foo MyMode _ mode func types args kwargs x = args x + x = torch randn y = foo x assert torch allclose y x MyMode y = foo x assert torch allclose y x + isinstance op str torch _ops OpOverload torch _library custom_ops CustomOpDef raise ValueError f register_torch_dispatch op got unexpected type op type op isinstance op torch _ops OpOverload op = op _name opdef = _maybe_get_opdef op opdef None opdef register_torch_dispatch torch_dispatch_class func assert isinstance op str register func namespace op_name = torch _library utils parse_namespace op lib None use_lib = Library namespace FRAGMENT _keep_alive append use_lib use_lib = lib use_lib _register_torch_dispatch_rule op_name torch_dispatch_class func func func None register register func register_vmap op _op_identifier func Optional Callable = None lib=None r Register vmap implementation support func ` torch vmap ` custom op This API may used decorator see examples In order operator work func ` torch vmap ` you may need register vmap implementation following signature ` ` vmap_func info in_dims Tuple Optional int args kwargs ` ` where ` ` args ` ` ` ` kwargs ` ` arguments kwargs ` ` op ` ` We do support kwarg-only Tensor args It specifies how do we compute batched version ` ` op ` ` given inputs additional dimension specified ` ` in_dims ` ` For each arg ` ` args ` ` ` ` in_dims ` ` has corresponding ` ` Optional int ` ` It ` ` None ` ` arg Tensor arg being vmapped over otherwise integer specifying what dimension Tensor being vmapped over ` ` info ` ` collection additional metadata may helpful ` ` info batch_size ` ` specifies size dimension being vmapped over while ` ` info randomness ` ` ` ` randomness ` ` option passed func ` torch vmap ` The function ` ` func ` ` tuple ` ` output out_dims ` ` Similar ` ` in_dims ` ` ` ` out_dims ` ` should same structure ` ` output ` ` contain one ` ` out_dim ` ` per output specifies output has vmapped dimension what index Examples torch numpy np torch Tensor typing Tuple to_numpy tensor tensor cpu numpy lib = torch library Library mylib FRAGMENT torch library custom_op mylib numpy_cube mutates_args= numpy_cube x Tensor - Tuple Tensor Tensor x_np = to_numpy x dx = torch tensor x_np device=x device torch tensor x_np device=x device dx numpy_cube_vmap info in_dims x result = numpy_cube x result in_dims in_dims torch library register_vmap numpy_cube numpy_cube_vmap x = torch randn torch vmap numpy_cube x torch library custom_op mylib numpy_mul mutates_args= numpy_mul x Tensor y Tensor - Tensor torch tensor to_numpy x to_numpy y device=x device torch library register_vmap mylib numpy_mul numpy_mul_vmap info in_dims x y x_bdim y_bdim = in_dims x = x movedim x_bdim - x_bdim None x unsqueeze - y = y movedim y_bdim - y_bdim None y unsqueeze - result = x y result = result movedim - result x = torch randn y = torch randn torch vmap numpy_mul x y note The vmap function should aim preserve semantics entire custom operator That ` ` grad vmap op ` ` should replaceable ` ` grad map op ` ` If your custom operator has any custom behavior backward pass please keep mind isinstance op str torch _ops OpOverload torch _library custom_ops CustomOpDef raise ValueError f register_vmap op got unexpected type op type op isinstance op torch _ops OpOverload op = op _name opdef = _maybe_get_opdef op opdef None opdef register_vmap func assert isinstance op str qualname = op op = torch _library utils lookup_op qualname schema = op _schema _library utils has_kwarg_only_tensors schema raise NotImplementedError f register_vmap kwarg-only Tensor args In original f definition op please make your tensors kwarg-only f Got schema register func nonlocal op lib namespace opname = torch _library utils parse_namespace qualname lib None lib = Library namespace FRAGMENT _keep_alive append lib torch _functorch autograd_function custom_function_call_vmap_helper torch _functorch pyfunctorch retrieve_current_functorch_interpreter wrapped_func keyset args kwargs interpreter = retrieve_current_functorch_interpreter custom_function_call_vmap_helper interpreter func op args kwargs lib impl opname wrapped_func FuncTorchBatched with_keyset=True func None register register func If op defined C++ then we want make sure there m set_python_module module call module same module called torch library register_fake _check_pystubs_once func qualname actual_module_name checked = False inner args kwargs nonlocal checked checked func args kwargs op = torch _library utils lookup_op qualname op _defined_in_python checked = True func args kwargs maybe_pystub = torch _C _dispatch_pystub op _schema name op _schema overload_name maybe_pystub None torch _library utils requires_set_python_module namespace = op namespace cpp_filename = op _handle debug raise RuntimeError f Operator qualname defined C++ has Python f fake impl In situation we require there also f companion C++ ` m set_python_module actual_module_name ` f call we could find one Please add f top C++ TORCH_LIBRARY namespace block f operator registered cpp_filename pystub_module = maybe_pystub actual_module_name = pystub_module cpp_filename = op _handle debug raise RuntimeError f Operator qualname specified its python fake impl f Python module pystub_module actually found f actual_module_name Please either move fake impl f correct m set_python_module call cpp_filename checked = True func args kwargs inner NOTE ctx inside fake implementation If user has operator data-dependent output shape then when writing fake implementation they must query current ctx use methods ctx construct new unbacked symint This done via us setting global_ctx_getter function every time fake implementation invoked get_ctx - torch _library fake_impl FakeImplCtx get_ctx returns current AbstractImplCtx object Calling ` ` get_ctx ` ` only valid inside fake impl see func ` torch library register_fake ` more usage details torch _library fake_impl global_ctx_getter get_kernel op _op_identifier dispatch_key Union str torch DispatchKey - torch _C _SafeKernelFunction Returns computed kernel given operator dispatch key This function retrieves kernel would executed given operator dispatch key combination The returned SafeKernelFunction can used call kernel boxed fashion The intended use case function retrieve original kernel given dispatch key then register another kernel same dispatch key calls into original kernel certain cases Args op Operator name along overload OpOverload object Can string e g aten add Tensor OpOverload CustomOpDef dispatch_key str &#124; torch DispatchKey The dispatch key get kernel Can string e g CPU CUDA DispatchKey enum value Returns torch _C _SafeKernelFunction A safe kernel function can used call kernel Raises RuntimeError If operator does exist Example Get CPU kernel torch add kernel = torch library get_kernel aten add Tensor CPU You can also use DispatchKey enum kernel = torch library get_kernel aten add Tensor torch DispatchKey CPU Or use OpOverload directly kernel = torch library get_kernel torch ops aten add Tensor CPU Example Using get_kernel custom op conditional dispatch Get original kernel torch sin original_sin_kernel = torch library get_kernel aten sin CPU If input has negative values use original sin otherwise zeros conditional_sin_impl dispatch_keys x x any original_sin_kernel call_boxed dispatch_keys x torch zeros_like x lib = torch library Library aten IMPL with_keyset=True so first argument impl current DispatchKeySet which needs first argument ` ` kernel call_boxed ` ` lib impl sin conditional_sin_impl CPU with_keyset=True Test conditional behavior x_positive = torch tensor x_mixed = torch tensor - torch sin x_positive tensor torch sin x_mixed tensor - isinstance op str torch _ops OpOverload raise ValueError f get_kernel op got unexpected type op type op isinstance op torch _ops OpOverload op = op _name isinstance dispatch_key str try dispatch_key = torch _C DispatchKey __members__ dispatch_key except KeyError raise ValueError f Invalid dispatch key dispatch_key None torch _C _dispatch_get_computed_kernel_for_dispatch_key op dispatch_key _OPCHECK_DEFAULT_UTILS = test_schema test_autograd_registration test_faketensor test_aot_dispatch_dynamic opcheck op Union torch _ops OpOverload torch _ops OpOverloadPacket CustomOpDef args tuple Any kwargs Optional dict str Any = None test_utils Union str Sequence str = _OPCHECK_DEFAULT_UTILS raise_exception bool = True atol=None rtol=None - dict str str Given operator some sample arguments tests operator registered correctly That when you use torch library TORCH_LIBRARY APIs create custom op you specified metadata e g mutability info about custom op these APIs require functions you pass them satisfy certain properties e g no data pointer access fake meta abstract kernel ` ` opcheck ` ` tests these metadata properties Concretely we test following - test_schema If schema matches implementation operator For example schema specifies Tensor mutated then we check implementation mutates Tensor If schema specifies we new Tensor then we check implementation returns new Tensor instead existing one view existing one - test_autograd_registration If operator supports training autograd we check its autograd formula registered via torch library register_autograd manual registration one more DispatchKey Autograd keys Any other DispatchKey-based registrations may lead undefined behavior - test_faketensor If operator has FakeTensor kernel correct The FakeTensor kernel necessary sufficient operator work PyTorch compilation APIs torch compile export FX We check FakeTensor kernel also sometimes known meta kernel registered operator correct This test takes result running operator real tensors result running operator FakeTensors checks they have same Tensor metadata sizes strides dtype device etc - test_aot_dispatch_dynamic If operator has correct behavior PyTorch compilation APIs torch compile export FX This checks outputs gradients applicable same under eager-mode PyTorch torch compile This test superset ` ` test_faketensor ` ` e e test other things tests operator supports functionalization backward pass exists also supports FakeTensor functionalization For best results please call ` ` opcheck ` ` multiple times representative set inputs If your operator supports autograd please use ` ` opcheck ` ` inputs ` ` requires_grad = True ` ` your operator supports multiple devices e g CPU CUDA please use ` ` opcheck ` ` inputs all supported devices Args op The operator Must either function decorated func ` torch library custom_op ` OpOverload OpOverloadPacket found torch ops e g torch ops aten sin torch ops mylib foo args The args operator kwargs The kwargs operator test_utils Tests we should run Default all them Example test_schema test_faketensor raise_exception If we should raise exception first error If False we will dict information each test passed rtol Optional float Relative tolerance floating point comparisons If specified ` ` atol ` ` must also specified If omitted default values based ` ` dtype ` ` selected see table func ` torch testing assert_close ` atol Optional float Absolute tolerance floating point comparisons If specified ` ` rtol ` ` must also specified If omitted default values based ` ` dtype ` ` selected see table func ` torch testing assert_close ` warning opcheck func ` torch autograd gradcheck ` test different things opcheck tests your usage torch library APIs correct while func ` torch autograd gradcheck ` tests your autograd formula mathematically correct Use both test custom ops support gradient computation Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA torch library custom_op mylib numpy_mul mutates_args= numpy_mul x Tensor y float - Tensor x_np = x numpy force=True z_np = x_np y torch from_numpy z_np x device numpy_mul register_fake _ x y torch empty_like x setup_context ctx inputs output y = inputs ctx y = y backward ctx grad grad ctx y None numpy_mul register_autograd backward setup_context=setup_context sample_inputs = torch randn torch randn device= cuda torch randn requires_grad=True torch randn device= cuda requires_grad=True args sample_inputs torch library opcheck numpy_mul args torch testing _internal optests optests optests opcheck op args kwargs test_utils=test_utils raise_exception=raise_exception rtol=rtol atol=atol