Copyright c Meta Platforms Inc affiliates Owner s oncall distributed itertools torch torch distributed tensor distribute_tensor DTensor Replicate Shard torch distributed tensor _dtensor_spec DTensorSpec torch distributed tensor experimental register_sharding torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase with_comms aten = torch ops aten TestRegisterSharding DTensorTestBase with_comms test_softmax_fwd After registering custom softmax sharding strategy original entry would have been replaced The following line showcasing purpose only DTensor _op_dispatcher sharding_propagator op_strategy_funcs pop aten _softmax default None register_sharding aten _softmax default custom_softmax_sharding x DTensorSpec dim int half_to_float torch dtype softmax_dim = dim dim = dim + x ndim acceptable_shardings = all_replicate = Replicate Replicate None None acceptable_shardings append all_replicate sharding_dim range x ndim sharding_dim = softmax_dim all_sharded = Shard sharding_dim Shard sharding_dim None None acceptable_shardings append all_sharded acceptable_shardings check RuntimeSchemaInfo derived correctly schema_info = DTensor _op_dispatcher sharding_propagator op_to_schema_info aten _softmax default assertEqual schema_info static_argnum device_mesh = build_device_mesh x = torch rand device=self device_type dims = range used convert - actual dim softmax_dims = - shard_dims = test_list = list itertools product softmax_dims shard_dims softmax_dim shard_dim test_list local_y = torch nn functional softmax x dim=softmax_dim dtype=torch float dist_x = distribute_tensor x device_mesh Shard shard_dim dist_y = torch nn functional softmax dist_x dim=softmax_dim dtype=torch float dims shard_dim == dims softmax_dim assertTrue dist_y placements is_replicate assertEqual dist_y to_local local_y assertTrue dist_y placements is_shard dim=shard_dim assertEqual dist_y full_tensor local_y with_comms test_argmax register_sharding aten argmax default custom_argmax_sharding x dim keepdim acceptable_shardings = all_replicate = Replicate Replicate None None acceptable_shardings append all_replicate keepdim sharding_dim range x ndim sharding_dim = dim all_sharded = Shard sharding_dim Shard sharding_dim None None acceptable_shardings append all_sharded acceptable_shardings check RuntimeSchemaInfo derived correctly when first int arg optional schema_info = DTensor _op_dispatcher sharding_propagator op_to_schema_info aten argmax default assertEqual schema_info static_argnum device_mesh = build_device_mesh x = torch rand device=self device_type dist_x = distribute_tensor x device_mesh Shard local_y = torch argmax x dim= keepdim=True dist_y = torch argmax dist_x dim= keepdim=True assertTrue dist_y placements is_shard dim= assertEqual dist_y full_tensor local_y __name__ == __main__ run_tests