Copyright c Meta Platforms Inc affiliates Owner s oncall distributed functools logging statistics time collections namedtuple torch torch distributed device_mesh init_device_mesh torch distributed tensor distribute_tensor DTensor Shard torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase with_comms torch utils _python_dispatch TorchDispatchMode logger = logging getLogger __name__ logging basicConfig level=logging INFO TimeCaptureMode TorchDispatchMode __init__ repeat_count= repeat each op call ` repeat_count ` times repeat_count = repeat_count recorded time scaled micro seconds time_list = op_to_time = __torch_dispatch__ func types args= kwargs=None time_list clear functools wraps func repeated_func args kwargs result = None _ range repeat_count start_time = time perf_counter result = func args kwargs end_time = time perf_counter elapsed_time = end_time - start_time time_list append elapsed_time result res = repeated_func args kwargs Timing = namedtuple Timing dispatch_with_cache_miss dispatch_with_cache_hit func __name__ op_to_time op_to_time func __name__ = op_to_time func __name__ append Timing round time_list e round statistics median time_list e res DistOpDispatchOverHead DTensorTestBase property world_size - int with_comms test_dtensor_add_op_dispatch_overhead torch cuda is_available device_props = torch cuda get_device_name gpu_name = device_props logger info running s gpu_name TODO adjust ` expected_propagate_time ` ` expected_dispatch_time ` target different hardware skipTest CUDA available expected_propagate_time = noqa F expected_dispatch_time = noqa F diff_percent_threshold = noqa F propagator = DTensor _op_dispatcher sharding_propagator device_mesh = init_device_mesh cuda world_size input_data = torch rand device= cuda = distribute_tensor input_data device_mesh Shard warm up TimeCaptureMode tcm _ range propagator propagate_op_sharding cache cache_clear _ = + record number propagator propagate_op_sharding cache cache_clear _ = + add_dispatch_cache_miss add_dispatch_cache_hit = tcm op_to_time add Tensor - all_miss_performance = world_size all_hit_performance = world_size torch distributed all_gather_object all_miss_performance add_dispatch_cache_miss torch distributed all_gather_object all_hit_performance add_dispatch_cache_hit rank == logger info add op dispatch cache miss s ranks s us \n add op dispatch cache hit s ranks s us world_size all_miss_performance world_size all_hit_performance compare median expected range miss_performance = statistics median all_miss_performance hit_performance = statistics median all_hit_performance extra_time_spend_on_strategy_propagate = miss_performance - hit_performance noqa F Do enabling assertion check due flaky performance concern assertTrue extra_time_spend_on_strategy_propagate - expected_propagate_time expected_propagate_time diff_percent_threshold msg= f extra time spend strategy propagate extra_time_spend_on_strategy_propagate us f performance diff diff_percent_threshold greater than expected expected_propagate_time us assertTrue hit_performance - expected_dispatch_time expected_dispatch_time diff_percent_threshold msg= f DTensor dispatch time hit_performance us f performance diff diff_percent_threshold greater than f expected expected_dispatch_time us __name__ == __main__ run_tests