Owner s module inductor functools sys unittest unittest mock patch torch torch _dynamo config dynamo_config torch _inductor config inductor_config torch _inductor select_algorithm select_algorithm torch _dynamo utils counters torch _inductor test_case run_tests torch testing _internal common_device_type dtypes instantiate_device_type_tests torch testing _internal common_quantized _calculate_dynamic_per_channel_qparams torch testing _internal common_utils parametrize TEST_CUDA TEST_WITH_SLOW_GRADCHECK try try test_cpu_select_algorithm test_torchinductor except ImportError test_cpu_select_algorithm manual=fbcode caffe test inductor test_cpu_select_algorithm-library test_torchinductor manual=fbcode caffe test inductor test_inductor-library except unittest SkipTest __name__ == __main__ sys exit raise check_model = test_torchinductor check_model BaseTestSelectAlgorithm = test_cpu_select_algorithm BaseTestSelectAlgorithm patches fn skip_cache choices name key benchmark hint_override=None benchmark None timings = benchmark choices choice timing timings items isinstance choice select_algorithm ExternKernelCaller timings choice = timing timings patcher dynamo_config patch verbose=True dynamo_config patch inline_inbuilt_nn_modules=True inductor_config patch debug=True max_autotune=True epilogue_fusion=True patch object select_algorithm VERIFY dict atol= e- rtol= e- patch object select_algorithm AlgorithmSelectorCache lookup skip_cache fn = patcher fn functools wraps fn wrapped args kwargs counters clear torch manual_seed fn args kwargs wrapped TestSelectAlgorithmCuda BaseTestSelectAlgorithm common = check_model inductor_config patch freezing True patches torch no_grad dtypes torch bfloat parametrize batch_size parametrize mid_dim parametrize in_features parametrize out_features unittest skipIf TEST_CUDA CUDA available unittest skipIf TEST_WITH_SLOW_GRADCHECK Leaking memory test_int _woq_mm_cuda dtype batch_size mid_dim in_features out_features _convert_weight_to_int pack w Move CPU quantization calculation then back original device device = w device w_cpu = w cpu scale zp = _calculate_dynamic_per_channel_qparams w_cpu torch float torch int scale = torch from_numpy scale device zp = torch from_numpy zp device w_int = torch ao quantization fx _decomposed quantize_per_channel input=w scales=scale zero_points=zp axis= quant_min=- quant_max= dtype=torch int w_int scale torch bfloat M torch nn Module __init__ w super __init__ linear_weight = torch nn Parameter w requires_grad=False forward x scale torch nn functional linear x linear_weight x dtype scale counters clear Currently corresponding torch fx pattern only supports D x Add D X case once corresponding pattern-matcher pattern added x = torch rand batch_size mid_dim in_features dtype=dtype device= cuda w = torch rand out_features in_features dtype=dtype device= cuda w_int pack w_scales = _convert_weight_to_int pack w w_scales = w_scales cuda mod = M w_int pack eval common mod x w_scales assertEqual counters inductor woq_matcher_count inductor_config patch freezing True cpp enable_concat_linear True patches torch no_grad dtypes torch bfloat parametrize batch_size parametrize mid_dim parametrize in_features parametrize out_features unittest skipIf TEST_CUDA CUDA available unittest skipIf TEST_WITH_SLOW_GRADCHECK Leaking memory test_int _woq_mm_concat_cuda dtype batch_size mid_dim in_features out_features _convert_weight_to_int pack w Move CPU quantization calculation then back original device device = w device w_cpu = w cpu scale zp = _calculate_dynamic_per_channel_qparams w_cpu torch float torch int scale = torch from_numpy scale device zp = torch from_numpy zp device w_int = torch ao quantization fx _decomposed quantize_per_channel input=w scales=scale zero_points=zp axis= quant_min=- quant_max= dtype=torch int w_int scale torch bfloat M torch nn Module __init__ w w w super __init__ w = torch nn Parameter w requires_grad=False w = torch nn Parameter w requires_grad=False w = torch nn Parameter w requires_grad=False forward x scale scale scale Ref _linear_fp_act_int _weight_impl torchao dtypes uintx plain_layout py y = torch mm x reshape - x shape - w t x dtype scale y = torch mm x reshape - x shape - w t x dtype scale y = torch mm x reshape - x shape - w t x dtype scale y reshape x shape - y shape - y reshape x shape - y shape - y reshape x shape - y shape - counters clear Currently corresponding torch fx pattern only supports D x Add D X case once corresponding pattern-matcher pattern added x = torch rand batch_size mid_dim in_features dtype=dtype device= cuda w = torch rand out_features in_features dtype=dtype device= cuda w = torch rand out_features in_features dtype=dtype device= cuda w = torch rand out_features in_features dtype=dtype device= cuda w _int pack w _scales = _convert_weight_to_int pack w w _int pack w _scales = _convert_weight_to_int pack w w _int pack w _scales = _convert_weight_to_int pack w mod = M w _int pack w _int pack w _int pack eval common mod x w _scales w _scales w _scales assertEqual counters inductor woq_matcher_count instantiate_device_type_tests TestSelectAlgorithmCuda globals only_for= cuda __name__ == __main__ run_tests