mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates logging operator collections abc Sequence typing Any Optional torch torch fx node map_aggregate torch nn attention flex_attention BlockMask torch utils _pytree tree_flatten tree_map tree_unflatten __all__ = TensorChunkSpec split_args_kwargs_into_chunks merge_chunks logger = logging getLogger __name__ _debug_mask_minibatches specifies send masked versions mini-batch through instead micro-batch slices -- can used more stable numerical testing see A Note About Correctness Testing _debug_mask_minibatches = False _CustomReducer Custom reducer can used specify custom operation reduces losses multiple microbatches into one value Example xdoctest +SKIP sum_reducer = _CustomReducer torch tensor lambda b + b __init__ init_value reduce_fn init_value = init_value reduce_fn = reduce_fn _LossReducer _CustomReducer pass sum_reducer = _LossReducer torch tensor operator add Default chunking dimension This used case where user did specify chunking dimension DEFAULT_CHUNK_DIM = TensorChunkSpec Class used specify chunking inputs __init__ split_dim split_dim = split_dim split_dim int __repr__ f __class__ __module__ __class__ __name__ split_dim __str__ f TensorChunkSpec split_dim staticmethod from_tuple chunk_dims tuple int A helper creating tuple ` TensorChunkSpec ` tuple chunk dimensions int s Example xdoctest +SKIP There three positional arguments model we chunking them along dimension respectively args_chunk_spec = TensorChunkSpec from_tuple args_chunk_spec = map_aggregate chunk_dims lambda dim TensorChunkSpec dim type ignore arg-type return-value args_chunk_spec staticmethod from_dict chunk_dims dict str int A helper creating dictionary ` TensorChunkSpec ` dictionary chunk dimensions int s Example xdoctest +SKIP Chunk dimension id argument mask argument kwargs_chunk_spec = TensorChunkSpec from_dict id mask kwargs_chunk_spec = map_aggregate chunk_dims lambda dim TensorChunkSpec dim type ignore arg-type return-value kwargs_chunk_spec Class used specify replication inputs _Replicate pass _split_block_mask block_mask BlockMask num_chunks int - list BlockMask Given block mask split block mask along batch dimension dim Args block_mask Block mask split num_chunks Number chunks split block mask into Returns chunk_block_masks List chunked block masks BlockMask will broadcast B block_mask kv_num_blocks size == block_mask num_chunks assert block_mask kv_num_blocks size = num_chunks Block mask has fewer batch size than number chunks batch_dim = kv_num_blocks_chunks = torch tensor_split block_mask kv_num_blocks num_chunks batch_dim kv_indices_chunks = torch tensor_split block_mask kv_indices num_chunks batch_dim full_kv_num_blocks_chunks = torch tensor_split block_mask full_kv_num_blocks num_chunks batch_dim block_mask full_kv_num_blocks None None num_chunks full_kv_indices_chunks = torch tensor_split block_mask full_kv_indices num_chunks batch_dim block_mask full_kv_indices None None num_chunks chunk_block_masks = batch_offset = chunk_idx range num_chunks create_mask_mod idx batch_offset_mask_mod b h q_idx kv_idx b_offset = torch full_like b idx block_mask mask_mod b + b_offset h q_idx kv_idx batch_offset_mask_mod chunk_block_masks append BlockMask from_kv_blocks kv_num_blocks=kv_num_blocks_chunks chunk_idx kv_indices=kv_indices_chunks chunk_idx full_kv_num_blocks=full_kv_num_blocks_chunks chunk_idx full_kv_indices=full_kv_indices_chunks chunk_idx BLOCK_SIZE=block_mask BLOCK_SIZE mask_mod=create_mask_mod batch_offset seq_lengths=block_mask seq_lengths batch_offset += kv_num_blocks_chunks chunk_idx size chunk_block_masks _split_tensor tensor torch Tensor spec TensorChunkSpec num_chunks int - Sequence torch Tensor Given tensor chunking spec split tensor Args tensor Tensor split spec Chunking spec num_chunks Number chunks split tensor into Returns chunk_tensors List chunked tensors assert tensor size spec split_dim = num_chunks f Tensor size tensor size spec split_dim smaller than num_chunks chunk_tensors = torch tensor_split tensor num_chunks spec split_dim _debug_mask_minibatches chunk_tensors expanded_chunks = split_dim_idx = chunk_tensor chunk_tensors new_val = torch zeros_like tensor upper_idx = split_dim_idx + chunk_tensor size spec split_dim slice_indices = slice None None None new_val ndim slice_indices spec split_dim = slice split_dim_idx upper_idx new_val slice_indices = chunk_tensor expanded_chunks append new_val split_dim_idx += chunk_tensor size spec split_dim expanded_chunks _shard_dict_of_args args_dict args_chunk_spec num_chunks Given dictionary args dictionary chunking specs shard args according chunking specs Args args_dict Dictionary args args_chunk_spec Dictionary chunking specs num_chunks Number chunks shard args into Returns args_split List sharded args args_dict _ range num_chunks assert len args_dict == len args_chunk_spec f args_dict keys = list args_dict keys f args_chunk_spec keys = list args_chunk_spec keys assert args_chunk_spec None Should have been set caller values tree_spec = tree_flatten args_dict is_leaf=lambda x isinstance x BlockMask chunk_specs _ = tree_flatten args_chunk_spec is_leaf=lambda x isinstance x BlockMask First check find actual number chunks split_sizes = v spec zip values chunk_specs strict=True The original logic spec _Replicate This doesn t seem correct But we keep backward compatibility spec _Replicate isinstance spec _Replicate split_sizes append num_chunks isinstance v torch Tensor assert isinstance spec TensorChunkSpec split_sizes append v size spec split_dim isinstance v BlockMask assert isinstance spec TensorChunkSpec assert spec split_dim == BlockMask only supports split_dim= BlockMask will broadcast B v kv_num_blocks size == split_sizes append num_chunks split_sizes append v kv_num_blocks size raise ValueError f Unsupported chunk spec spec value v combination result_num_chunks = min split_sizes num_chunks flat_split_results list Any = _ range result_num_chunks v spec zip values chunk_specs strict=True v_splits Sequence Any = spec _Replicate isinstance spec _Replicate v_splits = v result_num_chunks isinstance v torch Tensor v_splits = _split_tensor v spec result_num_chunks isinstance v BlockMask v_splits = _split_block_mask v result_num_chunks raise ValueError f Unsupported chunk spec spec value v combination _flat_split_result _v_split zip flat_split_results v_splits strict=True _flat_split_result append _v_split tree_unflatten _flat_split_result tree_spec _flat_split_result flat_split_results split_args_kwargs_into_chunks args tuple Any kwargs Optional dict str Any chunks int args_chunk_spec Optional tuple TensorChunkSpec = None kwargs_chunk_spec Optional dict str TensorChunkSpec = None - tuple list tuple list dict Given sequence args kwargs split them into number chunks according their respective chunking specs Args args Tuple args kwargs Dict kwargs chunks Number chunks split args kwargs into args_chunk_spec chunking specs args same shape args kwargs_chunk_spec chunking specs kwargs same shape kwargs Returns args_split List sharded args kwargs_split List sharded kwargs Given ` args ` ` kwargs ` we want yield set ` chunks ` args kwargs such constituent Tensor values have been sharded replicated according ` args_chunk_spec ` ` kwargs_chunk_spec ` specifications The steps follows Use pytree tree_flatten flatten each arg its spec into nto d array values To use running example suppose our inputs look like args = A B C D args_spec = None None TensorChunkSpec None kwargs shown s similar process Then step we would end up args = A B C D args_spec = None None TensorChunkSpec None Shard replicate arguments subject policy spec Suppose chunks = args = A A B B C_ C_ D D Rotate nesting order such chunks outer dimension args_chunks = A B C_ D A B C_ D Unflatten each chunk according spec args_chunks = A B C_ D A B C_ D TODO _debug_mask_minibatches Handle case where kwargs None kwargs None kwargs = If user did provide args_chunk_spec kwargs_chunk_spec we extend their format use default chunking along dim default_spec v isinstance v torch Tensor &#124; BlockMask TensorChunkSpec DEFAULT_CHUNK_DIM _Replicate args_chunk_spec None args_chunk_spec = tree_map default_spec args is_leaf=lambda v isinstance v BlockMask kwargs_chunk_spec None kwargs_chunk_spec = tree_map default_spec kwargs is_leaf=lambda v isinstance v BlockMask args_split_dict = _shard_dict_of_args dict enumerate args dict enumerate args_chunk_spec chunks real_num_chunks = len args_split_dict kwargs_split = _shard_dict_of_args kwargs kwargs_chunk_spec real_num_chunks len kwargs_split real_num_chunks In case kwargs sharded into less chunks e g when ` args ` has no tensor just values real_num_chunks = len kwargs_split Re-shard args args_split_dict = _shard_dict_of_args dict enumerate args dict enumerate args_chunk_spec real_num_chunks len args_split_dict = len kwargs_split raise RuntimeError args kwargs split into different number chunks f len args_split_dict len kwargs_split args_split = tuple chunk_args i i range len chunk_args chunk_args args_split_dict args_split kwargs_split merge_chunks chunks list Any chunk_spec Given list chunks merge them into single value according chunk spec Args chunks list chunks chunk_spec Chunking spec chunks Returns value Merged value This essentially inverse ` split_args_kwargs_into_chunks ` so steps similar steps function reverse Given input values chunks = A B C_ D A B C_ D args_spec = None None TensorChunkSpec None Flatten chunks according chunk_spec chunks_flat = A B C_ D A B C_ D Rotate nesting order such chunks inner dimension value_inner = A B C_ C_ D Concatenate sharded arguments value_combined = A B C D Unflatten combined args given spec value = A B C D Preliminary flatten chunk spec chunk_spec None spec_flattened flatten_spec = tree_flatten chunk_spec If chunk_spec provided we will merge chunks along default dimension all output fields We obtain output structure flattening chunk generate chunk_spec chunk _flat flatten_spec = tree_flatten chunks spec_flattened = TensorChunkSpec DEFAULT_CHUNK_DIM len chunk _flat Stage flatten chunks chunks_flattened num chunks num args chunks_flattened = chunk chunks chunk_flattened _ = tree_flatten chunk len chunk_flattened = len spec_flattened raise ValueError f Chunk chunk did match chunk spec chunk_spec chunks_flattened append chunk_flattened Stage Rotate nesting order s t chunks inner dimension concatenate sharded operands args_flattened num args args_flattened = arg_idx arg enumerate spec_flattened isinstance arg TensorChunkSpec partial_values = chunks_flattened chunk_idx arg_idx chunk_idx range len chunks_flattened _debug_mask_minibatches Infer size individual chunks running ` tensor_split ` again overall_shape = partial_values shape val partial_values assert val shape == overall_shape meta_chunks = torch tensor_split torch empty overall_shape device= meta sections=len partial_values dim=arg split_dim values_to_cat = chunk_start_idx = assert len partial_values == len meta_chunks partial_value meta_chunk zip partial_values meta_chunks strict=True chunk_end_idx = chunk_start_idx + meta_chunk size arg split_dim slice_indices = slice None None None partial_value ndim slice_indices arg split_dim = slice chunk_start_idx chunk_end_idx sliced = partial_value slice_indices values_to_cat append sliced chunk_start_idx = chunk_end_idx values_to_cat = partial_values args_flattened append torch cat values_to_cat dim=arg split_dim isinstance arg _CustomReducer reduced_val = arg init_value chunk_idx range len chunks_flattened reduced_val = arg reduce_fn reduced_val chunks_flattened chunk_idx arg_idx args_flattened append reduced_val value = chunks_flattened arg_idx chunk_idx range len chunks_flattened assert chunks_flattened chunk_idx arg_idx == value args_flattened append value Stage Unflatten combined args tree_unflatten args_flattened flatten_spec