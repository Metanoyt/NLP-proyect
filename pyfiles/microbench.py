usr bin env python argparse inspect sys numpy np tabulate torch torch _inductor torch _dynamo backends cudagraphs cudagraphs_inner torch _dynamo testing same torch _inductor compile_fx compile_fx torch _inductor utils timed aten = torch ops aten try test test_torchinductor tti except ImportError tti = None compute_speedups args models example_inputs expected = models example_inputs model models actual = model example_inputs assert same actual expected expected - actual timings = np zeros args repeat len models np float rep range args repeat interleave runs handle frequency scaling load changes m model enumerate models timings rep m = timed model example_inputs median = np median timings axis= median median tolist microbenchmark args model example_inputs compiled_fn = compile_fx torch fx symbolic_trace model example_inputs cudagraphs_eager = cudagraphs_inner model example_inputs copy_outputs=False cudagraphs_jit = cudagraphs_inner torch jit trace model example_inputs example_inputs copy_outputs=False compute_speedups args cudagraphs_eager cudagraphs_jit compiled_fn example_inputs MyModel torch nn Module __init__ super __init__ model = torch nn Sequential torch nn Linear torch nn ReLU forward input model input + model input MyModel torch nn Module forward x y x torch abs x + x + y MicroBenchmarks staticmethod add b + b staticmethod scale x m d x - m torch clip d e- staticmethod abs_norm x x torch abs x + staticmethod add_relu_softmax x torch softmax torch relu x + - staticmethod sum b + b sum staticmethod view x aten alias x main parser = argparse ArgumentParser parser add_argument -- filter -k action= append help= filter benchmarks regexp parser add_argument -- exclude -x action= append help= filter benchmarks regexp parser add_argument -- devices -d action= append help= cpu cuda parser add_argument -- size -s action= append help= cpu cuda parser add_argument -- repeat -n type=int default= help= number timing runs parser add_argument -- threads -t type=int help= number threads use eager parser add_argument -- verbose -v action= store_true help= enable verbose debug printouts parser add_argument -- nvfuser action= store_true help= enable nvfuser globally parser add_argument -- transpose action= store_true help= transpose one input parser add_argument -- broadcast action= store_true help= broadcast one input args = parser parse_args defaults args devices = args devices cpu cuda args filter = args filter r args exclude = args exclude r ^$ args size = args size args nvfuser torch _C _jit_override_can_fuse_on_cpu False torch _C _jit_override_can_fuse_on_gpu False torch _C _jit_set_texpr_fuser_enabled False torch _C _jit_set_nvfuser_enabled True torch _C _jit_override_can_fuse_on_cpu torch _C _llvm_enabled torch _C _jit_override_can_fuse_on_gpu True torch _C _jit_set_texpr_fuser_enabled True torch cuda is_available torch _C _jit_set_nvfuser_enabled False args threads torch set_num_threads args threads torch _inductor config cpp threads = args threads args verbose torch _inductor config debug = True torch _inductor config triton autotune_pointwise = True rows = model MicroBenchmarks sum MicroBenchmarks view nargs = len inspect signature model parameters device args devices n args size n = int n sys stdout write f model __name__ device n sys stdout flush inputs = torch rand n n device=device _ range nargs args broadcast inputs - = torch rand n device=device args transpose inputs - = inputs - transpose result = microbenchmark args model inputs rows append model __name__ device str n + result print join f v f x v result print tabulate tabulate rows headers= model dev n ts inductor __name__ == __main__ main