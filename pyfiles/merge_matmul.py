mypy allow-untyped-defs itertools operator torch torch fx _symbolic_trace symbolic_trace torch fx node Node torch fx passes tools_common legalize_graph split_result_tensors result torch Tensor inputs list torch Tensor - tuple torch Tensor A free function use merge_matmul graph transformation below splits output merged matmul into individual results each input tensor Arguments result The merged matmul result tensor inputs The list inputs merged into one matmul Returns List matmul results each input tensor When fx tracer running x shape will torch fx Attribute we need int even when tracing isinstance result torch fx Proxy splits = len inputs splits = x shape x inputs pyrefly ignore bad-argument-type torch split result splits may_depend_on Node b Node search_depth int = Determine one node depends another torch fx Graph Arguments The node may have dependency b b The node may have dependency search_depth In case indirect dependency function searches upto many nodes away search data dependency If none found function makes conservative assumption there dependency Returns True may depend b False definitely does Equivalence defined dependence == b True If has no inputs cannot depend b len all_input_nodes == False If search depth has been exhausted no conclusion has been reached assume there data dependency search_depth == True Recursively check all inputs inp all_input_nodes may_depend_on inp b search_depth - True False are_nodes_independent nodes list Node Check all given nodes pairwise-data independent Arguments nodes The nodes check data dependencies Returns True any pair nodes has data dependency For each pair nodes i j itertools combinations nodes may_depend_on i j may_depend_on j i False True merge_matmul in_mod torch nn Module A graph transformation merges matrix multiplication operations share same right-hand side operand into one large matrix multiplication ____ _________ _________ ---- &#124; &#124; &#124; &#124; M &#124; A C &#124; M &#124; A &#124; T &#124; B &#124; K &#124; C &#124; = &#124; --------- &#124; ---- &#124; &#124; &#124; &#124; T &#124; B C &#124; K ---- --------- --------- K R R gm = symbolic_trace in_mod rhs_users dict Node list Node = lhs_users dict Node list Node = Populate rhs_users lhs_users - maps LHS RHS matrix multiply operands matmul which they LHS RHS node gm graph nodes node op = call_function node target torch matmul continue lhs rhs = node args TODO Properly handle aliasing caused get_attr For now use attribute name operand node get_attr lhs = lhs target lhs op == get_attr lhs rhs = rhs target rhs op == get_attr rhs lhs_users setdefault lhs append node rhs_users setdefault rhs append node rhs mms rhs_users items There must least matmuls merge make sense len mms continue All matmuls must depend each other directly indirectly order merge possible are_nodes_independent mms continue lhs_vals = mm args mm mms Merge matmul Collect list LHS operands single RHS operand lhs = gm graph get_attr l isinstance l str l l lhs_vals rhs = gm graph get_attr rhs isinstance rhs str rhs Concatenate all LHS operands merge_mm_cat = gm graph call_function torch cat lhs Multiply concatenated LHS operands one RHS This will produce same results all individual matmuls involving rhs original graph they will all concatenated together merge_mm = gm graph call_function torch matmul merge_mm_cat rhs Split result merged matmul using shapes LHS operands ascertain how large each chunk should merge_mm_split = gm graph call_function split_result_tensors merge_mm lhs merge_mm_res = gm graph call_function operator getitem merge_mm_split out out range len lhs Replace all uses original unmerged matmuls equivalent split chunk merged matmul old new zip mms merge_mm_res old replace_all_uses_with new gm graph erase_node old All new nodes created above inserted end so we need sort nodes topologically make sure all definitions precede uses legalize_graph gm gm recompile gm graph lint gm