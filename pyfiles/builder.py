mypy ignore-errors This module contains classes utilities building variable trackers Dynamo Variable trackers used convert Python values into symbolic representations can traced transformed during graph capture The key classes - VariableBuilder Handles source-tracked objects need guards proper reconstruction output graph Used inputs module attributes etc - SourcelessBuilder Handles ephemeral objects created during tracing don t need source tracking guards Used temporary lists intermediate values etc Variable trackers enable Dynamo track flow values through program maintain guards dynamic properties reconstruct values output graph The builders module handle converting Python values into appropriate VariableTracker instances based their type usage context abc collections contextlib copy dataclasses enum functools inspect itertools logging math operator random re sys traceback types weakref collections abc Callable MutableMapping typing Any NamedTuple Optional TYPE_CHECKING Union sympy torch torch SymInt torch _dispatch python enable_python_dispatcher torch _dynamo graph_bytecode_inputs get_external_object_by_index register_user_object torch _dynamo utils get_metrics_context is_int_specialization_case is_torch_sym set_feature_use torch _guards TracingContext torch _higher_order_ops flat_apply flat_apply torch _higher_order_ops torchbind call_torchbind torch _ops HigherOrderOperator torch _subclasses fake_tensor FakeTensor is_fake maybe_get_fake_mode torch _subclasses meta_utils is_sparse_any safe_grad torch _utils_internal justknobs_check torch fx experimental _backward_state BackwardState torch fx experimental _dynamism normalize_source_name torch fx experimental sym_node _DynamicScalar DynamicInt torch fx experimental symbolic_shapes _constrain_range_for_size _nested_int_aware_sort DimDynamic RelaxedUnspecConstraint StatefulSymbolicContext SubclassSymbolicContext SymbolicContext SymIntSymbolicContext TrackedFake torch fx immutable_collections immutable_dict immutable_list torch nn utils _expanded_weights ExpandedWeight torch utils _python_dispatch is_traceable_wrapper_subclass is_traceable_wrapper_subclass_type torch utils _sympy value_ranges ValueRanges torch utils weak TensorWeakRef config graph_break_hints mutation_guard replay_record trace_rules device_interface get_registered_device_interfaces exc InternalTorchDynamoError raise_observed_exception unimplemented_v guards GuardBuilder install_guard make_dupe_guard pgo auto_dynamic auto_unset FrameStateSizeEntry InferStride process_automatic_dynamic side_effects SideEffects source AttrProxySource AttrSource CallMethodItemSource ChainedSource ConstDictKeySource ConvertIntSource DictGetItemSource DictSubclassGetItemSource DynamicScalarSource FloatTensorSource GetItemSource GradSource is_constant_source is_from_closure_source is_from_global_source is_from_nonlocal_source is_from_optimizer_source is_from_unspecialized_nn_module_source ListGetItemSource LocalSource NonSerializableSetGetItemSource NumpyTensorSource OptimizerSource RandomValueSource Source SubclassAttrListSource TupleIteratorGetItemSource UnspecializedBuiltinNNModuleSource UnspecializedNNModuleSource utils _extract_tensor_dict build_checkpoint_variable build_invoke_subgraph_variable clone_input common_constant_types dict_keys get_fake_value get_items_from_dict get_locals_to_steal get_static_address_type is_frozen_dataclass is_function is_function_or_wrapper is_invoke_subgraph is_lru_cache_wrapped_function is_namedtuple is_parameter_freezing is_typing is_utils_checkpoint is_wrapper_or_member_descriptor istype namedtuple_fields odict_values proxy_args_kwargs range_iterator set_example_value tensor_always_has_static_shape tuple_iterator tuple_iterator_getitem tuple_iterator_len unwrap_with_attr_name_if_wrapper wrap_fake_exception base AttributeMutationNew typestr ValueMutationExisting ValueMutationNew VariableTracker VariableTrackerMeta builtin BuiltinVariable constant ConstantVariable EnumVariable ctx_manager AutocastModeVariable DynamoConfigPatchVariable ErrorOnGraphBreakVariable NullContextVariable PreserveVersionContextVariable dicts ConstDictVariable DefaultDictVariable DictKeySetVariable FrozensetVariable MappingProxyVariable SetVariable distributed DeviceMeshVariable PlacementClassVariable PlacementVariable ProcessGroupVariable WorldMetaClassVariable functions BuiltinMethodVariable CollectionsNamedTupleFunction CollectiveFunctionRewriteVariable CreateTMADescriptorExperimentalVariable CreateTMADescriptorStableVariable FunctoolsPartialVariable FunctoolsWrapsVariable SysFunctionVariable TracebackVariable TritonKernelVariable UserFunctionVariable UserMethodVariable WrapperUserFunctionVariable higher_order_ops LocalMapWrappedHigherOrderVariable TorchHigherOrderOperatorVariable iter ItertoolsVariable lazy LazyVariableTracker lists BaseListVariable ListIteratorVariable ListVariable NamedTupleVariable RangeVariable SizeVariable SliceVariable TupleIteratorVariable TupleVariable misc AutogradEngineVariable AutogradFunctionContextVariable AutogradFunctionVariable ComptimeVariable DebuggingVariable DelayGraphBreakVariable GetAttrVariable GetSetDescriptorVariable LambdaVariable LoggingLoggerVariable MethodWrapperVariable NumpyDTypeVariable NumpyTypeInfoVariable NumpyVariable PythonModuleVariable RandomClassVariable RandomVariable RegexPatternVariable SavedTensorBox TorchVersionVariable TypingVariable WeakRefVariable nn_module FSDPManagedNNModuleVariable UnspecializedBuiltinNNModuleVariable UnspecializedNNModuleVariable optimizer OptimizerVariable script_object TorchScriptObjectVariable sdpa SDPAParamsVariable streams EventVariable StreamContextVariable StreamVariable tensor NumpyNdarrayVariable supported_const_comparison_op_values SymNodeVariable TensorSubclassVariable TensorVariable UnspecializedPythonVariable torch DispatchKeySetVariable FuncTorchInterpreterVariable TorchCtxManagerClassVariable TorchInGraphFunctionVariable torch_function TensorWithTFOverrideVariable torch_function_mode_stack_state_mgr TorchFunctionModeVariable user_defined FrozenDataClassVariable IntWrapperVariable KeyedJaggedTensorVariable MutableMappingVariable SourcelessGraphModuleVariable UserDefinedClassVariable UserDefinedDictVariable UserDefinedExceptionClassVariable UserDefinedListVariable UserDefinedObjectVariable UserDefinedSetVariable UserDefinedTupleVariable try numpy np except ModuleNotFoundError np = None TYPE_CHECKING torch _dynamo codegen PyCodegen torch _dynamo symbolic_convert InstructionTranslator log = logging getLogger __name__ static_inputs_log = torch _logging getArtifactLogger __name__ cudagraph_static_inputs DimList = list safe_has_grad t torch _logging hide_warnings torch _logging _internal safe_grad_filter hasattr t grad _missing pass dataclasses dataclass GraphArg source Source TODO storing SymInt here FakeTensor pretty strange thing do Probably should have example which stores int fake_example _example Union TensorWeakRef torch SymInt When True indicates GraphArg Python quantity e g float int which we pass FX graph Tensor This controls how we codegen calls into Dynamo graph we will call torch as_tensor quantity before passing Note we typically do pass dynamic integers tensors because they will most frequently just used size computation But policy decision we can change our mind particular when int comes random number generator e g random randint we DO pass tensor It s also worth noting our current tracing rules pass_arg_as_tensor subtly broken we just pun variable d scalar Tensor pray semantics same Which they often necessarily ezyang May plans fix soon pass_arg_as_tensor bool fake_tensor Optional torch _subclasses fake_tensor FakeTensor UnspecializedPythonVariable often masquerades tensor We MUST NOT generate shape guard code actually tries access tensor properties these values is_tensor lets us tell graph arg actually tensor is_tensor bool = True Sometimes Tensor we pass example freshly allocated smh Then we cannot only keep weak reference This lets you stash strong reference too example_strong_ref Optional torch Tensor = None property example isinstance _example TensorWeakRef r = _example assert r None r _example __post_init__ isinstance _example torch Tensor _example = TensorWeakRef _example assert is_fake fake_tensor reconstruct codegen PyCodegen codegen source erase _example = None example_strong_ref = None __eq__ other source name == other source name BackwardStateGraphArg GraphArg __init__ - None super __init__ source=None _example=BackwardState pass_arg_as_tensor=False fake_tensor=None is_tensor=False reconstruct codegen PyCodegen assert codegen tx output backward_state_var codegen add_push_null lambda codegen load_import_from BackwardState __module__ BackwardState codegen call_function False codegen dup_top codegen store codegen tx output backward_state_var All class-based iterators itertools NOTE use id because some objects hashable will raise error during lookup ITERTOOLS_TYPE_IDS frozenset int = frozenset id member name member vars itertools items name startswith _ inspect isclass member Will updated later substitute_in_graph torch _dynamo polyfills itertools py ITERTOOLS_POLYFILLED_TYPE_IDS set int = set Capture fn pointer time This guard against trying mark iterated tensors static case user overrides fn ptr og_module_named_buffers_fn_ptr = torch nn Module named_buffers og_module_named_parameters_fn_ptr = torch nn Module named_parameters VariableBuilder Wrap python value VariableTracker instance __init__ tx source Source - None assert source None Consider SourcelessBuilder ephemeral objects usually objects created locally assert TracingContext try_get None Expected active TracingContext super __init__ tx = tx source = source name = source name __call__ value value tx output side_effects side_effect_result = tx output side_effects value dup_guard = make_dupe_guard source side_effect_result source dup_guard install_guards dup_guard isinstance value torch nn Module isinstance side_effect_result UnspecializedNNModuleVariable This means two nn module instances different sources have same id NN modules somewhat special objects because we have track their nn_module_stack ease use But we don t do anything we will just older variable tracker older nn_module_stack So lets old variable tracker update its nn_module_stack side_effect_result set_nn_module_stack_source source side_effect_result cached_vt = tx output variable_tracker_cache lookup value source cached_vt cached_vt vt = _wrap value vt source None vt source = source _is_deduplicable_sym_variable value vt Constants like etc can unspecialized SymNodeVariables sometimes we should NOT track them If we use single SymNodeVariable instance track them across multiple uses then guards created one usage will incorrectly apply all other usages constant leading unnecessary recompilations is_torch_sym value isinstance value _DynamicScalar isinstance vt SymNodeVariable _can_lift_attrs_to_inputs vt _is_deduplicable_sym_variable value vt value tx output side_effects is_wrapper_or_member_descriptor value vt = tx output side_effects track_object_existing value vt tx output variable_tracker_cache add value source vt vt _can_lift_attrs_to_inputs vt type vt TensorVariable TensorWithTFOverrideVariable UserDefinedObjectVariable NumpyNdarrayVariable get_source source install_guards guards source = get_source try tmp = source make_guard guard guard guards except NotImplementedError None install_guard tmp skip= classmethod _type_dispatch cls cls _type_dispatch_impl config trace_numpy classmethod functools cache _type_dispatch_impl cls trace_numpy NB Careful close over avoid ref cycle lru_cache entries = torch Tensor torch nn Parameter torch _subclasses FakeTensor torch _subclasses functional_tensor FunctionalTensor cls wrap_tensor tuple list odict_values collections deque torch Size cls wrap_listlike tuple_iterator cls wrap_tuple_iterator range_iterator cls wrap_range_iterator slice range cls wrap_slice_range tuple common_constant_types cls wrap_literal re Pattern cls wrap_regex_pattern weakref ReferenceType cls wrap_weakref torch utils hooks RemovableHandle cls wrap_removable_handle torch jit ScriptFunction cls wrap_jit_function types MappingProxyType cls wrap_mapping_proxy trace_numpy np entries append np ndarray cls wrap_numpy_ndarray result = ts fn entries t ts isinstance ts tuple ts assert t result result t = fn result wrap_regex_pattern value re Pattern TODO jansel something like REPR_MATCH might more robust here install_guards GuardBuilder ID_MATCH RegexPatternVariable value wrap_weakref value weakref ReferenceType install_guards GuardBuilder TYPE_MATCH WeakRefVariable build tx value source=self source wrap_removable_handle value This means removable handle created some other frame Our current infra requires hook registered removed same frame So graph break Related test - PYTORCH_TEST_WITH_DYNAMO= python test test_autograd py -k TestAutograd test_hooks unimplemented_v gb_type= Attempted represent unregistered RemovableHandle context= explanation= Dynamo attempted build representation torch utils hooks RemovableHandle which supported This happens because RemovableHandle created another frame hints= wrap_jit_function value install_guards GuardBuilder TYPE_MATCH WrapperUserFunctionVariable value _torchdynamo_inline source=self source wrap_mapping_proxy value install_guards GuardBuilder TYPE_MATCH This might suboptimal compared dict guards But mappingproxy very common so its ok guard all keys install_guards GuardBuilder MAPPING_KEYS_CHECK all_const = all ConstantVariable is_literal k k value keys all_const unimplemented_v gb_type= non-const keys mappingproxy context=f non-const keys k k value keys ConstantVariable is_literal k explanation= Dynamo expects mappingproxy keys constants hints= Ensure your mappingproxy keys constants e g int float strings build_key_value k v key = ConstantVariable create k source_key = k source_value = GetItemSource get_source source_key res_value = LazyVariableTracker create v source_value key res_value items = dict build_key_value k v k v value items Create dict_vt used mapping proxy variable dict_vt = ConstDictVariable items source=None result = MappingProxyVariable dict_vt source=self source tx output side_effects track_mutable value result classmethod functools cache _id_dispatch cls - dict int Callable VariableBuilder Any VariableTracker comptime comptime entries = comptime lambda value ComptimeVariable dataclasses fields lambda value LambdaVariable _dataclasses_fields_lambda source=self source install_guards GuardBuilder CLOSURE_MATCH torch __version__ lambda value TorchVersionVariable result = ts fn entries t ts isinstance ts tuple list ts assert t result result id t = fn result _wrap value here avoid circular dependencies torch utils _triton has_triton has_triton_experimental_host_tma has_triton_tensor_descriptor_host_tma decorators DynamoConfigPatchProxy ErrorOnGraphBreakDecoratorContextManager has_triton triton runtime autotuner Autotuner triton runtime jit JITFunction JITFunction pass Autotuner pass default implementations case we don t have triton wrong triton version create_ d_tma_descriptor pass create_ d_tma_descriptor pass TensorDescriptor staticmethod from_tensor pass has_triton_experimental_host_tma triton tools experimental_descriptor noqa F create_ d_tma_descriptor create_ d_tma_descriptor has_triton_tensor_descriptor_host_tma triton tools tensor_descriptor TensorDescriptor noqa F Handle exact type match type_dispatch = _type_dispatch get type value type_dispatch None type_dispatch value Handle exact id match id_dispatch = _id_dispatch get id value id_dispatch None id_dispatch value Everything NB order matters isinstance value torch Tensor type value These torch-native subclasses have overly restrictive ` __torch_function__ ` which prevents Dynamo reading their tensor attributes like ` is_nested ` calling methods like ` _is_view ` torch nn parameter UninitializedBuffer torch nn parameter UninitializedParameter ExpandedWeight type value config nontraceable_tensor_subclasses type value __torch_dispatch__ torch Tensor __torch_dispatch__ is_traceable_wrapper_subclass value wrap_tensor value is_namedtuple value install_guards GuardBuilder SEQUENCE_LENGTH output = LazyVariableTracker create getattr value name source=AttrSource source name name namedtuple_fields type value result = NamedTupleVariable output tuple_cls=type value source=self source tx output side_effects track_object_existing value result istype value dict collections defaultdict collections OrderedDict install_guards GuardBuilder TYPE_MATCH all_const = all ConstantVariable is_literal k k value keys For all_const we don t have guard anything yet We guard keys lazily adding dict_getitem entry each accessed key For cases where we need guard all keys we lazily put guards during dict call_method check dicts py all_const Guard key order This ideal i e there no need guard key order But we guard key order because complexity For non-constant objects we can t save key guard context because can memory heavy We can add weakrefs complicates accesses For non-constant objects we also have guard keys like TENSOR_MATCH tensor We might also have guards attributes keys like tensor grad To make work tree structure complicated So instead we guard key order While guarding key order we just save indices use access keys values Indices cheap save tx output guard_on_key_order add source We need all keys hashable We do within _HashableTracker dicts py build_key_value i k v base = get_source all_const key = ConstantVariable create k source_key = k source_key = ConstDictKeySource base i key = LazyVariableTracker create k source_key source_value = DictGetItemSource base source_key res_value = LazyVariableTracker create v source_value key res_value Ensure we call dict keys value keys which can call overridden keys method In C++ guards we relied PyDict_Next traverse dictionary which uses internal data structure does call overridden keys method result = dict build_key_value i k v i k v enumerate get_items_from_dict value istype value collections defaultdict factory_source = AttrSource source default_factory result = DefaultDictVariable result type value default_factory=VariableBuilder tx factory_source value default_factory source=self source result = ConstDictVariable result user_cls=type value source=self source tx output side_effects track_mutable value result isinstance value torch nn Module wrap_module value ConstantVariable is_literal value non-atomic literals wrap_literal value isinstance value torch overrides TorchFunctionMode var = TorchFunctionModeVariable value source=self source tx output side_effects track_object_existing value var var istype value set any isinstance x torch Tensor x value unimplemented_v gb_type= Attempted wrap set tensors context= Python set containing torch Tensor elements explanation= Dynamo cannot trace sets tensors To get stable ordering Dynamo needs convert set into list order might stable set contains tensors hints= Use dictionary where keys tensors graph_break_hints SUPPORTABLE install_guards GuardBuilder TYPE_MATCH install_guards GuardBuilder SEQUENCE_LENGTH The list gives ordering set items The ordering based Python hash related object ordering inside set object The order being incorrect runtime will lead recompilation L = list value items = LazyVariableTracker create v source=NonSerializableSetGetItemSource source i i v enumerate L result = SetVariable items source=self source tx output side_effects track_object_existing value result istype value frozenset all For DBR quantization we could get frozenset torch funcs type x types BuiltinMethodType x __module__ == torch Another commonly used frozenset types x torch utils _pytree BUILTIN_TYPES x value For limited cases frozenset here we know items won t change across runs so we can safely create sourceless VTs them only guard frozenset id TODO support source sets remove special logics here items = SourcelessBuilder create tx v v value install_guards GuardBuilder ID_MATCH FrozensetVariable items source=self source isinstance value enum Enum torch DispatchKey torch _C _functorch TransformType install_guards GuardBuilder ID_MATCH EnumVariable value=value source=self source DebuggingVariable is_reorderable_logging_function value Put above builtin_callable so print can handled along other builtin debugging functions install_guards GuardBuilder BUILTIN_MATCH DebuggingVariable value source=self source isinstance value logging Logger install_guards GuardBuilder TYPE_MATCH LoggingLoggerVariable value source=self source is_utils_checkpoint value build_checkpoint_variable source=self source is_invoke_subgraph value build_invoke_subgraph_variable source=self source LocalMapWrappedHigherOrderVariable should_wrap_in_hop value LocalMapWrappedHigherOrderVariable build source=self source isinstance value functools partial func_src = AttrSource get_source func func_obj = VariableBuilder tx func_src value func args = args_source = AttrSource get_source args i arg enumerate value args args append VariableBuilder tx GetItemSource args_source i arg keywords = keywords_source = AttrSource get_source keywords k v value keywords items ConstantVariable is_literal k unimplemented_v gb_type= functools partial non-literal keyword context=f non-literal keyword k explanation= functools partial expects literal string keywords hints= graph_break_hints USER_ERROR keywords k = VariableBuilder tx DictGetItemSource keywords_source k v install_guard get_source make_guard GuardBuilder TYPE_MATCH keywords_source make_guard GuardBuilder DICT_KEYS_MATCH args_source make_guard GuardBuilder SEQUENCE_LENGTH FunctoolsPartialVariable func_obj args keywords is_typing value typing List typing Mapping etc install_guards GuardBuilder ID_MATCH TypingVariable value source=self source np None isinstance value np generic numpy array scalars convert D arrays wrap_numpy_ndarray np asarray value trace_rules is_numpy value assert np istype value types MethodType Dont guard cython functions they dont change ids inspect isfunction value __func__ install_guard AttrSource source __func__ make_guard GuardBuilder CLOSURE_MATCH inspect isclass value install_guards GuardBuilder CLASS_MATCH inspect isfunction value install_guards GuardBuilder CLOSURE_MATCH callable value install_guards GuardBuilder ID_MATCH install_guards GuardBuilder TYPE_MATCH NumpyVariable value source=self source trace_rules is_numpy_dtype value install_guards GuardBuilder ID_MATCH NumpyDTypeVariable value source=self source trace_rules is_numpy_type_info value isinstance value np iinfo install_guards GuardBuilder TYPE_MATCH dt_source = AttrSource source dtype install_guard dt_source make_guard GuardBuilder ID_MATCH install_guards GuardBuilder ID_MATCH NumpyTypeInfoVariable value source=self source NB These can t put type_dispatch they have run later CollectiveFunctionRewriteVariable can_rewrite value install_guards GuardBuilder CLOSURE_MATCH CollectiveFunctionRewriteVariable create tx value source=self source istype value torch autograd function FunctionMeta install_guards GuardBuilder CLASS_MATCH AutogradFunctionVariable value source=self source isinstance value torch autograd function FunctionCtx actual_saved_tensors = None try actual_saved_tensors = value saved_tensors except RuntimeError pass saved_tensors = guards = source make_guard GuardBuilder TYPE_MATCH isinstance actual_saved_tensors tuple saved_tensors_source = AttrSource source saved_tensors guards append saved_tensors_source make_guard GuardBuilder SEQUENCE_LENGTH i v enumerate actual_saved_tensors saved_tensors append VariableBuilder tx GetItemSource saved_tensors_source i v install_guard guards tx output side_effects track_object_existing value AutogradFunctionContextVariable value source=self source saved_tensors=SavedTensorBox saved_tensors isinstance value types MethodType istype getattr value __self__ None torch autograd function FunctionMeta getattr value __name__ == apply value == getattr value __self__ apply None handle aliased autograd function ` apply ` calls install_guard AttrSource get_source __func__ make_guard GuardBuilder CLOSURE_MATCH GetAttrVariable AutogradFunctionVariable value __self__ source=AttrSource source member= __self__ apply isinstance value torch _C _ImperativeEngine install_guards GuardBuilder ID_MATCH AutogradEngineVariable value source=self source value torch _dynamo external_utils FakeCompiledAutogradEngine _exec_final_callbacks_stub install_guards GuardBuilder CLOSURE_MATCH LambdaVariable lambda UserFunctionVariable torch _dynamo external_utils FakeCompiledAutogradEngine exec_final_callbacks call_function tx tx output side_effects get_ca_final_callbacks_var isinstance value DynamoConfigPatchProxy DynamoConfigPatchVariable value changes isinstance value ErrorOnGraphBreakDecoratorContextManager ErrorOnGraphBreakVariable value error_on_graph_break callable value trace_rules lookup_callable value None trace_rules is_callable_allowed value tx output has_user_defined_allowed_in_graph = True trace_rules lookup_callable value create_with_source value source=self source np isinstance value np number wrap_unspecialized_primitive value isinstance value HigherOrderOperator value torch _higher_order_ops invoke_subgraph unimplemented_v gb_type= Attempted wrap torch _higher_order_ops invoke_subgraph context= explanation= Directly using invoke_subgraph supported Use nested_compile_region hints= install_guards GuardBuilder TYPE_MATCH TorchHigherOrderOperatorVariable make value source=self source isinstance value torch cuda StreamContext install_guards GuardBuilder ID_MATCH stream_source = AttrSource source stream stream_var = VariableBuilder tx stream_source value stream StreamContextVariable create tx stream_var isinstance value torch Stream This refers device-agnostic torch Stream install_guards GuardBuilder TYPE_MATCH index = register_user_object value source stream_proxy = tx output create_proxy call_function get_external_object_by_index index set_example_value stream_proxy node value var = StreamVariable stream_proxy value source=self source tx output side_effects track_object_existing value var isinstance value torch _C _SDPAParams install_guards GuardBuilder TYPE_MATCH SDPAParamsVariable create tx value source isinstance value torch _functorch pyfunctorch FuncTorchInterpreter install_guards GuardBuilder ID_MATCH FuncTorchInterpreterVariable value isinstance value torch Event install_guards GuardBuilder TYPE_MATCH index = register_user_object value source event_proxy = tx output create_proxy call_function get_external_object_by_index index set_example_value event_proxy node value EventVariable event_proxy value source=self source istype value contextlib nullcontext inspect getattr_static value enter_result None None install_guards GuardBuilder TYPE_MATCH NullContextVariable source=self source KeyedJaggedTensorVariable is_matching_object value install_guards GuardBuilder TYPE_MATCH result = KeyedJaggedTensorVariable value source=self source TODO doing manually bad tx output side_effects track_object_existing value result isinstance value torch optim Optimizer install_guards GuardBuilder ID_MATCH source = OptimizerSource source OptimizerVariable value source=self source isinstance value torch DispatchKeySet install_guards GuardBuilder DISPATCH_KEY_SET_MATCH DispatchKeySetVariable value WorldMetaClassVariable is_group_member_type value WorldMetaClassVariable value source=self source ProcessGroupVariable is_process_group value install_guards GuardBuilder ID_MATCH ProcessGroupVariable value source=self source DeviceMeshVariable is_device_mesh value TODO see we need add custom guard instead simple ID_MATCH install_guards GuardBuilder EQUALS_MATCH DeviceMeshVariable value source=self source PlacementClassVariable is_placement_type value TODO see we need add custom guard instead simple ID_MATCH install_guards GuardBuilder ID_MATCH PlacementClassVariable value source=self source PlacementVariable is_placement value TODO see we need add custom guard instead simple ID_MATCH install_guards GuardBuilder EQUALS_MATCH PlacementVariable value source=self source id value ITERTOOLS_TYPE_IDS id value ITERTOOLS_POLYFILLED_TYPE_IDS install_guards GuardBuilder CLASS_MATCH ItertoolsVariable value source=self source isinstance value _DynamicScalar is_int = isinstance value DynamicInt source = DynamicScalarSource source is_int id value tx output root_tracer dynamic_scalar_nodes If we ve already seen dynamic scalar reuse existing SymInt SymFloat node node = tx output root_tracer dynamic_scalar_nodes id value sym = tx output shape_env create_unspecified_symbol value real source=source dynamic_dim=DimDynamic DYNAMIC node = tx output shape_env create_symintnode sym hint=value real source=source Bind graph input sym_node_proxy = tx output root_tracer create_graph_input re sub r ^a-zA-Z - + _ name type node node source=source sym_node_proxy node meta grapharg = GraphArg source node False None is_tensor=False example_strong_ref=node sym_expr = node node expr assert isinstance sym_expr sympy Symbol f sym_expr basic Symbol tx output tracked_fakes append TrackedFake node source None SymNodeVariable sym_node_proxy node is_torch_sym value Note doesn t handle nested symints For SymBool input we reuse infra SymInt simulating SymBool SymInt dynamo Concretely We create SymInt dynamo s shape_env whose source constructed ConvertIntSource source so guards SymInts can effectively applied original SymBool user program We create SymBool based SymInt dynamo s ShapeEnv Because original user program depends value being SymBool This allows dynamo interpret user s program correctly source = source isinstance value torch SymInt ConvertIntSource source value node has_hint new_symint = tx output shape_env create_unspecified_symint_and_symbol int value node hint source dynamic_dim=DimDynamic DYNAMIC isinstance value torch SymBool We need create unbacked symint replace unbacked symbool new_symint = tx output shape_env create_unbacked_symint TODO yidi we need figure out way propagate guards we accumulated when tracing subggraph outer shape_env For normal symints automatically done evaluating guards once will cause data-dependent error when we evaluate outer unbacked symints The test case triggers graph break test_cond_unbacked_symint_closure unimplemented_v gb_type= Attempted wrap unbacked SymInt context= explanation= Unbacked SymInt input supported yet hints= graph_break_hints SUPPORTABLE sym_node_proxy = tx output root_tracer create_graph_input re sub r ^a-zA-Z - + _ name type new_symint new_symint source=source sym_node_proxy node meta grapharg = GraphArg source new_symint False None is_tensor=False example_strong_ref=new_symint We bind new_symint graph input sym_expr = new_symint node expr assert isinstance sym_expr sympy Symbol f sym_expr basic Symbol tx output tracked_fakes append TrackedFake new_symint source None tracing_symint = new_symint isinstance value torch SymInt new_symint == cast back symbool tracing SymNodeVariable sym_node_proxy tracing_symint isinstance value JITFunction Autotuner install_guards GuardBuilder ID_MATCH TritonKernelVariable value None No kernel idx provided None No grid provided source=self source value create_ d_tma_descriptor CreateTMADescriptorExperimentalVariable rank= value create_ d_tma_descriptor CreateTMADescriptorExperimentalVariable rank= value TensorDescriptor from_tensor CreateTMADescriptorStableVariable isinstance value torch amp autocast_mode autocast install_guards GuardBuilder ID_MATCH AutocastModeVariable target_values= value device value fast_dtype value _enabled value _cache_enabled source=self source TorchCtxManagerClassVariable is_matching_cls value inspect isclass value install_guards GuardBuilder CLASS_MATCH inspect isfunction value install_guards GuardBuilder CLOSURE_MATCH TorchCtxManagerClassVariable value source=self source inspect getattr_static value __script_if_tracing_wrapper False install_guards GuardBuilder TYPE_MATCH WrapperUserFunctionVariable value __original_fn source=self source is_lru_cache_wrapped_function value install_guards GuardBuilder TYPE_MATCH WrapperUserFunctionVariable value __wrapped__ source=self source value traceback clear_frames TracebackVariable source=self source value sys exc_info sys version_info = value sys exception SysFunctionVariable value source=self source is_function_or_wrapper value inspect getattr_static value _torchdynamo_inline False install_guards GuardBuilder TYPE_MATCH WrapperUserFunctionVariable value _torchdynamo_inline source=self source value functools wraps install_guards GuardBuilder ID_MATCH FunctoolsWrapsVariable value source=self source value collections namedtuple install_guards GuardBuilder ID_MATCH CollectionsNamedTupleFunction value source=self source isinstance value types BuiltinMethodType BuiltinMethodVariable is_supported_builtin_method value install_guards GuardBuilder ID_MATCH BuiltinMethodVariable value source=self source is_function value value float fromhex float hex install_guards GuardBuilder ID_MATCH GetAttrVariable BuiltinVariable float source=self source value __name__ is_function_or_wrapper value value attr_name = unwrap_with_attr_name_if_wrapper value For these wrappers Dynamo points wrapped function so source needs updated well attr_name None source = AttrSource source attr_name trace_rules lookup value create_with_source value source=self source value random Random install_guards GuardBuilder ID_MATCH RandomClassVariable source=self source istype value random Random RandomVariable is_supported_random_obj value install_guards GuardBuilder TYPE_MATCH result = RandomVariable value source=self source tx output side_effects track_mutable value result result Don t use istype since some python modules subclasses types ModuleType directly E g type torch ops - torch _ops _Ops type torch backends cudnn - torch backends cudnn CudnnModule isinstance value types ModuleType replay_record DummyModule install_guards GuardBuilder MODULE_MATCH result = PythonModuleVariable value source=self source tx output side_effects track_object_existing value result result isinstance value types MethodType isinstance value __self__ torch nn Module torch utils _pytree TreeSpec don t let MethodTypes fall through UserDefinedObject which doesn t support CALL_FUNCTION TODO whc Why do we limit methods NNModules I don t have good reason preserves existing behavior MBartForConditionalGeneration which generates many graph breaks OOMs otherwise I suspect we probably want relax check dig deeper there In order construct MethodVariable Dynamo we start actual method obj python need separately wrap its underlying ` __func__ ` its ` ` argument We wrap ` ` here then ` __func__ ` gets wrapped inside UserMethodVariable self_obj = VariableBuilder tx source=AttrSource source __self__ value __self__ assert self_obj isinstance self_obj VariableTracker Failed produce valid obj UserMethodVariable value __func__ self_obj source=self source isinstance value types GetSetDescriptorType GetSet descriptors C functions attached attribute lookup using PyGetSetDef Python attribute lookup can decide create new object fly therefore ` id ` descriptors guaranteed same different attribute accesses Since these unlikely change during program execution we can skip guarding them GetSetDescriptorVariable value isinstance value types MethodWrapperType Method-wrappers written C they guaranteed same object attribute lookup Therefore we cannot insert ID_MATCH guard here method-wrappers very unlikely change so its ok skip guard here MethodWrapperVariable value issubclass type value type issubclass value BaseException match user defined exceptions install_guards GuardBuilder ID_MATCH UserDefinedExceptionClassVariable value issubclass type value type value torch utils hooks BackwardHook torch nn Parameter torch nn Buffer TODO jansel combine case one above trace_rules lookup value create_with_source value source=self source value torch autograd _unsafe_preserve_version_counter install_guards GuardBuilder CLASS_MATCH PreserveVersionContextVariable constructor tx ` value ` must strict subclass ` torch Tensor ` issubclass value torch Tensor value torch Tensor ` TensorSubclassVariable ` subclass overrides ` torch_dispatch ` value __torch_dispatch__ torch Tensor __torch_dispatch__ ` TensorSubclassVariable ` would lead construction ` TensorWithTFOverrideVariable ` we don t want traceable wrapper subclasses we wrap those subclass instances into ` TensorVariable ` is_traceable_wrapper_subclass_type value TensorSubclassVariable value source=self source is_from_closure_source source For closure source variable comes LOAD_SUPER_ATTR which calls __class__ This internal Cpython implementation rare user modify __class__ manually For other cases userdefined so install ID_MATCH even its global variable install_guards GuardBuilder CLASS_MATCH UserDefinedClassVariable value source=self source TorchScriptObjectVariable is_matching_cls type value source FlattenScriptObjectSource ScriptObjectQualifiedNameSource torch _library fake_class_registry tracing_with_real value proxy = tx output root_tracer create_graph_input re sub r ^a-zA-Z - + _ name type value value source=self source setting is_unspecialized=False insert as_tensor call reconstruct default setting example real value because these example values will used example_inputs user compiler proxy node meta grapharg = GraphArg source value False None False value TorchScriptObjectVariable create proxy value source=self source This exists allow smoother transition The implications The script objects won t tracked proxies Methods these objects won t show up graph The original script object might mutated hasattr value __obj_flatten__ wrap_user_defined value Install guards fully qualified name script object LazyVariableTracker realize_all VariableBuilder tx ScriptObjectQualifiedNameSource source value _type qualified_name type ignore attr-defined Install guards content script object setting source FlattenScriptObjectSource which calls __obj_flatten__ get contents LazyVariableTracker realize_all VariableBuilder tx FlattenScriptObjectSource source value __obj_flatten__ fake_script_obj = torch _library fake_class_registry maybe_to_fake_obj tx output fake_mode value proxy = tx output root_tracer create_graph_input re sub r ^a-zA-Z - + _ name type value fake_script_obj source=self source setting is_unspecialized=False insert as_tensor call reconstruct default setting example real value because these example values will used example_inputs user compiler proxy node meta grapharg = GraphArg source value False None False fake_script_obj TorchScriptObjectVariable create proxy fake_script_obj source=self source isinstance value dict collections OrderedDict type value __new__ dict __new__ Construct dict_vt will reside inside UserDefinedDictVariable install_guards GuardBuilder TYPE_MATCH install_guards GuardBuilder SEQUENCE_LENGTH Guard key order tx output guard_on_key_order add source We need all keys hashable We do within _HashableTracker dicts py build_key_value i k v base = get_source source_key = ConstDictKeySource base i key = LazyVariableTracker create k source_key source_value = DictSubclassGetItemSource base source_key res_value = LazyVariableTracker create v source_value key res_value Ensure we call dict keys value keys which can call overridden keys method In C++ guards we relied PyDict_Next traverse dictionary which uses internal data structure does call overridden keys method result = dict build_key_value i k v i k v enumerate get_items_from_dict value dict_vt = ConstDictVariable result user_cls= collections OrderedDict isinstance value collections OrderedDict dict mutation_type=ValueMutationExisting source=self source Force reconstruct mutation keep reconstruction bytecode simple dict_vt should_reconstruct_all = True result = UserDefinedDictVariable value dict_vt=dict_vt source=self source tx output side_effects track_object_existing value result isinstance value tuple install_guards GuardBuilder TYPE_MATCH install_guards GuardBuilder SEQUENCE_LENGTH NB - Be careful triggering user code Guards also work underlying tuple data structure output = LazyVariableTracker create tuple __getitem__ value i source=GetItemSource get_source i i range tuple __len__ value tuple_vt = TupleVariable output source=self source mutation_type=ValueMutationExisting result = UserDefinedTupleVariable value tuple_vt=tuple_vt source=self source tx output side_effects track_object_existing value result isinstance value list install_guards GuardBuilder TYPE_MATCH install_guards GuardBuilder SEQUENCE_LENGTH NB - Be careful triggering user code Guards also work underlying list data structure output = LazyVariableTracker create list __getitem__ value i source=ListGetItemSource get_source i i range list __len__ value list_vt = ListVariable output source=self source mutation_type=ValueMutationExisting result = UserDefinedListVariable value list_vt=list_vt source=self source tx output side_effects track_object_existing value result isinstance value set frozenset install_guards GuardBuilder TYPE_MATCH install_guards GuardBuilder SEQUENCE_LENGTH L = list dict fromkeys value output = LazyVariableTracker create list __getitem__ L i source=NonSerializableSetGetItemSource get_source i i range list __len__ L set_vt_cls = SetVariable isinstance value set FrozensetVariable set_vt = set_vt_cls output source=self source mutation_type=ValueMutationExisting result = UserDefinedSetVariable value set_vt=set_vt source=self source tx output side_effects track_object_existing value result issubclass type value MutableMapping install_guards GuardBuilder TYPE_MATCH result = MutableMappingVariable value source=self source tx output side_effects track_object_existing value result is_frozen_dataclass value install_guards GuardBuilder TYPE_MATCH result = FrozenDataClassVariable create tx value source=self source tx output side_effects track_object_existing value result isinstance value dict_keys all ConstantVariable is_literal k k value If dict_keys object passed outside compile region must either passed along corresponding dict object treated set when only keys passed into compiled region - If passed along dict dict object itself already guarded - If only dict_keys object passed we add EQUALS_MATCH SEQUENCE_LENGTH guards ensure remains unchanged across multiple runs items = SourcelessBuilder create tx v v value install_guard get_source make_guard GuardBuilder SEQUENCE_LENGTH get_source make_guard GuardBuilder EQUALS_MATCH DictKeySetVariable items source=self source unimplemented_v gb_type= non-const keys dict_keys context=f non-const keys k k value ConstantVariable is_literal k explanation= Dynamo expects dict_keys keys constants hints= Ensure your dict_keys keys constants e g int float strings IntWrapperVariable is_matching_object value torch export dynamic_shapes _DimHintType value dynamism None value dynamism type == _DimHintType STATIC wrap_symint value val value dynamism type == _DimHintType DYNAMIC log debug s marked s via IntWrapper source name DimDynamic DYNAMIC wrap_symint value val dynamism=DimDynamic DYNAMIC context=SymIntSymbolicContext constraint=RelaxedUnspecConstraint warn_only=False value dynamism type == _DimHintType AUTO log debug s marked s via IntWrapper source name DimDynamic DYNAMIC wrap_symint value val dynamism=DimDynamic DYNAMIC raise RuntimeError f Undefined dynamism value dynamism wrap_user_defined value wrap_user_defined value Any install_guards GuardBuilder TYPE_MATCH result = UserDefinedObjectVariable value source=self source SideEffects cls_supports_mutation_side_effects type value don t allow STORE_ATTR mutation custom __setattr__ result tx output side_effects track_object_existing value result wrap_listlike value Union tuple list odict_values NamedTuple item value item value unimplemented_v gb_type= list elements pointing list itself context= explanation= Dynamo does support lists whose items reference itself hints= Avoid using referential list config specialize_int type value torch Size install_guards GuardBuilder CONSTANT_MATCH ConstantVariable create value=value One can index tensor list tuple Therefore we need have stricter match install_guards GuardBuilder SEQUENCE_LENGTH Tuples immutable objects so we should mark its items static This avoids wrapping tuple items symints This helps nn module attributes like conv d strides dilations istype value tuple all ConstantVariable is_literal item item value source guard_source is_unspecialized_nn_module install_guards GuardBuilder CONSTANT_MATCH TupleVariable ConstantVariable create item item value output = LazyVariableTracker create item source=GetItemSource get_source i i item enumerate value maybe_gm = tx output local_scope get isinstance source LocalSource source local_name get_locals_to_steal maybe_gm The input tensor list dynamo compiled autograd may contain activations which freed they used inductor Dynamo s default behavior lift all tensors graph inputs will cause dynamo hold extra reference activation tensors increase peak memory usage To allow freeing ASAP we keep list graph argument dynamo output graph unpack locally e g instead ` forward L_inputs_ _ L_inputs_ _ ` we have ` forward L_inputs_ ` source = source assert isinstance value list tensor_list_proxy = tx output root_tracer create_graph_input re sub r ^a-zA-Z - + _ name type value value source=source tensor_list_proxy node meta steal_arg = True list_variable = wrap_fx_proxy_cls target_cls=TensorVariable tx=self tx proxy=tensor_list_proxy example_value=value subclass_type=None source=source Apply relevant logic ` VariableTracker build value i ` except ` create_graph_input ` stuff guards = i tensor_variable enumerate list_variable items source_i = GetItemSource base=source index=i index_is_slice=False access unpacked tensor list instead lifted arg tx output input_source_to_var source_i = tensor_variable tensor_variable proxy node meta tensor_dict = _extract_tensor_dict value i guard = functools partial GuardBuilder TENSOR_MATCH value=TensorWeakRef value i guards append source_i make_guard guard install_guard guards skip= grapharg = GraphArg source value pass_arg_as_tensor=False fake_tensor=None is_tensor=False tensor_list_proxy node meta grapharg = grapharg The following very important maintaining python object == variable tracker -to- mapping which mainly handled via ` side_effects ` Note constructing ` tensor_variable ` above already adds graph arg we never registered ` side_effects ` The preemptive ` realize ` calls here basically does registration end ` __call__ ` A slightly cleaner alternative register ` tensor_variable ` s above ` side_effects ` directly just ` list_variable ` breaks some tensor-subclass related tests like ` test_inputs_aliasing_bytecode_stack_restore ` because ` tensor_variable ` constructed via ` handle_traced_output ` which doesn t really expect handle tensor subclass Eventually we expect fix remove all these having Dynamo auto-boxing inputs compiled graph see https github com pytorch pytorch issues vt output vt realize result = BaseListVariable cls_for_instance value output source=self source istype value list collections deque tx output side_effects track_mutable value result result wrap_tuple_iterator value tuple_iterator install_guards GuardBuilder TUPLE_ITERATOR_LEN output = VariableBuilder tx TupleIteratorGetItemSource get_source i tuple_iterator_getitem value i i range tuple_iterator_len value result = TupleIteratorVariable output source=self source tx output side_effects track_mutable value result wrap_range_iterator value range_iterator install_guards GuardBuilder RANGE_ITERATOR_MATCH Get all values range iterator no need install guards items since ` RANGE_ITERATOR_MATCH ` guarantees same items items = ConstantVariable create v v copy deepcopy value result = ListIteratorVariable items source=self source tx output side_effects track_mutable value result wrap_slice_range value Union slice range items = VariableBuilder tx AttrSource get_source k getattr value k k start stop step install_guards GuardBuilder TYPE_MATCH isinstance value slice SliceVariable items tx source=self source RangeVariable items source=self source mark_static_input value torch Tensor guard bool decorators mark_static_address static_inputs_log debug Marking static input s id s source name id value mark_static_address value guard=guard Check we ve seen tensor before update graph metadata needed As long runs before AOT sound value tx output side_effects var = tx output side_effects value var proxy node meta tensor_dict _dynamo_static_input_type = value _dynamo_static_input_type wrap_module value torch nn Module eval_frame OptimizedModule len value __dict__ == unimplemented_v gb_type= Uninitialized nn Module context=typestr value explanation=f Attempted trace uninitialized nn Module type typestr value hints= graph_break_hints USER_ERROR Ensure your nn Module instance has called ` super __init__ ` istype value OptimizedModule Check optimized module disabled inspect getattr_static value forward _torchdynamo_disable False This bytecode mostly kind LOAD_ATTR LOAD_METHOD If we graph break here Dynamo does know how create continuation functions such bytecodes So we delay graph break CALL_FUNCTION msg = inspect getattr_static value forward _torchdynamo_disable_msg None DelayGraphBreakVariable source=self source msg=f Optimized ` nn Module ` wrapped ` torch compiler disable ` reason msg install_guards GuardBuilder TYPE_MATCH source = AttrSource source _orig_mod wrap_module value _orig_mod isinstance value torch nn RNN torch nn GRU torch nn LSTM config allow_rnn unimplemented_v gb_type= Attempted wrap RNN GRU LSTM context=str value explanation= Dynamo does support RNN GRU LSTM hints= graph_break_hints SUPPORTABLE getattr value _is_fsdp_managed_module False See note Dynamo treats FSDP wrapped modules UnspecializedNNModule fully_sharded_data_parallel py more information we can t do assert inside FSDP constructor since we don t know yet whether dynamo will used getattr value _fsdp_use_orig_params False unimplemented_v gb_type= FSDP use_orig_params=False context= explanation= Dynamo only supports FSDP use_orig_params=True hints= Note FSDP guarding Eager FSDP already assumes requires without enforcement users don t mutate their model parameters structure after FSDP wrapping because FSDP wouldn t notice update its FlatParams Therefore torch compile can skip guarding params submodule structure fsdp_managed modules using FSDPNNModuleSource guard source This behavior gated config skip_fsdp_guards install_guards GuardBuilder TYPE_MATCH result = FSDPManagedNNModuleVariable value source=self get_source SideEffects cls_supports_mutation_side_effects type value don t allow STORE_ATTR mutation custom __setattr__ result tx output side_effects track_object_existing value result mutation_guard is_dynamic_nn_module value tx export created dynamically don t specialize Note Tracing torch compiled function when make_fx tracing compiled function we need isinstance value torch fx experimental proxy_tensor _AttrProxy value = value get_base source = AttrProxySource source torch _dynamo config inline_inbuilt_nn_modules freezing = is_parameter_freezing Guard against case where user may overwrite named parameters named buffers NOTE This likely happen worth guarding avoid exception callable value named_parameters value named_parameters __func__ og_module_named_parameters_fn_ptr try catch TypeErrors named_parameters unserializable nn modules _ p value named_parameters mark_static_input p guard=freezing except TypeError e raise_observed_exception type e tx args=list e args callable value named_buffers value named_buffers __func__ og_module_named_buffers_fn_ptr try catch TypeErrors named_parameters unserializable nn modules _ b value named_buffers mark_static_input b guard=freezing except TypeError e raise_observed_exception type e tx args=list e args freezing we need add module tracing context order allow its params get invalidated will get cleaned up once compile ends tx output nn_modules name = value value __module__ startswith torch nn modules torch ao value __module__ startswith torch nn modules container getattr value __class__ _dynamo_marked_static False new_source = source config inline_inbuilt_nn_modules tx output export config install_free_tensors Export corner case - look test_repros py test_inlining_cornercase new_source = UnspecializedBuiltinNNModuleSource source result = UnspecializedBuiltinNNModuleVariable value source=new_source install_guard new_source make_guard GuardBuilder TYPE_MATCH new_source = source config inline_inbuilt_nn_modules tx output export config install_free_tensors Export corner case - look test_repros py test_inlining_cornercase new_source = UnspecializedNNModuleSource source result = UnspecializedNNModuleVariable value source=new_source install_guard new_source make_guard GuardBuilder TYPE_MATCH tx output add_fqn_info_for_inlined_modules value source SideEffects cls_supports_mutation_side_effects type value don t allow STORE_ATTR mutation custom __setattr__ result tx output side_effects track_object_existing value result issubclass value __class__ torch nn parallel distributed DistributedDataParallel install_guards GuardBuilder TYPE_MATCH UnspecializedNNModuleVariable value source=self get_source tx output register_attr_or_module value name source=self get_source Guards added inside register_attr_or_module wrap_literal value type value int allowlist has higher precedence over specialization control is_dynamic_source source name log debug s marked dynamic via source whitelist source name wrap_symint value dynamism=DimDynamic DYNAMIC is_unbacked_source source name log debug s marked unbacked via source whitelist source name wrap_symint value dynamism=DimDynamic SIZE_LIKE_UNBACKED config specialize_int unspecializing int default still specialize following conditions is_int_specialization_case value source recompile_hint = None source guard_source is_unspecialized_builtin_nn_module source guard_source is_unspecialized_nn_module This means integer NN module Dynamo considers nn module int attributes static good heuristic But user might want mark int attribute symint so track integer recompilation later recompile_hint = torch compile considers integer attributes nn Module static If you observing recompilation you might want make integer dynamic using torch _dynamo config allow_unspec_int_on_nn_module = True convert integer into tensor process_automatic_dynamic tx source name FrameStateSizeEntry make_scalar value is_unspecialized_nn_module=self source guard_source is_unspecialized_nn_module install_guards functools partial GuardBuilder EQUALS_MATCH recompile_hint=recompile_hint ConstantVariable create value=value source=self source wrap_symint value config specialize_float type value float wrap_symfloat value install_guards GuardBuilder CONSTANT_MATCH result = ConstantVariable create value=value source=self source isinstance value list set tx output side_effects track_mutable value result result assert_not_wrapped_by_this_graph value torch Tensor is_fake value maybe_get_fake_mode value tx fake_mode raise InternalTorchDynamoError Cannot wrap Tensor has already been wrapped instance Dynamo wrap_tensor value torch Tensor source = get_source We cannot already tracking tensor which implies would have already been wrapped assert value tx output side_effects is_static_input = get_static_address_type value None config inline_inbuilt_nn_modules is_static_input isinstance value torch nn Parameter mark tensor attributes nn modules static This done keep inline_inbuilt_nn_modules behavior compatible previous behavior source source guard_source is_unspecialized_nn_module mark_static_input value guard=is_parameter_freezing is_static_input = True Install any tensors which free variables Globals NonLocals tensors attributes nn module should_install_free_tensor = config install_free_tensors is_from_global_source source is_from_nonlocal_source source is_from_unspecialized_nn_module_source source make_graph_attribute = is_static_input config inline_inbuilt_nn_modules is_parameter_freezing torch _dynamo config prepare_freezing should_install_free_tensor source guard_source is_specialized_nn_module make_graph_attribute source guard_source is_fsdp_module assert_not_wrapped_by_this_graph value tx output register_attr_or_module value name source=source get_static_address_type value == guarded If s guarded tensor we can install parameter directly into Fx graph instead lifting input Lifting offers no benefit such regional compilation since we still guard tensor s ID Moreover installing Fx graph eliminates pre-graph bytecode required extract tensor locals globals reducing overhead This can lead significant cost savings especially optimizers handling many tensors install_guards GuardBuilder ID_MATCH assert_not_wrapped_by_this_graph value tx output register_attr_or_module value name source=source is_constant_source source assert_not_wrapped_by_this_graph value tx output register_attr_or_module value re sub r ^a-zA-Z - + _ name source=source Guards added inside register_attr_or_module NB just says we accessed tensor same source again e g tensor lives global foo we LOAD_GLOBAL twice This distinct two distinct sources mapping same Tensor per id No guard necessary here See below other case is_duplicate_tensor = source tx output input_source_to_var is_duplicate_tensor tx output input_source_to_var source options = subclass_type = infer_subclass_type value subclass_type None install_guards GuardBuilder TYPE_MATCH get_static_address_type value == guarded install_guards GuardBuilder ID_MATCH By point we should have deduplicated all tensors assert_not_wrapped_by_this_graph value isinstance value torch Tensor value is_nested isinstance value torch nested _internal nested_tensor NestedTensor unimplemented_v gb_type= Attempted wrap strided NestedTensor context= explanation= torch compile does support strided NestedTensor hints= TODO pearu sparse-team - Add corresponding SPARSE_TENSOR_MATCH guards isinstance value torch Tensor is_sparse_any value tx export config capture_sparse_compute A hot fix sparse tensors + torch compile Support export + sparsity being added we need create SPARSE_TENSOR_GUARDS guards work properly unimplemented_v gb_type= Attempted wrap sparse Tensor context= explanation= torch compile does support sparse Tensors hints= graph_break_hints SUPPORTABLE safe_has_grad value safe_grad value None value dtype = safe_grad value dtype unimplemented_v gb_type= dtype mismatch between tensor its gradient context=f tensor dtype value dtype grad dtype safe_grad value dtype explanation= Inconsistent dtype between tensor its gradient This can happen FSDP crashes meta tensor creation hints= graph_break_hints SUPPORTABLE tx output has multiple tracers we re introspecting HigherOrderOperator When we ve discovered untracked tensor then we actually need get Dynamo track tensor which what function does put graph input root tracer Later input actually used body HigherOrderOperator then relevant SubgraphTracer will lift being input subgraph See NOTE HigherOrderOperator tracing design more details example_value = wrap_to_fake_tensor_and_record value tx=self tx is_tensor=True source=source tensor_proxy = tx output root_tracer create_graph_input re sub r ^a-zA-Z - + _ name type value example_value source=source cache_real_value_when_export tx tensor_proxy value tensor_variable = wrap_fx_proxy tx=self tx proxy=tensor_proxy example_value=example_value subclass_type=subclass_type source=source options value _is_view If value view add its base tensor tracked fakes list This so we able access correct source its symbolic shape values case we need them wrap_to_fake_tensor_and_record value _base tx=self tx source=AttrSource source _base is_tensor=True guard_type = GuardBuilder TENSOR_MATCH isinstance source GradSource is_from_optimizer_source source guard_type = GuardBuilder NOT_NONE_MATCH install_guards functools partial guard_type value= value isinstance source NumpyTensorSource TensorWeakRef value We install TYPE_MATCH guards traceable wrapper subclass object recursively install corresponding guard each inner attribute is_traceable_wrapper_subclass value install_guards GuardBuilder TENSOR_SUBCLASS_METADATA_MATCH install_guards GuardBuilder TYPE_MATCH install_guard SubclassAttrListSource source make_guard GuardBuilder EQUALS_MATCH attrs _ = value __tensor_flatten__ attr attrs inner_value = getattr value attr inner_source = AttrSource source attr LazyVariableTracker realize_all VariableBuilder tx inner_source inner_value tx output input_source_to_var source = tensor_variable assert tensor_dict tensor_proxy node meta tensor_proxy node meta tensor_dict = _extract_tensor_dict value Note information conveyed via subclass_type now fake_tensor_value = tensor_variable proxy node meta example_value maybe_get_fake_mode fake_tensor_value tx fake_mode raise InternalTorchDynamoError Wrapped Tensor must graph s fake grapharg = GraphArg source value False fake_tensor_value tensor_proxy node meta grapharg = grapharg tensor_variable wrap_numpy_ndarray value assert np None assert isinstance value np ndarray source = NumpyTensorSource get_source torch _numpy _util readonly = value flags writeable readonly try value flags writeable = True except ValueError One can easily make nditer elements writable warning end world assert isinstance value base np nditer torch_function_mode_stack_state_mgr temp_restore_stack try tensor_value = _util _try_convert_to_tensor value readonly torch _prims_common clone_preserve_strides tensor_value = clone_preserve_strides tensor_value except NotImplementedError e failed convert tensor graph break unimplemented_v gb_type= failed convert numpy ndarray Tensor context=str value explanation= Exception encountered when attempting convert numpy ndarray Tensor hints= from_exc=e We do because we want full behavior guarding numpy ndarray tensor It s little annoying make VT throw out there s so many side effects here there s another great way do atm This creates right graphargs well registration guards tensor names shape env LazyVariableTracker realize_all VariableBuilder tx source tensor_value example_value = wrap_to_fake_tensor_and_record tensor_value tx=self tx is_tensor=False source=source proxy = tx output root_tracer create_graph_input re sub r ^a-zA-Z - + _ name type tensor_value example_value source=source cache_real_value_when_export tx proxy tensor_value options = source source numpy_ndarray_variable = wrap_fx_proxy_cls target_cls=NumpyNdarrayVariable tx=self tx proxy=proxy example_value=example_value options tx output input_source_to_var source = numpy_ndarray_variable example_value = numpy_ndarray_variable proxy node meta example_value pass_arg_as_tensor should true because we wrapping np ndarray argument input needs converted tensor grapharg = GraphArg source tensor_value pass_arg_as_tensor=True fake_tensor=example_value is_tensor=True example_strong_ref=tensor_value proxy node meta grapharg = grapharg TODO - Why do we need set source np ndarray vt back original source Many tests fails numpy_ndarray_variable source = source numpy_ndarray_variable wrap_symint value dynamism Optional DimDynamic = None context Optional SymIntSymbolicContext = None assert type value int name tx output unspec_variable_map tx output unspec_variable_map name shape_env = tx output shape_env TracingContext get force_unspec_int_unbacked_size_like wrapped_value = shape_env create_unbacked_symint _constrain_range_for_size wrapped_value tx output tracked_fakes append TrackedFake wrapped_value source None NB We do do float For motivation see https docs google com document d INSCdYu PxXcr HrD OudeEuS-qxQe yZmLg wy A edit general idea we generate kernels can take unspecialized floats use them sizevar computation is_constant_source get_source dynamism None torch _dynamo config specialize_int If specialize_int False also constant should have been handled caller TBH But ` dynamism ` set then actually turn into symint install_guards GuardBuilder CONSTANT_MATCH ConstantVariable create value=value source=self source name = source name frame_state_entry = process_automatic_dynamic tx name FrameStateSizeEntry make_scalar value is_unspecialized_nn_module=self source guard_source is_unspecialized_nn_module TODO This should dynamic we general do know bare integers actually going sizevars inappropriate eagerly duck size them real sizevars normalized_source_name = normalize_source_name source name base_source = source isinstance base_source ChainedSource base_source = base_source get_base dynamism None dynamic_dim = dynamism config automatic_dynamic_shapes frame_state_entry scalar auto_dynamic set_feature_use dynamo automatic_dynamic_shapes True dynamic_dim = get_automatic_dynamic_shapes_mark_as isinstance base_source LocalSource base_source dynamism None dict base_source dynamism get normalized_source_name False config assume_static_by_default dynamic_dim = DimDynamic DYNAMIC assume_static_by_default TODO dynamic_dim = DimDynamic STATIC should work some reason doesn t frame_state_entry scalar auto_dynamic set_feature_use dynamo automatic_dynamic_shapes False install_guards GuardBuilder CONSTANT_MATCH ConstantVariable create value=value wrapped_value = shape_env create_unspecified_symint_and_symbol value source=self source dynamic_dim=dynamic_dim tx output tracked_fakes append TrackedFake wrapped_value source context assert is_constant_source get_source TODO Do I actually need guard constant source install_guards GuardBuilder CONSTANT_MATCH ConstantVariable create value=value source=self source assert isinstance get_source RandomValueSource install_guard get_source make_guard GuardBuilder TYPE_MATCH options = source get_source proxy = tx output root_tracer create_graph_input re sub r ^a-zA-Z - + _ name type wrapped_value wrapped_value source=self get_source sym_expr = wrapped_value node expr assert isinstance sym_expr sympy Symbol f sym_expr basic Symbol tx output root_tracer bound_symbols sym_expr = proxy unspec_var = SymNodeVariable proxy wrapped_value options tx output unspec_variable_map name = unspec_var is_constant_source get_source proxy node meta grapharg = GraphArg get_source wrapped_value pass_arg_as_tensor=False fake_tensor=None is_tensor=False example_strong_ref=wrapped_value unspec_var wrap_symfloat value SymFloat wrapping special We first wrap same way we do unspecialized primitive then we item into SymFloat Removal item call left later FX pass mostly because pass more easily done after we have lowered ATen ops Dynamo doesn t do decomposition right now name tx output unspec_variable_map tx output unspec_variable_map name frame_state_entry = process_automatic_dynamic tx source name FrameStateSizeEntry make_scalar value is_unspecialized_nn_module=self source guard_source is_unspecialized_nn_module NB we specialize nan input because our guard modeling ShapeEnv cannot deal nan torch _dynamo config specialize_float is_constant_source get_source math isnan value math isinf value We don t support cudagraphs now Without cudagraphs break because they expect all cuda inputs our tensorified float will f cpu tensor Fixes following test when specialize_float=False python test inductor test_compiled_optimizers py CompiledOptimizerTests test_rmsprop_weight_decay_maximize_capturable_cuda noqa B torch _inductor config triton cudagraphs justknobs_check pytorch compiler unspecialize_float_killswitch False config assume_static_by_default frame_state_entry scalar auto_dynamic install_guards GuardBuilder CONSTANT_MATCH ConstantVariable create value=value source=self source NB At point we ve gotten here we don t assume static default Since we have guard mechanism there isn t really any downside trying dynamic float all time Unlike ints won t make codegen perf worse Modest cost compile time wrapped_value = torch tensor value dtype=torch float We don t support specializing floats grad checking tensors See https github com pytorch pytorch pull more context torch _C _functorch is_gradtrackingtensor wrapped_value install_guards GuardBuilder CONSTANT_MATCH ConstantVariable create value=value source=self source TODO Switch RandomValueSource over use more accurate assert isinstance get_source RandomValueSource install_guard get_source make_guard GuardBuilder TYPE_MATCH The FloatTensorSource here just pedantic correctness you guard against UnspecializedPythonVariable you need guard against tensor-ified version local otherwise s Tensor However we never let UnspecializedPythonVariable escape here so there should never actually any guards against source source = FloatTensorSource get_source options = source source raw_value value TODO Maybe tensor-ification should built into source rather than special pattern match example_value = wrap_to_fake_tensor_and_record wrapped_value tx=self tx is_tensor=False source=source proxy = tx output root_tracer create_graph_input re sub r ^a-zA-Z - + _ name type wrapped_value example_value source=source cache_real_value_when_export tx proxy wrapped_value unspec_var = wrap_fx_proxy_cls UnspecializedPythonVariable tx=self tx proxy=proxy example_value=example_value options assert isinstance unspec_var UnspecializedPythonVariable tx output unspec_variable_map name = unspec_var tx export isinstance get_source LocalSource raise AssertionError f Dynamo attempts add additional input during export value= wrapped_value source= get_source fake_tensor_value = None example_value = unspec_var proxy node meta example_value assert is_fake example_value fake_tensor_value = example_value assert fake_tensor_value fake_mode tx fake_mode f fake mode fake_tensor_value fake_mode fake tensor metadata doesn t match mode tx fake_mode InstructionTranslator There s something bit incoherent about pass_arg_as_tensor specifically regarding sources Specifically suppose we have x float local argument We eventually end up UnspecializedPythonVariable denoting torch as_tensor x s source still L x which you accessed directly float So you gotta careful when setting up your guards because s still going float point conversion happens only precisely point we re actually calling FX graph This happens what we want shape guard generation s kind unintuitive proxy node meta grapharg = GraphArg get_source wrapped_value pass_arg_as_tensor=True fake_tensor=fake_tensor_value is_tensor=False example_strong_ref=wrapped_value Directly do item bypass capture_scalar_outputs r = wrap_fx_proxy tx tx output create_proxy call_method item proxy_args_kwargs unspec_var tx output tracked_fakes append TrackedFake r sym_num source None get_metrics_context set tensorify_float_attempt True overwrite=True r wrap_unspecialized_primitive value name tx output unspec_variable_map tx output unspec_variable_map name wrapped_value = torch tensor value isinstance get_source RandomValueSource install_guard get_source make_guard GuardBuilder TYPE_MATCH options = source get_source options update raw_value value example_value = wrap_to_fake_tensor_and_record wrapped_value tx=self tx is_tensor=False source=self get_source proxy = tx output root_tracer create_graph_input re sub r ^a-zA-Z - + _ name type wrapped_value example_value source=self get_source cache_real_value_when_export tx proxy wrapped_value unspec_var = wrap_fx_proxy_cls UnspecializedPythonVariable tx=self tx proxy=proxy example_value=example_value options tx output unspec_variable_map name = unspec_var is_constant_source get_source tx export isinstance get_source LocalSource raise AssertionError f Dynamo attempts add additional input during export value= wrapped_value source= get_source fake_tensor_value = None isinstance unspec_var ConstantVariable TODO when can happen example_value = unspec_var value example_value = unspec_var proxy node meta example_value assert is_fake example_value fake_tensor_value = example_value assert fake_tensor_value fake_mode tx fake_mode f fake mode fake_tensor_value fake_mode fake tensor metadata doesn t match mode tx fake_mode InstructionTranslator proxy node meta grapharg = GraphArg get_source wrapped_value pass_arg_as_tensor=True fake_tensor=fake_tensor_value is_tensor=False example_strong_ref=wrapped_value unspec_var _dataclasses_fields_lambda obj isinstance obj UserDefinedObjectVariable value = obj value unimplemented_v gb_type= dataclass fields failure context=f obj obj variable type type obj explanation=f Dataclass fields handling fails obj Expected user-defined object hints= items = field dataclasses fields value source = None obj source base_src = AttrSource obj source __dataclass_fields__ source = DictGetItemSource base_src field name items append UserDefinedObjectVariable field source=source TupleVariable items _clone_input value fake_mode isinstance value torch Tensor tensor subclasses will converted FakeTensors need cloned isinstance value FakeTensor Is functional tensor fakeified instance Dynamo torch _is_functional_tensor value maybe_get_fake_mode value fake_mode value is_nested NB ensure strides preserved value = clone_input value value wrap_fx_proxy tx proxy example_value=None subclass_type=None options - VariableTracker kwargs = tx tx proxy proxy example_value example_value subclass_type subclass_type options subclass_type None wrap_fx_proxy_cls target_cls=TensorVariable kwargs result = wrap_fx_proxy_cls target_cls=TensorWithTFOverrideVariable kwargs result install_global tx result cache_real_value_when_export tx proxy example_value tx export The legacy behavior real value cache subclasses perform clone WITHOUT preserving subclass It s entirely clear what you actually want though torch _C DisableTorchFunctionSubclass proxy tracer real_value_cache proxy node = _clone_input example_value tx fake_mode Note Unfortunate split due some gross classes existing subclass TensorVariable Should compositional instead This horribly complicated function does too many things explain what does let s first talk about classic usage wrap_fx_proxy TensorVariable There two primary modes use Wrapping pre-existing Tensor In case example_value set pre-existing Tensor Note example_value will NOT final example_value we put into node meta example_value instead converted into fake tensor using wrap_to_fake_tensor_and_record registered graph input Wrapping result some Tensor operation Dynamo traced over In case example_value None we going figure out ourselves using FakeTensors via get_fake_value which will run operation represented singular FX node referenced passed proxy The expectation you end up Tensor output everything straightforwardly traced into graph In all cases returned ` TensorVariable ` subclass will have ` example_value ` ` example_value ` must ` FakeTensor ` produced currently running instance Dynamo Upon closer inspection you may notice there slurry non-Tensor output cases handle_traced_output What gives Well we sometimes trace operations into graph don t involve tensors Some operators tuples we need recursively handle their contents Some operators have side effects will affect subsequent AOTAutograd tracing don t otherwise anything Some operators symbolic ints floats bools which can go graph traced only they re actually symbolic If they re static you don t want put them graph which means you shouldn t call function The common theme you only use function WHEN YOU ARE TRACING SOMETHING INTO THE GRAPH This sort obvious because you can t call function without proxy wrap_fx_proxy_cls target_cls tx proxy example_value=None subclass_type=None options example_value None _wrap_fx_proxy target_cls tx proxy example_value subclass_type options isinstance example_value torch Tensor _wrap_fx_preexisting_tensor target_cls tx proxy example_value subclass_type options This will skip tracing op recursively reinvoke wrap_fx_proxy_cls supported data structures In essence just handles tracing some other value which may contain Fake Tensors otherwise proxyable handle_traced_output example_value tx proxy options subclass_type target_cls This above wrapping preexisting tensor _wrap_fx_preexisting_tensor target_cls tx proxy tensor subclass_type=None options symbolic_convert InstructionTranslatorBase assert isinstance tensor torch Tensor f _wrap_fx_preexisting_tensor expected tensor got type tensor assert isinstance tx InstructionTranslatorBase guards options options guards None tx output guards update options guards Placeholders always carry example_value node meta non-placeholders always have no example_value node meta proxy node op == placeholder assert example_value proxy node meta f placeholder proxy doesn t have example_value node meta assert example_value proxy node meta f proxy node meta example_value See NOTE Deferring tensor pack unpack hooks until runtime torch _dynamo utils _disable_saved_tensors_hooks_during_tracing Handle recursive calls here maybe_get_fake_mode tensor tx fake_mode pass cache_real_value_when_export tx proxy tensor tx export The legacy behavior real value cache subclasses perform clone WITHOUT preserving subclass It s entirely clear what you actually want though torch _C DisableTorchFunctionSubclass proxy tracer real_value_cache proxy node = _clone_input tensor tx fake_mode NB If we re ignoring subclass then expectation you will take returned TensorVariable wrap into more accurate TensorVariable able track subclass-ness otherwise wrong kwargs = is_tensor target_cls TensorVariable TensorWithTFOverrideVariable assert source options options source None kwargs source = options source tensor = wrap_to_fake_tensor_and_record tensor tx=tx kwargs tensor device type = meta maybe_get_fake_mode tensor tx fake_mode raise InternalTorchDynamoError ` tensor ` needs ` FakeTensor ` f wrapped instance Dynamo Found tensor construct_tensor_variable target_cls tx proxy tensor subclass_type options This above comment wrapping output traced op _wrap_fx_proxy target_cls tx proxy example_value=None subclass_type=None options symbolic_convert InstructionTranslatorBase assert isinstance tx InstructionTranslatorBase guards options options guards None tx output guards update options guards assert example_value proxy node meta f proxy node meta example_value See NOTE Deferring tensor pack unpack hooks until runtime torch _dynamo utils _disable_saved_tensors_hooks_during_tracing preserve_rng_state only allow_non_graph_fake instance because we handle non-fake cases properly below example_value = get_fake_value proxy node tx allow_non_graph_fake=True handle_traced_output example_value tx proxy options subclass_type target_cls This handles wrapping output op traced into graph handle_traced_output example_value tx proxy options subclass_type target_cls torch _functorch vmap torch _subclasses fake_tensor torch _utils isinstance example_value torch Tensor Check result sparse tensor - We generally don t support sparse tensor so better graph break here is_sparse_any example_value tx export config capture_sparse_compute unimplemented_v gb_type= Attempted wrap sparse Tensor VariableTracker context=str example_value explanation= torch compile does support sparse Tensors VariableTracker hints= graph_break_hints SUPPORTABLE var = construct_tensor_variable target_cls tx proxy example_value subclass_type options NOTE Side effect tracking newly constructed tensor For newly constructed objects have mutable attributes we usually construct their VariableTracker via ` track_object_new ` since tensor variable construction bit different we handle them specially here This ensures codegen will actually generate attribute mutations tensor NOTE we pass dummy object ` item ` argument avoid constructing dummy _tensor_ object The object isn t used newly constructed VTs anyways tx output side_effects _track_obj proxy var mutation_type_cls=AttributeMutationNew var hasattr proxy node target __name__ proxy node target __name__ == set_state isinstance proxy node target __self__ torch _C Generator proxy node target torch random set_rng_state TorchInGraphFunctionVariable proxy node target proxy node target torch _C _DisableFuncTorch proxy node target torch cuda _is_in_bad_fork UserDefinedObjectVariable example_value istype example_value torch Size all isinstance x int x example_value sizes = ConstantVariable create x x example_value SizeVariable sizes options isinstance example_value tuple list set_example_value proxy node example_value unpacked = i val enumerate example_value val None nn MultiheadAttention can None see issue unpacked append ConstantVariable create None options proxy_i = proxy tracer create_proxy kind= call_function target=operator getitem args= proxy i kwargs= source options This path should only trigger list stealing so s safe use ` GetItemSource ` assert isinstance example_value list source = options source options_i = options copy options_i source = GetItemSource base=source index=i index_is_slice=False use same options object parent options_i = options WARNING assumes same target_cls tuple list call unpacked append wrap_fx_proxy_cls target_cls=target_cls tx=tx proxy=proxy_i example_value=val options_i isinstance example_value torch Size NB Keep old proxy around See SizeVariable explanation why SizeVariable unpacked proxy options istype example_value tuple TupleVariable unpacked options istype example_value list immutable_list ListVariable unpacked options assert example_value __class__ __module__ == torch return_types hasattr example_value _fields f expected example_value __class__ __module__ == torch return_types named tuple got type example_value NamedTupleVariable unpacked example_value __class__ options example_value None proxy node target torch manual_seed ConstantVariable create None options isinstance example_value torch SymInt torch SymFloat torch SymBool tx output current_tracer track_produced_symints example_value proxy set_example_value proxy node example_value SymNodeVariable proxy example_value options isinstance example_value torch Stream proxy node target get_external_object_by_index torch accelerator current_stream proxy node target device_interface current_stream _ device_interface get_registered_device_interfaces set_example_value proxy node example_value StreamVariable proxy example_value options inspect isclass proxy node target issubclass proxy node target torch Event proxy node target device_interface Event _ device_interface get_registered_device_interfaces set_example_value proxy node example_value EventVariable proxy example_value options proxy node target == query proxy node op == call_method set_example_value proxy node example_value ConstantVariable example_value options example_value None isinstance example_value torch Event proxy node target == record_event proxy node op == call_method set_example_value proxy node example_value EventVariable proxy example_value options isinstance example_value int proxy node target torch sym_int getattr operator getitem torch _utils _element_size torch seed operator mod torch _functorch vmap _validate_and_get_batch_size torch _functorch predispatch _vmap_increment_nesting torch _functorch predispatch _vmap_decrement_nesting some mac builds missing torch distributed get_rank getattr torch distributed get_rank _missing getattr torch distributed get_world_size _missing This always wants graph even constraint results constant int torch _constrain_as_size TODO little sus because we didn t check what proxy node op == call_method proxy node target == bit_length set_example_value proxy node example_value ConstantVariable create example_value options isinstance example_value torch backends cuda SDPAParams sdpa SDPAParamsVariable set_example_value proxy node example_value SDPAParamsVariable proxy options isinstance example_value bool proxy node target torch _C _are_functorch_transforms_active torch _C _functorch is_batchedtensor torch backends cuda is_flash_attention_available torch backends cuda can_use_flash_attention torch backends cuda can_use_efficient_attention torch _C _get_cudnn_sdp_enabled torch _C _get_flash_sdp_enabled torch _C _get_mem_efficient_sdp_enabled torch _C _get_math_sdp_enabled torch _C _get_overrideable_sdp_enabled is_integer + list supported_const_comparison_op_values keys set_example_value proxy node example_value ConstantVariable create example_value options isinstance example_value int float bool proxy node target call_torchbind proxy node target flat_apply proxy node op == call_method proxy node target == item set_example_value proxy node example_value ConstantVariable create example_value options isinstance example_value float proxy node target hex __round__ set_example_value proxy node example_value ConstantVariable create example_value options unimplemented_v gb_type= torch op returned non-Tensor context=f example_value type typestr example_value op proxy node op target proxy node target explanation= torch ops non-Tensor cannot traced into Dynamo FX graph output hints= infer_subclass_type value type value torch Tensor torch nn Parameter torch _subclasses fake_tensor FakeTensor torch _subclasses functional_tensor FunctionalTensor is_traceable_wrapper_subclass value Ordinarily we would fakeify tensor so can get dynamic shapes computed without triggering actual operations However how can we fakeify tensor subclass Ordinary inheritance nor multiple inheritance won t work work Instead our plan manually simulate tensor subclass inheriting fake tensor dynamo This means our data representation tensor subclass will fake tensor + tensor subclass type + any extra data subclass may have been storing tensor Because all Python accesses mediated through TensorWithTFOverrideVariable we can ensure we dispatch differently e g according __torch_function__ To simplify things now __dict__ tracking bits haven t been implemented yet they can added into design later point time None type value get_specialized_props target_cls tx example_value subclass_type specialized_props = target_cls specialize example_value TODO sure about fake mode test isinstance example_value torch _subclasses fake_tensor FakeTensor example_value fake_mode tx fake_mode subclass_type tensor_type = subclass_type isinstance example_value torch nn Parameter tensor_type = torch nn Parameter isinstance example_value torch nn Buffer tensor_type = torch nn Buffer tensor_type = torch Tensor specialized_props class_type = tensor_type specialized_props construct_tensor_variable target_cls tx proxy example_value subclass_type options Actually construct tensor variable after all pre-processing wrapping pre-existing newly created tensor value NB In most all cases does actually do clone WARNING means we mutate metadata fake tensor stored example value will update too example_value = _clone_input example_value tx fake_mode set_example_value proxy node example_value We bind unbacked symints sizes trdies tensor lazily So subgraphs can access unbacked symbol s proxy parent graph when lifting unbacked symbols input tensors subgraph inputs We do lazily because tensor may used subgraphs proxy node op = placeholder tx output current_tracer track_produced_symints example_value proxy options update get_specialized_props target_cls tx example_value subclass_type target_cls proxy options get_automatic_dynamic_shapes_mark_as config automatic_dynamic_shapes_mark_as == dynamic DimDynamic DYNAMIC config automatic_dynamic_shapes_mark_as == unbacked DimDynamic SIZE_LIKE_UNBACKED config automatic_dynamic_shapes_mark_as == oblivious DimDynamic OBLIVIOUS_SIZE raise ValueError f invalid automatic_dynamic_shapes_mark_as = config automatic_dynamic_shapes_mark_as _DYNAMIC_SOURCES Optional set str = None _DYNAMIC_SOURCES_CONFIG_HASH Optional int = None get_dynamic_sources - set str global _DYNAMIC_SOURCES _DYNAMIC_SOURCES_CONFIG_HASH current_hash = hash torch compiler config dynamic_sources If we have already calculated sources config hasn t changed cached result _DYNAMIC_SOURCES None _DYNAMIC_SOURCES_CONFIG_HASH == current_hash _DYNAMIC_SOURCES Config has changed first time re calculate sources _DYNAMIC_SOURCES = s s torch compiler config dynamic_sources replace split s _DYNAMIC_SOURCES_CONFIG_HASH = current_hash _DYNAMIC_SOURCES is_dynamic_source source_name str - bool dynamic_sources = get_dynamic_sources pattern dynamic_sources pattern == source_name re match pattern source_name log debug s marked dynamic due dynamic source allowlist pattern s source_name pattern True False record_automatic_dynamic tx InstructionTranslator name str e torch Tensor - FrameStateSizeEntry This mimics stride inference algorithm _create_symbolic_sizes_strides_storage_offset ex_size = e size is_sparse_any e ex_stride = e stride dim = e dim stride = None dim pending = ex_stride i -i i range dim pending sort key=_nested_int_aware_sort candidates = i_stride neg_i pending i = -neg_i stride i = candidates get i_stride i_stride candidates setdefault i_stride ex_size i InferStride i stride = process_automatic_dynamic tx name FrameStateSizeEntry make_tensor tuple ex_size tuple stride _UNBACKED_SOURCES Optional set str = None _UNBACKED_SOURCES_CONFIG_HASH Optional int = None get_unbacked_sources - set str global _UNBACKED_SOURCES _UNBACKED_SOURCES_CONFIG_HASH current_hash = hash torch compiler config unbacked_sources If we have already calculated sources config hasn t changed cached result _UNBACKED_SOURCES None _UNBACKED_SOURCES_CONFIG_HASH == current_hash _UNBACKED_SOURCES Config has changed first time re calculate sources _UNBACKED_SOURCES = s s torch compiler config unbacked_sources replace split s _UNBACKED_SOURCES_CONFIG_HASH = current_hash _UNBACKED_SOURCES is_unbacked_source source_name str - bool unbacked_sources = get_unbacked_sources pattern unbacked_sources pattern == source_name re match pattern source_name log debug s marked unbacked due unbacked source allowlist pattern s source_name pattern True False Performs automatic dynamic dim determination Returns SymbolicContext _automatic_dynamic e tx source static_shapes outer_only=False - SymbolicContext strided NT supported e is_nested isinstance e torch nested _internal nested_tensor NestedTensor unimplemented_v gb_type= Encountered strided NestedTensor automatic dynamic dim determination context= explanation= torch compile does support strided NestedTensor hints= name = source name prior_policy = tx output tracing_context tensor_to_context get e None shape_env_to_source_to_symbol_cache = prior_policy shape_env_to_source_to_symbol_cache prior_policy None Get base context tensor view view_base_context Optional SymbolicContext = None e _is_view base_source = AttrSource source _base view_base_context = _automatic_dynamic e _base tx base_source static_shapes is_traceable_wrapper_subclass e outer_only Get symbolic context outer tensor outer_context = _automatic_dynamic e tx source static_shapes outer_only=True Get symbolic contexts inner tensors inner_contexts = mapping attr - symbolic context attrs _ = type e __tensor_flatten__ e attr attrs inner_tensor = getattr e attr inner_source = AttrSource source attr inner_contexts attr = _automatic_dynamic inner_tensor tx inner_source static_shapes SubclassSymbolicContext dynamic_sizes=outer_context dynamic_sizes dynamic_strides=outer_context dynamic_strides constraint_sizes=outer_context constraint_sizes constraint_strides=outer_context constraint_strides view_base_context=view_base_context tensor_source=outer_context tensor_source shape_env_to_source_to_symbol_cache=outer_context shape_env_to_source_to_symbol_cache inner_contexts=inner_contexts static_shapes is_dynamic_source name StatefulSymbolicContext dynamic_sizes= DimDynamic STATIC e dim dynamic_strides= DimDynamic INFER_STRIDE e dim constraint_sizes= None e dim constraint_strides= None e dim view_base_context=view_base_context tensor_source=source shape_env_to_source_to_symbol_cache=shape_env_to_source_to_symbol_cache We preserve dynamism inputs For example when users call make_fx torch cond tracing_mode= symbolic args inputs have SymInt sizes torch fx experimental symbolic_shapes is_nested_int any isinstance s SymInt is_nested_int s s e size StatefulSymbolicContext dynamic_sizes= DimDynamic DYNAMIC isinstance s SymInt DimDynamic STATIC s e size dynamic_strides= DimDynamic INFER_STRIDE e dim constraint_sizes= None e dim constraint_strides= None e dim view_base_context=view_base_context tensor_source=source shape_env_to_source_to_symbol_cache=shape_env_to_source_to_symbol_cache Prep automatic dynamic frame_state_entry = record_automatic_dynamic tx name e TODO index export_constraints ahead time so we don t have do linear scan every time here t_id = id e dim constraint = update_dim constraint dim constraint_range name dim dim constraint torch fx experimental symbolic_shapes StrictMinMaxConstraint old_constraint_range old_name = dim constraint dim new_constraint_range = StrictMinMaxConstraint vr=constraint_range vr old_constraint_range vr warn_only=False It possible non-None old_name name different will only happen corresponding Dims can derived equal new_name = old_name name dim constraint dim = new_constraint_range new_name dim constraint dim = constraint_range name torch export dynamic_shapes _RelaxedConstraint tx output export_constraints constraint tx output export_constraints isinstance constraint _RelaxedConstraint continue constraint t_id == t_id update_dim constraint constraint dim constraint constraint_range constraint name dynamic_sizes = dynamic_strides = constraint_sizes = constraint_strides = specialize_on = i range e dim NB mark dynamic has precedence over static marked_strict_unbacked = i getattr e _dynamo_strict_unbacked_indices set marked_unbacked = i getattr e _dynamo_unbacked_indices set marked_dynamic = i getattr e _dynamo_dynamic_indices set marked_weak_dynamic = i getattr e _dynamo_weak_dynamic_indices set marked_static = i getattr e _dynamo_static_indices set specialize_on append getattr e _specialize_on get i Reflect user directive frame_state For dynamic apply None always normalized_source_name = normalize_source_name source name base_source = source isinstance base_source ChainedSource base_source = base_source get_base marked_dynamic isinstance base_source LocalSource base_source dynamism None dict base_source dynamism get normalized_source_name i False i TODO This can batched TODO Doing here kind sus maybe better set up when we initially created FrameStateSizeEntry bong into mutable state log debug automatic dynamic s marked dynamic name mark_size = auto_unset e dim mark_size i = auto_dynamic frame_state_entry &#124; = FrameStateSizeEntry make_size size=mark_size NB both static dynamic have precedence over automatic_dynamic_size = config automatic_dynamic_shapes frame_state_entry is_size_dynamic i NB previously size dynamic we wouldn t make its stride dynamic But now because InferStride concept we will properly make stride dynamic even s wobbling automatic_dynamic_stride = config automatic_dynamic_shapes frame_state_entry is_stride_dynamic i is_dynamic_source name log debug s marked dynamic via source whitelist name automatic_dynamic_size = True is_unbacked_source name log debug s marked unbacked via source whitelist name automatic_dynamic_size = True automatic_dynamic = automatic_dynamic_size automatic_dynamic_stride We will process constraints first they will imply we have dynamic dimension Precedence export constraints eager constraints constraint = dim constraint get i constraint None constraint_size = None constraint_stride = None marked_dynamic config allow_ignore_mark_dynamic constraint_stride deliberaly kept None because no easy way provide value ranges mark dynamic constraint_stride = None hasattr e _dynamo_dynamic_range dim_range = dr dr e _dynamo_dynamic_range dr dim == i pop dim_range min None dim_range max None constraint_size = RelaxedUnspecConstraint warn_only=False torch fx experimental symbolic_shapes StrictMinMaxConstraint constraint_size = StrictMinMaxConstraint vr=ValueRanges lower=dim_range min upper=dim_range max warn_only=False constraint_size = RelaxedUnspecConstraint warn_only=False marked_strict_unbacked constraint_size = RelaxedUnspecConstraint warn_only=False marked_static automatic_dynamic set_feature_use dynamo automatic_dynamic_shapes True automatic_dynamic_size constraint_size = RelaxedUnspecConstraint warn_only=True automatic_dynamic_stride constraint_stride = RelaxedUnspecConstraint warn_only=True marked_static config automatic_dynamic_shapes set_feature_use dynamo automatic_dynamic_shapes False constraint_size = None constraint_stride = None constraint_size name_ = constraint constraint_stride = None dim_name = f name size i tx output shape_env source_name_to_debug_name dim_name = name_ constraint_sizes append constraint_size constraint_strides append constraint_stride marked_unbacked is_unbacked_source name dynamic_size = DimDynamic SIZE_LIKE_UNBACKED constraint_size None marked_dynamic marked_weak_dynamic is_nested_int e size i NB We could assert static_shapes False here seems better allow user override symbolic_context case automatic_dynamic dynamic_size = get_automatic_dynamic_shapes_mark_as dynamic_size = DimDynamic DYNAMIC static_shapes config assume_static_by_default marked_static dynamic_size = DimDynamic STATIC TODO When does show up dynamic_size = DimDynamic DUCK constraint_stride None dynamic_stride = DimDynamic DYNAMIC dynamic_stride = DimDynamic INFER_STRIDE dynamic_sizes append dynamic_size dynamic_strides append dynamic_stride StatefulSymbolicContext dynamic_sizes=dynamic_sizes dynamic_strides=dynamic_strides constraint_sizes=constraint_sizes constraint_strides=constraint_strides specialize_on=specialize_on view_base_context=view_base_context tensor_source=source shape_env_to_source_to_symbol_cache=shape_env_to_source_to_symbol_cache See note Tensor Fakification Symbol Caching wrap_to_fake_tensor_and_record e tx source Optional Source is_tensor bool parent_context=None type e torch Tensor torch nn Parameter FakeTensor isinstance e torch Tensor is_traceable_wrapper_subclass e assert source None static_shapes _reason = tensor_always_has_static_shape e is_tensor tensor_source=source parent_context symbolic_context = _automatic_dynamic e tx source static_shapes Parent contexts passed when we recursively creating fake tensors subclasses A better design would create parent child relationship recursively call _automatic_dynamic we recursively call wrap_to_fake_tensor_and_record This runs into bugs around how meta_utils knows works create fake tensors tensor subclasses Ideally dynamo would drive both recursive wrap_to_fake_tensor_and_record _automatic_dynamic policy creation assert isinstance source AttrSource inner_context_name = source member symbolic_context = parent_context inner_contexts inner_context_name log debug wrap_to_fake s s s s source name tuple e shape symbolic_context type e Note enable_python_dispatcher dynamo Dynamo disables itself when runs fake tensor prop which means tensor subclasses have no way know purely based off global state they currently being run under compile we use enable_python_dispatcher mainly tweak DispatchKeyState so subclass authors can check know they running eager context enable_python_dispatcher fake_e = wrap_fake_exception lambda tx fake_mode from_tensor e source=source symbolic_context=symbolic_context source None isinstance fake_e FakeTensor sym_val = fake_e item_memo None tx output tracked_fakes append TrackedFake sym_val CallMethodItemSource source symbolic_context is_traceable_wrapper_subclass fake_e attrs _ = fake_e __tensor_flatten__ attr attrs fake_inner = getattr fake_e attr inner = getattr e attr inner_source = AttrSource source attr wrap_to_fake_tensor_and_record inner tx source=inner_source is_tensor=isinstance fake_inner torch Tensor parent_context=symbolic_context tx output tracing_context tensor_to_context e = symbolic_context is_sparse_any fake_e TODO TensorGuards eventually may need more fields size stride any other constituents values = fake_e _values fake_e is_sparse fake_e values tx output input_source_to_sizes_strides source = size fake_e size TODO revise now stride instead avoids SegFault PYTORCH_TEST_WITH_DYNAMO= stride fake_e ndim values_size values size values_stride values stride tx output input_source_to_sizes_strides source = size fake_e size stride fake_e stride is_tensor static_shapes source is_specialized_nn_module is_constant_source source tx output tracked_fakes append TrackedFake fake_e source symbolic_context tx output tracked_fakes_id_to_source id e append source fake_e e SourcelessBuilder Like builder stateless does require source Useful simple type- VT objects objects being created evaporated during inlining ex consider locally made list tensors we then iterate over such list should show up artifact inputs nor reconstruction nor graph However there may reasons represent ListVariable internally NOTE - Objects produced here born UNGUARDED due nature sources NOTE - This very new It will have some rough edges created stem bleeding giant type- VariableTracker trees cropping up all over dynamo __init__ - None raise AssertionError Use SourcelessBuilder create staticmethod create tx InstructionTranslator value - VariableTracker value_type = type value fast_handler = SourcelessBuilder _type_handlers get value_type fast_handler fast_handler tx value isinstance value VariableTracker This always valid call useful recursive calls value isinstance value dataclasses _HAS_DEFAULT_FACTORY_CLASS UserDefinedObjectVariable value ConstantVariable is_literal value ConstantVariable create value callable value trace_rules lookup_callable value None trace_rules is_callable_allowed value tx output has_user_defined_allowed_in_graph = True trace_rules lookup_callable value value callable value UserDefinedClassVariable is_supported_new_method value NamedTuple _make uses alias tuple __new__ obj = trace_rules lookup_callable value __self__ value __self__ GetAttrVariable obj __new__ is_function_or_wrapper value trace_rules lookup value value isinstance value enum Enum torch DispatchKey torch _C _functorch TransformType EnumVariable value isinstance value type abc ABCMeta UserDefinedClassVariable value isinstance value types MethodWrapperType MethodWrapperVariable value isinstance value types MethodType We only want support sourceless objects here An instance variable allowed should have source isinstance value __self__ type abc ABCMeta value classmethod assert getattr value __self__ value __func__ __name__ == value cls_obj_vt = SourcelessBuilder create tx value __self__ try cls_obj_vt var_getattr tx value __func__ __name__ except NotImplementedError pass failthrough unimplemented branch isinstance value torch fx graph_module GraphModule SourcelessGraphModuleVariable value isinstance value torch utils _pytree TreeSpec UserDefinedObjectVariable value PlacementVariable is_placement value PlacementVariable value DeviceMeshVariable is_device_mesh value DeviceMeshVariable value value functools wraps FunctoolsWrapsVariable value isinstance value re Pattern RegexPatternVariable value isinstance value torch _dynamo variables lazy LazySymNodeFormatString ConstantVariable create str value isinstance value type torch _higher_order_ops flex_attention_backward torch _dynamo variables higher_order_ops FlexAttentionBackwardHighOrderVariable value isinstance value types GenericAlias types UnionType TypingVariable value is_namedtuple value output = SourcelessBuilder create tx getattr value name name namedtuple_fields type value NamedTupleVariable output tuple_cls=type value isinstance value torch SymInt value node expr tx output bound_symbols proxy = tx output bound_symbols value node expr SymNodeVariable create tx proxy unimplemented_v gb_type= Unexpected type sourceless builder context=f value_type __module__ value_type __qualname__ explanation=f SourcelessBuilder create does know how wrap value_type hints= graph_break_hints DYNAMO_BUG staticmethod wrap_constant_literal value assert ConstantVariable is_literal value ConstantVariable create value=value staticmethod make_type_handlers create = SourcelessBuilder create handlers = t common_constant_types handlers t = lambda tx value ConstantVariable value handlers set = lambda tx value SetVariable create tx x x value mutation_type=ValueMutationNew handlers dict = lambda tx value ConstDictVariable create tx k create tx v k v value items type value mutation_type=ValueMutationNew handlers list = lambda tx value ListVariable create tx x x value mutation_type=ValueMutationNew handlers tuple = lambda tx value TupleVariable create tx x x value handlers torch Size = lambda tx value SizeVariable create tx x x value handlers collections OrderedDict = handlers dict handlers immutable_dict = handlers dict handlers immutable_list = handlers list handlers random Random = lambda tx value RandomClassVariable handlers types ModuleType = lambda tx value PythonModuleVariable value handlers torch DispatchKeySet = lambda tx value DispatchKeySetVariable value mutation_type=ValueMutationNew handlers torch _functorch pyfunctorch FuncTorchInterpreter = lambda tx value FuncTorchInterpreterVariable value mutation_type=ValueMutationNew handlers torch distributions constraints _Real = lambda tx value UserDefinedObjectVariable value mutation_type=ValueMutationNew handlers torch distributions constraints _Interval = lambda tx value UserDefinedObjectVariable value mutation_type=ValueMutationNew handlers torch distributions constraints Constraint = lambda tx value UserDefinedObjectVariable value mutation_type=ValueMutationNew passthrough tx InstructionTranslator value value cls VariableTrackerMeta all_subclasses handlers cls = passthrough handlers SourcelessBuilder _type_handlers = SourcelessBuilder make_type_handlers SourcelessUserDefinedObjectBuilder SourceLessBuilder does UserDefinedObjectVariable some cases might ok UserDefinedObjects In such case use builder __init__ - None raise AssertionError Use SourcelessUserDefinedObjectBuilder create staticmethod create tx InstructionTranslator value - VariableTracker value_type = type value issubclass value_type MutableMapping MutableMappingVariable value mutation_type=ValueMutationNew isinstance value torch nn Module UnspecializedNNModuleVariable value mutation_type=ValueMutationNew UserDefinedObjectVariable value mutation_type=ValueMutationNew