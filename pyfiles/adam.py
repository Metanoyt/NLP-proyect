mypy allow-untyped-defs typing cast Optional Union torch torch Tensor optimizer _capturable_doc _default_to_fused_or_foreach _device_dtype_check_for_fused _differentiable_doc _disable_dynamo_if_unsupported _foreach_doc _fused_doc _get_capturable_supported_devices _get_scalar_dtype _get_value _maximize_doc _params_doc _stack_if_compiling _to_scalar _use_grad_for_differentiable _view_as_real DeviceDict DeviceDtypeDict Optimizer ParamsT __all__ = Adam adam Adam Optimizer __init__ params ParamsT lr Union float Tensor = e- betas tuple Union float Tensor Union float Tensor = eps float = e- weight_decay float = amsgrad bool = False foreach Optional bool = None maximize bool = False capturable bool = False differentiable bool = False fused Optional bool = None decoupled_weight_decay bool = False isinstance lr Tensor foreach capturable raise ValueError lr Tensor supported capturable=False foreach=True lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Invalid learning rate lr = eps raise ValueError f Invalid epsilon value eps = betas raise ValueError f Invalid beta parameter index betas = betas raise ValueError f Invalid beta parameter index betas = weight_decay raise ValueError f Invalid weight_decay value weight_decay isinstance betas float isinstance betas float isinstance betas Tensor isinstance betas Tensor raise ValueError betas must either both floats both Tensors isinstance betas Tensor capturable foreach raise ValueError betas Tensor supported capturable=False foreach=True betas numel = raise ValueError Tensor betas must -element isinstance betas Tensor capturable foreach raise ValueError betas Tensor supported capturable=False foreach=True betas numel = raise ValueError Tensor betas must -element betas = tuple map _to_scalar betas defaults = lr lr betas betas eps eps weight_decay weight_decay amsgrad amsgrad maximize maximize foreach foreach capturable capturable differentiable differentiable fused fused decoupled_weight_decay decoupled_weight_decay super __init__ params defaults fused differentiable raise RuntimeError ` fused ` does support ` differentiable ` _step_supports_amp_scaling = True TODO crcrpar low prec params their higher prec copy Support AMP FP BF model params which would need higher prec copy params do update math higher prec alleviate loss information foreach raise RuntimeError ` fused ` ` foreach ` cannot ` True ` together __setstate__ state super __setstate__ state group param_groups group setdefault amsgrad False group setdefault maximize False group setdefault foreach None group setdefault capturable False group setdefault differentiable False group setdefault decoupled_weight_decay False fused = group setdefault fused None p group params p_state = state get p len p_state = torch is_tensor p_state step step_val = float p_state step p_state step = torch tensor step_val dtype=_get_scalar_dtype is_fused=fused device=p device group capturable group fused torch tensor step_val dtype=_get_scalar_dtype _init_group group params_with_grad grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps has_complex = False p group params p grad None has_complex &#124; = torch is_complex p params_with_grad append p p grad is_sparse raise RuntimeError Adam does support sparse gradients please consider SparseAdam instead grads append p grad state = state p Lazy state initialization len state == group fused _device_dtype_check_for_fused p note crcrpar special device hosting step Deliberately host ` step ` CPU both capturable fused off This because kernel launches costly CUDA XLA state step = torch zeros dtype=_get_scalar_dtype is_fused=group fused device=p device group capturable group fused torch tensor dtype=_get_scalar_dtype Exponential moving average gradient values state exp_avg = torch zeros_like p memory_format=torch preserve_format Exponential moving average squared gradient values state exp_avg_sq = torch zeros_like p memory_format=torch preserve_format group amsgrad Maintains max all exp moving avg sq grad values state max_exp_avg_sq = torch zeros_like p memory_format=torch preserve_format exp_avgs append state exp_avg exp_avg_sqs append state exp_avg_sq group amsgrad max_exp_avg_sqs append state max_exp_avg_sq group differentiable state step requires_grad raise RuntimeError ` requires_grad ` supported ` step ` differentiable mode Foreach without capturable does support tensor lr group foreach torch is_tensor group lr group capturable raise RuntimeError lr Tensor supported capturable=False foreach=True state_steps append state step has_complex _use_grad_for_differentiable step closure=None Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss _cuda_graph_capture_health_check loss = None closure None torch enable_grad loss = closure group param_groups params_with_grad list Tensor = grads list Tensor = exp_avgs list Tensor = exp_avg_sqs list Tensor = max_exp_avg_sqs list Tensor = state_steps list Tensor = beta beta = group betas has_complex = _init_group group params_with_grad grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps adam params_with_grad grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps amsgrad=group amsgrad has_complex=has_complex beta =beta beta =beta lr=group lr weight_decay=group weight_decay eps=group eps maximize=group maximize foreach=group foreach capturable=group capturable differentiable=group differentiable fused=group fused grad_scale=getattr grad_scale None found_inf=getattr found_inf None decoupled_weight_decay=group decoupled_weight_decay loss Adam __doc__ = r Implements Adam algorithm math \begin aligned \rule mm pt \\ \textbf input \gamma \text lr \beta_ \beta_ \text betas \theta_ \text params f \theta \text objective \\ \hspace mm \lambda \text weight decay \ \textit amsgrad \ \textit maximize \ \epsilon \text epsilon \\ \textbf initialize m_ \leftarrow \text first moment v_ \leftarrow \text second moment \ v_ ^ max \leftarrow \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm \textbf \ \textit maximize \\ \hspace mm g_t \leftarrow -\nabla_ \theta f_t \theta_ t- \\ \hspace mm \textbf \\ \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \textbf \ \lambda \neq \\ \hspace mm g_t \leftarrow g_t + \lambda \theta_ t- \\ \hspace mm m_t \leftarrow \beta_ m_ t- + - \beta_ g_t \\ \hspace mm v_t \leftarrow \beta_ v_ t- + -\beta_ g^ _t \\ \hspace mm \widehat m_t \leftarrow m_t \big -\beta_ ^t \big \\ \hspace mm \textbf \ amsgrad \\ \hspace mm v_t^ max \leftarrow \mathrm max v_ t- ^ max v_t \\ \hspace mm \widehat v_t \leftarrow v_t^ max \big -\beta_ ^t \big \\ \hspace mm \textbf \\ \hspace mm \widehat v_t \leftarrow v_t \big -\beta_ ^t \big \\ \hspace mm \theta_t \leftarrow \theta_ t- - \gamma \widehat m_t \big \sqrt \widehat v_t + \epsilon \big \\ \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned For further details regarding algorithm we refer ` Adam A Method Stochastic Optimization ` _ + rf Args _params_doc lr float Tensor optional learning rate default e- A tensor LR yet supported all our implementations Please use float LR you also specifying fused=True capturable=True betas tuple Union float Tensor Union float Tensor optional coefficients used computing running averages gradient its square If tensor provided must -element default eps float optional term added denominator improve numerical stability default e- weight_decay float optional weight decay L penalty default decoupled_weight_decay bool optional True optimizer equivalent AdamW algorithm will accumulate weight decay momentum nor variance default False amsgrad bool optional whether use AMSGrad variant algorithm paper ` On Convergence Adam Beyond ` _ default False _foreach_doc _maximize_doc _capturable_doc _differentiable_doc _fused_doc Note A prototype implementation Adam AdamW MPS supports ` torch float ` ` torch float ` _Adam\ A Method Stochastic Optimization https arxiv org abs _On Convergence Adam Beyond https openreview net forum id=ryQu f-RZ _single_tensor_adam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor max_exp_avg_sqs list Tensor state_steps list Tensor grad_scale Optional Tensor found_inf Optional Tensor amsgrad bool has_complex bool beta Union float Tensor beta Union float Tensor lr Union float Tensor weight_decay float eps float maximize bool capturable bool differentiable bool decoupled_weight_decay bool grad_scale None found_inf None raise AssertionError Expected grad_scale found_inf None torch jit is_scripting assert due JIT being dumb realizing ops below have overloads handle both float Tensor lrs so we just assert s float since most people using JIT using floats isinstance lr float raise AssertionError f Expected lr float got type lr isinstance beta float raise AssertionError f Expected beta float got type beta isinstance beta float raise AssertionError f Expected beta float got type beta lr = _to_scalar lr beta = _to_scalar beta beta = _to_scalar beta We only shuffle around beta when Tensor otherwise we prefer treating scalar Note ensure type declaration under conditional check isinstance torchscript will get cranky about DeviceDict type isinstance beta Tensor beta _dict Optional DeviceDtypeDict = beta device beta dtype beta beta _dict = None i param enumerate params grad = grads i maximize -grads i exp_avg = exp_avgs i exp_avg_sq = exp_avg_sqs i step_t = state_steps i If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices param device type == step_t device type param device type capturable_supported_devices raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices update step step_t += weight_decay = decoupled_weight_decay Perform stepweight decay param mul_ - lr weight_decay Nested necessary bypass jitscript rules differentiable isinstance weight_decay Tensor weight_decay requires_grad grad = grad addcmul_ param clone weight_decay pyrefly ignore bad-argument-type grad = grad add param alpha=weight_decay grad = grad add param alpha=weight_decay torch is_complex param grad = torch view_as_real grad exp_avg = torch view_as_real exp_avg exp_avg_sq = torch view_as_real exp_avg_sq amsgrad max_exp_avg_sqs i = torch view_as_real max_exp_avg_sqs i param = torch view_as_real param device = param device beta _dict None dtype = param dtype type ignore union-attr cast workaround https github com pytorch pytorch issues key = device dtype key beta _dict beta _dict key = beta type ignore union-attr device=device dtype=dtype non_blocking=True device_beta Union float Tensor = beta _dict key device_beta = beta Decay first second moment running average coefficient exp_avg lerp_ grad - device_beta Nested necessary bypass jitscript rules differentiable isinstance beta Tensor beta requires_grad Using lerp only use operations bc addcmul s value cannot tensor Showing equivalence differentiable path nondifferentiable path expavg b + grad^ -b add expavg -b - expavg -b = expavg b + expavg -b - expavg -b + grad^ -b expavg - expavg -b + grad^ -b expavg + grad^ - expavg -b expavg lerp grad^ -beta exp_avg_sq lerp_ torch square grad weight= - beta exp_avg_sq mul_ beta addcmul_ grad grad value=cast float - beta exp_avg_sq mul_ beta addcmul_ grad grad value= - beta type ignore arg-type capturable differentiable step = step_t Nested necessary bypass jitscript rules differentiable isinstance beta Tensor beta requires_grad bias_correction = - beta step clone bias_correction = - beta step bias_correction = - beta step Nested necessary bypass jitscript rules differentiable isinstance beta Tensor beta requires_grad bias_correction = - beta step clone bias_correction = - beta step bias_correction = - beta step step_size = lr bias_correction step_size_neg = step_size neg bias_correction _sqrt = bias_correction sqrt amsgrad Maintains maximum all nd moment running avg till now differentiable max_exp_avg_sq = max_exp_avg_sqs i clone max_exp_avg_sq = max_exp_avg_sqs i max_exp_avg_sqs i copy_ torch maximum max_exp_avg_sq exp_avg_sq Uses max normalizing running avg gradient Folds admittedly ugly -elem step_size math here avoid extra param-set-sized read+write can t fold into addcdiv_ below because addcdiv_ requires value Number Tensor denom = max_exp_avg_sqs i sqrt bias_correction _sqrt step_size_neg add_ eps step_size_neg denom = exp_avg_sq sqrt bias_correction _sqrt step_size_neg add_ eps step_size_neg differentiable param addcdiv_ exp_avg clone denom param addcdiv_ exp_avg denom step = _get_value step_t bias_correction = - beta step bias_correction = - beta step step_size = lr bias_correction bias_correction _sqrt = bias_correction amsgrad Maintains maximum all nd moment running avg till now torch maximum max_exp_avg_sqs i exp_avg_sq out=max_exp_avg_sqs i Use max normalizing running avg gradient denom = max_exp_avg_sqs i sqrt bias_correction _sqrt add_ eps denom = exp_avg_sq sqrt bias_correction _sqrt add_ eps param addcdiv_ exp_avg denom value=-step_size type ignore arg-type Lastly switch back complex view amsgrad torch is_complex params i max_exp_avg_sqs i = torch view_as_complex max_exp_avg_sqs i _multi_tensor_adam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor max_exp_avg_sqs list Tensor state_steps list Tensor grad_scale Optional Tensor found_inf Optional Tensor amsgrad bool has_complex bool beta Union float Tensor beta Union float Tensor lr Union float Tensor weight_decay float eps float maximize bool capturable bool differentiable bool decoupled_weight_decay bool len params == isinstance lr Tensor capturable raise RuntimeError lr Tensor supported capturable=False foreach=True lr numel = raise ValueError Tensor lr must -element isinstance beta Tensor capturable raise ValueError beta Tensor supported capturable=False foreach=True beta numel = raise ValueError Tensor beta must -element isinstance beta Tensor capturable raise ValueError beta Tensor supported capturable=False foreach=True beta numel = raise ValueError Tensor beta must -element If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices supports_xla=False all p device type == step device type p device type capturable_supported_devices p step zip params state_steps strict=True raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices grad_scale None found_inf None raise AssertionError Expected grad_scale found_inf None differentiable raise AssertionError _foreach ops don t support autograd lr = _to_scalar lr beta = _to_scalar beta beta = _to_scalar beta grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps type ignore list-item We only shuffle around beta when Tensor CUDA otherwise we prefer treating scalar beta _dict Optional DeviceDict = type ignore attr-defined beta device beta isinstance beta Tensor str beta device = cpu None device_params_ device_grads_ device_exp_avgs_ device_exp_avg_sqs_ device_max_exp_avg_sqs_ device_state_steps_ _ grouped_tensors values device_params = cast list Tensor device_params_ device_grads = cast list Tensor device_grads_ device_exp_avgs = cast list Tensor device_exp_avgs_ device_exp_avg_sqs = cast list Tensor device_exp_avg_sqs_ device_state_steps = cast list Tensor device_state_steps_ device = device_params device beta _dict None device beta _dict beta _dict device = beta device=device non_blocking=True type ignore union-attr attr-defined device_beta = beta _dict device beta _dict beta Handle complex parameters has_complex amsgrad device_max_exp_avg_sqs = cast list Tensor device_max_exp_avg_sqs_ _view_as_real device_params device_grads device_exp_avgs device_exp_avg_sqs device_max_exp_avg_sqs _view_as_real device_params device_grads device_exp_avgs device_exp_avg_sqs maximize device_grads = torch _foreach_neg device_grads type ignore assignment Update steps If steps CPU foreach will fall back slow path which for-loop calling t add over over will then wrapped into Tensor over over again which slower than we just wrapped once now The alpha required assure we go right overload torch compiler is_compiling device_state_steps is_cpu torch _foreach_add_ device_state_steps torch tensor device= cpu alpha= torch _foreach_add_ device_state_steps weight_decay = decoupled_weight_decay Perform stepweight decay torch _foreach_mul_ device_params - lr weight_decay Reuse intermediate memory device_grads already allocated maximize maximize torch _foreach_add_ device_grads device_params alpha=weight_decay device_grads = torch _foreach_add type ignore assignment device_grads device_params alpha=weight_decay Decay first second moment running average coefficient Use device beta beta tensor ensure all tensors same device torch _foreach_lerp_ device_exp_avgs device_grads cast float - device_beta torch _foreach_mul_ device_exp_avg_sqs beta Due strictness _foreach_addcmul API we can t have single tensor scalar scalar arg only python number supported there result separate out value mul Filed https github com pytorch pytorch issues isinstance beta torch Tensor scaled_device_grads = torch _foreach_mul device_grads - beta type ignore assignment value = scaled_device_grads = device_grads type ignore assignment value = - beta torch _foreach_addcmul_ device_exp_avg_sqs scaled_device_grads device_grads value Delete local intermediate s since they won t used anymore save peak memory del device_grads del scaled_device_grads bias_correction Union tuple Tensor list Tensor bias_correction Union tuple Tensor list Tensor bias_correction _sqrt Union tuple Tensor list Tensor capturable bias_correction = torch _foreach_pow beta device_state_steps type ignore arg-type bias_correction = torch _foreach_pow beta device_state_steps type ignore arg-type foreach_sub doesn t allow scalar first arg torch _foreach_sub_ bias_correction torch _foreach_sub_ bias_correction we do negate bias_correction ll need negated later anyway torch _foreach_neg_ bias_correction foreach_div doesn t allow scalar first arg torch _foreach_div_ bias_correction lr torch _foreach_reciprocal_ bias_correction torch _foreach_sqrt_ bias_correction Re-assign clarity we maintain minimal intermediates we ll have step_size = - lr - beta ^ t where t = num_steps bias_correction _sqrt = sqrt - beta ^ t step_size = bias_correction bias_correction _sqrt = bias_correction amsgrad device_max_exp_avg_sqs = cast list Tensor device_max_exp_avg_sqs_ Maintains maximum all nd moment running avg till now torch _foreach_maximum_ device_max_exp_avg_sqs device_exp_avg_sqs type ignore assignment Set intermediate max normalizing running avg gradient when amsgrad exp_avg_sq_sqrt = torch _foreach_sqrt device_max_exp_avg_sqs exp_avg_sq_sqrt = torch _foreach_sqrt device_exp_avg_sqs torch _foreach_div_ exp_avg_sq_sqrt bias_correction _sqrt torch _foreach_add_ exp_avg_sq_sqrt eps torch _foreach_div_ exp_avg_sq_sqrt step_size point exp_avg_sq_sqrt = - - beta^t sqrt exp_avg_sq - beta ^t + eps lr torch _foreach_addcdiv_ device_params device_exp_avgs exp_avg_sq_sqrt bias_correction = - beta _get_value step step device_state_steps bias_correction = - beta _get_value step step device_state_steps step_size = _stack_if_compiling lr bc - bc bias_correction bias_correction _sqrt = bc bc bias_correction type ignore arg-type amsgrad device_max_exp_avg_sqs = cast list Tensor device_max_exp_avg_sqs_ Maintains maximum all nd moment running avg till now torch _foreach_maximum_ device_max_exp_avg_sqs device_exp_avg_sqs Use max normalizing running avg gradient exp_avg_sq_sqrt = torch _foreach_sqrt device_max_exp_avg_sqs exp_avg_sq_sqrt = torch _foreach_sqrt device_exp_avg_sqs torch _foreach_div_ exp_avg_sq_sqrt bias_correction _sqrt torch _foreach_add_ exp_avg_sq_sqrt eps torch _foreach_addcdiv_ device_params device_exp_avgs exp_avg_sq_sqrt step_size type ignore arg-type _fused_adam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor max_exp_avg_sqs list Tensor state_steps list Tensor grad_scale Optional Tensor found_inf Optional Tensor amsgrad bool has_complex bool Needed consistency beta Union float Tensor beta Union float Tensor lr Union float Tensor weight_decay float eps float maximize bool capturable bool Needed consistency differentiable bool decoupled_weight_decay bool - None params differentiable raise RuntimeError Adam fused=True does support differentiable=True beta = _to_scalar beta beta = _to_scalar beta grad_scale_dict DeviceDict = grad_scale device grad_scale grad_scale None found_inf_dict DeviceDict = found_inf device found_inf found_inf None We only shuffle around lr when Tensor CUDA otherwise we prefer treating scalar lr_dict Optional DeviceDict = lr device lr isinstance lr Tensor str lr device = cpu None grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps type ignore list-item device _ device_params_ device_grads_ device_exp_avgs_ device_exp_avg_sqs_ device_max_exp_avg_sqs device_state_steps_ _ grouped_tensors items device_params = cast list Tensor device_params_ device_grads = cast list Tensor device_grads_ device_exp_avgs = cast list Tensor device_exp_avgs_ device_exp_avg_sqs = cast list Tensor device_exp_avg_sqs_ device_state_steps = cast list Tensor device_state_steps_ device_grad_scale device_found_inf = None None grad_scale None device_grad_scale = grad_scale_dict setdefault device grad_scale device non_blocking=True found_inf None device_found_inf = found_inf_dict setdefault device found_inf device non_blocking=True lr_dict None device lr_dict lr_dict device = lr device=device non_blocking=True type ignore union-attr lr = lr_dict device torch _foreach_add_ device_state_steps func = torch _fused_adam_ decoupled_weight_decay torch _fused_adamw_ func device_params device_grads device_exp_avgs device_exp_avg_sqs device_max_exp_avg_sqs type ignore arg-type device_state_steps amsgrad=amsgrad lr=lr type ignore arg-type beta =beta beta =beta weight_decay=weight_decay eps=eps maximize=maximize grad_scale=device_grad_scale found_inf=device_found_inf device_found_inf None torch _foreach_sub_ device_state_steps device_found_inf len device_state_steps _disable_dynamo_if_unsupported single_tensor_fn=_single_tensor_adam adam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor max_exp_avg_sqs list Tensor state_steps list Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim foreach Optional bool = None capturable bool = False differentiable bool = False fused Optional bool = None grad_scale Optional Tensor = None found_inf Optional Tensor = None has_complex bool = False decoupled_weight_decay bool = False amsgrad bool beta Union float Tensor beta Union float Tensor lr Union float Tensor weight_decay float eps float maximize bool r Functional API performs Adam algorithm computation See ` ~torch optim Adam ` details Respect when user inputs False True foreach fused We only want change default when neither have been user-specified Note we default foreach pass False use_fused This mistake -- we want give fused impl bake-in time before making default even typically faster fused None foreach None _ foreach = _default_to_fused_or_foreach params differentiable use_fused=False Do flip foreach unsupported case where lr Tensor capturable=False foreach isinstance lr Tensor capturable foreach = False fused None fused = False foreach None foreach = False check slow during compilation so we skip s strictly needed we can add check back dynamo torch compiler is_compiling all isinstance t torch Tensor t state_steps raise RuntimeError API has changed ` state_steps ` argument must contain list singleton tensors foreach torch jit is_scripting raise RuntimeError torch jit script supported foreach optimizers fused torch jit is_scripting raise RuntimeError torch jit script supported fused optimizers fused torch jit is_scripting func = _fused_adam foreach torch jit is_scripting func = _multi_tensor_adam func = _single_tensor_adam func params grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps amsgrad=amsgrad has_complex=has_complex beta =beta beta =beta lr=lr weight_decay=weight_decay eps=eps maximize=maximize capturable=capturable differentiable=differentiable grad_scale=grad_scale found_inf=found_inf decoupled_weight_decay=decoupled_weight_decay