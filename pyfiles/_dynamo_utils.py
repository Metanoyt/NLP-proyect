torch nn nn _annotate_modules_for_dynamo module nn Module ignored_modules set nn Module use_orig_params bool - None Annotates submodules ` ` module ` ` s tree except those ` ` ignored_modules ` ` indicating submodules FSDP-managed saving ` ` use_orig_params ` ` setting passed FSDP constructor submodule module modules submodule ignored_modules note Dynamo treats FSDP wrapped modules UnspecializedNNModule Dynamo doesn t get see instance FullyShardedDataParallel during tracing since skips tracing all torch distributed fsdp code - Why Running FSDP code eagerly avoids lots issues trying trace complex hooks also gets us graph-breaks FSDP module boundaries which we want anyway comm ops - However we _also_ want dynamo treat wrapped module inside FSDP unspecially we need way indicate dynamo which modules wrapped FSDP UnspecializedNNModules dynamo traced-through without any assumptions thorough guards NNModules otherwise specialized meaning there less overhead due assuming their code well-behaved One particular issue specialized NNModules FSDP views created orig_params captured into compiled graph first iteration while they always going point correct flatparameter give correct results their order creation influences order backward execution preventing overlap comm computation during backward We need _use_ new parameter views created each forward iteration order backward interleave hooks compute per layer UnspecializedNNModule lets us achieve capturing module code more functionally passing parameters inputs each time submodule _is_fsdp_managed_module = True type ignore assignment Dynamo only supports FSDP use_orig_params=True This hacky I could think another way add assertion dynamo since Dynamo skips all FSDP code frames thus can t inspect FSDP module directly submodule _fsdp_use_orig_params = use_orig_params type ignore assignment