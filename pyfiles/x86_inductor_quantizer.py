mypy allow-untyped-defs functools itertools operator warnings collections abc Callable Sequence dataclasses dataclass typing Any Optional TYPE_CHECKING TypeAlias Union torch torch nn functional F torch ao quantization fake_quantize FakeQuantize FusedMovingAvgObsFakeQuantize torch ao quantization observer HistogramObserver MovingAverageMinMaxObserver MovingAveragePerChannelMinMaxObserver PerChannelMinMaxObserver PlaceholderObserver torch ao quantization pt e graph_utils find_sequential_partitions torch ao quantization quantizer quantizer QuantizationAnnotation QuantizationSpec Quantizer SharedQuantizationSpec torch ao quantization quantizer utils _get_module_name_filter torch ao quantization quantizer xnnpack_quantizer_utils get_bias_qspec get_input_act_qspec get_output_act_qspec get_weight_qspec QuantizationConfig torch fx Node torch fx passes utils source_matcher_utils get_source_partitions SourcePartition FilterFn TypeAlias = Callable list Node bool TYPE_CHECKING torch ao quantization qconfig _ObserverOrFakeQuantizeConstructor __all__ = X InductorQuantizer get_default_x _inductor_quantization_config get_x _inductor_linear_dynamic_fp _config dataclass _X InductorQuantizationAnnotation QuantizationAnnotation _is_output_of_quantized_pattern Node output node fusion pattern The fusion pattern supports int data type The fusion pattern has inputs annotated insert observer The quantization_config ` None ` _is_output_of_quantized_pattern bool = False Operators Operators optimized run int when int input provided Operators do support int input produce fp output int _in_int _out_ops set = torch ops aten max_pool d default torch ops aten cat default torch ops aten avg_pool d default torch ops aten adaptive_avg_pool d default torch ops aten flatten using_ints Operators support int data type quantization config propagation A superset int _in_int _out_ops incorporating additional operators propagation_quantizable_ops = int _in_int _out_ops Operators support int data type recipe configured default X InductorQuantizer default_quantizable_ops = propagation_quantizable_ops &#124; torch ops aten conv d default torch ops aten conv d default torch ops aten linear default A superset default_quantizable_ops includes operators support int data type enabled default recipe X InductorQuantizer quantizable_ops = default_quantizable_ops &#124; torch ops aten matmul default QUANT_ANNOTATION_KEY = quantization_annotation _skip_annotate nodes list Node filter_fn Optional FilterFn = None - bool Determine whether skip annotation list nodes Skip annotate any node already annotated _is_any_annotated nodes True Proceed annotate filter function provided b given nodes list passes filter function check filter_fn filter_fn nodes False True _create_module_name_filter module_name str - FilterFn Create filter function given module name The filter function takes list nodes determined annotate function True all nodes come specified module name False otherwise For example linear_ f = torch ops aten linear default comes module name ` sub linear ` relu f = torch ops aten relu default linear_ comes module name ` sub relu ` module_name_filter = _create_module_name_filter_inner sub print module_name_filter relu linear_ True These two nodes determined ` _annotate_linear_unary ` function sub filter_fn = _get_module_name_filter module_name check_all_nodes_from_module nodes list Node - bool all_nodes_from_module_name bool = all filter_fn n n nodes all_nodes_from_module_name check_all_nodes_from_module _create_operator_type_filter operator_type Callable - FilterFn Create filter function given operator type The filter function takes list nodes returns True contains exactly one node specified operator type False otherwise For example linear_ f = torch ops aten linear default comes module name ` sub linear ` relu f = torch ops aten relu default linear_ comes module name ` sub relu ` operator_type_filter = _create_operator_type_filter torch ops aten linear default print operator_type_filter relu linear_ True These two nodes determined ` _annotate_linear_unary ` function second node ` linear ` operator_type_filter nodes list Node num_nodes_with_operator_type = sum node target == operator_type node nodes num_nodes_with_operator_type raise NotImplementedError f Several nodes within single pattern operator_type num_nodes_with_operator_type == operator_type_filter _global_config_filter nodes list Node - bool Filter function global configuration This filter function takes list nodes returns True there exactly one node list default quantizable operation False otherwise num_nodes_in_default_quantizable_ops = sum node target default_quantizable_ops node nodes num_nodes_in_default_quantizable_ops raise NotImplementedError Several nodes within single pattern default quantizable operations num_nodes_in_default_quantizable_ops == _map_module_function_to_aten_operator_type module_function_to_aten_operator dict Callable torch _ops OpOverloadPacket = map_list = torch nn Conv d F conv d torch ops aten conv d default torch nn Conv d F conv d torch ops aten conv d default torch nn Linear F linear torch ops aten linear default torch nn MaxPool d F max_pool d torch ops aten max_pool d default torch cat torch ops aten cat default torch nn AvgPool d F avg_pool d torch ops aten avg_pool d default torch nn AdaptiveAvgPool d F adaptive_avg_pool d torch ops aten adaptive_avg_pool d default torch flatten torch ops aten flatten using_ints torch matmul torch ops aten matmul default map_item map_list module_function_to_aten_operator update dict fromkeys map_item map_item type ignore arg-type call-overload module_function_to_aten_operator _mark_nodes_as_annotated nodes list Node node nodes node None QUANT_ANNOTATION_KEY node meta node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation node meta QUANT_ANNOTATION_KEY _annotated = True _is_node_annotated _node True node annotated otherwise False QUANT_ANNOTATION_KEY _node meta _node meta QUANT_ANNOTATION_KEY _annotated _is_any_annotated nodes list Node Given list nodes represents operator pattern check any node annotated True any node annotated otherwise False any _is_node_annotated node node nodes _is_all_annotated nodes list Node Given list nodes represents operator pattern True all node annotated otherwise False all _is_node_annotated node node nodes _is_quantized_op_pt e node torch fx Node Used pt e flow check node quantized node Case node has been annotated output node fusion pattern Case node has been annotated single quantized node _is_any_annotated node The node has been annotated directly False False quantization_annotation = node meta get QUANT_ANNOTATION_KEY None assert isinstance quantization_annotation _X InductorQuantizationAnnotation quantization_annotation _is_output_of_quantized_pattern functools lru_cache get_default_x _inductor_quantization_config is_qat bool = False is_dynamic bool = False reduce_range bool = False reduce_range False default Set True earlier CPUs without VNNI avoid accuracy issue extra_args dict str Any = eps - is_qat is_dynamic act_observer_or_fake_quant_ctr = FakeQuantize dynamic_quant_observer = MovingAverageMinMaxObserver with_args averaging_constant= extra_args observer = dynamic_quant_observer act_observer_or_fake_quant_ctr = FusedMovingAvgObsFakeQuantize type ignore assignment is_dynamic act_observer_or_fake_quant_ctr = PlaceholderObserver type ignore assignment act_observer_or_fake_quant_ctr = HistogramObserver type ignore assignment Copy x default qconfig torch ao quantization qconfig py act_quantization_spec = QuantizationSpec dtype=torch uint quant_min= quant_max= reduce_range qscheme=torch per_tensor_affine is_dynamic=is_dynamic observer_or_fake_quant_ctr=act_observer_or_fake_quant_ctr with_args extra_args weight_observer_or_fake_quant_ctr _ObserverOrFakeQuantizeConstructor = FusedMovingAvgObsFakeQuantize is_qat PerChannelMinMaxObserver is_qat Only support per channel quant now extra_args observer = MovingAveragePerChannelMinMaxObserver type ignore dict-item weight_quantization_spec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_channel_symmetric ch_axis= corresponding weight shape = oc ic kh kw conv is_dynamic=False observer_or_fake_quant_ctr=weight_observer_or_fake_quant_ctr with_args extra_args bias_quantization_spec = None will use placeholder observer default quantization_config = QuantizationConfig act_quantization_spec act_quantization_spec weight_quantization_spec bias_quantization_spec is_qat quantization_config functools lru_cache get_x _inductor_linear_dynamic_fp _config For linear_dynamic_fp The name may confusing The op s behavior fp _input fp _weight - to_fp - fp _output weight_quantization_spec = QuantizationSpec dtype=torch float observer_or_fake_quant_ctr=PlaceholderObserver quantization_config = QuantizationConfig None input_quantization_spec None output_quantization_spec weight_quantization_spec None bias_quantization_spec quantization_config _annotate_nodes_not_quantize nodes Union Node list Node - None Annotate nodes exclude them quantization their ` quantization_config ` ` None ` isinstance nodes list nodes = nodes node nodes node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation _annotated=True _config_checker method Callable - Callable functools wraps method wrapper quantizer X InductorQuantizer name Any quantization_config Optional QuantizationConfig - X InductorQuantizer quantizer _need_skip_config quantization_config warnings warn f Skip quantization config name stacklevel= quantizer method quantizer name quantization_config wrapper dataclass _CurrentQuantizationMode r Configuration defining current quantization mode quantizer All possible current quantization modes listed below ---------------------------------------------------------------------------------------------------------- &#124; dynamic_state qat_state &#124; --------------------------------------------------------------------------------------------- &#124; None &#124; True &#124; False ---------------------------------------------------------------------------------------------------------- None &#124; quantizer does receive non-None ` quantization_config ` &#124; \ &#124; \ False &#124; quantizer will do QAT &#124; dynamic &#124; static True &#124; quantizer will do QAT &#124; QAT + dynamic &#124; QAT + static qat_state Optional bool dynamic_state Optional bool X InductorQuantizer Quantizer module_function_to_aten_operator_type = _map_module_function_to_aten_operator_type __init__ - None super __init__ global_config Optional QuantizationConfig = None operator_type_qconfig dict torch _ops OpOverloadPacket Optional QuantizationConfig = module_name_qconfig dict str Optional QuantizationConfig = _get_current_quantization_mode - _CurrentQuantizationMode Retrieves current quantization mode based all configurations qat_state = None dynamic_state = None As we use ` _need_skip_config ` skip all invalid configurations we can safely assume all existing non-None configurations have same quantization mode pyrefly ignore bad-assignment qconfig list module_name_qconfig values + list operator_type_qconfig values + global_config qconfig None Query ` is_qat ` state qat_state None qat_state = qconfig is_qat assert qat_state == qconfig is_qat f All non-None quantization configs should have same ` is_qat ` f got qat_state qconfig is_qat Query ` is_dynamic ` state input_activation_spec = qconfig input_activation input_activation_spec None dynamic_state None dynamic_state = input_activation_spec is_dynamic assert dynamic_state == input_activation_spec is_dynamic f All non-None ` input_activation_spec ` should have same ` is_dynamic ` f got dynamic_state input_activation_spec is_dynamic _CurrentQuantizationMode qat_state=qat_state dynamic_state=dynamic_state _need_skip_config quantization_config Optional QuantizationConfig - bool Check provided quantization config valid X InductorQuantizer Mixed static dynamic configurations mixed QAT non-QAT configurations supported To avoid such mix we compare incoming configuration current configuration status Refer ` _CurrentQuantizationMode ` definition all possible modes quantization_config None False need_skip = False current_mode = _get_current_quantization_mode current_mode qat_state None current_mode qat_state = quantization_config is_qat warnings warn Mixed QAT Non-QAT quantization config supported stacklevel= need_skip = True current_mode dynamic_state None input_activation_spec = quantization_config input_activation input_activation_spec None current_mode dynamic_state = input_activation_spec is_dynamic warnings warn Mixed dynamic static quantization config supported stacklevel= need_skip = True need_skip set_global quantization_config QuantizationConfig _need_skip_config quantization_config warnings warn Skip global quantization config stacklevel= global_config = quantization_config get_global_quantization_config isinstance global_config QuantizationConfig warnings warn The global_config X InductorQuantizer currently invalid \ Please ensure you use set_global establish global quantization configuration stacklevel= global_config _config_checker set_function_type_qconfig function_type Callable quantization_config Optional QuantizationConfig - X InductorQuantizer function_type X InductorQuantizer module_function_to_aten_operator_type _set_aten_operator_qconfig X InductorQuantizer module_function_to_aten_operator_type function_type quantization_config warnings warn f function Unable customize quantization config function_type X InductorQuantizer stacklevel= _config_checker set_module_type_qconfig module_type torch nn Module quantization_config Optional QuantizationConfig - X InductorQuantizer module_type X InductorQuantizer module_function_to_aten_operator_type _set_aten_operator_qconfig X InductorQuantizer module_function_to_aten_operator_type module_type quantization_config warnings warn f Module Unable customize quantization config module_type X InductorQuantizer stacklevel= _config_checker set_module_name_qconfig module_name str quantization_config Optional QuantizationConfig Set quantization_config submodule name ` module_name ` example quantizer set_module_name_qconfig blocks sub will quantize all supported operator operator patterns submodule module name given ` quantization_config ` The supported operators include ` quantizable_ops ` ` propagation_quantizable_ops ` module_name_qconfig module_name = quantization_config _set_aten_operator_qconfig operator_type torch _ops OpOverloadPacket quantization_config Optional QuantizationConfig - X InductorQuantizer operator_type quantizable_ops operator_type_qconfig operator_type = quantization_config warnings warn f operator Unable quantize operator X InductorQuantizer stacklevel= _annotate_conv_node_helper conv_node torch fx Node annotate_output bool quantization_config Optional QuantizationConfig - None Helper function annotate conv node quantization_config None _annotate_nodes_not_quantize conv_node input_qspec_map = input_node = conv_node args assert isinstance input_node Node input_qspec_map input_node = get_input_act_qspec quantization_config weight_node = conv_node args assert isinstance weight_node Node input_qspec_map weight_node = get_weight_qspec quantization_config bias_node = None len conv_node args == conv_node args isinstance bias_node Node input_qspec_map bias_node = get_bias_qspec quantization_config annotate_output conv_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True _is_output_of_quantized_pattern=True conv_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True _annotate_linear_node_helper linear_node torch fx Node annotate_output bool quantization_config Optional QuantizationConfig - None Helper function annotate linear node quantization_config None _annotate_nodes_not_quantize linear_node input_qspec_map = assert linear_node target torch ops aten linear default has_bias = len linear_node args == input_index = weight_index = bias_index = input_node = linear_node args input_index assert isinstance input_node Node input_qspec_map input_node = get_input_act_qspec quantization_config weight_node = linear_node args weight_index assert isinstance weight_node Node input_qspec_map weight_node = get_weight_qspec quantization_config bias_node = linear_node args bias_index has_bias None isinstance bias_node Node input_qspec_map bias_node = get_bias_qspec quantization_config annotate_output linear_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True _is_output_of_quantized_pattern=True linear_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True _get_output_nodes_of_partitions partition_list list SourcePartition - list torch fx Node Helper function get output node list partition list output_node_list = partition partition_list len partition output_nodes raise ValueError Input partition has more than one output node output_node = partition output_nodes assert isinstance output_node Node output_node_list append output_node len output_node_list = len partition_list raise ValueError length output_node_list should equal length partition_list output_node_list _get_input_idx_for_binary_node conv_gemm_node torch fx Node binary_node torch fx Node Helper function check conv_gemm extra input node index binary node fused conv_gemm conv_gemm_node_idx = None extra_input_node_idx = None binary_node args op == call_function type ignore union-attr binary_node args == conv_gemm_node conv_gemm_node_idx = extra_input_node_idx = binary_node args op == call_function type ignore union-attr binary_node args == conv_gemm_node conv_gemm_node_idx = extra_input_node_idx = extra_input_node = binary_node args extra_input_node_idx type ignore index assert isinstance extra_input_node Node conv_gemm_node_idx extra_input_node_idx annotate model torch fx GraphModule - torch fx GraphModule Annotate given model quantization configurations Annotation contracts Annotate each node according user s qconfig following order ` module_name_qconfig ` ` operator_type_qconfig ` ` global_config ` Avoid re-annotating nodes already annotated prior stages For example ` linear ` has been annotated ` module_name_qconfig ` won t annotated again during processing operator_type_qconfig global_config For config ` None ` node will annotated ` _X InductorQuantizationAnnotation _annotated=True ` For each pair module_name_or_operator_type_or_global qconfig filter function created This filter function checks node marked current stage annotated previous stage module_name quantization_config module_name_qconfig items _annotate_with_config model quantization_config _create_module_name_filter module_name operator_type quantization_config operator_type_qconfig items _annotate_with_config model quantization_config _create_operator_type_filter operator_type global_config _annotate_with_config model global_config _global_config_filter Once we ve annotated model quantization configurations we also need annotate output quantizable operations For example we annotated ` maxpool d ` quantize its inputs we will quantize its output accordingly This enables us fuse dq-operator-q into quantized op Refer https github com intel intel-extension-for-pytorch blob d d afc fcc ba bb fb fdd c c intel_extension_for_pytorch quantization _recipe py#L noqa B _annotate_output_for_int _in_int _out_pattern_entry model model _annotate_with_config model torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn FilterFn - None Annotate model given quantization configuration High-level description quantization recipe X Inductor Backend Step Apply quantization recipe fusion patterns conv linear enable int data type actively Step Propagate quantization annotation patterns besides conv linear Go through pattern model start end If pattern supports computation int data type inputs connected quantized patterns annotate its inputs quantized pattern Step Recipe fusion patterns like conv linear _annotate_conv d_fusion_pattern model quantization_config filter_fn _annotate_linear_fusion_pattern model quantization_config filter_fn _annotate_matmul model quantization_config filter_fn Step Recipe propagate annotation patterns beside conv linear Go through all nodes start end Recipe refer https github com intel intel-extension-for-pytorch blob d d afc fcc ba bb fb fdd c c intel_extension_for_pytorch quantization _recipe py#L noqa B _annotate_propagation_quantizable_pattern_entry model quantization_config filter_fn _annotate_qat_conv d_fusion_pattern model torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None Annotate QAT Specific patterns _annotate_qat_conv d_bn_binary_unary model quantization_config filter_fn _annotate_qat_conv d_bn_binary model quantization_config filter_fn _annotate_qat_conv d_bn_unary model quantization_config filter_fn _annotate_qat_conv d_bn model quantization_config filter_fn _annotate_qat_conv d_bn_binary_unary gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None fused_partitions = find_sequential_partitions gm torch nn Conv d torch nn BatchNorm d operator add torch nn ReLU fused_partition fused_partitions conv_partition bn_partition binary_partition unary_partition = fused_partition conv_node bn_output_node binary_node unary_node = _get_output_nodes_of_partitions conv_partition bn_partition binary_partition unary_partition len bn_output_node users = Conv BN pattern should only has user continue bn_output_node_idx extra_input_node_idx = _get_input_idx_for_binary_node bn_output_node binary_node bn_output_node_idx None extra_input_node_idx None continue bn_output_node = binary_node args bn_output_node_idx raise ValueError f bn_output_node doesn t match input binary node extra_input_node = binary_node args extra_input_node_idx conv_node op = call_function conv_node target = torch ops aten conv d default continue _skip_annotate unary_node binary_node bn_output_node conv_node filter_fn continue _annotate_conv_node_helper conv_node False quantization_config quantization_config None binary_node_input_qspec_map = binary_node_input_qspec_map extra_input_node = get_input_act_qspec quantization_config binary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation pyrefly ignore bad-argument-type input_qspec_map=binary_node_input_qspec_map _annotated=True unary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation TODO leslie Remove annotate output QAT when qat util support pattern matcher output_qspec=get_output_act_qspec quantization_config type ignore arg-type _annotated=True _is_output_of_quantized_pattern=True _annotate_nodes_not_quantize binary_node unary_node nodes_to_mark_annotated = list conv_partition nodes nodes_to_mark_annotated extend list bn_partition nodes nodes_to_mark_annotated extend list binary_partition nodes nodes_to_mark_annotated extend list unary_partition nodes _mark_nodes_as_annotated nodes_to_mark_annotated _annotate_qat_conv d_bn_binary gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None fused_partitions = find_sequential_partitions gm torch nn Conv d torch nn BatchNorm d operator add fused_partition fused_partitions conv_partition bn_partition binary_partition = fused_partition conv_node bn_output_node binary_node = _get_output_nodes_of_partitions conv_partition bn_partition binary_partition len bn_output_node users = Conv BN pattern should only has user continue bn_output_node_idx extra_input_node_idx = _get_input_idx_for_binary_node bn_output_node binary_node bn_output_node_idx None extra_input_node_idx None continue bn_output_node = binary_node args bn_output_node_idx raise ValueError f bn_output_node doesn t match input binary node extra_input_node = binary_node args extra_input_node_idx conv_node op = call_function conv_node target = torch ops aten conv d default continue _skip_annotate binary_node bn_output_node conv_node filter_fn continue _annotate_conv_node_helper conv_node False quantization_config quantization_config None binary_node_input_qspec_map = binary_node_input_qspec_map extra_input_node = get_input_act_qspec quantization_config binary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation pyrefly ignore bad-argument-type input_qspec_map=binary_node_input_qspec_map TODO leslie Remove annotate output QAT when qat util support pattern matcher output_qspec=get_output_act_qspec quantization_config type ignore arg-type _annotated=True _is_output_of_quantized_pattern=True _annotate_nodes_not_quantize binary_node nodes_to_mark_annotated = list conv_partition nodes nodes_to_mark_annotated extend list bn_partition nodes nodes_to_mark_annotated extend list binary_partition nodes _mark_nodes_as_annotated nodes_to_mark_annotated _annotate_qat_conv d_bn_unary gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None fused_partitions = unary_patterns = torch nn Conv d torch nn BatchNorm d torch nn ReLU torch nn Conv d torch nn BatchNorm d torch nn Hardtanh torch nn Conv d torch nn BatchNorm d torch nn Hardswish torch nn Conv d torch nn BatchNorm d torch nn ReLU torch nn Conv d torch nn BatchNorm d torch nn SiLU unary_pattern unary_patterns partitions = find_sequential_partitions gm unary_pattern partitions Extend fused_partitions partitions empty fused_partitions extend partitions fused_partition fused_partitions conv_partition bn_partition unary_partition = fused_partition conv_node bn_output_node unary_node = _get_output_nodes_of_partitions conv_partition bn_partition unary_partition conv_node op = call_function conv_node target = torch ops aten conv d default continue _skip_annotate unary_node bn_output_node conv_node filter_fn continue _annotate_conv_node_helper conv_node False quantization_config quantization_config None unary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation TODO leslie Remove annotate output QAT when qat util support pattern matcher output_qspec=get_output_act_qspec quantization_config type ignore arg-type _annotated=True _is_output_of_quantized_pattern=True _annotate_nodes_not_quantize unary_node nodes_to_mark_annotated = list conv_partition nodes nodes_to_mark_annotated extend list bn_partition nodes nodes_to_mark_annotated extend list unary_partition nodes _mark_nodes_as_annotated nodes_to_mark_annotated _annotate_qat_conv d_bn gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None fused_partitions = find_sequential_partitions gm torch nn Conv d torch nn BatchNorm d fused_partition fused_partitions conv_partition bn_partition = fused_partition conv_node bn_output_node = _get_output_nodes_of_partitions conv_partition bn_partition conv_node op = call_function conv_node target = torch ops aten conv d default continue _skip_annotate bn_output_node conv_node filter_fn continue _annotate_conv_node_helper conv_node False quantization_config quantization_config None bn_output_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation TODO leslie Remove annotate output QAT when qat util support pattern matcher output_qspec=get_output_act_qspec quantization_config type ignore arg-type _annotated=True _is_output_of_quantized_pattern=True _annotate_nodes_not_quantize bn_output_node nodes_to_mark_annotated = list conv_partition nodes nodes_to_mark_annotated extend list bn_partition nodes _mark_nodes_as_annotated nodes_to_mark_annotated _annotate_conv d_fusion_pattern model torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None quantization_config None quantization_config is_qat Annotate QAT specific pattern mainly due BN folded prepare_qat _annotate_qat_conv d_fusion_pattern model quantization_config filter_fn _annotate_conv d_binary_unary model quantization_config filter_fn _annotate_conv d_binary model quantization_config filter_fn _annotate_conv d_unary model quantization_config filter_fn _annotate_conv d model quantization_config filter_fn _annotate_linear_fusion_pattern model torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None _annotate_linear_binary_unary model quantization_config filter_fn _annotate_linear_unary model quantization_config filter_fn _annotate_linear model quantization_config filter_fn _annotate_matmul model torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None node model graph nodes node target = torch ops aten matmul default continue _skip_annotate node filter_fn continue quantization_config None _annotate_nodes_not_quantize node continue input_qspec_map = matmul_node = node input_node matmul_node args input_qspec_map input_node = get_input_act_qspec quantization_config matmul_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True _is_output_of_quantized_pattern=True _annotate_conv d_binary_unary gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None Conv d + add + unary op fused_partitions = find_sequential_partitions gm torch nn Conv d operator add torch nn ReLU fused_partition fused_partitions conv_partition binary_partition unary_partition = fused_partition conv_node binary_node unary_node = _get_output_nodes_of_partitions conv_partition binary_partition unary_partition len conv_node users = Conv Node should only has user node continue conv_node_idx extra_input_node_idx = _get_input_idx_for_binary_node conv_node binary_node conv_node_idx None extra_input_node_idx None continue conv_node = binary_node args conv_node_idx raise ValueError f conv_node doesn t match input binary node extra_input_node = binary_node args extra_input_node_idx conv_node op = call_function conv_node target = torch ops aten conv d default No conv node found fused add continue _skip_annotate unary_node binary_node conv_node filter_fn continue quantization_config None _annotate_nodes_not_quantize conv_node binary_node unary_node continue _annotate_conv_node_helper conv_node False quantization_config binary_node_input_qspec_map = binary_node_input_qspec_map extra_input_node = get_input_act_qspec quantization_config binary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation pyrefly ignore bad-argument-type input_qspec_map=binary_node_input_qspec_map _annotated=True unary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation _annotated=True _is_output_of_quantized_pattern=True _annotate_conv d_binary gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None Conv d + add fused_partitions = find_sequential_partitions gm torch nn Conv d operator add fused_partition fused_partitions conv_partition binary_partition = fused_partition conv_node binary_node = _get_output_nodes_of_partitions conv_partition binary_partition len conv_node users = Conv Node should only has user node continue conv_node_idx extra_input_node_idx = _get_input_idx_for_binary_node conv_node binary_node conv_node_idx None extra_input_node_idx None continue conv_node = binary_node args conv_node_idx raise ValueError f conv_node doesn t match input binary node extra_input_node = binary_node args extra_input_node_idx assert isinstance conv_node Node conv_node op = call_function conv_node target = torch ops aten conv d default No conv node found fused add continue _skip_annotate binary_node conv_node filter_fn continue quantization_config None _annotate_nodes_not_quantize conv_node binary_node continue _annotate_conv_node_helper conv_node False quantization_config binary_node_input_qspec_map = binary_node_input_qspec_map extra_input_node = get_input_act_qspec quantization_config binary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation pyrefly ignore bad-argument-type input_qspec_map=binary_node_input_qspec_map _annotated=True _is_output_of_quantized_pattern=True _annotate_conv d_unary gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None fused_partitions = unary_patterns = torch nn Conv d torch nn ReLU torch nn Conv d torch nn Hardtanh torch nn Conv d torch nn Hardswish torch nn Conv d torch nn ReLU torch nn Conv d torch nn SiLU torch nn Conv d torch nn ReLU unary_pattern unary_patterns partitions = find_sequential_partitions gm unary_pattern partitions Extend fused_partitions partitions empty fused_partitions extend partitions fused_partition fused_partitions conv_partition unary_partition = fused_partition conv_node unary_node = _get_output_nodes_of_partitions conv_partition unary_partition conv_node op = call_function conv_node target torch ops aten conv d default torch ops aten conv d default continue _skip_annotate unary_node conv_node filter_fn continue quantization_config None _annotate_nodes_not_quantize conv_node unary_node continue _annotate_conv_node_helper conv_node False quantization_config unary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation _annotated=True _is_output_of_quantized_pattern=True _annotate_conv d gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None conv_partitions = get_source_partitions gm graph torch nn Conv d torch nn functional conv d conv_partitions = list itertools chain from_iterable conv_partitions values conv_partition conv_partitions len conv_partition output_nodes raise ValueError conv partition has more than one output node conv_node = conv_partition output_nodes conv_node op = call_function conv_node target = torch ops aten conv d default raise ValueError f conv_node aten conv d operator skip annotation already annotated _skip_annotate conv_node filter_fn continue _annotate_conv_node_helper conv_node True quantization_config _annotate_maxpool d node Node quantization_config Optional QuantizationConfig - None node target torch ops aten max_pool d default quantization_config None _annotate_nodes_not_quantize node maxpool_node = node _is_any_annotated maxpool_node input_node = maxpool_node args assert isinstance input_node Node input_qspec_map = input_qspec_map input_node = get_input_act_qspec quantization_config maxpool_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True _is_output_of_quantized_pattern=True _annotate_cat node Node quantization_config QuantizationConfig - None quantization_config None _annotate_nodes_not_quantize node cat_node = node input_nodes = cat_node args assert isinstance input_nodes Sequence first_input_node = input_nodes input_qspec_map = assert isinstance first_input_node Node assert isinstance cat_node Node input_qspec_map first_input_node = get_input_act_qspec quantization_config share_qparams_with_input_act _qspec = SharedQuantizationSpec first_input_node cat_node input_node input_nodes input_node input_qspec_map There has case cat same nodes torch cat input input assert isinstance input_node Node input_qspec_map input_node = share_qparams_with_input_act _qspec cat_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True _is_output_of_quantized_pattern=True _annotate_propagation_quantizable_pattern_entry gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None node gm graph nodes _annotate_propagation_quantizable_pattern node quantization_config filter_fn _annotate_propagation_quantizable_pattern node Node quantization_config filter_fn - None Propagate annotation quantizable patterns node target propagation_quantizable_ops _is_any_annotated node node op == call_function is_all_inputs_connected_to_quantized_op input_nodes Ensure all inputs connect fusion pattern quantized node input_node input_nodes _is_quantized_op_pt e input_node False True _skip_annotate node filter_fn quantization_config None _annotate_nodes_not_quantize node node target torch ops aten max_pool d default Recipe maxpool d check input arg maxpool d quantized input_nodes_to_check = node all_input_nodes is_all_inputs_connected_to_quantized_op input_nodes_to_check quantization_config None warnings warn f The input maxpool d quantized skip annotate maxpool d config quantization_config stacklevel= _annotate_maxpool d node quantization_config node target torch ops aten cat default input_nodes_to_check = node all_input_nodes is_all_inputs_connected_to_quantized_op input_nodes_to_check _annotate_cat node quantization_config node target torch ops aten flatten using_ints len node users any user target quantizable_ops user node users keys Recipe flatten check any users flatten node quantizable ops input_node = node all_input_nodes is_all_inputs_connected_to_quantized_op input_node input_qspec_map = input_qspec_map input_node = get_input_act_qspec quantization_config node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation input_qspec_map=input_qspec_map _annotated=True _is_output_of_quantized_pattern=True _annotate_output_share_observer_as_input input_node Node source_node Node source_node_quantization_annotation = source_node meta get QUANT_ANNOTATION_KEY source_node_quantization_annotation source_node_quantization_annotation _is_output_of_quantized_pattern edge_or_node = input_node source_node source_node_quantization_annotation output_qspec = SharedQuantizationSpec edge_or_node _annotate_output_for_int _in_int _out_pattern_entry model torch fx GraphModule node model graph nodes _annotate_output_for_int _in_int _out_pattern node _annotate_output_for_int _in_int _out_pattern node Node - None r Check insert observer output node int _in_int _out_ops needed Recipe refers https github com intel intel-extension-for-pytorch blob d d afc fcc ba bb fb fdd c c intel_extension_for_pytorch quantization _utils py#L noqa B edge_or_node tuple Node Node node target int _in_int _out_ops _is_any_annotated node node target torch ops aten max_pool d default maxpool_node = node _is_all_annotated maxpool_node Get quantization_annotation getitem_node maxpool_node_quantization_annotation = maxpool_node meta get QUANT_ANNOTATION_KEY maxpool_node_quantization_annotation maxpool_node_quantization_annotation _is_output_of_quantized_pattern Annotate output_qspec getitem_node input_act = maxpool_node args assert isinstance input_act Node assert isinstance maxpool_node Node edge_or_node = input_act maxpool_node maxpool_node_quantization_annotation output_qspec = SharedQuantizationSpec edge_or_node input_node = node all_input_nodes _annotate_output_share_observer_as_input input_node node _annotate_linear gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None linear_partitions = get_source_partitions gm graph torch nn Linear torch nn functional linear linear_partitions = list itertools chain from_iterable linear_partitions values partition linear_partitions len partition output_nodes raise ValueError Linear partition cannot have more than one output node linear_node = partition output_nodes linear_node op = call_function linear_node target = torch ops aten linear default raise ValueError f linear_node aten linear operator skip annotation already annotated _skip_annotate linear_node filter_fn continue _annotate_linear_node_helper linear_node True quantization_config _annotate_linear_unary gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None postop_list = torch nn ReLU torch nn LeakyReLU torch nn Tanh torch nn GELU fused_partitions list tuple = postop postop_list fused_partitions = fused_partitions + find_sequential_partitions gm torch nn Linear postop fused_partition fused_partitions linear_partition unary_partition = fused_partition linear_node unary_node = _get_output_nodes_of_partitions linear_partition unary_partition linear_node op = call_function linear_node target = torch ops aten linear default continue _skip_annotate unary_node linear_node filter_fn continue quantization_config None _annotate_nodes_not_quantize linear_node unary_node continue _annotate_linear_node_helper linear_node False quantization_config unary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation _annotated=True _is_output_of_quantized_pattern=True _annotate_linear_binary_unary gm torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None - None linear + binary_op + optional unary op binary_op_list = operator add unary_op_list = torch nn ReLU None combinations = itertools product binary_op_list unary_op_list binary_op unary_op combinations has_unary = unary_op None seq_partition = torch nn Linear binary_op has_unary pyrefly ignore bad-argument-type seq_partition append unary_op fused_partitions = find_sequential_partitions gm seq_partition fused_partition fused_partitions unary_partition unary_node = None None has_unary linear_partition binary_partition unary_partition = fused_partition linear_node binary_node unary_node = _get_output_nodes_of_partitions linear_partition binary_partition unary_partition linear_partition binary_partition = fused_partition linear_node binary_node = _get_output_nodes_of_partitions linear_partition binary_partition len linear_node users = Linear Node should only has user node continue linear_node_idx extra_input_node_idx = _get_input_idx_for_binary_node linear_node binary_node linear_node_idx None extra_input_node_idx None continue linear_node = binary_node args linear_node_idx raise ValueError f linear_node doesn t match input binary node assert isinstance linear_node Node linear_node op = call_function linear_node target = torch ops aten linear default No linear node found fused add continue node_list = binary_node linear_node unary_node None unary_node binary_node linear_node _skip_annotate node_list filter_fn continue quantization_config None _annotate_nodes_not_quantize node_list continue _annotate_linear_node_helper linear_node False quantization_config We don t insert q-dq before binary input node due accuracy issues binary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation input_qspec_map= _annotated=True _is_output_of_quantized_pattern= has_unary unary_node None unary_node meta QUANT_ANNOTATION_KEY = _X InductorQuantizationAnnotation _annotated=True _is_output_of_quantized_pattern=True validate model torch fx GraphModule - None pass