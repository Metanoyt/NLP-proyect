Owner s module sparse copy random torch torch nn torch ao pruning _experimental pruner BaseStructuredSparsifier FakeStructuredSparsity FPGMPruner LSTMSaliencyPruner SaliencyPruner torch nn utils parametrize torch testing _internal common_pruning Conv dActivation Conv dBias Conv dPadBias Conv dPool Conv dPoolFlatten Conv dPoolFlattenFunctional LinearActivation LinearActivationFunctional LinearBias LSTMLayerNormLinearModel LSTMLinearModel rows_are_subset SimpleConv d SimpleLinear torch testing _internal common_utils raise_on_run_directly skipIfTorchDynamo TestCase DEVICES = torch device cpu torch device cuda torch cuda is_available torch device cpu SimplePruner BaseStructuredSparsifier update_mask module tensor_name kwargs getattr module parametrizations tensor_name mask = False ImplementedPruner BaseStructuredSparsifier update_mask module tensor_name kwargs Prunes weight output channels so resulting module has pruning num_rows = len module parametrizations tensor_name mask prune = random sample list range num_rows num_rows module parametrizations tensor_name mask prune = False BottomHalfLSTMPruner BaseStructuredSparsifier Pruner will remove bottom half rows This primarily meant testing purposes update_mask module tensor_name kwargs p getattr module parametrizations tensor_name isinstance p FakeStructuredSparsity mask = p mask masks = torch split mask len mask small masks num = len small small num = False new_mask = torch cat masks mask data = new_mask data TestSaliencyPruner TestCase test_saliency_pruner_update_mask Test we prune out row lowest saliency first row model = SimpleLinear torch no_grad model linear weight = nn Parameter torch Tensor pruning_config = tensor_fqn linear weight sparsity_level pruner = SaliencyPruner pruner prepare model pruning_config pruner enable_mask_update = True pruner step pruned_model = pruner prune expected = torch Tensor pruned = pruned_model linear weight assert expected shape == pruned shape assert torch isclose expected pruned rtol= e- atol= e- all test_lstm_saliency_pruner_update_mask model = LSTMLinearModel input_dim= hidden_dim= output_dim= num_layers= manual_weights = torch Tensor - - - - - - - - torch no_grad model lstm weight_ih_l = nn Parameter manual_weights model lstm weight_hh_l = nn Parameter torch Tensor manual_weights model lstm bias_ih_l = nn Parameter manual_weights model lstm bias_hh_l = nn Parameter manual_weights config = tensor_fqn lstm weight_ih_l tensor_fqn lstm weight_hh_l lstm_input = torch ones fx_pruner = LSTMSaliencyPruner sparsity_level fx_pruner prepare model config fx_pruner enable_mask_update = True fx_pruner step model eval pruned_model = fx_pruner prune pruned_model eval make sure both models run model lstm_input pruned_model lstm_input make sure lowest saliency rows pruned expected = torch Tensor - - - - pruned = model lstm weight_ih_l assert expected shape == pruned shape assert torch isclose expected pruned rtol= e- atol= e- all expected = torch Tensor - - pruned = model lstm weight_hh_l assert expected shape == pruned shape assert torch isclose expected pruned rtol= e- atol= e- all expected = torch Tensor - - pruned model lstm bias_ih_l model lstm bias_hh_l assert expected shape == pruned shape assert torch isclose expected pruned rtol= e- atol= e- all TestBaseStructuredSparsifier TestCase _check_pruner_prepared model pruner device config pruner groups module = config module assert module weight device type == device type Check mask exists assert config tensor_fqn pruner state Check parametrization exists correct assert parametrize is_parametrized module assert hasattr module parametrizations Assume st only parametrization assert type module parametrizations weight FakeStructuredSparsity _check_pruner_valid_before_step model pruner device config pruner groups modules = type config module tuple modules extend config module module = config module modules append module module modules assert module weight device type == device type assert module parametrizations weight mask dtype == torch bool _check_pruner_valid_after_step model pruner mask device config pruner groups modules = type config module tuple modules extend config module module = config module modules append module module modules assert module weight device type == device type total = module parametrizations weight mask numel assert module parametrizations weight mask count_nonzero == total - mask _test_constructor_on_device model device assertRaisesRegex TypeError BaseStructuredSparsifier update_mask BaseStructuredSparsifier model = copy deepcopy model device pruner = SimplePruner None pruner prepare model None pruner enable_mask_update = True g pruner groups module = g module assert module weight device type == device type assert len pruner groups == pruner step Can instantiate model configs model = copy deepcopy model device pruner = SimplePruner test pruner prepare model tensor_fqn seq weight assert len pruner groups == assert pruner groups module_fqn == seq assert test pruner groups assert pruner groups test == test_constructor model = SimpleLinear device DEVICES _test_constructor_on_device model torch device device _test_prepare_linear_on_device model device model = copy deepcopy model device x = torch ones device=device pruner = SimplePruner None pruner prepare model None _check_pruner_prepared model pruner device assert model x shape == test_prepare_linear models = SimpleLinear LinearBias LinearActivation LinearActivationFunctional without bias device DEVICES model models _test_prepare_linear_on_device model torch device device _test_prepare_conv d_on_device model expected_shape config device x = torch ones device=device pruner = SimplePruner None pruner prepare model config _check_pruner_prepared model pruner device assert model x shape == expected_shape test_prepare_conv d models = SimpleConv d Conv dBias Conv dActivation Conv dPadBias Conv dPool shapes = configs = None None None None None device DEVICES model shape config zip models shapes configs model = model device _test_prepare_conv d_on_device model shape config torch device device _test_step_linear_on_device model device model = model device pruner = SimplePruner None pruner prepare model None pruner enable_mask_update = True _check_pruner_valid_before_step model pruner device pruner step _check_pruner_valid_after_step model pruner device test_step_linear models = SimpleLinear LinearBias LinearActivation LinearActivationFunctional device DEVICES model models _test_step_linear_on_device model torch device device _test_step_conv d_on_device model expected_shape config device model = model device x = torch ones device=device pruner = SimplePruner None pruner prepare model config pruner enable_mask_update = True _check_pruner_valid_before_step model pruner device pruner step _check_pruner_valid_after_step model pruner device assert model x shape == expected_shape skipIfTorchDynamo TorchDynamo fails unknown reason test_step_conv d models = SimpleConv d Conv dBias Conv dActivation Conv dPadBias Conv dPool shapes = configs = None None None None None device DEVICES model shape config zip models shapes configs _test_step_conv d_on_device model shape config torch device device _check_pruner_pruned model pruner device config pruner groups module = config module assert hasattr module parametrizations assert hasattr module mask _test_linear_on_device model config expected_shape device also_prune_bias model = model device model eval num_original_params = sum p numel p model parameters x = torch ones device=device pruner = ImplementedPruner prune_bias also_prune_bias pruner prepare model config pruner enable_mask_update = True pruner step y_expected = model x assert y_expected shape == _check_pruner_prepared model pruner device Pruning step pruned = pruner prune y_pruned = pruned x num_pruned_params = sum p numel p pruned parameters assert y_pruned shape == expected_shape _check_pruner_pruned model pruner device y_pruned shape == y_expected shape assert torch isclose y_expected y_pruned rtol= e- atol= e- all assert num_pruned_params num_original_params test_prune_linear_linear r test pruning linear- linear modules configs shapes = configs append tensor_fqn seq weight tensor_fqn seq weight tensor_fqn seq weight shapes append configs append tensor_fqn seq weight tensor_fqn seq weight tensor_fqn seq weight tensor_fqn linear weight shapes append configs append tensor_fqn seq weight tensor_fqn seq weight shapes append device DEVICES also_prune_bias True False config shape zip configs shapes _test_linear_on_device SimpleLinear config shape torch device device also_prune_bias test_prune_linear_bias_linear linear bias - linear no bias configs shapes = configs append tensor_fqn seq weight tensor_fqn seq weight shapes append linear bias - linear bias configs append tensor_fqn seq weight tensor_fqn seq weight shapes append linear no bias - linear bias configs append tensor_fqn seq weight tensor_fqn seq weight tensor_fqn seq weight shapes append device DEVICES also_prune_bias True False config shape zip configs shapes _test_linear_on_device LinearBias config shape torch device device also_prune_bias test_prune_linear_activation_linear config = tensor_fqn seq weight tensor_fqn seq weight tensor_fqn seq weight tensor_fqn linear weight shape = device DEVICES also_prune_bias True False test version nn Modules _test_linear_on_device LinearActivation config shape torch device device also_prune_bias test functional version _test_linear_on_device LinearActivationFunctional config shape torch device device also_prune_bias _test_conv d_on_device model config x expected_shape device also_prune_bias model = model device num_original_params = sum p numel p model parameters model eval pruner = ImplementedPruner prune_bias also_prune_bias pruner prepare model config pruner enable_mask_update = True pruner step y_expected = model x assert y_expected shape == expected_shape _check_pruner_prepared model pruner device Fusion step pruned = pruner prune y_pruned = pruned x num_pruned_params = sum p numel p pruned parameters assert y_pruned shape == expected_shape _check_pruner_pruned model pruner device y_pruned shape == y_expected shape TODO This rtol little high need double check something specific causing fail assert torch isclose y_expected y_pruned rtol= e- atol= e- all f fail type model only time should equal when all layers have padding we can t prune assert num_pruned_params = num_original_params test_prune_conv d_conv d configs shapes = all within sequential blocks configs append tensor_fqn seq weight shapes append prune across sequential blocks configs append tensor_fqn seq weight tensor_fqn seq weight tensor_fqn conv d weight shapes append device DEVICES x = torch ones device=device also_prune_bias True False config shape zip configs shapes _test_conv d_on_device SimpleConv d config x shape torch device device also_prune_bias test_prune_conv d_bias_conv d Conv d Bias no Activation configs shapes = conv d bias - conv d bias configs append tensor_fqn seq weight tensor_fqn seq weight shapes append conv d no bias - conv d bias configs append tensor_fqn seq weight tensor_fqn seq weight tensor_fqn conv d weight shapes append conv d bias - conv d no bias configs append tensor_fqn seq weight tensor_fqn seq weight tensor_fqn seq weight shapes append device DEVICES x = torch ones device=device also_prune_bias True False config shape zip configs shapes _test_conv d_on_device Conv dBias config x shape torch device device also_prune_bias test_prune_conv d_activation_conv d Conv d Activation no Bias configs shapes = conv d no bias - activation - conv d no bias configs append tensor_fqn seq weight shapes append conv d bias - activation - conv d bias configs append tensor_fqn seq weight tensor_fqn seq weight shapes append conv d bias - activation - conv d no bias configs append tensor_fqn seq weight tensor_fqn seq weight shapes append conv d no bias - activation - conv d bias configs append tensor_fqn conv d weight shapes append device DEVICES x = torch ones device=device also_prune_bias True False config shape zip configs shapes _test_conv d_on_device Conv dActivation config x shape torch device device also_prune_bias test_prune_conv d_padding_conv d Conv d Padded layers after Bias layers configs shapes = conv padded bias - conv padded bias configs append tensor_fqn seq weight shapes append conv no bias no pad - conv padded bias configs append tensor_fqn seq weight shapes append conv padded bias - conv no bias no pad configs append tensor_fqn seq weight shapes append conv pad bias - conv no pad bias configs append tensor_fqn seq weight shapes append conv no pad bias - conv pad bias configs append tensor_fqn seq weight shapes append device DEVICES x = torch ones device=device also_prune_bias True False config shape zip configs shapes _test_conv d_on_device Conv dPadBias config x shape torch device device also_prune_bias test_prune_conv d_pool_conv d Conv d Pooling layers config = tensor_fqn seq weight tensor_fqn seq weight tensor_fqn conv d weight tensor_fqn conv d weight shape = device DEVICES x = torch ones device=device also_prune_bias True False _test_conv d_on_device Conv dPool config x shape torch device device also_prune_bias skipIfTorchDynamo TorchDynamo fails unknown reason test_complex_conv d Test fusion models contain Conv d Linear modules Currently supports Conv d-Pool d-Flatten-Linear Skip-add config = tensor_fqn seq weight tensor_fqn seq weight tensor_fqn conv d weight tensor_fqn conv d weight shape = device DEVICES x = torch ones device=device also_prune_bias True False _test_conv d_on_device Conv dPoolFlattenFunctional config x shape torch device device also_prune_bias _test_conv d_on_device Conv dPoolFlatten config x shape torch device device also_prune_bias test_prune_lstm_linear_multiple_layer Test fusion support LSTM multi-layer - Linear model = LSTMLinearModel input_dim= hidden_dim= output_dim= num_layers= config = tensor_fqn lstm weight_ih_l tensor_fqn lstm weight_hh_l tensor_fqn lstm weight_ih_l tensor_fqn lstm weight_hh_l lstm_input = torch ones fx_pruner = BottomHalfLSTMPruner sparsity_level fx_pruner prepare model config fx_pruner enable_mask_update = True fx_pruner step model eval _ _ = model lstm_input pruned_model = fx_pruner prune pruned_model eval _ _ = pruned_model lstm_input expected_params = dict model named_parameters name param model named_parameters assert name expected_params We cannot compare y_expected == y_pruned elements mess up numerics Instead we check weights new LSTM subset weights old LSTM assert rows_are_subset param expected_params name del expected_params name assert we haven t deleted any keys assert len expected_params == test_prune_lstm_linear_single_layer Test fusion support LSTM single-layer - Linear model = LSTMLinearModel input_dim= hidden_dim= output_dim= num_layers= config = tensor_fqn lstm weight_ih_l tensor_fqn lstm weight_hh_l lstm_input = torch ones fx_pruner = BottomHalfLSTMPruner sparsity_level fx_pruner prepare model config fx_pruner enable_mask_update = True fx_pruner step model eval out_expected lstm_out_expected = model lstm_input pruned_model = fx_pruner prune pruned_model eval out_pruned lstm_out_pruned = pruned_model lstm_input _ c = lstm_out_expected size We cannot check y_expected == y_pruned usual because zeros vs missing elements yield different numerical results Instead we check pruned elements first half results since we using BottomHalfLSTMPruner assert torch isclose lstm_out_expected c lstm_out_pruned rtol= e- atol= e- all also check output linear same shape means we ve resized linear columns correctly assert out_expected shape == out_pruned shape test_prune_lstm_layernorm_linear_multiple_layer Test fusion support LSTM multi-layer - Linear model = LSTMLayerNormLinearModel input_dim= output_dim= hidden_dim= num_layers= config = tensor_fqn lstm weight_ih_l tensor_fqn lstm weight_hh_l tensor_fqn lstm weight_ih_l tensor_fqn lstm weight_hh_l lstm_input = torch ones fx_pruner = BottomHalfLSTMPruner sparsity_level fx_pruner prepare model config fx_pruner enable_mask_update = True fx_pruner step model eval _ _ = model lstm_input pruned_model = fx_pruner prune pruned_model eval _ _ = pruned_model lstm_input expected_params = dict model named_parameters name param model named_parameters assert name expected_params We cannot compare y_expected == y_pruned elements mess up numerics Instead we check weights new LSTM subset weights old LSTM assert rows_are_subset param expected_params name del expected_params name assert we haven t deleted any keys assert len expected_params == test_prune_lstm_layernorm_linear_single_layer Test fusion support LSTM single-layer - Linear model = LSTMLinearModel input_dim= hidden_dim= output_dim= num_layers= config = tensor_fqn lstm weight_ih_l tensor_fqn lstm weight_hh_l lstm_input = torch ones fx_pruner = BottomHalfLSTMPruner sparsity_level fx_pruner prepare model config fx_pruner enable_mask_update = True fx_pruner step model eval out_expected lstm_out_expected = model lstm_input pruned_model = fx_pruner prune pruned_model eval out_pruned lstm_out_pruned = pruned_model lstm_input _ c = lstm_out_expected size We cannot check y_expected == y_pruned usual because zeros vs missing elements yield different numerical results Instead we check pruned elements first half results since we using BottomHalfLSTMPruner assert torch isclose lstm_out_expected c lstm_out_pruned rtol= e- atol= e- all also check output linear same shape means we ve resized linear columns correctly assert out_expected shape == out_pruned shape TestFPGMPruner TestCase Test case implementation paper ` Filter Pruning via Geometric Median Deep Convolutional Neural Networks Acceleration https arxiv org abs ` _ SimpleConvFPGM nn Module __init__ - None super __init__ conv d = nn Conv d in_channels= out_channels= kernel_size= padding= bias=False Manually set filter weights demonstration purposes Three filters weight manually set values Different norm-based decision prunes filter value FPGM will prune one value weights = torch tensor Weight weights each filter weights = weights None None None broadcasting conv d weight data copy_ torch ones conv d weight shape weights Second Convolutional Layer conv d = nn Conv d in_channels= out_channels= kernel_size= padding= bias=False weights = torch tensor weights = weights None None None conv d weight data copy_ torch ones conv d weight shape weights forward x x = conv d x x = conv d x x test_compute_distance device= cpu Test distance computation function model = TestFPGMPruner SimpleConvFPGM device pruner = FPGMPruner dist_conv = pruner _compute_distance model conv d weight compute distance matrix using torch cdist flattened_filters = torch Tensor Expected distance matrix should have following values distance should therefore expected_dist_matrix_conv = torch cdist flattened_filters flattened_filters p= expected_dist_conv = torch sum torch abs expected_dist_matrix_conv assert torch isclose dist_conv expected_dist_conv rtol= e- atol= e- all _test_update_mask_on_single_layer expected_conv device Test pruning conducted based pair-wise distance measurement instead absolute norm value test pruning one layer conv d model = TestFPGMPruner SimpleConvFPGM device x = torch ones device=device pruner = FPGMPruner config = tensor_fqn conv d weight pruner prepare model config pruner enable_mask_update = True pruner step assert pruner groups module parametrizations weight mask - item False do prune least-norm filter fusion step pruned_model = pruner prune pruned_y = pruned_model x assert shapes expected_conv = expected_conv device assert pruned_y shape == assert pruned_model conv d weight shape == expected_conv shape assert pruned_model conv d weight shape == conv d should have input channel pruned assert value assert torch isclose pruned_model conv d weight expected_conv rtol= e- atol= e- all _test_update_mask_on_multiple_layer expected_conv expected_conv device second setting model = TestFPGMPruner SimpleConvFPGM device x = torch ones device=device pruner = FPGMPruner config = tensor_fqn conv d weight tensor_fqn conv d weight sparsity_level pruner prepare model config pruner enable_mask_update = True pruner step Get masks two least-norm filters mask = pruner groups module parametrizations weight mask - mask = pruner groups module parametrizations weight mask - Check either least-norm filters pruned assert mask item False mask item False Do prune all least-norm filters fusion step pruned_model = pruner prune pruned_y = pruned_model x assert shapes expected_conv = expected_conv device expected_conv = expected_conv device assert pruned_y shape == assert pruned_model conv d weight shape == expected_conv shape assert pruned_model conv d weight shape == expected_conv shape assert values assert torch isclose pruned_model conv d weight expected_conv rtol= e- atol= e- all assert torch isclose pruned_model conv d weight expected_conv rtol= e- atol= e- all test_update_mask weights = torch tensor expected_conv = torch ones weights None None None weights = torch tensor expected_conv = torch ones weights None None None device DEVICES _test_update_mask_on_single_layer expected_conv device _test_update_mask_on_multiple_layer expected_conv expected_conv device __name__ == __main__ raise_on_run_directly test test_ao_sparsity py