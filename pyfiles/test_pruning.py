Owner s module nn pickle unittest unittest mock mock torch torch nn nn torch nn utils prune prune torch testing _internal common_nn NNTestCase torch testing _internal common_utils instantiate_parametrized_tests run_tests TemporaryFileName TEST_NUMPY TestPruningNN NNTestCase _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True torch nn utils prune py unittest skipIf TEST_NUMPY numpy found test_validate_pruning_amount_init r Test first util function validates pruning amount requested user moment pruning method initialized This test checks expected errors raised whenever amount invalid The original function runs basic type checking + value range checks It doesn t check validity pruning amount respect size tensor prune That s left ` _validate_pruning_amount ` tested below neither float int should raise TypeError assertRaises TypeError prune _validate_pruning_amount_init amount= I m string float should raise ValueError assertRaises ValueError prune _validate_pruning_amount_init amount= assertRaises ValueError prune _validate_pruning_amount_init amount= negative int should raise ValueError assertRaises ValueError prune _validate_pruning_amount_init amount=- all these should pass without errors because they re valid amounts prune _validate_pruning_amount_init amount= prune _validate_pruning_amount_init amount= prune _validate_pruning_amount_init amount= prune _validate_pruning_amount_init amount= prune _validate_pruning_amount_init amount= prune _validate_pruning_amount_init amount= assertTrue True unittest skipIf TEST_NUMPY numpy found test_validate_pruning_amount r Tests second util function validates pruning amount requested user time respect size tensor prune The rationale pruning amount converted absolute value units prune larger than number units tensor then we expect util function raise value error amount int amount tensor_size raise ValueError assertRaises ValueError prune _validate_pruning_amount amount= tensor_size= amount float so should raise error prune _validate_pruning_amount amount= tensor_size= okay prune _validate_pruning_amount amount= tensor_size= prune _validate_pruning_amount amount= tensor_size= prune _validate_pruning_amount amount= tensor_size= assertTrue True unittest skipIf TEST_NUMPY numpy found test_compute_nparams_to_prune r Test requested pruning ` amount ` gets translated into correct absolute number units prune assertEqual prune _compute_nparams_toprune amount= tensor_size= assertEqual prune _compute_nparams_toprune amount= tensor_size= int means unit assertEqual prune _compute_nparams_toprune amount= tensor_size= float means units assertEqual prune _compute_nparams_toprune amount= tensor_size= assertEqual prune _compute_nparams_toprune amount= tensor_size= test_random_pruning_sizes r Test new parameters buffers created pruning method have same size input tensor prune These fact correspond pruned version tensor itself its mask its original copy so size must match fixturize test TODO add other modules modules = nn Linear nn Conv d names = weight bias m modules name names subTest m=m name=name original_tensor = getattr m name prune random_unstructured m name=name amount= mask has same size tensor being pruned assertEqual original_tensor size getattr m name + _mask size orig tensor has same size original tensor assertEqual original_tensor size getattr m name + _orig size new tensor has same size original tensor assertEqual original_tensor size getattr m name size test_random_pruning_orig r Test original tensor correctly stored orig after pruning applied Important make sure we don t lose info about original unpruned parameter fixturize test TODO add other modules modules = nn Linear nn Conv d names = weight bias m modules name names subTest m=m name=name tensor prior pruning original_tensor = getattr m name prune random_unstructured m name=name amount= assertEqual original_tensor getattr m name + _orig test_random_pruning_new_weight r Test module name now contains pruned version original tensor obtained multiplying mask fixturize test TODO add other modules modules = nn Linear nn Conv d names = weight bias m modules name names subTest m=m name=name tensor prior pruning original_tensor = getattr m name prune random_unstructured m name=name amount= weight = weight_orig weight_mask assertEqual getattr m name getattr m name + _orig getattr m name + _mask dtype=original_tensor dtype test_identity_pruning r Test mask s does change forward backward input_ = torch ones m = nn Linear y_prepruning = m input_ output prior pruning compute grad pre-pruning check s equal all ones y_prepruning sum backward old_grad_weight = m weight grad clone don t grab pointer assertEqual old_grad_weight torch ones_like m weight old_grad_bias = m bias grad clone assertEqual old_grad_bias torch ones_like m bias remove grads m zero_grad force mask made all s prune identity m name= weight mask s output should identical no mask y_postpruning = m input_ assertEqual y_prepruning y_postpruning mask s grad should identical no mask y_postpruning sum backward assertEqual old_grad_weight m weight_orig grad assertEqual old_grad_bias m bias grad calling forward twice row shouldn t change output y = m input_ y = m input_ assertEqual y y test_random_pruning_ perc r Test mask s does change forward backward input_ = torch ones m = nn Linear y_prepruning = m input_ output prior pruning compute grad pre-pruning check s equal all ones y_prepruning sum backward old_grad_weight = m weight grad clone don t grab pointer assertEqual old_grad_weight torch ones_like m weight old_grad_bias = m bias grad clone assertEqual old_grad_bias torch ones_like m bias remove grads m zero_grad force mask made all s mock patch torch nn utils prune RandomUnstructured compute_mask compute_mask compute_mask return_value = torch ones_like m weight prune random_unstructured m name= weight amount= amount won t count mask s output should identical no mask y_postpruning = m input_ assertEqual y_prepruning y_postpruning mask s grad should identical no mask y_postpruning sum backward assertEqual old_grad_weight m weight_orig grad assertEqual old_grad_bias m bias grad calling forward twice row shouldn t change output y = m input_ y = m input_ assertEqual y y test_random_pruning input_ = torch ones m = nn Linear define custom mask assign mock mask = torch ones_like m weight mask = mask = check grad zero masked weights mock patch torch nn utils prune RandomUnstructured compute_mask compute_mask compute_mask return_value = mask prune random_unstructured m name= weight amount= y_postpruning = m input_ y_postpruning sum backward weight_orig parameter so s tensor will accumulate grad assertEqual m weight_orig grad mask all s except masked units assertEqual m bias grad torch ones_like m bias make sure weight_orig update doesn t modify old_weight_orig = m weight_orig clone update weights learning_rate = p m parameters p data sub_ p grad data learning_rate since these pruned they should updated assertEqual old_weight_orig m weight_orig assertEqual old_weight_orig m weight_orig test_random_pruning_forward r check forward mask hand input_ = torch ones m = nn Linear define custom mask assign mock mask = torch zeros_like m weight mask = mask = mock patch torch nn utils prune RandomUnstructured compute_mask compute_mask compute_mask return_value = mask prune random_unstructured m name= weight amount= yhat = m input_ assertEqual yhat m weight_orig + m bias assertEqual yhat m weight_orig + m bias test_remove_pruning_forward r Remove pruning check forward unchanged previous pruned state input_ = torch ones m = nn Linear define custom mask assign mock mask = torch ones_like m weight mask = mask = check grad zero masked weights mock patch torch nn utils prune RandomUnstructured compute_mask compute_mask compute_mask return_value = mask prune random_unstructured m name= weight amount= y_postpruning = m input_ prune remove m weight y_postremoval = m input_ assertEqual y_postpruning y_postremoval test_pruning_id_consistency r Test pruning doesn t change id parameters which would otherwise introduce issues pre-existing optimizers point old parameters m = nn Linear bias=False tensor_id = id next iter m parameters prune random_unstructured m name= weight amount= assertEqual tensor_id id next iter m parameters prune remove m weight assertEqual tensor_id id next iter m parameters test_random_pruning_pickle modules = nn Linear nn Conv d names = weight bias m modules name names subTest m=m name=name prune random_unstructured m name=name amount= m_new = pickle loads pickle dumps m assertIsInstance m_new type m test_multiple_pruning_calls you call pruning twice hook becomes PruningContainer m = nn Conv d prune l _unstructured m name= weight amount= weight_mask = m weight_mask save later sanity check prune again prune ln_structured m name= weight amount= n= dim= hook = next iter m _forward_pre_hooks values assertIsInstance hook torch nn utils prune PruningContainer check container _tensor_name correctly set no matter how many pruning methods container assertEqual hook _tensor_name weight check pruning container has right length equal number pruning iters assertEqual len hook m weight has been pruned twice check entries pruning container expected type expected order assertIsInstance hook torch nn utils prune L Unstructured assertIsInstance hook torch nn utils prune LnStructured check all entries st mask nd mask too assertTrue torch all m weight_mask weight_mask == == prune again prune ln_structured m name= weight amount= n=float inf dim= check container _tensor_name correctly set no matter how many pruning methods container hook = next iter m _forward_pre_hooks values assertEqual hook _tensor_name weight test_pruning_container create empty container container = prune PruningContainer container _tensor_name = test assertEqual len container p = prune L Unstructured amount= p _tensor_name = test test adding pruning method container container add_pruning_method p test error raised tensor name different q = prune L Unstructured amount= q _tensor_name = another_test assertRaises ValueError container add_pruning_method q test adding non-pruning method object pruning container raises TypeError assertRaises TypeError container add_pruning_method assertRaises TypeError container add_pruning_method ugh test_pruning_container_compute_mask r Test ` compute_mask ` pruning container known ` t ` ` default_mask ` Indirectly checks Ln structured pruning acting right axis create empty container container = prune PruningContainer container _tensor_name = test test unstructured pruning create new pruning method p = prune L Unstructured amount= p _tensor_name = test add pruning method container container add_pruning_method p create tensor pruned t = torch tensor dtype=torch float create prior mask hand default_mask = torch tensor since we pruning two lowest magnitude units outcome calculation should expected_mask = torch tensor dtype=torch float computed_mask = container compute_mask t default_mask assertEqual expected_mask computed_mask test structured pruning q = prune LnStructured amount= n= dim= q _tensor_name = test container add_pruning_method q since we pruning lowest magnitude one two rows outcome calculation should expected_mask = torch tensor dtype=torch float computed_mask = container compute_mask t default_mask assertEqual expected_mask computed_mask test structured pruning along another axis r = prune LnStructured amount= n= dim= r _tensor_name = test container add_pruning_method r since we pruning lowest magnitude four columns outcome calculation should expected_mask = torch tensor dtype=torch float computed_mask = container compute_mask t default_mask assertEqual expected_mask computed_mask test_l _unstructured_pruning r Test l unstructured pruning actually removes lowest entries l norm hand It also checks applying l unstructured pruning more than once respects previous mask m = nn Linear modify its weight matrix hand m weight = torch nn Parameter torch tensor - - - - dtype=torch float prune l _unstructured m weight amount= expected_weight = torch tensor - - - dtype=m weight dtype assertEqual expected_weight m weight check pruning again removes next two smallest entries prune l _unstructured m weight amount= expected_weight = torch tensor - - dtype=m weight dtype assertEqual expected_weight m weight test_l _unstructured_pruning_with_importance_scores r Test l unstructured pruning actually removes lowest entries importance scores parameter l norm hand It also checks applying l unstructured pruning more than once respects previous mask m = nn Linear modify its weight matrix hand m weight = torch nn Parameter torch tensor - - - - dtype=torch float importance_scores = torch tensor - - - - dtype=torch float prune l _unstructured m weight amount= importance_scores=importance_scores expected_weight = torch tensor - - - dtype=m weight dtype assertEqual expected_weight m weight check pruning again removes two entries m weight colocated next two smallest absolute values importance scores prune l _unstructured m weight amount= importance_scores=importance_scores expected_weight = torch tensor - - dtype=m weight dtype assertEqual expected_weight m weight test_unstructured_pruning_same_magnitude r Since may happen tensor prune has entries same exact magnitude important check pruning happens consistenly based bottom weights threshold which would instead kill off all units magnitude = threshold AMOUNT = p = prune L Unstructured amount=AMOUNT create random tensors entries - t = torch randint low=- high= size= nparams_toprune = prune _compute_nparams_toprune AMOUNT t nelement computed_mask = p compute_mask t default_mask=torch ones_like t nparams_pruned = torch sum computed_mask == assertEqual nparams_toprune nparams_pruned test_random_structured_pruning_amount AMOUNT = AXIS = p = prune RandomStructured amount=AMOUNT dim=AXIS t = torch randint low=- high= size= dtype=torch float prune _compute_nparams_toprune AMOUNT t shape AXIS computed_mask = p compute_mask t default_mask=torch ones_like t check column fully prune others left untouched remaining_axes = _ _ range len t shape _ = AXIS per_column_sums = sorted torch sum computed_mask == axis=remaining_axes assert per_column_sums == test_ln_structured_pruning r Check Ln structured pruning hand m = nn Conv d m weight data = torch tensor - - - expected effect pruning channels L -norm expected_mask_axis = torch ones_like m weight expected_mask_axis = prune ln_structured m weight amount= n= dim= assertEqual expected_mask_axis m weight_mask expected effect pruning columns along axis - L -norm expected_mask_axis = expected_mask_axis expected_mask_axis = prune ln_structured m weight amount= n= dim=- assertEqual expected_mask_axis m weight_mask test_ln_structured_pruning_importance_scores r Check Ln structured pruning hand m = nn Conv d m weight data = torch tensor - - - importance_scores = torch tensor - - - - expected effect pruning channels L -norm expected_mask_axis = torch ones_like m weight expected_mask_axis = prune ln_structured m weight amount= n= dim= importance_scores=importance_scores assertEqual expected_mask_axis m weight_mask expected effect pruning columns along axis - L -norm expected_mask_axis = expected_mask_axis expected_mask_axis = prune ln_structured m weight amount= n= dim=- importance_scores=importance_scores assertEqual expected_mask_axis m weight_mask test_remove_pruning r ` prune remove ` removes hook reparametrization makes pruning final original parameter modules = nn Linear nn Conv d names = weight bias m modules name names subTest m=m name=name first prune prune random_unstructured m name amount= assertIn name + _orig dict m named_parameters assertIn name + _mask dict m named_buffers assertNotIn name dict m named_parameters assertTrue hasattr m name pruned_t = getattr m name then remove pruning prune remove m name assertIn name dict m named_parameters assertNotIn name + _orig dict m named_parameters assertNotIn name + _mask dict m named_buffers final_t = getattr m name assertEqual pruned_t final_t test_remove_pruning_exception r Removing unpruned tensor throws assertion error modules = nn Linear nn Conv d names = weight bias m modules name names subTest m=m name=name check module isn t pruned assertFalse prune is_pruned m since isn t pruned pruning can t removed assertRaises ValueError prune remove m name test_global_pruning r Test global l unstructured pruning over parameters removes ` amount= ` smallest global weights across parameters m = nn Linear n = nn Linear modify weight matrices hand m weight = torch nn Parameter torch tensor - - - - dtype=torch float n weight = torch nn Parameter torch tensor - dtype=torch float params_to_prune = m weight n weight prune smallest weights globally L magnitude prune global_unstructured params_to_prune pruning_method=prune L Unstructured amount= expected_mweight = torch tensor - - - dtype=m weight dtype assertEqual expected_mweight m weight expected_nweight = torch tensor - dtype=n weight dtype assertEqual expected_nweight n weight test_global_pruning_importance_scores r Test global l unstructured pruning over parameters removes ` amount= ` smallest global weights across parameters m = nn Linear n = nn Linear modify weight matrices hand m weight = torch nn Parameter torch tensor - - - - dtype=torch float m_importance_scores = torch tensor - - - - dtype=torch float n weight = torch nn Parameter torch tensor - dtype=torch float n_importance_scores = torch tensor - dtype=torch float params_to_prune = m weight n weight importance_scores = m weight m_importance_scores n weight n_importance_scores prune smallest weights globally L magnitude prune global_unstructured params_to_prune pruning_method=prune L Unstructured amount= importance_scores=importance_scores expected_m_weight = torch tensor - - - dtype=m weight dtype assertEqual expected_m_weight m weight expected_n_weight = torch tensor dtype=n weight dtype assertEqual expected_n_weight n weight test_custom_from_mask_pruning r Test CustomFromMask capable receiving input instantiation time custom mask combining previous default mask generate correct final mask new mask mask = torch tensor old mask default_mask = torch tensor some tensor actually used t = torch rand_like mask dtype=torch float p = prune CustomFromMask mask=mask computed_mask = p compute_mask t default_mask expected_mask = torch tensor dtype=computed_mask dtype assertEqual computed_mask expected_mask test_pruning_rollback r Test something fails when we try compute mask then model isn t left some intermediate half-pruned state The try except statement ` apply ` should handle rolling back previous state before pruning began modules = nn Linear nn Conv d names = weight bias m modules name names subTest m=m name=name mock patch torch nn utils prune L Unstructured compute_mask compute_mask compute_mask side_effect = Exception HA assertRaises Exception prune l _unstructured m name=name amount= assertTrue name dict m named_parameters assertFalse name + _mask dict m named_buffers assertFalse name + _orig dict m named_parameters test_pruning_serialization_model create model model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear check everything looks normal before pruning assertNotIn weight_orig model state_dict assertNotIn weight_mask model state_dict assertIn weight model state_dict prune one its parameters prune l _unstructured module=model name= weight amount= check original weight new mask present assertIn weight_orig model state_dict assertIn weight_mask model state_dict assertNotIn weight model state_dict assertTrue hasattr model weight pruned_weight = model weight TemporaryFileName fname torch save model fname weights_only=False legacy code saves model new_model = torch load fname weights_only=False check original weight new mask present assertIn weight_orig new_model state_dict assertIn weight_mask new_model state_dict assertNotIn weight new_model state_dict assertTrue hasattr new_model weight assertEqual pruned_weight new_model weight test_pruning_serialization_state_dict create model model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear check everything looks normal before pruning assertNotIn weight_orig model state_dict assertNotIn weight_mask model state_dict assertIn weight model state_dict prune one its parameters prune l _unstructured module=model name= weight amount= check original weight new mask present assertIn weight_orig model state_dict assertIn weight_mask model state_dict assertNotIn weight model state_dict assertTrue hasattr model weight pruned_weight = model weight make pruning permanent restore parameter names base architecture prune remove module=model name= weight check original weight new mask no longer present assertNotIn weight_orig model state_dict assertNotIn weight_mask model state_dict assertIn weight model state_dict save state dict model reload into new_model new_model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear TemporaryFileName fname torch save model state_dict fname new_model load_state_dict torch load fname check original weight new mask present new_model either assertNotIn weight_orig new_model state_dict assertNotIn weight_mask new_model state_dict assertIn weight new_model state_dict assertEqual pruned_weight new_model weight test_prune create new pruning method p = prune L Unstructured amount= create tensor pruned t = torch tensor dtype=torch float create prior mask hand default_mask = torch tensor since we pruning two lowest magnitude units outcome calculation should expected_mask = torch tensor pruned_tensor = p prune t default_mask assertEqual t expected_mask pruned_tensor test_prune_importance_scores create new pruning method p = prune L Unstructured amount= create tensor pruned t = torch tensor dtype=torch float importance_scores = torch tensor dtype=torch float create prior mask hand default_mask = torch tensor since we pruning two lowest magnitude units outcome calculation should expected_mask = torch tensor pruned_tensor = p prune t default_mask importance_scores=importance_scores assertEqual t expected_mask pruned_tensor test_prune_importance_scores_mimic_default create new pruning method p = prune L Unstructured amount= create tensor pruned t = torch tensor dtype=torch float create prior mask hand default_mask = torch tensor since we pruning two lowest magnitude units outcome calculation should expected_mask = torch tensor pruned_tensor_without_importance_scores = p prune t default_mask pruned_tensor_with_importance_scores = p prune t default_mask importance_scores=t assertEqual pruned_tensor_without_importance_scores pruned_tensor_with_importance_scores assertEqual t expected_mask pruned_tensor_without_importance_scores test_rnn_pruning l = torch nn LSTM This Module has parameters called weight_ih_l weight_hh_l bias_ih_l bias_hh_l Pruning one them causes one weights become tensor prune l _unstructured l weight_ih_l assert sum isinstance p torch nn Parameter p l _flat_weights == Removing pruning reparameterization restores Parameter prune remove l weight_ih_l assert sum isinstance p torch nn Parameter p l _flat_weights == Make sure upon removal reparameterization ` _parameters ` ` named_parameters ` contain right params Specifically original weight weight_ih_l should placed back parameters while reparameterization component weight_ih_l _orig should removed assert weight_ih_l l _parameters assert l _parameters weight_ih_l None assert weight_ih_l _orig l _parameters assert weight_ih_l dict l named_parameters assert dict l named_parameters weight_ih_l None assert weight_ih_l _orig dict l named_parameters instantiate_parametrized_tests TestPruningNN __name__ == __main__ run_tests