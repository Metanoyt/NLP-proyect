argparse datetime tempfile collections defaultdict dataclasses dataclass types ModuleType typing Any Optional Protocol torch torch autograd DeviceType torch utils _ordered_set OrderedSet runtime benchmarking benchmarker runtime runtime_utils create_bandwidth_info_str get_num_bytes BenchmarkCallableType Protocol __call__ times int repeat int - float _kernel_category_choices = foreach persistent_reduction pointwise reduction split_scan template get_kernel_category_by_source_code src_code str - str Similar get_kernel_category use source code Call API we have compile src_code module yet choices = ch ch _kernel_category_choices f triton_heuristics ch src_code len choices == choices unknown get_kernel_category kernel_mod ModuleType - str Given module defining triton kernel category kernel Category can one - pointwise - reduction - persistent_reduction Currently we simply decide category depending what decorator imported kernel choices = ch ch _kernel_category_choices ch kernel_mod __dict__ len choices == choices unknown get_triton_kernel mod ModuleType type ignore no-untyped-def torch _inductor runtime triton_heuristics CachingAutotuner cand_list = v k v mod __dict__ items k startswith triton_ isinstance v CachingAutotuner assert len cand_list == cand_list benchmark_all_kernels benchmark_name str benchmark_all_configs Optional dict Any Any - None An experimental API used only when config benchmark_kernel true Run kernel benchmarks all kernels cached PyCodeCache Used compiled modules Put method here rather than codegen convenience since its implementation does change based different graph modules being compiled torch _inductor codecache PyCodeCache nfound = kernel_mod PyCodeCache modules kernel_key = kernel_mod key hasattr kernel_mod get_args hasattr kernel_mod call continue triton_kernel = get_triton_kernel kernel_mod kernel_category = get_kernel_category kernel_mod args = kernel_mod get_args num_in_out_ptrs = len arg_name arg_name triton_kernel fn arg_names arg_name startswith in_out_ptr num_gb = triton_kernel inductor_meta get kernel_num_gb None num_gb None num_gb = get_num_bytes args num_in_out_args=num_in_out_ptrs e get_info_str ms float n_regs Optional Any n_spills Optional Any shared Optional Any prefix str = - str any x None x n_regs n_spills shared kernel_detail_str = f n_regs regs n_spills spills shared shared mem kernel_detail_str = gb_per_s = num_gb ms e create_bandwidth_info_str ms num_gb gb_per_s prefix=prefix suffix=kernel_detail_str kernel_desc = f benchmark_name kernel_category upper kernel_key benchmark_all_configs assert hasattr kernel_mod benchmark_all_configs bench_result = kernel_mod benchmark_all_configs args print kernel_desc launcher ms bench_result items print f get_info_str ms launcher n_regs launcher n_spills launcher shared launcher config ms = benchmarker benchmark_gpu lambda kernel_mod call args rep= assert len triton_kernel launchers == Autotuner should have selected best config launcher = triton_kernel launchers print get_info_str ms launcher n_regs launcher n_spills launcher shared prefix=f kernel_desc nfound += nfound == print No kernel benchmark functionality found Make sure you run inductor config benchmark_kernel being True dataclass ProfileEvent category str key str self_device_time_ms float benchmark run multiple times we average count across all runs It should integer define float just case count float parse_profile_event_list benchmark_name str event_list torch autograd profiler_util EventList wall_time_ms float nruns int device_name str - None Parse generate report event_list get_self_device_time ev torch autograd profiler_util EventList - float ev self_device_time_total microsecond Convert millisecond ev self_device_time_total nruns type ignore attr-defined all_events dict str list ProfileEvent = defaultdict list add_event ev torch autograd profiler_util EventList category str - None profile_ev = ProfileEvent category=category key=ev key type ignore attr-defined self_device_time_ms=get_self_device_time ev count=ev count nruns type ignore operator average across all runs all_events category append profile_ev ev event_list assert ev is_legacy Don t support legacy profiler ev device_type == DeviceType CPU ignore event CPU side continue category = unknown ev key startswith triton_ ev key startswith triton_poi category = triton_pointwise ev key startswith triton_red category = triton_reduction ev key startswith triton_per category = triton_persistent_reduction category = triton_unknown add_event ev category report_category category str profile_events list ProfileEvent - float device_name tabulate tabulate profile_events sort key=lambda ev ev self_device_time_ms reverse=True rows = total_time = print f \n == category category kernels == ev profile_events total_time += ev self_device_time_ms percent = f ev self_device_time_ms wall_time_ms f rows append ev key ev self_device_time_ms ev count percent rows append Total total_time f total_time wall_time_ms f print tabulate rows headers= Kernel f Self device_name upper TIME ms Count Percent total_time report - None category_list = triton_pointwise triton_reduction triton_persistent_reduction triton_unknown unknown assert OrderedSet all_events keys issubset OrderedSet category_list f list all_events keys per_category_wall_time = total_device_ms = category category_list category all_events _time = report_category category all_events category per_category_wall_time category = _time total_device_ms += _time device_busy_percent = f total_device_ms wall_time_ms f device_name print f \nPercent time when device_name upper busy device_busy_percent print No device detected print f Total wall time wall_time_ms f ms output such line so we can gather such line all compiled modules all benchmarks tabulate Columns benchmark_name pointwise_percent reduction_percent persistent_reduction_percent unknown_category_percent device_busy_percent wall_time_ms tabulate_line = f Output tabulate benchmark_name category category_list percent = f per_category_wall_time get category wall_time_ms f tabulate_line += f percent tabulate_line += f device_busy_percent wall_time_ms f ms print tabulate_line report PROFILE_DIR = tempfile gettempdir PROFILE_PATH = f PROFILE_DIR compiled_module_profile json perf_profile wall_time_ms float times int repeat int benchmark_name str benchmark_compiled_module_fn BenchmarkCallableType - None torch profiler profile record_shapes=True p benchmark_compiled_module_fn times=times repeat=repeat path = PROFILE_PATH p export_chrome_trace path print f Profiling result compiled module benchmark benchmark_name print f Chrome trace profile written path event_list = p key_averages group_by_input_shape=True print event_list table sort_by= self_device_time_total row_limit= parse_profile_event_list benchmark_name event_list wall_time_ms times repeat p use_device ncu_analyzer benchmark_name str benchmark_compiled_module_fn BenchmarkCallableType args argparse Namespace - None inspect os subprocess kernel_regex = args ncu_kernel_regex metrics = args ncu_metrics module_file = inspect getfile benchmark_compiled_module_fn module_dir = os path dirname module_file module_name = os path splitext os path basename module_file ncu_dir = tempfile gettempdir timestamp = datetime datetime now strftime Y m d_ H M S ncu_output = os path join ncu_dir f ncu_output_ timestamp ncu-rep python_cmd = f sys sys path insert module_dir f module_name benchmark_compiled_module benchmark_compiled_module times= repeat= ncu_cmd = ncu -- target-processes all -- replay-mode kernel -- kernel-name-base function -- print-units base -- import-source yes -- force-overwrite -- export ncu_output kernel_regex ncu_cmd extend -- kernel-name f regex kernel_regex metrics ncu_cmd extend -- metrics metrics ncu_cmd extend -- set full ncu_cmd extend python -c python_cmd try subprocess run ncu_cmd check=True print f \nNCU profiling results benchmark benchmark_name print f NCU report has been written ncu_output except subprocess CalledProcessError e print f NCU profiling failed error e collect_memory_snapshot benchmark_compiled_module_fn BenchmarkCallableType - None assert torch cuda is_available torch cuda memory _record_memory_history max_entries= benchmark_compiled_module_fn times= repeat= run times snapshot_path = f tempfile gettempdir memory_snapshot pickle torch cuda memory _dump_snapshot snapshot_path torch cuda memory _record_memory_history enabled=None print f The collect memory snapshot has been written snapshot_path With AOTAutograd cache we directly call compiled module So prevent Dynamo reentering torch compiler disable type ignore misc compiled_module_main benchmark_name str benchmark_compiled_module_fn BenchmarkCallableType - None This function called __main__ block compiled module argparse parser = argparse ArgumentParser parser add_argument -- benchmark-kernels -k action= store_true help= Whether benchmark each individual kernels parser add_argument -- benchmark-all-configs -c action= store_true help= Whether benchmark each individual config kernel parser add_argument -- profile -p action= store_true help= Whether profile compiled module parser add_argument -- cuda-memory-snapshot action= store_true help= Whether collect CUDA memory snapshot Refer https pytorch org blog understanding-gpu-memory- details about how visualize collected snapshot parser add_argument -- ncu action= store_true help= Whether run ncu analysis parser add_argument -- ncu-kernel-regex type=str default=None help= Filter kernels profiled NCU using regex e g ^triton_ Maps -- kernel-name regex regex If None NCU will profile all kernels parser add_argument -- ncu-metrics type=str default=None help= Comma-separated list NCU metrics collect e g dram__bytes sum per_second If None NCU will use -- set full parser add_argument -- times type=int default= help= Number times run each benchmark iteration parser add_argument -- repeat type=int default= help= Number repetitions each benchmark run args = parser parse_args args benchmark_kernels benchmark_all_kernels benchmark_name args benchmark_all_configs times = args times repeat = args repeat torch cuda is_available torch cuda reset_peak_memory_stats wall_time_ms = benchmark_compiled_module_fn times=times repeat=repeat torch cuda is_available peak_mem = torch cuda max_memory_allocated print f Peak GPU memory usage peak_mem e f MB torch cuda is_available args cuda_memory_snapshot collect_memory_snapshot benchmark_compiled_module_fn args profile perf_profile wall_time_ms times repeat benchmark_name benchmark_compiled_module_fn args ncu ncu_analyzer benchmark_name benchmark_compiled_module_fn args=args