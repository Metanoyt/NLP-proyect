ast copy csv functools json os platform timeit collections namedtuple dataclasses asdict dataclass typing Any Optional benchmark_utils numpy np torch needs imported after torch torch utils cpp_extension cpp_extension noqa F torch utils benchmark Timer Performance microbenchmarks This module contains core functionalities performance microbenchmark tests This used store configs tests An example input TestConfig test_name= add_M _N _K input_config= M N K tag= long run_backward=False TestConfig = namedtuple TestConfig test_name input_config tag run_backward BENCHMARK_TESTER = SKIP_OP_LISTS = weight_norm_sparsifier_step _register_test test_metainfo save metainfo needed create test Currently test_metainfo takes two different inputs This input when adds single op benchmark _register_test configs pt_bench_op create_pytorch_op_test_case run_backward=True This input when adds list ops benchmark _register_test configs pt_bench_op create_pytorch_op_test_case run_backward=False op_name_function=op BENCHMARK_TESTER append test_metainfo _create_test bench_op_obj orig_test_attrs tags OperatorTestCase run_backward bwd_input Create tests benchmark backend Args bench_op_obj object which instantiated subclass TorchBenchmarkBase which includes tensor creation operator execution orig_test_attrs dictionary includes test configs tags attribute test config filter inputs OperatorTestCase named tuple save metadata test run_backward bool parameter indicating backward path test_attrs = copy deepcopy orig_test_attrs test_attrs = k str v k v test_attrs items ascii_test_attrs = ast literal_eval json dumps test_attrs input_config = str ascii_test_attrs - replace bwd_input When auto_set used test name needs include input test_attrs update bwd bwd_input test_name = bench_op_obj test_name test_attrs test_config = TestConfig test_name input_config tags run_backward OperatorTestCase bench_op_obj test_config _build_test configs bench_op OperatorTestCase run_backward op_name_function=None Generate PyTorch Caffe tests operators different inputs Args configs dictionary has input shapes bench_op subclass TorchBenchmarkBase which includes tensor creation operator execution OperatorTestCase named tuple save metadata test run_backward bool parameter indicating backward path op_name_function dictionary includes operator name function config configs test_attrs = tags = None keep_config = True attr config tags only used our benchmark backend filter tests will removed config which then passed init function example config atrr config M N K tags short attr tags short tags attr tags = attr tags continue cuda specified input shape testing machines doesn t support we will skip input cuda attr values torch cuda is_available keep_config = False break test_attrs update attr keep_config continue tags None raise ValueError Missing tags configs op = bench_op assert op None Can t create test op_name_function dictionary which has op_name op_function example op_name_function op_name abs op_function torch abs op_function concatenated input dict then passed init function op_name passed set_module_name function init_dict = copy deepcopy test_attrs op_name_function None op_name = op_name_function op_name init_dict update op_func op_name_function op_func op set_module_name op_name op _set_backward_test run_backward op init init_dict op extract_inputs_tuple run_backward attr vars op values isinstance attr torch nn Module param attr parameters param requires_grad = False input_name = None _num_inputs_require_grads used track number tensors which use auto_set op _num_inputs_require_grads input_name = all yield _create_test op test_attrs tags OperatorTestCase run_backward input_name This loop only used when auto_set used _pass_count counts how many times init has been called _auto_set_counter reset after init called i range op _num_inputs_require_grads op _pass_count += op _auto_set_counter = TODO mingzhe remove deepcopy when we encounter performance issue new_op = copy deepcopy op new_op init init_dict Input name index will start input input_name = i + yield _create_test new_op test_attrs tags OperatorTestCase run_backward input_name BenchmarkRunner BenchmarkRunner responsible benchmarking all registered benchmark test groups Attributes tag_filter str control benchmarks which matches tag operator str only run benchmark test cases contains filter string test case s id test_name str only run benchmark test cases matches filter case-sensitive substring match happens _keep_test method __init__ args TODO consider time-bound constraints well args = args iters = has_explicit_iteration_count = False multiplier = predefined_minimum_secs = max_iters = e use_jit = args use_jit use_compile = args use_compile use_jit use_compile raise ValueError use_jit use_compile mutually exclusive please specify one num_runs = args num_runs print_per_iter = False output_csv = args output_csv operator_range = benchmark_utils get_operator_range args operator_range default warmup iterations args warmup_iterations == - args warmup_iterations = args iterations args iterations = - has_explicit_iteration_count = True iters = args iterations when specific test selected user we don t need match tag anymore args test_name None args tag_filter = None _print_header DASH_LINE = - print f DASH_LINE \n PyTorch Caffe Operator Micro-benchmarks\n f DASH_LINE \n f Tag args tag_filter \n args list_tests print List tests args list_ops print List Operators run printed_ops_list = set args operators print f args operators _print_perf_result results test_case args report_aibench Output AIBench Print out per iteration execution time instead avg time test_name = _ join test_case framework test_case test_config test_name run range num_runs print f test_case framework Observer + json dumps type test_name metric latency unit us value str results reported_run_time_us run print f Mode JIT use_jit Compile use_compile Eager print f Name test_case test_config test_name \n# Input test_case test_config input_config mode = Backward test_case test_config run_backward Forward num_runs run range num_runs print f Run run mode Execution Time us results reported_run_time_us run f print print f mode Execution Time us results reported_run_time_us f print f Peak Memory KB results peak_memory \n _perf_result_to_dict results test_case This function parallel _print_perf_result which instead writing information terminal returns dictionary args report_aibench out = test_name test_case test_config test_name input_config test_case test_config input_config runtime JIT use_jit Compile use_compile Eager run Backward test_case test_config run_backward Forward latency round results reported_run_time_us latency unit us peak memory results peak_memory memory unit KB parsing test_case test_config input_config adding entries out dictionary input M N K device cpu output M N K device cpu splitting string unnested commas split s open_to_close = break_idxs = - curr_brackets = i c enumerate s c open_to_close keys curr_brackets append c c open_to_close values assert curr_brackets open_to_close curr_brackets - == c ERROR able parse string curr_brackets pop c == curr_brackets break_idxs append i break_idxs append len s out = i range len break_idxs - start end = break_idxs i break_idxs i + out append s start + end out key_vals = split test_case test_config input_config M ZPB - M ZPB key_vals = key strip value strip key value map lambda str str split key_vals noqa C M ZPB - M ZPB out update key_vals out _predict_num_iter_needed i i multiplier _iteration_result_is_significant iters run_time_sec curr_test_total_time has_explicit_iteration_count This function decides whether measured time can reported based following conditions number iterations larger than max_iters execution time larger than predefined minimum_time execution time larger than user defined minimum_time iters max_iters run_time_sec predefined_minimum_secs has_explicit_iteration_count curr_test_total_time args min_time_per_test _launch_forward test_case iters print_per_iter Use Python s timeit module measure execution time unit second cuda_sync = cuda test_case test_config test_name func = test_case run_forward use_jit func = test_case run_jit_forward use_compile func = test_case run_compile_forward cuda_sync forward_time = timeit timeit functools partial func iters print_per_iter cuda_sync number= forward_time Stable timing Timer timer = Timer stmt= func iters print_per_iter cuda_sync globals= func func iters iters print_per_iter print_per_iter cuda_sync cuda_sync result = timer adaptive_autorange min_run_time= result median iters _launch_backward test_case iters print_per_iter=False This function runs forward path op get output Then backward path executed execution time reported test_case run_forward num_runs= print_per_iter=False cuda_sync=False test_case _output_mean backward_time = timeit timeit functools partial test_case run_backward iters print_per_iter number= backward_time _measure_metrics launch_test test_case iters print_per_iter This function execute operator iters iterations then look time If s significant number iterations will increased before rerun The execution stops when time becomes significant curr_test_total_time = time_trace = peak_memory = input_values = test_case op_bench inputs values device device_module = None None input_values isinstance next iter input_values torch Tensor The device device module information crucial memory metric calculation In case ops where inputs integers tensor memory metrics need calculated sample_input = next iter input_values device = sample_input device device_module = torch get_device_module device type TODO add support cpu memory measurement while True hasattr device_module reset_peak_memory_stats device_module reset_peak_memory_stats device run_time_sec = launch_test test_case iters print_per_iter hasattr device_module synchronize device_module synchronize device Memory measurement process hasattr device_module max_memory_allocated peak_memory = device_module max_memory_allocated device curr_test_total_time += run_time_sec Analyze time after each run decide result stable results_are_significant = _iteration_result_is_significant iters run_time_sec curr_test_total_time has_explicit_iteration_count report_run_time = e run_time_sec iters time_trace append report_run_time Print out time spent each epoch ms args report_aibench mode = JIT use_jit Compile use_compile Eager test_name = _ join test_case framework test_case test_config test_name mode print PyTorchObserver + json dumps type test_name metric latency unit ms value str report_run_time e results_are_significant break Re-estimate hopefully-sufficient iteration count run benchmark again iters = _predict_num_iter_needed iters reported_run_time_us = np percentile np array time_trace reported_run_time_us peak_memory _check_keep test_flag cmd_flag cmd_flag None test_flag == cmd_flag _check_operator_first_char test_flag cmd_flag cmd_flag None test_flag lower cmd_flag _check_keep_list test_flag cmd_flag_list cmd_flag_list None any test_flag == cmd_flag cmd_flag cmd_flag_list _check_skip test_module cmd_flag cmd_flag None test_module cmd_flag _keep_test test_case TODO consider regex matching test filtering Currently sub-string matching op_test_config = test_case test_config operators = benchmark_utils process_arg_list args operators args operators None Filter framework operator test_name tag forward_only _check_keep op_test_config test_name args test_name _check_keep_list test_case op_bench module_name operators _check_skip test_case op_bench module_name SKIP_OP_LISTS _check_operator_first_char test_case op_bench module_name operator_range args tag_filter == all _check_keep op_test_config tag args tag_filter args forward_only op_test_config run_backward = args forward_only args device == None device test_case test_config input_config args device op_test_config test_name _print_test_case_info test_case Print out test name skip real execution args list_tests print f test_case test_config test_name True args list_ops args operators None op_name = test_case op_bench module_name op_name printed_ops_list print f op_name printed_ops_list add op_name True False _output_csv filename headers row os path exists filename open filename fd lines = list csv reader fd headers len headers len lines prior results failed header might filled yet lines = headers headers = lines lines = headers lines append f x f isinstance x float x x row open filename w fd writer = csv writer fd lineterminator= \n line lines writer writerow list line + len headers - len line _output_json perf_list output_file benchmark_name= PyTorch operator benchmark Write result into JSON format so can uploaded benchmark database displayed OSS dashboard The JSON format defined https github com pytorch pytorch wiki How-to-integrate-with-PyTorch-OSS-benchmark-database perf_list Prepare headers records JSON output records = perf_item perf_list Extract data perf_item test_name = perf_item get test_name unknown input_config = perf_item get input_config run_type = perf_item get run latency = perf_item get latency peak_memory = perf_item get peak memory device = perf_item get device unknown dtype = perf_item get dtype torch float split runtime = perf_item get runtime None Extract mode based run_type mode = None run_type == Forward mode = inference run_type == Backward mode = training Extract use_compile runtime == Compile use_compile = True runtime == Eager use_compile = False use_compile = None device_arch = torch cuda get_device_name device == cuda platform processor device == cpu unknown Extract operator name test_name operator_name = test_name split _ Create record dataclass BenchmarkInfo name str mode Optional str dtype str extra_info dict str Any dataclass ModelInfo name str type str origins list str extra_info dict str Any dataclass MetricInfo name str unit str benchmark_values list float target_value Optional float dataclass BenchmarkRecord benchmark BenchmarkInfo model ModelInfo metric MetricInfo Add record latency record_latency = BenchmarkRecord benchmark=BenchmarkInfo name=benchmark_name mode=mode dtype=dtype extra_info= input_config input_config device device arch device_arch use_compile use_compile operator_name operator_name model=ModelInfo name=test_name type= micro-benchmark origins= pytorch extra_info= operator_name operator_name metric=MetricInfo name= latency unit= us benchmark_values= latency target_value=None records append asdict record_latency Add record peak memory record_memory = copy deepcopy record_latency record_memory metric = MetricInfo name= peak memory unit= KB benchmark_values= peak_memory target_value=None records append asdict record_memory Write all records output file open output_file w encoding= utf- f json dump records f indent= run _print_header output_csv_filename = args output_csv headers = Benchmarking Framework Benchmarking Module Name Case Name tag run_backward Execution Time Peak Memory KB args output_json args output_json_for_dashboard perf_list = test_metainfo BENCHMARK_TESTER test _build_test test_metainfo full_test_id test_case = test op_test_config = test_case test_config _print_test_case_info test_case continue _keep_test test_case continue To reduce variance fix numpy randseed test case so randomly generated input tensors remain same each test case The random seed limited -bit because numpy requirement np random seed seed=hash full_test_id - print f Benchmarking test_case framework test_case op_bench module_name op_test_config run_backward launch_func = _launch_backward launch_func = _launch_forward Warmup launch_func test_case args warmup_iterations print_per_iter=False Actual Execution results = _measure_metrics launch_func test_case iters print_per_iter _ range num_runs result_dict = dict result_dict reported_run_time_us = r r results result_dict peak_memory = results _print_perf_result results=result_dict test_case=test_case output results csv _output_csv output_csv_filename headers test_case framework test_case op_bench module_name test_case test_config test_name + _BACKWARD test_case test_config run_backward True test_case test_config test_name test_case test_config tag test_case test_config run_backward result_dict reported_run_time_us result_dict peak_memory args output_json args output_json_for_dashboard perf_list append _perf_result_to_dict result_dict test_case args output_json_for_dashboard _output_json perf_list args output_json_for_dashboard args benchmark_name args output_json open args output_json w f json dump perf_list f