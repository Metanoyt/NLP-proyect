mypy allow-untyped-defs dataclasses operator sys enum Enum typing Callable Optional torch cpp_builder ir cpu_vec_isa pick_vec_isa VecAMX VecAVX VecAVX VecISA VecNEON VecSVE utils IndentedBuffer parallel_num_threads virtualized V common KernelTemplate cpp_template_kernel CppTemplateKernel cpp_utils DTYPE_TO_CPP GemmBlocking value_to_cpp LayoutType Enum NORMAL = VNNI = VNNI = _IS_WINDOWS = sys platform == win get_restrict_keyword - str _IS_WINDOWS https learn microsoft com en-us cpp cpp extension-restrict view=msvc- __restrict __restrict__ CppMicroGemm A codegens kernel computes small-sized matrix multiplication A micro GEMM kernel responsible register blocking instruction selection other CPU architecture-specific optimizations The subclasses need override ` codegen_define ` define kernel function called code generated ` codegen_call ` TODO jgong support constant shapes lds template args DECLARE_KERNEL = r template bool accum bool prefetch=false inline void kernel_name - kernel_extra_args_declare kernel_extra_args_declare - endif const input_t restrict_keyword A const input _t restrict_keyword B output_t restrict_keyword C int _t M int _t N int _t K int _t lda int _t ldb int _t ldc __init__ name input_dtype input _dtype output_dtype compute_dtype register_blocking alpha= - None name = name input_dtype = input_dtype assert input _dtype None input _dtype = input _dtype output_dtype = output_dtype compute_dtype = compute_dtype register_blocking = register_blocking alpha = alpha pack_vnni_B_locally = False get_common_options input_dtype torch uint torch int assert compute_dtype == torch int assert output_dtype == torch int assert input _dtype == torch int torch torch kernel_name name input_dtype input_dtype input _dtype input _dtype output_dtype output_dtype compute_dtype compute_dtype input_t DTYPE_TO_CPP input_dtype input _t DTYPE_TO_CPP input _dtype output_t DTYPE_TO_CPP output_dtype compute_t DTYPE_TO_CPP compute_dtype alpha alpha kernel_extra_args_declare get_kernel_extra_args_declare int _gemm input_dtype torch uint torch int vnni_size input_dtype torch uint torch int restrict_keyword get_restrict_keyword pack_vnni_B_locally pack_vnni_B_locally template is_woq_int is_woq_int get_kernel_declaration options = get_common_options KernelTemplate _template_from_string DECLARE_KERNEL render options get_kernel_extra_args_declare - str get_kernel_extra_args kwargs - list str codegen_define kernel CppTemplateKernel - str raise NotImplementedError codegen_call kernel CppTemplateKernel A ir Buffer B ir Buffer C ir Buffer accum bool prefetch bool = False kwargs_for_extra_args - str Generate code calling templated kernel computes ` C += alpha A B ` ` accum ` True ` C = alpha A B ` otherwise A_ptr = f kernel index A B_ptr = f kernel index B C_ptr = f kernel index C M = kernel size C N = kernel size C K = kernel size A lda = kernel stride A ldb = kernel stride B ldc = kernel stride C res = IndentedBuffer res writeline f name value_to_cpp accum bool value_to_cpp prefetch bool res indent kwargs_for_extra_args update kernel kernel extra_args = get_kernel_extra_args kwargs_for_extra_args arg extra_args res writeline arg res writeline f A_ptr res writeline f B_ptr res writeline f C_ptr res writeline f M res writeline f N res writeline f K res writeline f lda res writeline f ldb res writeline f ldc res writeline res getvalue use_local_vnni_blocking should_block_weight bool pack_vnni_B_locally = should_block_weight codegen_init kernel CppTemplateKernel - str codegen_finalize kernel CppTemplateKernel - str get_b_layout - LayoutType LayoutType NORMAL ALLOCATE_WEIGHT_BUFFER = r - is_msvc_compiler MSVC doesn t support stack-allocated dynamic-sized arrays so using heap memory here auto heap_deq_b_buf_ptr = std make_unique buffer_dtype buffer_size buffer_dtype buffer_name = heap_deq_b_buf_ptr get - It s safe use stack-allocated array since blocking strategy would require us allocate array s smaller than size L D cache default per thread max stack size Linux quite higher so we need worry about stack overflow alignas buffer_dtype buffer_name buffer_size - endif codegen_allocate_weight_buffer buffer_name str buffer_dtype str size_args - str buffer_size = join map str size_args KernelTemplate _template_from_string ALLOCATE_WEIGHT_BUFFER render buffer_name buffer_name buffer_dtype buffer_dtype buffer_size buffer_size is_msvc_compiler cpp_builder is_msvc_cl is_woq_int False dataclasses dataclass CppMicroGemmConfig input_dtype torch dtype input _dtype torch dtype output_dtype torch dtype compute_dtype torch dtype vec_isa_cls type VecISA register_blocking GemmBlocking extra_check Optional Callable bool = None micro_gemm_configs dict type CppMicroGemm list CppMicroGemmConfig = register_micro_gemm configs inner cls assert cls micro_gemm_configs f Duplicate micro_gemm registration cls assert len configs f No micro_gemm configs provided cls micro_gemm_configs cls = list configs cls inner generate_gemm_config vec_isa_cls register_blockings input_dtype=torch float input _dtype=None output_dtype=None compute_dtype=None extra_check=None output_dtype None output_dtype = input_dtype compute_dtype None compute_dtype = output_dtype input _dtype None input _dtype = input_dtype CppMicroGemmConfig input_dtype input _dtype output_dtype compute_dtype vec_isa_cls GemmBlocking blocking extra_check blocking register_blockings CppMicroGemmRef CppMicroGemm A reference implementation CppMicroGemm naive C++ code It used correctness debugging TEMPLATE_ENTRY = r declare_kernel int _t m = m M ++m int _t n = n N ++n compute_t result = accum C m ldc + n int _t k = k K ++k result += compute_t A m lda + k compute_t B k ldb + n alpha C m ldc + n = result __init__ name input_dtype input _dtype output_dtype compute_dtype alpha - None super __init__ name input_dtype input _dtype output_dtype compute_dtype GemmBlocking alpha codegen_define kernel CppTemplateKernel - str options = declare_kernel get_kernel_declaration get_common_options KernelTemplate _template_from_string TEMPLATE_ENTRY render options is_int _woq_gemm_small_m_dim_corner_case config m n k k config register_blocking block_k == n config register_blocking block_n == m extra check small M dimension int WoQ case check_int _woq_small_m_dim config m n k alpha num_threads kwargs is_int _woq_gemm_small_m_dim_corner_case config m n k kwargs get dynamic_M False For int WoQ GEMM small M we use different blockings shouldn t used otherwise do_not_use_with_small_m_for_int _woq config m n k alpha num_threads kwargs check_int _woq_small_m_dim config m n k alpha num_threads kwargs register_micro_gemm generate_gemm_config VecAVX input_dtype=torch float generate_gemm_config VecAVX input_dtype=torch bfloat output_dtype=torch float generate_gemm_config VecAVX input_dtype=torch half output_dtype=torch float generate_gemm_config VecAVX input_dtype=torch bfloat input _dtype=torch int output_dtype=torch float compute_dtype=torch float extra_check=do_not_use_with_small_m_for_int _woq generate_gemm_config VecAVX input_dtype=torch bfloat input _dtype=torch int output_dtype=torch float compute_dtype=torch float extra_check=check_int _woq_small_m_dim generate_gemm_config VecAVX input_dtype=torch float generate_gemm_config VecAVX input_dtype=torch bfloat output_dtype=torch float generate_gemm_config VecAVX input_dtype=torch half output_dtype=torch float generate_gemm_config VecAVX input_dtype=torch bfloat input _dtype=torch int output_dtype=torch float compute_dtype=torch float extra_check=do_not_use_with_small_m_for_int _woq generate_gemm_config VecAVX input_dtype=torch bfloat input _dtype=torch int output_dtype=torch float compute_dtype=torch float extra_check=check_int _woq_small_m_dim generate_gemm_config VecNEON input_dtype=torch float input _dtype=torch float output_dtype=torch float compute_dtype=torch float generate_gemm_config VecSVE input_dtype=torch float input _dtype=torch float output_dtype=torch float compute_dtype=torch float CppMicroGemmFP Vec CppMicroGemm This generates code micro gemm using fp vec instructions compute It supports input types torch float torch bfloat torch half fp output The output microkernel FP would converted BF FP template desired output BF FP TEMPLATE_ENTRY = r declare_kernel using Vectorized = vec Vectorized compute_t constexpr auto VLEN = Vectorized size kernel assert_function block_n VLEN == block_n dimension must multiple Vector size kernel assert_function K block_k == K dimension must multiple block_k TODO jgong loop unroll M N int _t m = m M m += block_m int _t block_m = std min int _t M - m block_m int _t n = n N n += block_n int _t block_n = std min int _t N - n block_n block_m == block_m block_n == block_n - trans_b kernel_name _kernel block_m block_n accum prefetch - kernel_name _transpose_b_kernel block_m block_n accum prefetch - endif A + m lda - trans_b B + n - B + n ldb - endif C + m ldc + n K lda ldb ldc - tail_n block_n == block_n - - endif switch block_m - b range block_m - - case b - trans_b kernel_name _kernel b block_n accum prefetch - kernel_name _transpose_b_kernel b block_n accum prefetch - endif A + m lda - trans_b B + n - B + n ldb - endif C + m ldc + n K lda ldb ldc break - endfor default kernel assert_function false Unsupported block_m block_m - tail_n switch block_m - b range block_m - case b - trans_b kernel_name _ntail_kernel b block_n accum prefetch - kernel_name _ntail_transpose_b_kernel b block_n accum prefetch - endif A + m lda - trans_b B + n - B + n ldb - endif C + m ldc + n block_n K lda ldb ldc break - endfor default kernel assert_function false Unsupported block_m block_m - - endif TEMPLATE_KERNEL = r template int _t BLOCK_M int _t BLOCK_N bool accum bool prefetch=false - trans_b - tail_n inline void kernel_name _ntail_kernel - inline void kernel_name _kernel - endif - - tail_n inline void kernel_name _ntail_transpose_b_kernel - inline void kernel_name _transpose_b_kernel - endif - endif const input_t restrict_keyword A const input _t restrict_keyword B output_t restrict_keyword C - tail_n int _t N - endif int _t K int _t lda int _t ldb int _t ldc using Vectorized = vec Vectorized compute_t - input _dtype torch bfloat torch float using VectorizedIn = vec Vectorized input_t - endif - trans_b constexpr auto VLEN = Vectorized size constexpr auto ROWS = BLOCK_M constexpr auto COLS = BLOCK_N VLEN Vectorized va vec VectorizedN compute_t COLS vb vec VectorizedN compute_t ROWS COLS vc - tail_n int _t rCOLS = N + VLEN - VLEN int ntail = N VLEN - endif auto loadc = auto i constexpr accum constexpr int row = i COLS constexpr int col = i COLS - tail_n int load_size = col == rCOLS - ntail = ntail VLEN col rCOLS vc i = Vectorized loadu C + row ldc + col VLEN load_size - vc i = Vectorized loadu C + row ldc + col VLEN - endif vc i = Vectorized f c ForcedUnroll ROWS COLS loadc auto compute = COLS auto i int k constexpr int row = i COLS constexpr int col = i COLS - tail_n int load_size = col == rCOLS - ntail = ntail VLEN - endif constexpr col == - alpha = va = Vectorized static_cast compute_t A row lda + k alpha - va = Vectorized static_cast compute_t A row lda + k - endif constexpr row == - tail_n col rCOLS - input _dtype torch bfloat torch float auto b = VectorizedIn loadu B + k ldb + col VLEN load_size vb col = vec convert compute_t b - input _dtype == torch int Convert VLEN int elements int then fp auto b = vec convert_to_int int _t B + k ldb + col VLEN load_size vb col = vec convert float b - vb col = Vectorized loadu B + k ldb + col VLEN load_size - endif vb col = Vectorized f - - input _dtype torch bfloat torch float auto b = VectorizedIn loadu B + k ldb + col VLEN VLEN vb col = vec convert compute_t b - input _dtype == torch int Convert VLEN int elements int then fp auto b = vec convert_to_int int _t B + k ldb + col VLEN constexpr prefetch _mm_prefetch B + k + block_k ldb + col VLEN _MM_HINT_T vb col = vec convert float b - vb col = Vectorized loadu B + k ldb + col VLEN - endif - endif constexpr int idx = row COLS + col - tail_n col rCOLS vc idx = vec fmadd va vb col vc idx - vc idx = vec fmadd va vb col vc idx - endif int k = k K ++k c ForcedUnroll ROWS COLS compute k store C auto storec = auto i constexpr int row = i COLS constexpr int col = i COLS - tail_n int store_size = col == rCOLS - ntail = ntail VLEN col rCOLS vc i store C + row ldc + col VLEN store_size - vc i store C + row ldc + col VLEN - endif c ForcedUnroll ROWS COLS storec - Use implementations transposed B First implementation Transpose first then perform outer product calculation sub-blocks which introduces additional transpose overhead K N compared non-transpose version Second implementation Directly perform inner product calculation sub-blocks which introduces additional vector reduction M N compared non-tranpose version Therefore when M N K N large first implementation has better performance - tail_n K Vectorized size == N Vectorized size == BLOCK_M K - K Vectorized size == BLOCK_M K - endif First implementation constexpr auto VLEN = Vectorized size constexpr auto ROWS = BLOCK_M constexpr auto COLS = BLOCK_N VLEN int _K = K VLEN Vectorized va vec VectorizedN compute_t VLEN vb vec VectorizedN compute_t ROWS COLS vc auto loadc = auto i constexpr accum constexpr int row = i COLS constexpr int col = i COLS vc i = Vectorized loadu C + row ldc + col VLEN vc i = Vectorized f c ForcedUnroll ROWS COLS loadc auto unroll_loadB = auto i const input _t restrict_keyword src_ptr - input _dtype torch bfloat torch float auto b = VectorizedIn loadu src_ptr + i ldb VLEN vb i = vec convert compute_t b - input _dtype == torch int auto b = vec convert_to_int int _t src_ptr + i ldb VLEN vb i = vec convert float b - vb i = Vectorized loadu src_ptr + i ldb VLEN - endif auto compute_trans = COLS auto i int k constexpr int row = i ROWS constexpr int col = i ROWS constexpr int e_col = col VLEN int idk = k VLEN constexpr row == c ForcedUnroll VLEN unroll_loadB B + e_col ldb + idk vec transpose_block vb constexpr int idx = row COLS + col kernel unroll_pragma int j = j VLEN j++ - alpha = va = Vectorized static_cast compute_t A row lda + idk + j alpha - va = Vectorized static_cast compute_t A row lda + idk + j - endif vc idx = vec fmadd va vb j vc idx int k = k _K ++k c ForcedUnroll ROWS COLS compute_trans k store C auto storec = auto i constexpr int row = i COLS constexpr int col = i COLS vc i store C + row ldc + col VLEN c ForcedUnroll ROWS COLS storec Second implementation - input _dtype torch bfloat torch float constexpr auto VLEN = VectorizedIn size - constexpr auto VLEN = Vectorized size - endif int _K = K + VLEN - VLEN sub-block size BLOCK_N BLOCK_M constexpr int sM = sub_block_m constexpr int sN = sub_block_n - tail_n int bN = N + sN - sN - constexpr int bN = BLOCK_N + sN - sN - endif constexpr int bM = BLOCK_M + sM - sM - input _dtype torch bfloat torch float vec VectorizedN compute_t va vec VectorizedN compute_t sN vb - vec Vectorized compute_t va vec VectorizedN compute_t sN vb - endif vec VectorizedN compute_t sN sM vmid - tail_n int ntail = N sN - constexpr int ntail = BLOCK_N sN - endif constexpr int mtail = BLOCK_M sM int ktail = K VLEN auto compute_trans = int m int n int k - tail_n int e_n = n == bN - ntail = N - n sN sN - int e_n = n == bN - ntail = BLOCK_N - n sN sN - endif int e_m = m == bM - mtail = BLOCK_M - m sM sM int e_k = k == _K - ktail = K - k VLEN VLEN kernel unroll_pragma sub_block_n int i = i e_n i++ - input _dtype torch bfloat torch float auto b = VectorizedIn loadu B + sN n + i ldb + k VLEN e_k std tie vb i vb i + = vec convert_to_float input_t b - input _dtype == torch int auto b = vec convert_to_int int _t B + sN n + i ldb + k VLEN e_k vb i = vec convert float b - vb i = Vectorized loadu B + sN n + i ldb + k VLEN e_k - endif kernel unroll_pragma sub_block_m int s = s e_m s++ - input _dtype torch bfloat torch float auto = VectorizedIn loadu A + sM m + s lda + k VLEN e_k std tie va va = vec convert_to_float input_t - input _dtype == torch int auto = vec convert_to_int int _t A + sM m + s lda + k VLEN e_k va = vec convert float - va = Vectorized loadu A + sM m + s lda + k VLEN e_k - endif - alpha = va = va Vectorized alpha - endif k == kernel unroll_pragma sub_block_n int i = i e_n i++ - input _dtype torch bfloat torch float vmid sN s + i = vec fmadd va vb i Vectorized f vmid sN s + i = vec fmadd va vb i + vmid sN s + i - vmid sN s + i = vec fmadd va vb i Vectorized f - endif kernel unroll_pragma sub_block_n int i = i e_n i++ - input _dtype torch bfloat torch float vmid sN s + i = vec fmadd va vb i vmid sN s + i vmid sN s + i = vec fmadd va vb i + vmid sN s + i - vmid sN s + i = vec fmadd va vb i vmid sN s + i - endif store C k == _K - kernel unroll_pragma sub_block_m int s = s e_m s++ kernel unroll_pragma sub_block_n int i = i e_n i++ auto v = vec vec_reduce_all Vectorized x Vectorized y x + y vmid sN s + i constexpr accum auto c = C + sM m + s ldc + sN n + i C + sM m + s ldc + sN n + i = c + v C + sM m + s ldc + sN n + i = v int n = n bN ++n int m = m bM ++m int k = k _K ++k compute_trans m n k - endif set trans_b generate gemm supports transposed B matrix set tail_n support tail N TODO add trans_b support other micro gemms move setting trans_b init CppMicroGemm __init__ name input_dtype input _dtype output_dtype compute_dtype register_blocking alpha= tail_n=False trans_b=False - None super __init__ name input_dtype input _dtype output_dtype compute_dtype register_blocking alpha tail_n = tail_n trans_b only supported platforms support avx avx since transpose_block only implemented these platforms trans_b vec_isa = pick_vec_isa assert issubclass vec_isa __class__ VecAVX issubclass vec_isa __class__ VecAVX trans_b = trans_b codegen_define kernel CppTemplateKernel - str options = declare_kernel get_kernel_declaration kernel kernel block_m register_blocking block_m block_n register_blocking block_n block_k register_blocking block_k trans_b False tail_n False restrict_keyword get_restrict_keyword get_common_options trans_b TODO supports tuning sub_block_m sub_block_n get better performance specific shapes sub_block_m = min register_blocking block_m sub_block_n = min register_blocking block_n update options generate kernel trans_b sub-block size options update trans_b trans_b sub_block_m sub_block_m sub_block_n sub_block_n result = KernelTemplate _template_from_string TEMPLATE_KERNEL render options update options generate kernel tail N tail_n options update tail_n tail_n result += KernelTemplate _template_from_string TEMPLATE_KERNEL render options result += KernelTemplate _template_from_string TEMPLATE_ENTRY render options result extra check CppMicroGemmAMX check_amx_extra config m n k alpha num_threads kwargs vnni_size = config input_dtype torch uint torch int k vnni_size == alpha == check_int _bf _amx_extra config m n k alpha num_threads kwargs We need avx _bf dequant int bf vec_isa = kwargs get vec_isa assert vec_isa None vec_isa is_avx _bf _supported check_amx_extra config m n k alpha num_threads kwargs amx_fp need checked separately since always supported when amx supported check_amx_fp _extra config m n k alpha num_threads kwargs assert config input_dtype == torch float config output_dtype == torch float vec_isa = kwargs get vec_isa assert vec_isa None vnni_size = vec_isa is_amx_fp _supported k vnni_size == alpha == register_micro_gemm generate_gemm_config VecAMX input_dtype=torch int input _dtype=torch int output_dtype=torch int compute_dtype=torch int extra_check=check_amx_extra generate_gemm_config VecAMX input_dtype=torch bfloat input _dtype=torch int output_dtype=torch float compute_dtype=torch float extra_check=check_int _bf _amx_extra generate_gemm_config VecAMX input_dtype=torch bfloat output_dtype=torch float extra_check=check_amx_extra generate_gemm_config VecAMX input_dtype=torch float output_dtype=torch float extra_check=check_amx_fp _extra generate_gemm_config VecAMX input_dtype=torch uint input _dtype=torch int output_dtype=torch int compute_dtype=torch int extra_check=check_amx_extra CppMicroGemmAMX CppMicroGemm This generates code micro gemm using Advanced Matrix extension AMX instructions available th generation Intel Xeon compute It supports input types torch bfloat fp output TEMPLATE_ENTRY = r declare_kernel kernel assert_function N block_n == N dimension must multiple block_n kernel assert_function K == K dimension must multiple - pack_vnni_B_locally template codegen_allocate_weight_buffer packed_B_buf input _t K block_n - endif - use_cached_dequantized_B Create stack-allocated buffer tiles B Except maybe tail-case AMX tile B has x BF elements we cache K block_n elements dequantized B template codegen_allocate_weight_buffer dequantized_B_buf input_t K block_n const auto buf_size = K block_n auto load_dequantized_B = int base_idx Load tile B cache L D input _t base_addr = const_cast input _t B + base_idx int idx_dq = idx_q = idx_dq buf_size idx_q += ldb idx_dq += block_n - vec_idx range block_n _mm_prefetch base_addr + idx_q + ldb _MM_HINT_T - block_n - vec_idx = Load x int __m i v = _mm _loadu_si const __m i base_addr + idx_q + vec_idx Extract two halves __m i v _lo = _mm _extracti _si v __m i v _hi = _mm _extracti _si v Widen each half i __m i v _lo = _mm _cvtepi _epi v _lo __m i v _hi = _mm _cvtepi _epi v _hi Convert f __m f_lo = _mm _cvtepi _ps v _lo __m f_hi = _mm _cvtepi _ps v _hi f - bf round-to-nearest-even pack lanes b Packs second operand f_lo into lower bf lanes first f_hi into upper __m i bf = __m i _mm _cvtne ps_pbh f_hi f_lo Store x bf bits _mm _storeu_si __m i dequantized_B_buf + idx_dq + vec_idx bf - block_n - vec_idx = Load x int bits __m i v = _mm_loadu_si const __m i base_addr + idx_q + vec_idx Widen x i - x i __m i v = _mm _cvtepi _epi v Convert f __m f = _mm _cvtepi _ps v Convert f - bf round-to-nearest-even __m i bf = __m i _mm _cvtneps_pbh f Store x bf bits _mm _storeu_si __m i dequantized_B_buf + idx_dq + vec_idx bf - auto b_int _tail = vec Vectorized int _t loadu base_addr + idx_q + block_n - block_n static_cast int _t block_n auto b_bf _tail = vec convert input_t b_int _tail b_bf _tail store dequantized_B_buf + idx_dq + block_n - block_n static_cast int _t block_n - endif - endfor - endif The ldb would block_n N = block_n - use_cached_dequantized_B pack_vnni_B_locally const int _t updated_ldb = block_n - const int _t updated_ldb = ldb - endif TODO jgong loop unroll M N int _t n = n N n += block_n - pack_vnni_B_locally Pack non-constant weights into VNNI interleaved format packed_B_buf vec pack_vnni B + n packed_B_buf ldb K block_n - use_cached_dequantized_B Dequantize K block_n int B elements into BF load_dequantized_B n - endif int _t m = m M m += block_m int _t block_m = std min int _t M - m block_m int _t m_tail = m - num_rows range block_m - - num_rows = block_m - endif block_m = num_rows kernel_name _amx_kernel_ num_rows _ num_columns accum amx_state A + m lda - use_cached_dequantized_B dequantized_B_buf - pack_vnni_B_locally packed_B_buf - B + n - endif C + m ldc + n K lda updated_ldb ldc block_m -= num_rows m_tail += num_rows - endfor block_m kernel_name _amx_kernel_ _ num_columns accum amx_state A + m_tail lda - use_cached_dequantized_B dequantized_B_buf - pack_vnni_B_locally packed_B_buf - B + n - endif C + m_tail ldc + n K lda updated_ldb ldc block_m TEMPLATE_KERNEL = r template bool accum bool prefetch=false inline void kernel_name _amx_kernel_ num_rows _ num_columns AMXState amx_state const input_t restrict_keyword A - use_cached_dequantized_B const input_t restrict_keyword B - const input _t restrict_keyword B - endif output_t restrict_keyword C int _t K int _t lda int _t ldb int _t ldc uint _t tilecfg_rows TODO jgong add prefetch hint A B C auto loadconfig = const amx_tilecfg cfg _tile_loadconfig cfg const auto last_k_offset = K block_k block_k const auto tail_k_size = K - last_k_offset C _LIKELY last_k_offset amx_state configure tilecfg_rows num_rows num_columns loadconfig amx_state configure tilecfg_rows tail_k_size sizeof input_t num_rows num_columns loadconfig auto load_c = - tile_row range num_rows - tile_col range num_columns - set tile_idx = tile_row num_columns + tile_col _tile_loadd tile_idx C + tile_row ldc + tile_col ldc sizeof output_t - endfor - endfor auto zero_c = - tile_row range num_rows - tile_col range num_columns - set tile_idx = tile_row num_columns + tile_col _tile_zero tile_idx - endfor - endfor constexpr accum load_c zero_c auto compute = int k - set tile_offset_a = num_rows num_columns - set tile_offset_b = tile_offset_a + num_rows - tile_row range num_rows - tile_col range num_columns - set tile_idx_a = tile_offset_a + tile_row - set tile_idx_b = tile_offset_b + tile_col - set tile_idx_c = tile_row num_columns + tile_col - tile_col == _tile_stream_loadd tile_idx_a A + tile_row lda + k lda sizeof input_t - endif - tile_row == _tile_loadd tile_idx_b B + k ldb + tile_col vnni_size ldb vnni_size sizeof input_t - endif - int _gemm - input_dtype == torch int _tile_dpbssd tile_idx_c tile_idx_a tile_idx_b - _tile_dpbusd tile_idx_c tile_idx_a tile_idx_b - endif - - input_dtype == torch float _tile_dpfp ps tile_idx_c tile_idx_a tile_idx_b - _tile_dpbf ps tile_idx_c tile_idx_a tile_idx_b - endif - endif - endfor - endfor kernel unroll_pragma int k = k last_k_offset k += block_k compute k auto store_c = store C - tile_row range num_rows - tile_col range num_columns - set tile_idx = tile_row num_columns + tile_col _tile_stored tile_idx C + tile_row ldc + tile_col ldc sizeof output_t - endfor - endfor TODO jgong move tail k computation separate loopnest save tile configuration overhead C _UNLIKELY tail_k_size C _LIKELY last_k_offset store_c amx_state configure tilecfg_rows tail_k_size sizeof input_t num_rows num_columns loadconfig load_c compute last_k_offset store_c codegen_define kernel CppTemplateKernel - str block_m block_n block_k = register_blocking assert block_m == Only support block_m == AMX assert block_n == Only support block_n == AMX input_dtype torch uint torch int assert block_k == Only support block_k = AMX INT assert block_k == Only support block_k = AMX Bfloat Float num_columns = block_n options = declare_kernel get_kernel_declaration use_cached_dequantized_B input_dtype == torch bfloat input _dtype torch int torch uint kernel kernel block_m block_m block_n block_n block_k block_k num_columns num_columns restrict_keyword get_restrict_keyword get_common_options result = num_rows range block_m - amx_kernel_options = options num_rows num_rows result += KernelTemplate _template_from_string TEMPLATE_KERNEL render amx_kernel_options result += KernelTemplate _template_from_string TEMPLATE_ENTRY render options result codegen_init kernel CppTemplateKernel - str AMXState amx_state codegen_finalize kernel CppTemplateKernel - str amx_state release _tile_release get_kernel_extra_args_declare - str AMXState amx_state get_kernel_extra_args kwargs - list str amx_state get_b_layout input_dtype torch uint torch int LayoutType VNNI LayoutType VNNI extra check CppMicroBrgemm check_brgemm_extra config m n k alpha num_threads kwargs assert config input_dtype == torch half config output_dtype == torch float vnni_size = use brgemm Half when amx_fp supported torch cpu _is_amx_fp _supported k vnni_size == alpha == register_micro_gemm generate_gemm_config VecAMX input_dtype=torch half output_dtype=torch float extra_check=check_brgemm_extra CppMicroBrgemm CppMicroGemm This generates code micro gemm using oneDNN brgemm It supports input types torch half TEMPLATE_ENTRY = r #include ATen native CPUBlas h declare_kernel - pack_vnni_B_locally template codegen_allocate_weight_buffer packed_B_buf input _t K N vec pack_vnni B packed_B_buf ldb K N - endif native cpublas brgemm M N K - pack_vnni_B_locally lda N ldc - lda ldb ldc - endif accum A - pack_vnni_B_locally packed_B_buf - B - endif C codegen_define kernel CppTemplateKernel - str options = declare_kernel get_kernel_declaration kernel kernel block_m register_blocking block_m block_n register_blocking block_n block_k register_blocking block_k restrict_keyword get_restrict_keyword get_common_options result = result += KernelTemplate _template_from_string TEMPLATE_ENTRY render options result codegen_finalize kernel CppTemplateKernel - str native cpublas brgemm_release get_b_layout assert input_dtype == torch half torch cpu _is_amx_fp _supported LayoutType VNNI check_woq_int _extra config m n k alpha num_threads kwargs alpha = False q_group_size = kwargs get q_group_size assert q_group_size None q_group_size k q_group_size = config register_blocking block_k q_group_size False k config register_blocking block_k == n == register_micro_gemm TODO support float half input generate_gemm_config VecAVX input_dtype=torch bfloat input _dtype=torch uint output_dtype=torch float compute_dtype=torch float extra_check=check_woq_int _extra CppMicroGemmWoQInt Avx CppMicroGemmFP Vec This generates code WoQ int micro gemm using AVX intrinsics It based corresponding ATen kernel Shape packed weight = N K viewed N K Shape packed ScalesAndZeros = K group_size N TEMPLATE_ENTRY = r declare_kernel kernel assert_function N block_n == N dimension must multiple block_n kernel assert_function K block_k == K dimension must multiple block_k auto group_size = q_group_size int _t m = m M m += block_m int _t block_m = std min int _t M - m block_m int _t n = n N n += block_n block_m == block_m kernel_name _kernel block_m block_n accum A + m lda reinterpret_cast const uint _t B + n ldb C + m ldc + n K lda ldb block_n ldc group_size ScaleAndZeros + n lds k_start switch block_m - b range block_m - - case b kernel_name _kernel b block_n accum A + m lda reinterpret_cast const uint _t B + n ldb C + m ldc + n K lda ldb block_n ldc group_size ScaleAndZeros + n lds k_start break - endfor default kernel assert_function false Unsupported block_m block_m TEMPLATE_KERNEL = r inline bool kernel_name _is_block_start int index int k_start int group_size k_start + index group_size == inline __m i kernel_name _convert_int _to_int const uint _t data __m i tmp = _mm_loadu_si const __m i data __m i bytes = _mm_cvtepu _epi tmp const __m i lowMask = _mm_set _epi xF __m i high = _mm_andnot_si lowMask bytes __m i low = _mm_and_si lowMask bytes high = _mm_slli_epi high bytes = _mm_or_si low high bytes template int _t BLOCK_M int _t BLOCK_N bool accum inline void kernel_name _kernel const input_t restrict_keyword A const uint _t restrict_keyword B output_t restrict_keyword C int _t K int _t lda int _t ldb int _t ldc int _t q_group_size const BFloat restrict_keyword ScaleAndZeros int _t lds leading dimension ScaleAndZeros int _t k_start constexpr int BLOCK_K = block_k constexpr int ROWS = BLOCK_M constexpr int COLS = BLOCK_N const int PREFETCH_SIZE_K = const int PREFETCH_SIZE_KB = PREFETCH_SIZE_K + BLOCK_K - BLOCK_K number blocks K const int KB = K BLOCK_K __m va __m vb COLS __m vc ROWS COLS __m scale COLS __m zero COLS Lookup table de-quantize int values bf Values dequantized truly int - range dequant = bf int _value bf _scale + bf _zero static const __m lut = _mm _set_ps f f f f f f f f - f - f - f - f - f - f - f - f index transpose static const __m i idx = _mm _set_epi static const __m i idx = _mm _set_epi load scale zero point auto load_scale_and_zeros = int i int _kb load x bfloat vector __m i t = _mm _loadu_si __m i ScaleAndZeros + _kb lds + i _mm_prefetch ScaleAndZeros + _kb + PREFETCH_SIZE_KB lds + i _MM_HINT_T convert x f vector __m b vec cvtbf _fp t b transpose scale_and_zero inputs s z s z s z b s z s z s z output scale s s s s zero z z z z scale i = _mm _mask_permutex var_ps xffff idx b zero i = _mm _mask_permutex var_ps xffff idx b auto loadc = auto i constexpr accum constexpr int row = i COLS constexpr int col = i COLS vc i = _mm _loadu_ps C + row ldc + col vc i = _mm _setzero_ps c ForcedUnroll ROWS COLS loadc auto compute = COLS auto i int k constexpr int row = i COLS constexpr int col = i COLS constexpr col == float aa = static_cast float A row lda + k _mm_prefetch A + row lda + k + PREFETCH_SIZE_K _MM_HINT_T va = _mm _set _ps aa constexpr row == constexpr COLS == when BLOCK_N = handle each row time reduce de-quantize overhead constexpr col == __m i b = _mm _loadu_si __m i B + k ldb _mm_prefetch B + k + PREFETCH_SIZE_K ldb _MM_HINT_T __m i b = _mm _cvtepu _epi _mm _castsi _si b vb = _mm _permutexvar_ps b lut vb = _mm _fmadd_ps vb scale zero vb = _mm _permutexvar_ps _mm _srli_epi b lut vb = _mm _fmadd_ps vb scale zero b = _mm _cvtepu _epi _mm _extracti _si b vb = _mm _permutexvar_ps b lut vb = _mm _fmadd_ps vb scale zero vb = _mm _permutexvar_ps _mm _srli_epi b lut vb = _mm _fmadd_ps vb scale zero __m i b = kernel_name _convert_int _to_int B + k ldb + col __m i b = _mm _cvtepu _epi b vb col = _mm _permutexvar_ps b lut vb col = _mm _fmadd_ps vb col scale col zero col constexpr int idx = row COLS + col vc idx = _mm _fmadd_ps va vb col vc idx int k = kb = k K ++k kernel_name _is_block_start k k_start q_group_size c ForcedUnroll COLS load_scale_and_zeros kb++ c ForcedUnroll ROWS COLS compute k store C auto storec = COLS auto i constexpr int row = i COLS constexpr int col = i COLS _mm _storeu_ps C + row ldc + col vc i c ForcedUnroll ROWS COLS storec get_kernel_extra_args_declare - str const int _t q_group_size \n const BFloat __restrict__ ScaleAndZeros \n const int _t lds \n int _t k_start get_kernel_extra_args kwargs - list str assert kernel kwargs assert qscale_and_zeros kwargs kernel = kwargs kernel qscale_and_zeros = kwargs qscale_and_zeros group_size f kernel index qscale_and_zeros N lds k_start is_woq_int True register_micro_gemm generate_gemm_config VecAMX block_m block_n block_k input_dtype=torch bfloat input _dtype=torch uint output_dtype=torch float compute_dtype=torch float extra_check=check_amx_extra CppMicroGemmWoQInt Amx CppMicroGemmAMX This generates code WoQ int micro gemm using AMX intrinsics which available th newer generations Intel Xeon Shape packed weight = N K viewed N K Shape packed ScalesAndZeros = K group_size N Reuse TEMPLATE_KERNEL CppMicroGemmAMX TEMPLATE_ENTRY = r inline bool kernel_name _is_block_start int index int k_start int group_size check k_start + index group_size == assuming group_size = k_start + index group_size - == declare_kernel kernel assert_function N block_n == N dimension must multiple block_n kernel assert_function K == K dimension must multiple kernel assert_function block_n == block_n must WOQ int Create stack-allocated buffer tiles B Except maybe tail-case AMX tile B has x BF elements we cache K block_n elements dequantized B template codegen_allocate_weight_buffer dequantized_B_buf input_t K block_n constexpr int BLOCK_K = block_k constexpr int _t BLOCK_N = block_n constexpr int COLS = BLOCK_N const int PREFETCH_SIZE_K = const int PREFETCH_SIZE_KB = PREFETCH_SIZE_K + BLOCK_K - BLOCK_K const int KB = K BLOCK_K __m i b COLS __m vb COLS __m scale COLS __m zero COLS Lookup table de-quantize int values bf Values dequantized truly int - range dequant = bf int _value bf _scale + bf _zero static const __m lut = _mm _set_ps f f f f f f f f - f - f - f - f - f - f - f - f index transpose static const __m i idx = _mm _set_epi static const __m i idx = _mm _set_epi Indices VNNI layout conversion __m i idx_low = _mm _set_epi x x x x x x x x x x x x x x x x __m i idx_high = _mm _set_epi x f x f x e x e x d x d x c x c x b x b x x x x x x load scale zero point auto load_scale_and_zeros = int i int _kb load x bfloat vector __m i t = _mm _loadu_si __m i ScaleAndZeros + _kb lds + i _mm_prefetch ScaleAndZeros + _kb + PREFETCH_SIZE_KB lds + i _MM_HINT_T convert x f vector __m b vec cvtbf _fp t b transpose scale_and_zero inputs s z s z s z b s z s z s z output scale s s s s zero z z z z scale i = _mm _mask_permutex var_ps xffff idx b zero i = _mm _mask_permutex var_ps xffff idx b Dequantize B block block_n into bf So handles k k+ same time auto dequantize_B = int n constexpr int _t ldb_int = BLOCK_N int k = kb = k K k += Since block_k must AMX microkernels k_start may multiple q_group_size In case we need load scales zero points immediately when k == here kernel_name _is_block_start k k_start q_group_size &#124; &#124; k == c ForcedUnroll COLS load_scale_and_zeros kb++ _mm_prefetch B + k + PREFETCH_SIZE_K ldb_int _MM_HINT_T load bits = elements int __m i b = _mm_loadu_si __m i B + n K + k ldb_int b = _mm _cvtepu _epi b b = _mm _srli_epi b vb = _mm _permutexvar_ps b lut vb = _mm _fmadd_ps vb scale zero vb = _mm _permutexvar_ps b lut vb = _mm _fmadd_ps vb scale zero __m i b _ = _mm_loadu_si __m i B + n K + k + ldb_int b + COLS = _mm _cvtepu _epi b _ b + COLS = _mm _srli_epi b + COLS vb + COLS = _mm _permutexvar_ps b + COLS lut vb + COLS = _mm _fmadd_ps vb + COLS scale zero vb + COLS = _mm _permutexvar_ps b + COLS lut vb + COLS = _mm _fmadd_ps vb + COLS scale zero int i = i COLS i++ convert VNNI auto low = _mm _permutex var_ps vb i idx_low vb i + COLS auto high = _mm _permutex var_ps vb i idx_high vb i + COLS convert lower float values bfloat auto v _bf = reinterpret_cast __m i _mm _cvtneps_pbh low convert higher float values bfloat auto v _bf = reinterpret_cast __m i _mm _cvtneps_pbh high combine lower higher bfloat values auto v = _mm _castsi _si v _bf v = _mm _inserti x v v _bf store VNNI format bfloat values input_t addr = dequantized_B_buf + k + i _mm _storeu_si addr v int _t n = n N n += block_n Dequantize K block_n int B elements into BF dequantize_B n int _t m = m M m += block_m int _t block_m = std min int _t M - m block_m int _t m_tail = m - num_rows range block_m - - num_rows = block_m - endif block_m = num_rows kernel_name _amx_kernel_ num_rows _ num_columns accum amx_state A + m lda dequantized_B_buf + n K C + m ldc + n K lda block_n ldc block_m -= num_rows m_tail += num_rows - endfor block_m kernel_name _amx_kernel_ _ num_columns accum amx_state A + m_tail lda dequantized_B_buf + n K C + m_tail ldc + n K lda block_n ldc block_m m n get_kernel_extra_args_declare - str AMXState amx_state \n const int _t q_group_size \n const c BFloat __restrict__ ScaleAndZeros \n const int _t lds \n int _t k_start get_kernel_extra_args kwargs - list str assert kernel kwargs assert qscale_and_zeros kwargs kernel = kwargs kernel qscale_and_zeros = kwargs qscale_and_zeros amx_state group_size f kernel index qscale_and_zeros N lds k_start is_woq_int True create_micro_gemm name m n k input_dtype input _dtype output_dtype=None compute_dtype=None alpha= num_threads=- use_ref=True q_group_size=None - Optional CppMicroGemm Based provided info try find config micro-kernel would deliver best performance terms lower latency case create_from_config cls config CppMicroGemmConfig cls name config input_dtype config input _dtype config output_dtype config compute_dtype config register_blocking alpha skip_amx_kernel_for_woq dynamic_M For WoQ GEMM AMX micro-kernel may perform well m small Exception dynamic shapes we consider using AMX micro-kernel dynamic_M input_dtype = torch bfloat input _dtype torch int torch uint False m_threshold = m m_threshold assert isinstance n int n is_number n assert isinstance k int k is_number k utils has_free_symbols dynamic_M = has_free_symbols m m = V graph sizevars size_hint m fallback= dynamic_M m assert isinstance m int m is_number m output_dtype None output_dtype = input_dtype compute_dtype None compute_dtype = output_dtype num_threads num_threads = parallel_num_threads vec_isa = pick_vec_isa matched_configs = cls configs micro_gemm_configs items config configs issubclass vec_isa __class__ config vec_isa_cls continue config input_dtype == input_dtype config compute_dtype == compute_dtype config input _dtype == input _dtype config output_dtype == output_dtype The output_dtype here output dtype micro-kernel In some cases actual output dtype op which micro-kernel being created would same activation micro-kernels compute output Float int which converted GEMM template This subject change future config extra_check None config extra_check config m n k alpha num_threads dynamic_M=dynamic_M q_group_size=q_group_size vec_isa=vec_isa continue block_m block_n block_k = config register_blocking config vec_isa_cls == VecAMX skip_amx_kernel_for_woq dynamic_M continue Criteria ranking configurations ISA AMX VEC Dividable block sizes block_m block_n block_k Number mxn blocks large enough occupy all threads Register blocks larger isa_score = config vec_isa_cls == VecAMX isa_score += dividable_score = m block_m == dividable_score += n block_n == dividable_score += k block_k == dividable_score += occupancy_score = n_blocks = n + block_n - block_n total_mxn_blocks = n_blocks m + block_m - block_m n_blocks = num_threads occupancy_score += total_mxn_blocks = num_threads occupancy_score += register_bytes = block_m block_n config compute_dtype itemsize + block_m block_k + block_k block_n config input_dtype itemsize size_score = register_bytes number mxn blocks can occupy all threads we favor smaller register blocks occupancy_score == size_score = - register_bytes matched_configs append isa_score dividable_score occupancy_score size_score cls config len matched_configs == use_ref CppMicroGemmRef name input_dtype input _dtype output_dtype compute_dtype alpha None TODO jgong allow autotuning choices configs create_from_config max matched_configs key=operator itemgetter