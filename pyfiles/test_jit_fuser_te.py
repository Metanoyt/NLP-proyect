Owner s NNC ruff noqa F contextlib math operator os unittest warnings torch torch nn functional F torch testing FileCheck these needs set before ` common_utils ` infers ` GRAPH_EXECUTOR ` file requires these settings setting them after ` GRAPH_EXECUTOR ` inferred erroneously runs skips some tests torch _C _jit_set_profiling_executor True torch _C _get_graph_executor_optimize True __name__ == __main__ torch testing _internal common_utils parse_cmd_line_args The value GRAPH_EXECUTOR depends command line arguments so make sure they re parsed before instantiating tests parse_cmd_line_args itertools combinations permutations product textwrap dedent jit test_fuser_common TestFuserCommon noqa F test_jit backward_graph get_lstm_inputs get_milstm_inputs LSTMCellC LSTMCellF LSTMCellS MiLSTMCell torch testing _internal common_device_type instantiate_device_type_tests onlyCPU OpDTypes ops torch testing _internal common_jit JitCommonTestCase torch testing _internal common_methods_invocations op_db torch testing _internal common_utils enable_profiling_mode_for_profiling_tests GRAPH_EXECUTOR IS_FBCODE ProfilingMode run_tests skipIfTorchDynamo slowTest TEST_WITH_ASAN TEST_WITH_ROCM torch testing _internal jit_metaprogramming_utils create_traced_fn torch testing _internal jit_utils clone_inputs get_traced_sample_variant_pairs JitTestCase NoTracerWarnContextManager RUN_CUDA RUN_CUDA_HALF RUN_CUDA_MULTI_GPU set_fusion_group_inlining TensorExprTestOptions warmup_backward FUSION_GROUP = prim TensorExprGroup LLVM_ENABLED = torch _C _llvm_enabled autograd_check_set = aten __is__ prim AutogradAllNonZero prim AutogradAllZero prim ListConstruct strip_profiling_nodes nodes profiling_opcodes = prim BailoutTemplate prim BailOut n n nodes n kind profiling_opcodes warmup_forward f args profiling_count= _ range profiling_count results = f args results contextlib contextmanager texpr_reductions_enabled old = torch _C _jit_set_texpr_reductions_enabled True try yield finally torch _C _jit_set_texpr_reductions_enabled old contextlib contextmanager texpr_enable_strategy strategy old = torch _C _jit_set_fusion_strategy strategy try yield finally torch _C _jit_set_fusion_strategy old contextlib contextmanager inline_fusion_groups old_inlining = torch _C _debug_get_fusion_group_inlining torch _C _debug_set_fusion_group_inlining True try yield finally torch _C _debug_set_fusion_group_inlining old_inlining TestTEFuser JitTestCase setUp super setUp tensorexpr_options = TensorExprTestOptions note ` dynamic_shapes ` instantiated specialization defined below fusion_strategy = DYNAMIC dynamic_shapes STATIC old_fusion_strategy = torch _C _jit_set_fusion_strategy fusion_strategy devices = cpu torch cuda is_available cpu cuda int_dtypes = torch int torch int torch int torch int torch bool fp_dtypes = torch float torch float torch float torch bfloat dtypes = int_dtypes + fp_dtypes tearDown tensorexpr_options restore torch _C _jit_set_fusion_strategy old_fusion_strategy super tearDown assertAllFused graph except_for=None except_for = except_for except_for None set TODO - upstream guards = prim TypeCheck prim RequiresGradCheck prim TensorExprDynamicGuard guard_found = False autodiff_guard node node kind = aten all False inps = list node inputs len inps = inps node kind = prim ListConstruct False li_inps = list inps node inputs li_inp li_inps li_inp node kind prim AutogradAllNonZero prim AutogradAllZero True False is_guard node node kind guards autodiff_guard node node graph block nodes node kind == prim Constant continue is_guard node assertFalse guard_found guard_found = True continue node kind except_for continue node kind == prim If assertTrue is_guard node prev continue assertTrue False Found unexpected node + node kind assertTrue guard_found assertLastGraphAllFused assertAllFused torch jit last_executed_optimized_graph findFusionGroups graph result = n graph nodes n kind == FUSION_GROUP result append n g Subgraph continue block n blocks result += findFusionGroups block result test_typecheck = torch ones fused_kernel b + b scripted = checkScript fused_kernel graph = scripted graph_for double check we fused fusion_groups = findFusionGroups graph assertEqual len fusion_groups we use bigger tensor now size we won t trigger recompilation we will still create tensor up size type check fails = torch ones shape changed we don t trigger recompilation we would compute wrong result silently assertEqual scripted fused_kernel test_sum_simple func x x = x x x sum texpr_reductions_enabled = torch tensor list range dtype=torch float device= cpu = reshape scripted = checkScript func assertLastGraphAllFused test_nop pass test_sum_dim func x x sum func_neg x x sum - texpr_reductions_enabled = torch tensor list range dtype=torch float device= cpu = reshape scripted = checkScript func assertLastGraphAllFused scripted = checkScript func_neg assertLastGraphAllFused test_sum_keepdim_cast func x x sum keepdim=True dtype=torch double texpr_reductions_enabled = torch tensor list range dtype=torch float device= cpu = reshape checkScript func assertLastGraphAllFused test_abs device devices func x x abs = torch randn device=device scripted = checkScript func assertLastGraphAllFused test_unsqueeze_size_calculation device devices foo b d x = d unsqueeze y = x z = b + y r = z r inputs = torch rand device=device requires_grad=True torch rand device=device scripted = checkScript foo inputs assertAllFused scripted graph_for inputs test_zero_element_tensors device devices decode sin_t cos_t theta = torch atan sin_t float cos_t float theta sin = torch zeros device=device cos = torch zeros device=device inputs = sin cos ge = checkScript decode inputs test_arg_configurations_smoke dynamic_shapes skipTest TODO chunk dynamic shapes A smoke test make sure we won t use same kernel contiguous non-contiguous arguments TODO add optionally enabled debug counters fuser verify we really can tell difference between configurations device devices f x y z z = x + y chunk dim= z z x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device traced_f = torch jit trace f x y assertEqual traced_f x t contiguous y traced_f x t y test_broadcast device devices scaleshift x scale shift x scale + shift inputs = torch randn dtype=torch float device=device torch randn dtype=torch float device=device torch randn dtype=torch float device=device checkScript scaleshift inputs unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf RUN_CUDA_HALF no half support unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY no half support profiling test_cuda_half x = torch randn dtype=torch half device= cuda y = torch randn dtype=torch half device= cuda funcs = fn_test_comparison_gt_lt fn_test_relu fn_test_exp Note Non fused inputs must float prevent loss precision inputs = x float y float fusion_inputs = x y fn funcs local_inputs = t clone requires_grad_ t inputs local_fusion_inputs = t clone requires_grad_ t fusion_inputs Verifies outputs fusion = torch jit trace fn local_fusion_inputs check_trace=False outputs = fn local_inputs fusion_outputs = fusion local_fusion_inputs outputs_half = t half t outputs assertEqual outputs_half fusion_outputs Verifies gradients output fusion_output zip outputs_half fusion_outputs grads = torch autograd grad output float sum local_inputs allow_unused=True retain_graph=True fusion_grads = torch autograd grad fusion_output sum local_fusion_inputs allow_unused=True retain_graph=True grads_half = t half t grads assertEqual grads_half fusion_grads test_checks_cat_inputs single fusion node causes error set_fusion_group_inlining True device devices We shouldn t treat cat nodes broadcasting All their inputs need checked having same map size before we can run kernel f x y torch cat x + x + x y + y + y dim= NOTE y broadcastable x output f x y should have shape x x x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device scripted = checkScript f x y assertEqual scripted x y shape assertAllFused scripted graph_for x y test_chunk dynamic_shapes skipTest TODO chunk dynamic shapes device devices fn x b c = x chunk b + c inputs = torch randn dtype=torch float device=device checkScript fn inputs assertLastGraphAllFused test_chunk_correctness dynamic_shapes skipTest TODO chunk dynamic shapes device devices chunk_ _ x x x x x = x chunk x + x + x + x chunk_ _ x x x x x = x chunk x + x + x + x chunk_ _last x x x x x = x chunk x + x + x + x fns = chunk_ _ chunk_ _ chunk_ _last tensors = splitSize = torch randn dtype=torch float device=device contiguous case torch randn dtype=torch float device=device non-contiguous case torch randn dtype=torch float device=device transpose tensor tensors fn fns checkScript fn tensor assertLastGraphAllFused test_chunk_distributes dynamic_shapes skipTest TODO chunk dynamic shapes dynamic_shapes skipTest TODO chunk dynamic shapes device devices f x y z z = x + y chunk dim= z z x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device ge = checkTrace f x y graph = ge graph_for x y XXX The old fuser does broadcast_tensors new fuser doesn t FileCheck check broadcast_tensors check + FUSION_GROUP + _ \ check_count ConstantChunk exactly=True run str graph FileCheck check + FUSION_GROUP + _ check_count ConstantChunk exactly=True run str graph test_chunk_motion_deduplicates_inputs dynamic_shapes skipTest TODO chunk dynamic shapes device devices func x z = x x z z = z chunk z z func x z = x x x z z = z chunk z z inputs = torch tensor device=device dtype=torch float func func func checkScript func inputs assertLastGraphAllFused test_chunk_multiple dynamic_shapes skipTest TODO chunk dynamic shapes device devices The arguments intentionally used out order test see fusion compiler adds extra args correct order fn s x y z z z = z chunk x x x = x chunk y y = y chunk s + x + x + x + y + y + z + z inputs = torch randn dtype=torch float device=device torch randn dtype=torch float device=device torch randn dtype=torch float device=device torch randn dtype=torch float device=device ge = checkScript fn inputs assertAllFused ge graph_for inputs test_minmax device devices tmax b torch max b tmin b torch min b = torch randn dtype=torch float b = torch randn dtype=torch float nan = torch tensor float nan dtype=torch float f inputs device product tmax tmin b nan b nan devices inputs = t device t inputs s = checkScript f inputs assertAllFused s graph_for inputs test_clamp device devices func b torch clamp + b min= max= funcInf b torch clamp + b min= max=float inf funcNegInf b torch clamp + b min=float -inf max= funcOptMin b torch clamp + b max= funcOptMax b torch clamp + b min= = torch randn dtype=torch float device=device requires_grad=True b = torch randn dtype=torch float device=device nan = torch tensor float nan dtype=torch float device=device funcs = func funcInf funcNegInf funcOptMin funcOptMax f inputs product funcs b nan inp inp = inputs s = checkScript f inp inp profiling=ProfilingMode PROFILING assertAllFused s graph_for inp inp except_for= aten size aten _size_if_not_equal c = s inp inp enable_profiling_mode_for_profiling_tests warmup_backward c sum graph = backward_graph s assertAllFused graph except_for= aten Float aten _grad_sum_to_size union autograd_check_set test_clamp_double device devices clamp_double x eta float - x clamp eta - eta x = torch tensor dtype=torch double device=device eta = e- s = checkScript clamp_double x eta profiling=ProfilingMode PROFILING atol= e- rtol= e- assertAllFused s graph_for x eta except_for= aten sub test_clamp_int device devices clamp_int x eta int x clamp eta x = torch tensor device=device eta = s = checkScript clamp_int x eta profiling=ProfilingMode PROFILING assertAllFused s graph_for x eta test_add_bool sizes = device size product devices sizes f x y z x + y + z x = torch randint size dtype=torch bool device=device y = torch randint size dtype=torch bool device=device z = torch randint size dtype=torch bool device=device ge = checkTrace f x y z inputs_require_grads=False assertAllFused ge graph_for x y z test_mul_bool device devices f x y z x y z x = torch randint dtype=torch bool device=device y = torch randint dtype=torch bool device=device z = torch randint dtype=torch bool device=device ge = checkTrace f x y z inputs_require_grads=False assertAllFused ge graph_for x y z test_div_bool device devices f x y z x + y z x = torch randint dtype=torch bool device=device y = torch randint dtype=torch bool device=device z = torch ones_like x dtype=torch bool device=device ge = checkTrace f x y z inputs_require_grads=False assertAllFused ge graph_for x y z test_bitwise_ops apply fn lambda x y z fn fn x y z binary_ops = operator __and__ operator __or__ operator __xor__ operator __lshift__ operator __rshift__ devices = devices dtype op device product int_dtypes binary_ops devices try x = data_for dtype device y = data_for dtype device z = data_for dtype device fn = apply op ref = fn x y z except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x y z assertEqual ref t x y z assertAllFused t graph_for x y z except Exception e raise RuntimeError join Failed str dtype op __name__ device e test_minmax_int_ops apply fn lambda x y z fn fn x y z binary_ops = torch min torch max devices = devices dtype op device product int_dtypes binary_ops devices try x = data_for dtype device y = data_for dtype device z = data_for dtype device fn = apply op ref = fn x y z except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x y z assertEqual ref t x y z assertAllFused t graph_for x y z except Exception e raise RuntimeError join Failed str dtype op __name__ device e test_comparison_eq_ne device devices f x y mask = x == type_as x z = x mask + y mask = x = type_as x z = z mask + y z x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device ge = checkTrace f x y assertAllFused ge graph_for x y staticmethod fn_test_comparison_gt_lt x y mask = x type_as x z = x mask + y mask = x type_as x z = z mask + y z test_comparison_gt_lt device devices x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device ge = checkTrace fn_test_comparison_gt_lt x y assertAllFused ge graph_for x y test_comparison_ge_le device devices f x y mask = x = type_as x z = x mask + y mask = x = type_as x z = z mask + y z x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device ge = checkTrace f x y assertAllFused ge graph_for x y x requires_grad_ True y requires_grad_ True assertAllFused ge graph_for x y except_for= aten size prim BroadcastSizes aten _size_if_not_equal test_addcmul device devices t = torch randn dtype=torch float device=device t = torch randn dtype=torch float device=device t = torch randn dtype=torch float device=device foo t t t t addcmul t + t value= ge = checkTrace foo t t t allow_unused=True graph = ge graph_for t t t fusion_groups = findFusionGroups graph assertEqual len fusion_groups FileCheck check aten add check aten addcmul run str fusion_groups TODO We leak CUDA memory here because traced graph holds onto constant-ified tensor Since Python-global CompilationUnit alive until end process memory effectively leaked Removed ` _cuda ` suffix test which disables leak-checking If real problem we ll need revisit Torchscript Function lifetimes Python test_lerp device devices start = torch randn dtype=torch float device=device end = torch randn dtype=torch float device=device weight = torch tensor dtype=torch float device=device scalar weight overload foo_weight_scalar start end torch lerp start + end tensor weight overload foo_weight_tensor start end torch lerp start + end weight ge_weight_scalar = checkTrace foo_weight_scalar start end graph = ge_weight_scalar graph_for start end assertAllFused graph TODO uncomment when TE enables support scalar tensors ge_weight_tensor = checkTrace foo_weight_tensor start end graph = ge_weight_tensor graph_for start end assertAllFused graph test_concat disabling concat causes error single concat node set_fusion_group_inlining True device devices hx = torch randn dtype=torch float device=device cx = torch randn dtype=torch float device=device foo hx cx torch cat hx + cx hx cx ge = checkTrace foo hx cx graph = ge graph_for hx cx assertAllFused graph XXX TE fuser can handle concats fusion group FileCheck check FusedConcat check_next run str graph test_remove_output_used_only_in_size device devices test_fuse b c = + b d = c + b d scripted_f = torch jit script test_fuse x = torch ones requires_grad=True device=device y = torch ones requires_grad=True device=device warmup_forward scripted_f x y profiling_count= g = scripted_f graph_for x y diff_nodes = g findAllNodes prim DifferentiableGraph assertEqual len diff_nodes g = diff_nodes g Subgraph if_nodes = n n g nodes n kind == prim If assertEqual len if_nodes node fusion group inside should only have one output assertEqual len list if_nodes outputs test_concat_invariant device devices Invariant output prim FusedConcat may input any node inside FusionGroup fn x y z x = x + y y = x - y w = torch cat x y w + z x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device z = torch randn dtype=torch float device=device ge = checkTrace fn x y z graph = ge graph_for x y z assertAllFused graph except_for= aten add XXX TE fuser can handle concats inside fusion group FileCheck check FusedConcat check_next run str graph staticmethod fn_test_exp x y x + y exp test_exp device devices x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device ge = checkTrace fn_test_exp x y assertAllFused ge graph_for x y test_threshold device devices f x torch threshold x - + x + x + x x = torch tensor - - device=device scripted = checkScript f x assertAllFused scripted graph_for x test_scalar_arg device devices fn_test_scalar_arg x torch Tensor p float - torch Tensor p x x + x x = torch randn dtype=torch float device=device p = scripted = checkScript fn_test_scalar_arg x p assertAllFused scripted graph_for x p x requires_grad_ True use another function otherwise we will bailout won t able do fused checks fn_test_scalar_arg_requires_grad x torch Tensor p float - torch Tensor p x x + x scripted = torch jit script fn_test_scalar_arg_requires_grad out = scripted x p out = scripted x p out = scripted x p assertAllFused scripted graph_for x p except_for= aten size prim BroadcastSizes aten _size_if_not_equal unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf RUN_CUDA_MULTI_GPU needs non-zero device test_fusion_reuse_multi_gpu fn x y x y x y inputs_cpu = torch randn dtype=torch float torch randn dtype=torch float inputs_cuda = x cuda x inputs_cpu inputs_cuda = y cuda y inputs_cpu Should crash these should compile different kernels ge = checkScript fn inputs_cpu assertAllFused ge graph_for inputs_cpu ge inputs_cuda ge inputs_cuda TODO we re currently checking device type info when pulling nodes into fusion group We should fix re-enable test unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf RUN_CUDA_MULTI_GPU needs non-zero device test_kernel_cache_multi_gpu not_fusible x x fn x y z x_out = x x x x x fusion lambda x x x x x x y_out = y y y y y z_out = z z z z z not_fusible x_out not_fusible y_out not_fusible z_out inputs = torch randn dtype=torch float torch randn dtype=torch float device= cuda torch randn dtype=torch float device= cuda prev_cache_size = torch _C _jit_debug_fuser_num_cached_kernel_specs There FusionGroups Because they have same graph they should reuse same KernelSpec KernelSpec cache ge = checkScript fn inputs assertGraphContainsExactly ge graph_for inputs FUSION_GROUP True new_cache_size = torch _C _jit_debug_fuser_num_cached_kernel_specs XXX This assumes same kernel isn t already used another test FIXME Use TE fuser s way querying cache assertEqual new_cache_size - prev_cache_size unittest skipIf RUN_CUDA_MULTI_GPU needs non-zero device test_nonzero_device_cuda device = cuda + str x = torch tensor dtype=torch float device=device y = torch tensor dtype=torch float device=device doit x y torch sigmoid torch tanh x x + y + x ge = checkTrace doit x y assertAllFused ge graph_for x y test_lstm device devices inputs = get_lstm_inputs device training=True module = checkScript LSTMCellS inputs assertAllFused module graph_for inputs except_for= prim TupleConstruct test_lstm_concat single fusion node causes error set_fusion_group_inlining True device devices inputs = get_lstm_inputs device ge = checkTrace LSTMCellC inputs graph = ge graph_for inputs except_nodes = prim TupleConstruct aten linear TODO Chunk dynamic_shapes except_nodes = except_nodes union aten add prim ConstantChunk assertAllFused ge graph_for inputs except_for=except_nodes XXX TE fuser can handle concats inside fusion group FileCheck check FusedConcat check_next run str graph test_lstm_gates_permutations device devices lstm has gates = x mm w_ih t + hx mm w_hh t + b_ih + b_hh Test any permutation will still result one FusionGroup choices = x mm w_ih t hx mm w_hh t b_ih b_hh template = dedent cell x hx cx w_ih w_hh b_ih b_hh gates = + + + ingate forgetgate cellgate outgate = gates chunk ingate forgetgate cellgate outgate permutation permutations choices len choices code = template format permutation scope = exec code globals scope cu = torch jit CompilationUnit code fusion_group_len = dynamic_shapes inputs = get_lstm_inputs device training=False assertEqual cu cell inputs scope cell inputs forward_graph = cu cell graph_for inputs assertGraphContainsExactly forward_graph FUSION_GROUP fusion_group_len TODO Fuser doesn t work all when inputs require grad Fix test_lstm_traced device devices inputs = get_lstm_inputs device ge = checkTrace LSTMCellF inputs graph = ge graph_for inputs fusion_groups = findFusionGroups graph TODO chunk fusion_group_len = dynamic_shapes assertEqual len fusion_groups fusion_group_len f = FileCheck dynamic_shapes f check Chunk f check aten sigmoid check aten tanh run str fusion_groups dynamic_shapes test_milstm dynamic_shapes skipTest don t run conv dynamic shapes device devices inputs = get_milstm_inputs device training=True module = checkScript MiLSTMCell inputs forward_graph = module graph_for inputs TODO chunk fusion_group_len = dynamic_shapes assertGraphContainsExactly forward_graph FUSION_GROUP fusion_group_len consider_subgraphs=True FileCheck check DifferentiableGraph check TupleConstruct check_next check FUSION_GROUP run str forward_graph hy cy = module inputs warmup_backward hy + cy sum unittest skipIf RUN_CUDA fuser requires CUDA unittest skip rand_like supported yet test_rand_cuda M torch jit ScriptModule __constants__ = d __init__ - None super __init__ d = torch device cuda torch jit script_method create x x x + x + torch rand_like x x = torch zeros dtype=torch float device= cuda m = M out = m create x out = m create x assertNotEqual out out assertTrue torch all out = assertTrue torch all out assertTrue torch all out = assertTrue torch all out assertAllFused m create graph_for x staticmethod fn_test_relu x y F relu x + y test_relu device devices x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device ge = checkTrace fn_test_relu x y assertAllFused ge graph_for x y test_erf device devices only enabled gpu device == cpu continue fn_test_erf x F relu torch erf x - torch erfc x x = torch randn dtype=torch float device=device ge = checkScript fn_test_erf x profiling=ProfilingMode PROFILING assertAllFused ge graph_for x x requires_grad_ True ge = checkScript fn_test_erf x profiling=ProfilingMode PROFILING assertAllFused ge graph_for x except_for= aten size prim BroadcastSizes aten _size_if_not_equal unittest skipIf RUN_CUDA fuser requires CUDA unittest skip rand_like supported yet test_rand_broadcast_cuda fn_test_rand x y r = torch rand_like y r x + x If using profiling different function needed test different shapes we ll use cached script fn_test_rand x y r = torch rand_like y r x x x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda script_f = torch jit script fn_test_rand warmup_forward script_f x y out = script_f x y assertAllFused script_f graph_for x y x requires_grad_ True out = script_f x y assertAllFused script_f graph_for x y except_for= aten size prim BroadcastSizes aten _size_if_not_equal test broadcasting random produces correct results x = torch ones dtype=torch float device= cuda y = torch ones dtype=torch float device= cuda script_f = torch jit script fn_test_rand warmup_forward script_f x y out = script_f x y assertEqual out + torch zeros device= cuda out unittest skipIf RUN_CUDA fuser requires CUDA unittest skip rand_like supported yet test_rand_diamond fn_test_diamond x y r = torch rand_like y = x + r b = y - r + b x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda script_f = torch jit script fn_test_diamond warmup_forward script_f x y out = script_f x y assertEqual out x + y test_scalar fn x y x + y x = torch tensor dtype=torch float device= cpu y = torch tensor dtype=torch float device= cpu ge = checkScript fn x y assertAllFused ge graph_for x y test_inlined_optimized_graph torch jit script foo x torch relu x + x _ range foo torch rand _ range foo torch rand _ range foo torch rand g = torch jit last_executed_optimized_graph FileCheck check_count prim If exactly=True check prim TensorExpr run g torch _C _jit_pass_inline g f = FileCheck _ range f check prim If check prim TensorExpr f run g test_small_constant device devices fn_test_small_constant x y e- x + e- y e x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device ge = checkTrace fn_test_small_constant x y assertAllFused ge graph_for x y Currently we don t pull constants into fusion groups because some cases could remove constant original graph now our fusion group needs constant its other users Instead never pulling constants into fusion group we should just more careful how we rewrite its users TODO fix reenable test test_tensor_scalar_ops device devices should_fuse x z = y = x + z x y should_fuse_scalar x z y = x + int z x y inputs = torch randn dtype=torch float device=device ge = checkScript should_fuse inputs graph = ge graph_for inputs fusion_groups = findFusionGroups graph assertEqual len fusion_groups FileCheck check aten add check aten mul run str fusion_groups inputs = torch randn dtype=torch float device=device torch tensor dtype=torch float device=device ge = checkScript should_fuse_scalar inputs Check fused graph computes correct results when scalar input changes inputs = torch randn dtype=torch float device=device torch tensor dtype=torch float device=device assertEqual ge inputs should_fuse_scalar inputs The TE fuser supports fusion non-constant scalars assertGraphContainsExactly ge graph_for inputs FUSION_GROUP consider_subgraphs=True test_where_and_typing device devices f x y mask = x y res = torch where mask x y mask res x = torch randn dtype=torch double device=device y = torch randn dtype=torch double device=device script_f = checkScript f x y assertAllFused script_f graph_for x y except_for= prim TupleConstruct test_disabled old_cpu_fuser_state = torch _C _jit_can_fuse_on_cpu torch _C _jit_override_can_fuse_on_cpu False fn + x = torch randn dtype=torch float device= cpu s = checkScript fn x g = s graph_for x assertEqual len findFusionGroups g torch _C _jit_override_can_fuse_on_cpu old_cpu_fuser_state data_for dtype device= cuda size=None size None v = torch arange dtype=torch float device=device v = torch rand size device=device dtype == torch bool v dtype torch qint torch quint torch qint torch quantize_per_tensor v dtype=dtype v dtype test_torch_to test no op torch jit script foo x x torch float foo torch tensor dtype=torch float foo torch tensor dtype=torch float FileCheck check_not TensorExpr run torch jit last_executed_optimized_graph test fusing non-const inputs torch jit script foo x dtype int x dtype foo torch tensor dtype=torch float torch int foo torch tensor dtype=torch float torch int FileCheck check_not TensorExpr run torch jit last_executed_optimized_graph test fusing to_pinned inputs torch jit script foo x dtype int x pin_memory=True foo torch tensor dtype=torch float torch int foo torch tensor dtype=torch float torch int FileCheck check_not TensorExpr run torch jit last_executed_optimized_graph test across-device supported torch cuda is_available torch jit script foo x x device= cuda foo torch tensor dtype=torch float foo torch tensor dtype=torch float FileCheck check_not TensorExpr run torch jit last_executed_optimized_graph sizes = reuses cast impl smaller dtype set faster test dtypes = torch bool torch int torch float torch float torch float MyMod torch nn Module __init__ dtype super __init__ dtype = dtype forward x x dtype bad_dtypes = dtype output_dtype device size product dtypes dtypes devices sizes TODO Add back when https github com pytorch pytorch issues closed dtype torch float torch bfloat device == cpu continue dtype == output_dtype continue x = data_for dtype device size=size mod = MyMod output_dtype ref = mod forward x use freezing make non-Tensor args ` ` constant mod = torch jit freeze torch jit script mod eval warmup_forward mod forward x assertEqual ref mod forward x assertLastGraphAllFused unittest skip Temporarily disabled test_masked_fill dtypes = torch int torch int torch int torch int TODO Add back when https github com pytorch pytorch issues closed torch float torch float torch float torch bool sizes = self_dtype device scalar_val size product dtypes devices sizes input_v = data_for self_dtype device size=size mask = data_for torch bool device size=size fn input_v mask torch masked_fill input_v mask scalar_val ref = fn input_v mask try t = torch jit trace fn input_v mask torch testing assert_close ref t input_v mask assertLastGraphAllFused except Exception e raise RuntimeError join Failed str self_dtype op __name__ noqa F device str size e test_isnan x = torch rand x = float nan inputs = x torch tensor float nan dtypes = torch int torch int torch int torch int torch float torch float torch float torch bool inp device dtype product inputs devices dtypes TODO Add back when https github com pytorch pytorch issues closed dtype torch float torch bfloat device == cpu continue inp = inp device=device dtype=dtype try f = torch jit trace lambda x x isnan inp warmup_forward f inp assertEqual f inp inp isnan assertLastGraphAllFused except Exception e raise RuntimeError join Failed str dtype isnan device e test_gelu apply fn lambda x approximate fn x approximate unary_ops = F gelu sizes = dtype op device size product dtypes unary_ops devices sizes TODO Add back when https github com pytorch pytorch issues closed dtype torch float torch bfloat device == cpu continue try x = data_for dtype device size=size cond = data_for torch bool device fn = apply op ref = fn x cond except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x cond torch testing assert_close ref t x cond assertAllFused t graph_for x cond except Exception e raise RuntimeError join Failed str dtype op __name__ device str size e test_unary_ops torch _jit_internal _disable_emit_hooks apply fn lambda x fn x unary_ops = torch lgamma torch sigmoid torch reciprocal torch neg torch relu F relu torch log torch log torch log p torch log torch exp torch expm torch erf torch erfc torch cos torch sin torch tan torch acos torch asin torch cosh torch sinh torch atan torch tanh F hardtanh F hardsigmoid F hardswish F softplus F silu F mish F elu torch sqrt torch rsqrt torch abs TODO broken int since https github com pytorch pytorch pull RuntimeError Invalid integral op_type torch ceil torch floor torch round torch trunc torch frac TODO broken ROCm F hardshrink F leaky_relu lambda x torch threshold x - TODO broken since type promotion added lambda x torch clamp x - gpu_only = torch erf torch erfc sizes = dtype op device size product dtypes unary_ops devices sizes TODO Add back when https github com pytorch pytorch issues closed dtype torch float torch bfloat device == cpu continue todo - re-enable fails dtype == torch bfloat op == torch round continue op gpu_only device == cpu continue try x = data_for dtype device size=size fn = apply op ref = fn x except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x torch testing assert_close ref t x assertAllFused t graph_for x except Exception e raise RuntimeError join Failed str dtype op __name__ device str size e test_binary_ops apply fn lambda x y fn x y binary_ops = operator __and__ operator __or__ operator __xor__ torch add torch sub torch mul torch min torch max lambda x y torch lerp x y torch atan torch div torch eq torch ne torch ge torch gt torch lt torch fmod torch remainder lambda x y y type_as x fp_only = torch fmod torch remainder devices = devices dtype op device product dtypes binary_ops devices dtype torch float torch bfloat device == cpu continue try x = data_for dtype device y = data_for dtype device fn = apply op ref = fn x y except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x y assertEqual ref t x y op fp_only dtype is_floating_point assertAllFused t graph_for x y except Exception e raise RuntimeError join Failed str dtype op __name__ device e test_binary_scalar_ops apply fn lambda x y fn x y ir_template = graph x dtype_x y dtype_y z = op x y z binary_ops = aten mul aten add aten sub aten div aten lt aten le aten eq aten ne aten gt aten ge aten __or__ aten __xor__ aten __and__ aten __lshift__ aten __rshift__ dtypes = int float bool values = int float bool True False devices = devices dtype_x dtype_y op device product dtypes dtypes binary_ops devices code = ir_template format locals Interpret graph try graph = torch _C parse_ir code x y product values dtype_x values dtype_y ref = torch _C _jit_interpret_graph graph x y except Exception If we can t interpret IR don t bother checking NNC continue Compile graph try k = torch _C _te TensorExprKernel graph except Exception e raise RuntimeError join Compilation failed device str code e Run graph x y product values dtype_x values dtype_y ref = torch _C _jit_interpret_graph graph x y try res = k run x y assertEqual ref res except Exception e raise RuntimeError join Failed runtime device str x str y str code e test_matmul dynamic_shapes skipTest don t run conv dynamic shapes fn x y torch matmul x y devices = cpu No cuda support ext calls yet sizes = Only D x D matrix multiply supported For non-supported sizes we still want run results verification test we didn t accidentally fuse we skip is-fused check TODO add support other shape combinations make set empty skip_is_fused_check_sizes = dtype size device product dtypes sizes devices dtype torch float torch bfloat device == cpu continue try size_x size_y = size x = data_for dtype device size=size_x y = data_for dtype device size=size_y ref = fn x y except Exception e If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x y t x y assertEqual ref t x y str size skip_is_fused_check_sizes assertAllFused t graph_for x y except Exception e raise RuntimeError join Failed str dtype device e test_binary_tensor_scalar_ops torch _jit_internal _disable_emit_hooks apply_with_scalar fn scalar lambda x fn x scalar FIXME Fails IR Eval torch int and_ cpu binary_ops = operator __and__ operator __or__ operator __xor__ torch add torch sub torch mul torch eq torch ne torch ge torch lt torch gt devices = devices Maybe we should split into separate tests speed up only using scalar values relevant particular ops scalars = - - dtype op device scalar product dtypes binary_ops devices scalars dtype torch float torch bfloat device == cpu continue try x = data_for dtype device fn = apply_with_scalar op scalar ref = fn x except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x assertEqual ref t x assertAllFused t graph_for x except Exception e raise RuntimeError join Failed str dtype op __name__ device e test_binary_div_ops apply_with_scalar fn scalar lambda x fn x scalar binary_ops = torch div torch remainder torch fmod devices = devices Maybe we should split into separate tests speed up only using scalar values relevant particular ops scalars = - - skip dtype op device scalar product dtypes binary_ops devices scalars dtype torch float torch bfloat device == cpu continue try x = data_for dtype device fn = apply_with_scalar op scalar ref = fn x except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x assertEqual ref t x except Exception e raise RuntimeError f Failed dtype op __name__ device scalar e test_binary_pow apply_with_scalar fn scalar lambda x fn x scalar dtypes = FIXME pow fails dtype=torch float device=cuda scalar= torch float torch float torch float torch bool intentionally included binary_ops = torch pow Maybe we should split into separate tests speed up only using scalar values relevant particular ops scalars = - - dtype op device scalar product dtypes binary_ops devices scalars dtype torch float torch bfloat device == cpu continue try x = data_for dtype device fn = apply_with_scalar op scalar ref = fn x except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x assertEqual ref t x assertAllFused t graph_for x except Exception e raise RuntimeError join Failed str dtype op __name__ device e test_ternary_ops apply fn lambda x y z fn x y z ternary_ops = torch lerp torch addcmul devices = devices dtype op device product dtypes ternary_ops devices dtype torch float torch bfloat device == cpu continue try x = data_for dtype device y = data_for dtype device z = data_for dtype device fn = apply op ref = fn x y z except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x y z assertEqual ref t x y z assertAllFused t graph_for x y z except Exception e raise RuntimeError join Failed str dtype op __name__ device e test_ternary_norm_ops apply fn lambda x y z fn x y z ternary_ops = F batch_norm devices = devices dtype op device product dtypes ternary_ops devices dtype torch float torch bfloat device == cpu continue try x = data_for dtype device size= y = data_for dtype device size= z = data_for dtype device size= fn = apply op ref = fn x y z except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x y z assertEqual ref t x y z assertAllFused t graph_for x y z except Exception e raise RuntimeError join Failed str dtype op __name__ device e unittest skip FIXME fuser doesn t include ListConstruct nodes group causing failure test_list_ops apply fn lambda x y z fn x x y y z z devices = devices list_ops = torch cat dtype op device product dtypes list_ops devices dtype torch float torch bfloat device == cpu continue try x = data_for dtype device size= y = data_for dtype device size= z = data_for dtype device size= fn = apply op ref = fn x y z except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn x y z assertEqual ref t x y z assertAllFused t graph_for x y z except Exception e raise RuntimeError join Failed str dtype op __name__ device e test_where_ops apply fn lambda cond x y fn cond x y ops = torch where lambda cond x y torch where cond x lambda cond x y torch where cond y devices = devices dtype op device product dtypes ops devices dtype torch float torch bfloat device == cpu continue try cond = data_for torch bool device x = data_for dtype device y = data_for dtype device fn = apply op ref = fn cond x y except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue try t = torch jit trace fn cond x y assertEqual ref t cond x y assertAllFused t graph_for cond x y except Exception e raise RuntimeError join Failed str dtype op __name__ device e test_unsupported_dtypes device devices fn x x x + x unsupported_dtypes = torch uint torch complex torch complex torch complex torch qint torch quint torch qint dtype unsupported_dtypes try x = data_for dtype device ref = fn x except Exception If eager mode doesn t support dtype op device combo neither does fuser Catch everything avoid needing guess what errors might thrown eager continue t = torch jit trace fn x assertEqual ref t x assertEqual len findFusionGroups t graph_for x test_superslomo devices = devices copy LLVM_ENABLED devices remove cpu device devices Test extracted Super-SloMo https github com avinashpaliwal Super-SloMo A few interesting things happen here strided inputs mixed size plus outputs mixed shapes The latter characteristic happened expose memory corruption bug due properly guarding outputs eager t t t t t t = torch mul t t t = torch mul t t t = torch mul t t t = torch add t t t = torch add t t ft_p = torch div t t ft_p t t t t = torch rand device=device transpose t = torch rand device=device t = torch rand device=device None None None permute t = torch rand device=device t = torch rand device=device inputs = t t t t t script = torch jit script eager _ range pair zip script inputs eager inputs test ref = pair torch testing assert_close test ref assertAllFused script graph_for inputs except_for= prim TupleConstruct test_sub_gt_and device devices eager t t t t t float w = t - t h = t - t k = w t h t assert k dtype == torch bool t Putting use k never-executed conditional prevents profiling its type which leaves Tensor If we propagate Tensor back definition k we have careful create fusion group containing k + w t = torch rand dtype=torch float device=device scripted = checkScript eager t t t t skipIfTorchDynamo too slow test_chunk_mul_one dynamic_shapes skipTest TODO chunk dynamic shapes device devices eager x z y w = torch chunk x - z y w x = torch rand dtype=torch float device=device z y w = eager x script = checkScript eager x test_eq_unsqueeze_type_as device devices eager b mask = b == mask = torch unsqueeze mask - x = mask type_as x mask = torch rand device=device dtype=torch float b = torch randint - device=device dtype=torch long script = checkScript eager b test_neg_pow eager_tt torch Tensor b torch Tensor torch neg torch pow b eager_ts torch Tensor b float torch neg torch pow b eager_st float b torch Tensor torch neg torch pow b = torch rand dtype=torch float b = torch rand dtype=torch float s = b item script = checkScript eager_tt b TODO re-enable fusion which doesn t work right now just test correctness now assertAllFused script graph_for b script = checkScript eager_ts s assertAllFused script graph_for s script = checkScript eager_st s b assertAllFused script graph_for s b unittest skipIf LLVM_ENABLED Too slow run TE interpreter test_conv d_depthwise dynamic_shapes skipTest don t run conv dynamic shapes eager input weight bias torch conv d input weight bias stride= padding= groups= input = torch rand dtype=torch float weight = torch rand dtype=torch float bias = torch rand dtype=torch float script = checkScript eager input weight bias assertAllFused script graph_for input weight bias test_conv d dynamic_shapes skipTest don t run conv dynamic shapes eager input weight bias torch conv d input weight bias stride= padding= groups= input = torch rand dtype=torch float weight = torch rand dtype=torch float bias = torch rand dtype=torch float script = checkScript eager input weight bias FileCheck check_not TensorExpr run torch jit last_executed_optimized_graph test_type_as_cat inline_fusion_groups eager x y torch cat x y type_as x dim= dtypes = dtypes copy CPU fuser doesn t support float dtypes remove torch float dtypes remove torch bfloat dtype dtype product dtypes dtypes x = torch randint dtype zero = torch tensor dtype one = torch tensor dtype script = torch jit trace eager x zero _ range torch testing assert_close script x zero eager x zero torch testing assert_close script x one eager x one assertAllFused script graph_for x one test_to_device eager x x device= cpu relu x = torch rand script = checkScript eager x assertAllFused script graph_for x test_dims eager x y x y + x = torch linspace - dtype=torch float as_strided y = torch tensor dtype=torch float script = checkScript eager x y assertAllFused script graph_for x y unittest skipIf RUN_CUDA fuser requires CUDA test_channels_last_dims_dynamic eager x y x + y + indices = sets = i range len indices + subset combinations indices i sets append subset noqa PERF set sets size = index set size index = inp = torch rand size memory_format=torch channels_last cuda texpr_enable_strategy DYNAMIC foo_s = torch jit trace eager inp inp _ range out = foo_s inp inp out_eager = eager inp inp assertEqual out_eager out assertTrue out is_contiguous memory_format=torch channels_last g = torch jit last_executed_optimized_graph FileCheck check TensorExpr run g test_exhaust_specializations texpr_enable_strategy STATIC torch jit script foo x x + x + x _ range foo torch rand _ range foo torch rand g = torch jit last_executed_optimized_graph torch _C _jit_pass_inline g FileCheck check_count TensorExpr exactly=True run g test_unsqueeze_var_dim eager x y z int x torch unsqueeze y dim=z x = torch rand permute y = torch rand z = script = checkScript eager x y z _test_fwd_bwd fn x = torch arange - dtype=torch float requires_grad=True xs = torch arange - dtype=torch float requires_grad=True script = torch jit script fn _ range y = fn x g = torch rand_like y y backward g ys = script xs ys backward g torch no_grad x -= x grad xs -= xs grad x grad = None xs grad = None torch testing assert_close y ys test_relu_fwd_bwd eager x torch relu x _test_fwd_bwd eager test_hardswish_fwd_bwd eager x F hardswish x _test_fwd_bwd eager test_hardsigmoid_fwd_bwd eager x F hardsigmoid x _test_fwd_bwd eager test_cat_graph_opt foo x y z torch log torch cat x y z checkScript foo torch rand torch rand torch rand TODO sure why updated graph isn t reflected last_optimized_graph assertLastGraphAllFused test_dynamic_cat inline_fusion_groups torch jit script repro xs list torch Tensor ys list torch Tensor zs list torch Tensor torch cat x torch cat y z dim=- dim=- x y z zip xs ys zs _ range N = xs = torch ones _ range N Note concat ys zs will have same size each pair even though individual ys zs do ys = torch ones N - i i range N zs = torch ones i i range N repro xs ys zs test_scalar_only_inputs eager b float = torch ones b script = checkScript eager test_cat_ k_args inline_fusion_groups eager x torch relu torch cat x _ range x = torch randn trace = checkTrace eager x fusion_groups = findFusionGroups trace graph_for x assertEqual len fusion_groups test_adaptive_avg_pool d TODO once adaptive_avg_pool d available OpInfo DB test should moved there inline_fusion_groups foo x torch nn functional adaptive_avg_pool d x foo x torch nn functional adaptive_avg_pool d x x = torch randn foo foo foo f = torch jit trace foo x kernel = torch _C _te TensorExprKernel f graph correct_val = f x assertEqual kernel run x correct_val test_unrolled_cat inline_fusion_groups eager x ret = torch empty i range x shape ret = torch cat ret x i relu ret script = torch jit script eager Warm up size= tensor since loop iterates once profile data will burned assuming size= then unrolled x = torch ones _ range script x torch testing assert_close eager x script x Now when input hits unrolled path will produce incorrectly-sized tensor since size= has been burned x = torch ones torch testing assert_close eager x script x skipIfTorchDynamo too slow unittest skipIf TEST_WITH_ASAN takes + minutes asan unittest skipIf TEST_WITH_ROCM Tensor-likes close nans test_batch_norm test fn args trace = torch jit trace fn args assertAllFused trace graph_for args TODO Are ` NaN ` s actually ok here did pass silently before because ` equal_nan=True ` default torch testing assert_close fn args trace args equal_nan=True bn i x torch batch_norm i x x x x False e- False relu bn_no_weight i x torch batch_norm i None x x x False e- False relu bn_no_bias i x torch batch_norm i x None x x False e- False relu bn_neither i x torch batch_norm i None None x x False e- False relu device devices i = torch randn device=device x = torch randn device=device fn bn bn_no_weight bn_no_bias bn_neither test fn i x test_profiler torch jit script test x y z x y + z args = torch randn _ range torch autograd profiler profile prof _ range test args assertIn fused_mul_add prof table test_skip_grad_in_check torch jit script foo x x + inp = torch rand _ range foo inp inp requires_grad_ True torch inference_mode _ range foo inp g = torch jit last_executed_optimized_graph torch _C _jit_pass_inline g torch _C _jit_pass_inline g FileCheck check_count prim If exactly=True run g test_dynamic_shapes functools partial n = gen_tensor = lambda n R n lambda n R n n lambda n R n n transpose lambda n R n + n + n n lambda n R n n lambda n R n n + n + n + memory_format=torch channels_last texpr_enable_strategy DYNAMIC foo x y z torch sigmoid torch tanh x foo __disable_jit_function_caching__ = True fi x y z torch tanh x + y fi __disable_jit_function_caching__ = True fum x y z torch tanh x + y + z fum __disable_jit_function_caching__ = True funcs = foo fi fum inline_fusion_groups device devices I = partial torch randint device=device R = partial torch randn device=device i func enumerate funcs num_args = i + gen gen_tensor inps = gen n gen n gen n func_s = torch jit trace func inps check_trace=False torch _C _jit_pass_erase_shape_information func_s graph _ range x y z = gen n gen n gen n func_s x y z _incr range func_s gen n + _ range g = torch jit last_executed_optimized_graph torch _C _jit_pass_inline g torch _C _jit_pass_dce g We should see only one optimized kernel FileCheck check_count TensorExprDynamicGuard exactly=True run g assertEqual func inps func_s inps gen = gen_tensor inps = gen n gen n gen n foo_s = torch jit trace foo inps torch _C _jit_pass_erase_shape_information foo_s graph g_prev = None gen gen_tensor i range foo_s gen n + i _ range inps = gen n gen n gen n assertEqual foo_s inps foo inps g = torch jit last_executed_optimized_graph torch _C _jit_pass_inline g torch _C _jit_pass_dce g FileCheck check_count TensorExprDynamicGuard len gen_tensor exactly=True run g unittest skipIf RUN_CUDA half-precision NNC fusion requires CUDA test_autocast_up f x y = x _autocast_to_full_precision True True z = torch exp y z x = torch rand dtype=torch half device= cuda scr = torch jit script f scr x scr x assertLastGraphAllFused unittest skipIf RUN_CUDA half-precision NNC fusion requires CUDA test_autocast_down f x y = torch sigmoid x z = y _autocast_to_reduced_precision True True torch half torch half z x = torch rand dtype=torch float device= cuda scr = torch jit script f scr x scr x assertLastGraphAllFused unittest skipIf LLVM_ENABLED Compiles TensorExprKernel test_to_dtype f x y = torch sigmoid x z = y _autocast_to_reduced_precision True True torch half torch bfloat h = z _autocast_to_full_precision True True i = h dtype=torch bfloat j = i dtype=torch float j x = torch rand dtype=torch float scr = torch jit trace f x scr x scr x assertLastGraphAllFused assertEqual f x scr x atol= e- rtol= e- bf_x = torch rand dtype=torch bfloat bf_scr = torch jit trace f bf_x bf_scr bf_x bf_scr bf_x graph = bf_scr graph_for bf_x fusion_groups = findFusionGroups graph assertEqual len fusion_groups assertEqual f bf_x bf_scr bf_x atol= e- rtol= e- test_with_strict_fusion success x torch jit strict_fusion x + x + x scripted = checkScript success torch rand g = torch jit last_executed_optimized_graph FileCheck check_not aten add check prim TensorExprGroup run g foo x torch jit strict_fusion x + x + torch rand + assertRaises Exception error_out foo_s = torch jit script foo foo_s torch rand foo_s torch rand print torch jit last_executed_optimized_graph fc = FileCheck check Found unfused operators fc check aten rand SymInt size fc check torch rand run str error_out exception warnings catch_warnings record=True warns foo torch rand FileCheck check Only works script mode run str warns test_autodiff x torch jit strict_fusion torch rand + x + x + x foo_s = torch jit script test_autodiff inp = torch rand requires_grad=True assertRaises Exception error_out _ range foo_s inp f = FileCheck check unfused operators check aten rand f run str error_out exception test_separate_fusions x y torch jit strict_fusion x + x + x y + y + y inp = torch rand requires_grad=True assertRaises Exception error_out _ range foo_s = torch jit script test_separate_fusions foo_s inp inp f = FileCheck check Found multiple fusions f run str error_out exception test_constant_chunk_shapes We had issue where buildShapeExpressions would fail show below Tensor = Constant supported we don t build shape Tensor = Constant supported Tensor = aten add inputs supported we don t build shape = prim ConstantChunk forgets check whether input shapes exist fails dynamic_shapes skipTest TODO chunk dynamic shapes device devices f x y r = torch tensor z z = x + y + r chunk dim= z z x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device ge = checkTrace f x y graph = ge graph_for x y make sure we actually testing right scenario FileCheck check + FUSION_GROUP + _ check_count ConstantChunk exactly=True run str graph f_traced = torch jit trace f x y _ range make sure doesn t error out res = f_traced x y assertEqual res f x y unittest skipIf RUN_CUDA_HALF half-precision NNC fusion requires CUDA test_pow_multiple_dtype https github com pytorch pytorch issues fn p torch Tensor gamma float = - torch Tensor p = torch sigmoid p result = p gamma result x = torch rand dtype=torch half device= cuda ref = fn x script_fn = torch jit script fn _ range res = script_fn x assertEqual ref res TestTEFuserStatic TestTEFuser dynamic_shapes = False TestTEFuserDynamic TestTEFuser dynamic_shapes = True del TestTEFuser works_list = __radd__ __rdiv__ __rmul__ __rmod__ abs acos add addcmul addmm decomposed asin atan atan ceil clamp clamp scalar contiguous cos cosh div no_rounding_mode div true_rounding div floor_rounding div trunc_rounding eq erf erfc exp expand expand_as expm floor fmod fmod autodiffed ge gt isnan le lerp lgamma log log log p log lt masked_fill max binary mean min binary mm mul ne neg nn functional hardshrink nn functional hardsigmoid nn functional hardswish nn functional softplus nn functional hardtanh nn functional leaky_relu nn functional relu nn functional relu nn functional softsign nn functional tanhshrink nn functional threshold permute pow reciprocal remainder remainder autodiffed reshape reshape_as round rsub rsub rsub_tensor rsqrt sigmoid sign sin sinh sqrt sub sum t tan tanh transpose true_divide trunc unsqueeze view view_as where bool byte char double float half int long short bool channels_last byte channels_last char channels_last double channels_last float channels_last half channels_last int channels_last long channels_last short channels_last known_failures = __rmatmul__ frac matmul If your OpInfo test causes test fail add here skip_ops = conj get_name op l = op name op variant_test_name = l append op variant_test_name join l Purpose allow super calls super no arguments fails presumably because how instantiate_device_type_tests works super TestNNCOpInfo fails because TestNNCOpInfo gets deleted global scope super JitCommonTestCase fn would skip JitCommonTestCase fn implementation TestNNCOpInfoParent JitCommonTestCase pass TestNNCOpInfo TestNNCOpInfoParent setUp super TestNNCOpInfoParent setUp tensorexpr_options = TensorExprTestOptions tearDown tensorexpr_options restore super TestNNCOpInfoParent tearDown te_compile device dtype op op name skip_ops sample_inputs_itr = op sample_inputs device dtype requires_grad=False sample_input sample_inputs_itr arg_values = sample_input input + list sample_input args kwarg_values = sample_input kwargs param_names = param_values = fx_args = idx v enumerate arg_values isinstance v torch Tensor param_names append f arg_ idx param_values append v fx_args append param_names - fx_args append f repr v k v kwarg_values items isinstance v torch Tensor param_names append k param_values append v fx_args append f k = k fx_args append f k = repr v code = f f join param_names op op join fx_args g = torch torch inf math inf op op exec code g f = g f f __module__ = test out = f param_values ts_g = torch jit trace f param_values kernel = torch _C _te TensorExprKernel ts_g graph correct_val = f param_values assertEqual kernel run tuple param_values correct_val assertEqual kernel fallback tuple param_values correct_val onlyCPU unittest skipIf LLVM_ENABLED Compiles TensorExprKernel ops op op op_db get_name op works_list allowed_dtypes= torch float test_working device dtype op te_compile device dtype op onlyCPU unittest skipIf LLVM_ENABLED Compiles TensorExprKernel ops op op op_db get_name op known_failures allowed_dtypes= torch float test_failures device dtype op try te_compile device dtype op except Exception e pass raise RuntimeError Expected test fail If now works move op into works_list onlyCPU unittest skipIf LLVM_ENABLED Compiles TensorExprKernel ops op op op_db get_name op works_list + known_failures allowed_dtypes= torch float test_unsupported device dtype op get_name op skip_ops try warnings catch_warnings warnings simplefilter ignore TracerWarning noqa F te_compile device dtype op except Exception e pass raise RuntimeError Expected test fail If now works move op into works_list slowTest onlyCPU ops op op op_db get_name op known_failures dtypes=OpDTypes supported test_nnc_correctness device dtype op op supports_tracing skipTest Requires tracing support NoTracerWarnContextManager no_warn variant_sample_pairs = get_traced_sample_variant_pairs device dtype op variant sample variant_sample_pairs trace = create_traced_fn variant cache_traced_fn=True ref = variant clone_inputs sample input sample args sample kwargs trace clone_inputs sample input sample args sample kwargs val = trace clone_inputs sample input sample args sample kwargs atol = e- dtype == torch bfloat e- rtol = e- dtype == torch bfloat e- assertEqual ref val atol=atol rtol=rtol https github com pytorch pytorch issues each torch jit trace adds state _python_cu compilation unit since test traces lot functions out-of-memory can occur CU cleared torch jit _state _python_cu drop_all_functions CPU fuser currently used fbcode only_for = cuda IS_FBCODE cpu cuda instantiate_device_type_tests TestNNCOpInfo globals only_for=only_for Purpose allow super calls See TestNNCOpInfoParent TestLoopnestRandomizationParent JitTestCase pass TestLoopnestRandomization TestLoopnestRandomizationParent setUp super TestLoopnestRandomizationParent setUp old_cpu_fuser_state = torch _C _jit_can_fuse_on_cpu old_must_use_cpu_state = torch _C _jit_get_te_must_use_llvm_cpu old_gpu_fuser_state = torch _C _jit_can_fuse_on_gpu torch _C _jit_override_can_fuse_on_cpu True TODO force LLVM need add asan mac windows builds + sandcastle torch _C _jit_set_te_must_use_llvm_cpu True torch _C _jit_override_can_fuse_on_gpu True old_profiling_executor = torch _C _jit_set_profiling_executor True old_profiling_mode = torch _C _get_graph_executor_optimize True old_fusion_inlining = torch _C _debug_get_fusion_group_inlining torch _C _debug_set_fusion_group_inlining False texpr_fuser_state = torch _C _jit_texpr_fuser_enabled torch _C _jit_set_texpr_fuser_enabled True old_te_must_use_llvm_cpu = torch _C _jit_get_te_must_use_llvm_cpu torch _C _jit_set_te_must_use_llvm_cpu False Set seed This tests codepath through random transformation os environ PYTORCH_TENSOREXPR_RANDOM_TRANSFORM_SEED = tearDown torch _C _jit_set_profiling_executor old_profiling_executor torch _C _get_graph_executor_optimize old_profiling_mode torch _C _jit_override_can_fuse_on_gpu old_gpu_fuser_state torch _C _jit_override_can_fuse_on_cpu old_cpu_fuser_state torch _C _jit_set_te_must_use_llvm_cpu old_must_use_cpu_state torch _C _debug_set_fusion_group_inlining old_fusion_inlining torch _C _jit_set_texpr_fuser_enabled texpr_fuser_state torch _C _jit_set_te_must_use_llvm_cpu old_te_must_use_llvm_cpu Set back os environ PYTORCH_TENSOREXPR_RANDOM_TRANSFORM_SEED = super TestLoopnestRandomizationParent tearDown onlyCPU unittest skipIf LLVM_ENABLED Compiles TensorExprKernel test_relu device fn_test_relu x y F relu x + y x = torch randn dtype=torch float device=device y = torch randn dtype=torch float device=device fn = fn_test_relu traced_fn = torch jit trace fn x y ref = fn x y res = traced_fn x y assert torch allclose ref res instantiate_device_type_tests TestLoopnestRandomization globals only_for= cpu __name__ == __main__ run_tests