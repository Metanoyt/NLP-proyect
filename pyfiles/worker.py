mypy allow-untyped-defs r Contains definitions methods used _BaseDataLoaderIter workers These needs global scope since Py doesn t support serializing static methods os queue random dataclasses dataclass typing Optional TYPE_CHECKING Union torch torch _utils ExceptionWrapper HAS_NUMPY IS_WINDOWS MP_STATUS_CHECK_INTERVAL signal_handling TYPE_CHECKING torch utils data Dataset IS_WINDOWS ctypes ctypes wintypes BOOL DWORD HANDLE On Windows parent ID worker process remains unchanged when manager process gone only way check through OS let worker have process handle manager ask process status has changed ManagerWatchdog __init__ - None manager_pid = os getppid mypy cannot detect code windows only kernel = ctypes WinDLL kernel use_last_error=True type ignore attr-defined kernel OpenProcess argtypes = DWORD BOOL DWORD kernel OpenProcess restype = HANDLE kernel WaitForSingleObject argtypes = HANDLE DWORD kernel WaitForSingleObject restype = DWORD Value obtained https msdn microsoft com en-us library ms aspx SYNCHRONIZE = x manager_handle = kernel OpenProcess SYNCHRONIZE manager_pid manager_handle raise ctypes WinError ctypes get_last_error type ignore attr-defined manager_dead = False is_alive manager_dead Value obtained https msdn microsoft com en-us library windows desktop ms aspx manager_dead = kernel WaitForSingleObject manager_handle == manager_dead ManagerWatchdog type ignore no-redef __init__ - None manager_pid = os getppid manager_dead = False is_alive manager_dead manager_dead = os getppid = manager_pid manager_dead _worker_info Optional WorkerInfo = None WorkerInfo id int num_workers int seed int dataset Dataset __initialized = False __init__ kwargs k v kwargs items setattr k v __keys = tuple kwargs keys __initialized = True __setattr__ key val __initialized raise RuntimeError f Cannot assign attributes __class__ __name__ objects super __setattr__ key val __repr__ items = f k = getattr k k __keys f __class__ __name__ join items get_worker_info - Optional WorkerInfo r Returns information about current ` ~torch utils data DataLoader ` iterator worker process When called worker returns object guaranteed have following attributes attr ` id ` current worker id attr ` num_workers ` total number workers attr ` seed ` random seed set current worker This value determined main process RNG worker id See ` ~torch utils data DataLoader ` s documentation more details attr ` dataset ` copy dataset object process Note will different object different process than one main process When called main process returns ` ` None ` ` note When used attr ` worker_init_fn ` passed over ` ~torch utils data DataLoader ` method can useful set up each worker process differently instance using ` ` worker_id ` ` configure ` ` dataset ` ` object only read specific fraction sharded dataset use ` ` seed ` ` seed other libraries used dataset code _worker_info r Dummy used signal end IterableDataset dataclass frozen=True _IterableDatasetStopIteration worker_id int r Dummy used resume fetching when worker reuse enabled dataclass frozen=True _ResumeIteration seed Optional int = None The function ` _generate_state ` adapted ` numpy random SeedSequence ` https github com numpy numpy blob main numpy random bit_generator pyx It s MIT licensed here copyright Copyright c Melissa E O Neill Copyright c NumPy Developers Permission hereby granted free charge any person obtaining copy software associated documentation files Software deal Software without restriction including without limitation rights use copy modify merge publish distribute sublicense sell copies Software permit persons whom Software furnished do so subject following conditions The above copyright notice permission notice shall included all copies substantial portions Software THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE This function generates array int seed ` numpy random ` order prevent state collision due same seed algorithm ` numpy random ` ` random ` modules TODO Implement ` SeedSequence ` like object ` torch random ` _generate_state base_seed worker_id INIT_A = x B D E MULT_A = x E INIT_B = x B F DD MULT_B = x F DED MIX_MULT_L = xCA F DD MIX_MULT_R = x F XSHIFT = MASK = xFFFFFFFF entropy = worker_id base_seed MASK base_seed pool = hash_const_A = INIT_A hash value nonlocal hash_const_A value = value ^ hash_const_A MASK hash_const_A = hash_const_A MULT_A MASK value = value hash_const_A MASK value = value ^ value XSHIFT MASK value mix x y result_x = MIX_MULT_L x MASK result_y = MIX_MULT_R y MASK result = result_x - result_y MASK result = result ^ result XSHIFT MASK result Add entropy pool i range len pool pool i = hash entropy i Mix all bits together so late bits can affect earlier bits i_src range len pool i_dst range len pool i_src = i_dst pool i_dst = mix pool i_dst hash pool i_src hash_const_B = INIT_B state = i_dst range data_val = pool i_dst data_val = data_val ^ hash_const_B MASK hash_const_B = hash_const_B MULT_B MASK data_val = data_val hash_const_B MASK data_val = data_val ^ data_val XSHIFT MASK state append data_val state _worker_loop dataset_kind dataset index_queue data_queue done_event auto_collation collate_fn drop_last base_seed init_fn worker_id num_workers persistent_workers shared_seed See NOTE Data Loader Multiprocessing Shutdown Logic details logic function try Initialize C side signal handlers SIGBUS SIGSEGV Python signal module s handlers executed after Python returns C low-level handlers likely when same fatal signal had already happened again https docs python org library signal html#execution-of-python-signal-handlers signal_handling _set_worker_signal_handlers torch multiprocessing _set_thread_name pt_data_worker torch set_num_threads seed = base_seed + worker_id random seed seed torch manual_seed seed HAS_NUMPY np_seed = _generate_state base_seed worker_id numpy np np random seed np_seed torch utils data IterDataPipe torch utils data graph_settings apply_random_seed shared_rng = torch Generator isinstance dataset IterDataPipe shared_seed None raise AssertionError shared_seed must provided IterDataPipe workers shared_rng manual_seed shared_seed dataset = apply_random_seed dataset shared_rng global _worker_info _worker_info = WorkerInfo id=worker_id num_workers=num_workers seed=seed dataset=dataset torch utils data _DatasetKind init_exception = None try init_fn None init_fn worker_id fetcher = _DatasetKind create_fetcher dataset_kind dataset auto_collation collate_fn drop_last except Exception init_exception = ExceptionWrapper where=f DataLoader worker process worker_id When using Iterable mode some worker can exit earlier than others due IterableDataset behaving differently different workers When such things happen ` _IterableDatasetStopIteration ` object sent over main process ID worker so main process won t send more tasks worker will send ` None ` worker properly exit Note we cannot set ` done_event ` worker shared among all processes Instead we set ` iteration_end ` flag signify iterator exhausted When either ` done_event ` ` iteration_end ` set we skip all processing step just wait ` None ` iteration_end = False watchdog = ManagerWatchdog while watchdog is_alive try r = index_queue get timeout=MP_STATUS_CHECK_INTERVAL except queue Empty continue isinstance r _ResumeIteration Acknowledge main process data_queue put r None iteration_end = False isinstance dataset IterDataPipe r seed None raise AssertionError resume iteration seed None IterDataPipe shared_rng manual_seed r seed dataset = apply_random_seed dataset shared_rng Recreate fetcher worker-reuse policy fetcher = _DatasetKind create_fetcher dataset_kind dataset auto_collation collate_fn drop_last continue r None Received final signal done_event is_set iteration_end raise AssertionError Received final signal neither done_event nor iteration_end set break done_event is_set iteration_end ` done_event ` set But I haven t received final signal None yet I will keep continuing until get skip processing steps continue idx index = r data Union _IterableDatasetStopIteration ExceptionWrapper init_exception None data = init_exception init_exception = None try data = fetcher fetch index type ignore possibly-undefined except Exception e isinstance e StopIteration dataset_kind == _DatasetKind Iterable data = _IterableDatasetStopIteration worker_id Set ` iteration_end ` save future ` next ` calls avoid sending multiple ` _IterableDatasetStopIteration ` s iteration_end = True It important we don t store exc_info variable ` ExceptionWrapper ` does correct thing See NOTE Python Traceback Reference Cycle Problem data = ExceptionWrapper where=f DataLoader worker process worker_id data_queue put idx data del data idx index r save memory except KeyboardInterrupt Main process will raise KeyboardInterrupt anyways pass done_event is_set data_queue cancel_join_thread data_queue close