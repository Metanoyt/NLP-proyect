mypy allow-untyped-defs logging time collections defaultdict collections abc Iterator contextlib contextmanager enum Enum torch torch distributed dist torch distributed fsdp _flat_param flat_param_file torch distributed fsdp _common_utils _apply_to_modules _get_module_fsdp_state clean_tensor_name logger = logging getLogger __name__ SimpleProfiler Type str Enum ALL = all ALLGATHER = all_gather ALLGATHER_OBJ = all_gather_object RESHARDING = resharding H D = H D D H = D H results dict str float = defaultdict float profiling set str = set classmethod reset cls - None cls results clear cls profiling clear classmethod contextmanager profile cls profile_type str - Iterator None profile_type cls profiling raise AssertionError f profile_type already being profiled SimpleProfiler does support profiling multiple instances same time cls profiling add profile_type begin = time monotonic try yield finally end = time monotonic cls results profile_type += end - begin cls profiling remove profile_type classmethod dump_and_reset cls msg str - None This cannot combined DETAIL distributed log profiling will very incorrect dist get_rank == dist get_debug_level == dist DebugLevel INFO logger info s s msg cls results cls reset _get_sharded_module_tree_with_module_name_to_fqns model torch nn Module - tuple str dict str list str It used composable fully_shard code path returns sharded module tree info each line represents submodule name contains submodule s FQN its submodule name submodule sharded ` fully_shard ` submodule name will add postfix FULLY SHARDED Each increased tree level adds spaces before printed name A printed sharded module tree info toy model like CompositeModel FULLY SHARDED l Linear u UnitModule FULLY SHARDED u l Linear u seq Sequential u seq ReLU u seq Linear u seq ReLU u l Linear u UnitModule FULLY SHARDED u l Linear u seq Sequential u seq ReLU u seq Linear u seq ReLU u l Linear l Linear dict mapping concated module FQN name list its managed original parameters FQNs An example dict above toy sharded model like CompositeModel l weight l bias l weight l bias u UnitModule u l weight u l bias u seq weight u seq bias u l weight u l bias u UnitModule u l weight u l bias u seq weight u seq bias u l weight u l bias All FQNs prefixed starting ` ` model ` ` Args model torch nn Module Root module which may may passed composable ` fully_shard ` module_fn module prefix tree_level sharded_tree_info sharded_module_name_to_fqns num_spaces = tree_level trimed_prefix = prefix - len prefix prefix - == prefix prefixed_module_name = trimed_prefix + + module __class__ __name__ + printed_prefixed_module_name = num_spaces + prefixed_module_name state = _get_module_fsdp_state module state None sharded_tree_info += printed_prefixed_module_name + \n handle = state _fully_sharded_module_to_handle get module None handle sharded_tree_info += printed_prefixed_module_name + FULLY SHARDED + \n sharded_tree_info += printed_prefixed_module_name + \n handle param = handle flat_param isinstance param flat_param_file FlatParameter raise AssertionError f Expected FlatParameter got type param global_fqns = clean_tensor_name prefix + name name param _fqns prefixed top level ` model ` i e including ` prefix ` prefixed_module_name sharded_module_name_to_fqns sharded_module_name_to_fqns prefixed_module_name extend global_fqns sharded_module_name_to_fqns prefixed_module_name = global_fqns return_fn sharded_tree_info sharded_module_name_to_fqns sharded_tree_info sharded_module_name_to_fqns Use List mutate its value place while running recursive functions sharded_tree_info list str = sharded_module_name_to_fqns dict str list str = _apply_to_modules model module_fn return_fn key key _ model named_parameters sharded_tree_info sharded_module_name_to_fqns