Variable-length attention implementation using Flash Attention This module provides high-level Python interface variable-length attention calls into optimized Flash Attention kernels logging functools lru_cache typing Any NamedTuple Optional Union torch log = logging getLogger __name__ __all__ = varlen_attn AuxRequest lru_cache maxsize= _should_use_cudnn device_index int - bool Cache device capability check avoid repeated CUDA calls False AuxRequest NamedTuple Request which auxiliary outputs compute varlen_attn Each field boolean indicating whether auxiliary output should computed lse bool = False torch library custom_op torch_attn _varlen_attn mutates_args= _varlen_attn query torch Tensor key torch Tensor value torch Tensor cu_seq_q torch Tensor cu_seq_k torch Tensor max_q int max_k int is_causal bool = False - tuple torch Tensor torch Tensor torch Tensor Private custom op variable-length attention This internal implementation Users should use public varlen_attn function instead use_cudnn = query is_cuda _should_use_cudnn query device index use_cudnn log info Using cuDNN backend varlen_attn result = torch ops aten _cudnn_attention_forward query key value None attn_bias cu_seq_q cu_seq_k max_q max_k True compute_log_sumexp dropout_p hardcoded is_causal False return_debug_mask cuDNN returns output logsumexp cum_seq_q cum_seq_k max_q max_k philox_seed philox_offset debug_attn_mask output softmax_lse rng_state = result result result log info Using Flash Attention backend varlen_attn output softmax_lse rng_state _ _ = torch ops aten _flash_attention_forward query key value cu_seq_q cu_seq_k max_q max_k dropout_p hardcoded is_causal return_debug_mask=False rng_state_ = torch zeros dtype=torch uint device=query device hardcoded since dropout hardcoded output softmax_lse rng_state_ _varlen_attn register_fake _varlen_attn_fake query torch Tensor key torch Tensor value torch Tensor cu_seq_q torch Tensor cu_seq_k torch Tensor max_q int max_k int is_causal bool = False - tuple torch Tensor torch Tensor torch Tensor Fake implementation meta tensor computation tracing Based D varlen path meta__flash_attention_forward - query shape total num_heads head_dim - logsumexp shape num_heads total_q Output has same shape query output = torch empty_like query For varlen path logsumexp shape num_heads total_q total_q = query size num_heads = query size logsumexp = torch empty num_heads total_q dtype=torch float device=query device rng_state = torch empty dtype=torch uint device=query device output logsumexp rng_state varlen_attn query torch Tensor key torch Tensor value torch Tensor cu_seq_q torch Tensor cu_seq_k torch Tensor max_q int max_k int is_causal bool = False return_aux Optional AuxRequest = None - Union torch Tensor tuple torch Tensor torch Tensor Compute variable-length attention using Flash Attention This function similar scaled_dot_product_attention optimized variable-length sequences using cumulative sequence position tensors Args - query Tensor Query tensor shape math ` T_q H D ` - key Tensor Key tensor shape math ` T_k H D ` - value Tensor Value tensor shape math ` T_k H D ` - cu_seq_q Tensor Cumulative sequence positions queries shape math ` N+ ` - cu_seq_k Tensor Cumulative sequence positions keys values shape math ` N+ ` - max_q int Maximum query sequence length batch - max_k int Maximum key value sequence length batch - is_causal bool optional If set True applies causal masking default False - return_aux Optional AuxRequest If None ` ` return_aux lse ` ` True also returns logsumexp tensor Shape legend - math ` N ` Batch size - math ` T_q ` Total number query tokens batch sum all query sequence lengths - math ` T_k ` Total number key value tokens batch sum all key value sequence lengths - math ` H ` Number attention heads - math ` D ` Head dimension Returns - Tensor Output tensor attention computation - If ` ` return_aux ` ` None ` ` return_aux lse ` ` True returns tuple Tensors output lse where lse logsumexp Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA batch_size max_seq_len embed_dim num_heads = head_dim = embed_dim num_heads seq_lengths = _ range batch_size length = torch randint max_seq_len + item seq_lengths append min length max_seq_len seq_lengths = torch tensor seq_lengths device= cuda total_tokens = seq_lengths sum item Create packed query key value tensors query = torch randn total_tokens num_heads head_dim dtype=torch float device= cuda key = torch randn total_tokens num_heads head_dim dtype=torch float device= cuda value = torch randn total_tokens num_heads head_dim dtype=torch float device= cuda Build cumulative sequence tensor cu_seq = torch zeros batch_size + device= cuda dtype=torch int cu_seq = seq_lengths cumsum max_len = seq_lengths max item Call varlen_attn output = varlen_attn query key value cu_seq cu_seq max_len max_len is_causal=False out lse _ = torch ops torch_attn _varlen_attn query key value cu_seq_q cu_seq_k max_q max_k is_causal return_aux None return_aux lse out lse out _setup_context ctx Any inputs tuple Any output Any - None query key value cu_seq_q cu_seq_k max_q max_k is_causal = inputs out lse rng_state = output ctx save_for_backward query key value cu_seq_q cu_seq_k out lse rng_state ctx max_q = max_q ctx max_k = max_k ctx is_causal = is_causal torch library custom_op torch_attn _varlen_attn_backward mutates_args= _varlen_attn_backward grad_out torch Tensor query torch Tensor key torch Tensor value torch Tensor out torch Tensor lse torch Tensor cu_seq_q torch Tensor cu_seq_k torch Tensor max_q int max_k int is_causal bool rng_state torch Tensor - tuple torch Tensor torch Tensor torch Tensor unused = torch empty device=query device use_cudnn = query is_cuda _should_use_cudnn query device index use_cudnn log info Using cuDNN backend varlen_attn dq dk dv = torch ops aten _cudnn_attention_backward grad_out query key value out lse cu_seq_q cu_seq_k max_q max_k is_causal rng_state unused log info Using Flash Attention backend varlen_attn dq dk dv = torch ops aten _flash_attention_backward grad_out query key value out lse cu_seq_q cu_seq_k max_q max_k is_causal rng_state unused dq dk dv _varlen_attn_backward register_fake _varlen_attn_backward_fake grad_out torch Tensor query torch Tensor key torch Tensor value torch Tensor out torch Tensor lse torch Tensor cu_seq_q torch Tensor cu_seq_k torch Tensor max_q int max_k int is_causal bool rng_state torch Tensor - tuple torch Tensor torch Tensor torch Tensor Fake implementation meta tensor computation tracing grad_query = torch empty_like query grad_key = torch empty_like key grad_value = torch empty_like value grad_query grad_key grad_value _backward ctx Any grad_out torch Tensor grad_lse torch Tensor grad_rng torch Tensor - tuple Optional torch Tensor query key value cu_seq_q cu_seq_k out lse rng_state = ctx saved_tensors max_q = ctx max_q max_k = ctx max_k is_causal = ctx is_causal dq dk dv = torch ops torch_attn _varlen_attn_backward grad_out query key value out lse cu_seq_q cu_seq_k max_q max_k is_causal rng_state dq dk dv None None None None None None _varlen_attn register_autograd _backward setup_context=_setup_context