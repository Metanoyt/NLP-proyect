mypy allow-untyped-defs typing Optional torch fx torch _subclasses fake_tensor FakeTensor FakeTensorMode torch fx Node torch fx _compatibility compatibility torch fx experimental proxy_tensor py_sym_types snapshot_fake torch fx node map_aggregate torch utils _ordered_set OrderedSet __all__ = FakeTensorProp compatibility is_backward_compatible=False FakeTensorProp torch fx Interpreter Execute FX graph Node-by-Node record fake tensor representing metadata node Unlike ShapeProp propagation cheap -- does propagation meta tensors which do actually store data fake tensors have much more fine grained information e g they have accurate alias information can consulted looking storages Args module GraphModule The module executed mode Optional FakeTensorMode The dispatch mode used execute computation indicated each FX Node __init__ module torch fx GraphModule mode Optional FakeTensorMode = None super __init__ module mode None mode = FakeTensorMode _mode = mode mode epoch += mode reset_nt_tensor_id_counter seen_subgraphs OrderedSet str = OrderedSet run_node n Node torch fx experimental symbolic_shapes compute_unbacked_bindings rebind_unbacked n op == call_function n target torch ops higher_order invoke_subgraph n args seen_subgraphs Prevent redundant fake tensor prop invoke_subgraphs Note there also fake tensor caching entire subgraph This happens next time we call ` run_node ` same subgraph which goes through super run_node caches fake tensor prop Therefore we propagating fake tensor through subgraphs twice assert isinstance n args str assert isinstance n args torch fx Node n args op == get_attr isinstance n args target str seen_subgraphs add n args operands = n args example_inputs = operand operands assert isinstance operand torch fx Node val operand meta example_inputs append operand meta val FakeTensorProp getattr module n args target mode=self _mode propagate example_inputs result = super run_node n rebind_unbacked _mode shape_env n result extract_val obj isinstance obj FakeTensor snapshot_fake obj isinstance obj torch Tensor TODO How possible we get non fake tensor We should running under mode snapshot_fake _mode from_tensor obj static_shapes=True isinstance obj py_sym_types obj None meta = map_aggregate result extract_val meta None n meta val = meta shape_env = _mode shape_env symbol_to_path = compute_unbacked_bindings shape_env result n meta unbacked_bindings = symbol_to_path result propagate args fake_args = _mode from_tensor isinstance torch Tensor args propagate_dont_convert_inputs fake_args propagate_dont_convert_inputs args _mode super run args