mypy allow-untyped-defs collections abc Sequence typing Any Optional Union torch torch nn nn torch Tensor torch _functorch utils exposed_in exposed_in torch func functional_call module torch nn Module parameter_and_buffer_dicts Union dict str Tensor Sequence dict str Tensor args Optional Union Any tuple = None kwargs Optional dict str Any = None tie_weights bool = True strict bool = False r Performs functional call module replacing module parameters buffers provided ones note If module has active parametrizations passing value attr ` parameter_and_buffer_dicts ` argument name set regular parameter name will completely disable parametrization If you want apply parametrization function value passed please set key ` ` submodule_name parametrizations parameter_name original ` ` note If module performs in-place operations parameters buffers these will reflected ` ` parameter_and_buffer_dicts ` ` input Example = foo torch zeros xdoctest +SKIP mod = Foo does foo = foo + print mod foo tensor functional_call mod torch ones print mod foo tensor print foo tensor note If module has tied weights whether functional_call respects tying determined tie_weights flag Example = foo torch zeros xdoctest +SKIP mod = Foo has both foo foo_tied which tied Returns x + foo + foo_tied print mod foo tensor mod torch zeros tensor functional_call mod torch zeros tensor since will change foo_tied too functional_call mod torch zeros tie_weights=False tensor -- foo_tied updated new_a = foo torch zeros foo_tied torch zeros functional_call mod new_a torch zeros tensor An example passing multiple dictionaries code-block python = weight torch ones buffer torch zeros two separate dictionaries mod = nn Bar weight x + buffer print mod weight tensor print mod buffer tensor x = torch randn print x functional_call mod x same x print mod weight same before functional_call And here example applying grad transform over parameters model code-block python torch torch nn nn torch func functional_call grad x = torch randn t = torch randn model = nn Linear compute_loss params x t y = functional_call model params x nn functional mse_loss y t grad_weights = grad compute_loss dict model named_parameters x t note If user does need grad tracking outside grad transforms they can detach all parameters better performance memory usage Example detached_params = k v detach k v model named_parameters grad_weights = grad compute_loss detached_params x t grad_weights grad_fn None -- s tracking gradients outside grad This means user cannot call ` ` grad_weight backward ` ` However they don t need autograd tracking outside transforms will result less memory usage faster speeds Args module torch nn Module module call parameters_and_buffer_dicts Dict str Tensor tuple Dict str Tensor parameters will used module call If given tuple dictionaries they must have distinct keys so all dictionaries can used together args Any tuple arguments passed module call If tuple considered single argument kwargs dict keyword arguments passed module call tie_weights bool optional If True then parameters buffers tied original model will treated tied reparameterized version Therefore True different values passed tied parameters buffers will error If False will respect originally tied parameters buffers unless values passed both weights same Default True strict bool optional If True then parameters buffers passed must match parameters buffers original module Therefore True there any missing unexpected keys will error Default False Returns Any result calling ` ` module ` ` isinstance parameter_and_buffer_dicts dict parameters_and_buffers = parameter_and_buffer_dicts isinstance parameter_and_buffer_dicts Sequence all isinstance d dict d parameter_and_buffer_dicts raise ValueError Expected all elements parameter_and_buffer_dicts dictionaries all_keys = k d parameter_and_buffer_dicts k d keys all_keys_counter dict str int = k all_keys v = all_keys_counter get k all_keys_counter k = v + repeated_keys = key key n all_keys_counter items n len repeated_keys raise ValueError f repeated_keys appeared multiple dictionaries behavior functional call ambiguous parameters_and_buffers = k v d parameter_and_buffer_dicts k v d items raise ValueError f Expected parameter_and_buffer_dicts dict list tuple dicts f got type parameter_and_buffer_dicts nn utils stateless _functional_call module parameters_and_buffers args kwargs tie_weights=tie_weights strict=strict exposed_in torch func stack_module_state models Union Sequence nn Module nn ModuleList - tuple dict str Any dict str Any stack_module_state models - params buffers Prepares list torch nn Modules ensembling func ` vmap ` Given list ` ` M ` ` ` ` nn Modules ` ` same returns two dictionaries stack all their parameters buffers together indexed name The stacked parameters optimizable i e they new leaf nodes autograd history unrelated original parameters can passed directly optimizer Here s example how ensemble over very simple model code-block python num_models = batch_size = in_features out_features = models = torch nn Linear in_features out_features i range num_models data = torch randn batch_size wrapper params buffers data torch func functional_call models params buffers data params buffers = stack_module_state models output = vmap wrapper None params buffers data assert output shape == num_models batch_size out_features When there s submodules follows state dict naming conventions code-block python torch nn nn Foo nn Module __init__ in_features out_features super __init__ hidden = l = nn Linear in_features hidden l = nn Linear hidden out_features forward x l l x num_models = in_features out_features = models = Foo in_features out_features i range num_models params buffers = stack_module_state models print list params keys l weight l bias l weight l bias warning All modules being stacked together must same except values their parameters buffers For example they should same mode training vs eval len models == raise RuntimeError stack_module_state Expected least one model got all m training m models all m training m models raise RuntimeError stack_module_state Expected all models have same training eval mode model _typ = type models all type m model _typ m models raise RuntimeError stack_module_state Expected all models same all_params = dict model named_parameters model models params = k construct_stacked_leaf tuple params k params all_params k k all_params all_buffers = dict model named_buffers model models buffers = k construct_stacked_leaf tuple buffers k buffers all_buffers k k all_buffers params buffers construct_stacked_leaf tensors Union tuple Tensor list Tensor name str - Tensor all_requires_grad = all t requires_grad t tensors none_requires_grad = all t requires_grad t tensors all_requires_grad none_requires_grad raise RuntimeError f Expected name each model have same requires_grad result = torch stack tensors all_requires_grad result = result detach requires_grad_ result