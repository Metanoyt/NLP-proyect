Owner s module masked operators Tests masked operations itertools torch typing Any functools wraps unittest torch testing _internal common_utils skipIfTorchDynamo torch testing _internal common_utils \ TestCase parametrize suppress_warnings _TestParametrizer run_tests torch testing _internal common_methods_invocations \ op_db SampleInput torch testing _internal common_device_type \ instantiate_device_type_tests ops onlyNativeDeviceTypes precisionOverride apply_masked_reduction_along_dim op input args kwargs Applies reduction op along given dimension strided x elements valid according mask tensor The op applied each elementary slice input args kwargs following constraints Prior applying op A kwargs contains item key dim_position then removed kwargs The value dim_position integer describes dim argument position while typically dim argument appears -th position op arguments excluding input instance sum input dim then there exists reductions have extra arguments prior dim argument instance norm input ord dim B args kwargs contains dim keepdim arguments these will removed replaced None so op applied elementary slice using default dim keepdim value The elementary slice input defined flattened slice has no masked out elements when op applied result will scalar value assuming keepdim=False For example input tensor reduction operation op having dim= keepdim=True argument denotes masked out elements has following elementary slices The result apply_masked_reduction_along_dim op args kwargs dim=None keepdim=False op args kwargs dim=None keepdim=False where args args where dim value replaced None present Using same example data op called dim= keepdim=False there one elementary slice corresponding result op op args kwargs dim=None keepdim=False If elementary slice empty corresponding output value nan dtype float otherwise An empty elementary slice corresponds fully masked-out output so corresponding specific value output will important because we used masked equality check comparing results masked operations eliminate mask dim_position keyword arguments mask = kwargs pop mask None dim_pos = kwargs pop dim_position dtype = kwargs get dtype input dtype input ndim == scalar input elementary slice op input args kwargs dtype=dtype eliminate keepdim keyword argument specified keepdim = kwargs pop keepdim False eliminate dim argument may appear both args kwargs element dim_pos len args dim specified args assert dim kwargs args kwargs dim = args dim_pos args = args dim_pos + None + args dim_pos + dim may specified kwargs dim = kwargs pop dim None args = args dimensions along which reduction operation applied dim_ = torch masked _canonical_dim dim input ndim slices product ranges define all elementary slices ranges list Any = shape output keepdim=True case shape = i range input ndim i dim_ ranges append slice None shape append ranges append range input shape i shape append input shape i keepdim=True version output filled nan output = input new_full shape float nan dtype is_floating_point dtype=dtype apply op all elementary slices mask None inpmask = input new_ones dtype=torch bool expand input shape inpmask = torch masked _input_mask input mask=mask s itertools product ranges data elementary slice D sequence has only masked-in elements data = input s flatten inpmask s flatten argwhere data numel empty elementary slice continue output s = op data args kwargs keepdim reshape output keepdim=False case shape = shape i i range len shape i dim_ output = output reshape shape output apply_masked_normalization_along_dim op input args kwargs Applies normalization op along given dimension strided x elements valid according mask tensor mask = kwargs pop mask None dim_pos = kwargs pop dim_position input ndim == scalar input op input args kwargs dtype = kwargs get dtype input dtype dim = args dim_pos args = args dim_pos + + args dim_pos + output = torch zeros_like input dtype=dtype mask None inpmask = input new_ones dtype=torch bool expand input shape inpmask = torch masked _input_mask input mask=mask dim_ = dim input ndim left_ranges = tuple map range input shape dim_ right_ranges = tuple map range input shape dim_ + s itertools product left_ranges + slice None + right_ranges indices = inpmask s argwhere output s indices = op input s indices args kwargs output reference_functions = dict norm=lambda args kwargs apply_masked_reduction_along_dim torch linalg vector_norm args dict kwargs dim_position= var=lambda args kwargs apply_masked_reduction_along_dim torch var args dict kwargs dim_position= std=lambda args kwargs apply_masked_reduction_along_dim torch std args dict kwargs dim_position= softmax=lambda args kwargs apply_masked_normalization_along_dim torch softmax args kwargs log_softmax=lambda args kwargs apply_masked_normalization_along_dim torch log_softmax args kwargs softmin=lambda args kwargs apply_masked_normalization_along_dim torch nn functional softmin args kwargs normalize=lambda args kwargs apply_masked_normalization_along_dim torch nn functional normalize args dict kwargs dim_position= masked_ops = op op op_db op name startswith masked masked_ops_with_references = op op masked_ops op name rsplit - reference_functions masked_ops_with_non_strided_support = op op masked_ops op supports_sparse op supports_sparse_csr _tensor_to_strided obj after gh- resolved replace usage function torch Tensor to_dense torch is_tensor obj obj layout == torch strided obj obj to_dense obj to_strided obj Convert tensor content object strided tensor content torch utils _pytree tree_map _tensor_to_strided obj to_sparse_coo obj Convert tensor content object sparse coo tensor content torch utils _pytree tree_map torch Tensor to_sparse obj to_sparse_csr obj Convert tensor content object sparse csr tensor content torch utils _pytree tree_map torch Tensor to_sparse_csr obj mask_layouts _TestParametrizer Decorator parametrization test function input layout argument extra argument sample inputs generator The sample_inputs generator provides samples all supported layouts mask argument _parametrize_test test generic_cls device_cls wraps test wrap layout device dtype op layout_name = str layout lstrip torch layout == torch strided strided layouts always supported sample_inputs_func = op sample_inputs layout == torch sparse_coo op supports_sparse raise unittest SkipTest f op name does support inputs layout_name layout sample_inputs_func = op sample_inputs_sparse_coo layout == torch sparse_csr op supports_sparse_csr raise unittest SkipTest f op name does support inputs layout_name layout sample_inputs_func = op sample_inputs_sparse_csr raise NotImplementedError f layout sample_inputs_generator sample_input sample_inputs_func device dtype mask = sample_input kwargs get mask mask None yield sample_input layout == sample_input input layout yield sample_input layout = torch strided sample_input_kwargs = sample_input kwargs copy sample_input_kwargs update mask=mask to_dense yield SampleInput sample_input input clone args=sample_input args kwargs=sample_input_kwargs layout = torch sparse_coo op supports_sparse sample_input_kwargs = sample_input kwargs copy sample_input_kwargs update mask=mask to_sparse yield SampleInput sample_input input clone args=sample_input args kwargs=sample_input_kwargs layout = torch sparse_csr op supports_sparse_csr sample_input input ndim == sample_input_kwargs = sample_input kwargs copy sample_input_kwargs update mask=mask to_sparse_csr yield SampleInput sample_input input clone args=sample_input args kwargs=sample_input_kwargs test layout device dtype op sample_inputs_generator layout torch strided torch sparse_coo torch sparse_csr yield wrap str layout lstrip torch layout layout lambda _ TestMasked TestCase assertEqualMasked actual expected mask strided = to_strided actual mask None strided = torch where mask strided strided new_zeros expected = torch where mask expected expected new_zeros assertEqual strided expected exact_device=False onlyNativeDeviceTypes suppress_warnings ops masked_ops_with_references precisionOverride torch bfloat e- torch float e- test_reference_masked device dtype op op_name = op name rsplit - ref_op = reference_functions op_name sample_inputs = op sample_inputs device dtype sample_input sample_inputs t_inp t_args t_kwargs = sample_input input sample_input args sample_input kwargs op_name var std t_inp dtype is_floating_point t_inp dtype is_complex torch var torch std does support integer inputs continue actual = op op t_inp t_args t_kwargs expected = ref_op t_inp t_args t_kwargs t_kwargs get mask None outmask = None outmask = torch masked _output_mask op op t_inp t_args t_kwargs assertEqualMasked actual expected outmask mask_layouts onlyNativeDeviceTypes suppress_warnings ops masked_ops_with_non_strided_support precisionOverride torch bfloat e- torch float e- test_mask_layout layout device dtype op sample_inputs sample sample_inputs t_inp t_args t_kwargs = sample input sample args sample kwargs actual = op op t_inp t_args t_kwargs assert actual layout == layout check masked invariance op inp mask to_dense == op inp to_dense mask to_dense outmask r_inp r_args r_kwargs = to_strided t_inp t_args t_kwargs r_kwargs get mask None outmask = None outmask = torch masked _output_mask op op r_inp r_args r_kwargs expected = op op r_inp r_args r_kwargs assertEqualMasked actual expected outmask skipIfTorchDynamo https github com pytorch torchdynamo issues parametrize sparse_kind fill_value coo hybrid_coo coo hybrid_coo csr csr name_fn=lambda sparse_kind fill_value f sparse_kind _fill_value_ fill_value test_where sparse_kind fill_value is_hybrid = False sparse_kind == coo to_sparse dense dense to_sparse set_values sparse index value sparse _values index = value sparse_kind == hybrid_coo is_hybrid = True to_sparse dense dense to_sparse set_values sparse index value sparse _values index = value sparse_kind == csr to_sparse dense dense to_sparse_csr set_values sparse index value sparse values index = value assert sparse_kind mask = torch tensor dtype=bool mask = to_sparse mask make some specified mask elements explicit masked-out masks is_hybrid set_values mask False set_values mask - - False set_values mask False set_values mask - False input = torch tensor - - - - - input = to_sparse input make specified input elements have zero values is_hybrid set_values input set_values input - F = fill_value set_values input set_values input - F = expected where result Z = Z value corresponds masked-in elements specified input will replaced zero tmp = torch tensor F Z F F F Z Z F F F Z F F F F F Z F F F tmp = to_sparse tmp sparse = torch masked _where mask input torch tensor fill_value dtype=input dtype device=input device tmp layout == torch sparse_coo expected_sparse = torch sparse_coo_tensor tmp indices torch where tmp values = Z tmp values tmp values new_full input shape outmask = torch sparse_coo_tensor sparse indices sparse values new_full sparse values shape dtype=bool sparse shape _coalesced_ True tmp layout == torch sparse_csr expected_sparse = torch sparse_csr_tensor tmp crow_indices tmp col_indices torch where tmp values = Z tmp values tmp values new_full input shape outmask = torch sparse_csr_tensor sparse crow_indices sparse col_indices sparse values new_full sparse values shape dtype=bool sparse shape assert assertEqual sparse expected_sparse check invariance torch where mask to_dense input to_dense fill_value == where mask input fill_value to_dense fill_value expected = torch where mask to_dense input to_dense torch full input shape F dense = torch where outmask to_dense sparse to_dense torch full sparse shape F assertEqual dense expected instantiate_device_type_tests TestMasked globals except_for= meta __name__ == __main__ run_tests