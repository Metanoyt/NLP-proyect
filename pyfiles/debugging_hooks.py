typing Any torch torch distributed GradBucket __all__ = noop_hook noop_hook _ Any bucket GradBucket - torch futures Future torch Tensor Return future wraps input so no-op does incur any communication overheads This hook should only used headroom analysis allreduce optimization instead normal gradient synchronization For example only less than speedup training time can observed after hook registered usually implies allreduce performance bottleneck case Such instrumentation can particularly useful GPU traces cannot easily retrieved trace analysis complicated some factors such overlap between allreduce computation desynchronization across ranks Example xdoctest +SKIP ddp_model register_comm_hook None noop_hook fut torch futures Future torch Tensor = torch futures Future fut set_result bucket buffer fut