mypy allow-untyped-defs functools collections abc Sequence typing Any Callable Optional Protocol TYPE_CHECKING TypeVar Union sympy torch torch _prims_common ELEMENTWISE_TYPE_PROMOTION_KIND type_to_dtype torch utils _ordered_set OrderedSet ops_handler OP_NAMES OpsHandler utils upcast_compute_type virtualized OpsValue V T = TypeVar T DTypeVar Protocol property dtype - torch dtype DTypeArg = Union DTypeVar torch types Number str OpsValue Inputs need cacheable e g CSEVar order cache effective So first decompose CSEVars - tuple before calling functools cache get_promoted_dtype args Sequence tuple torch dtype bool type_promotion_kind Optional ELEMENTWISE_TYPE_PROMOTION_KIND = None construct_input inp inp torch empty dtype=inp torch empty dtype=inp inps = construct_input arg arg args _ dtype = torch _prims_common elementwise_dtypes inps type_promotion_kind= type_promotion_kind type_promotion_kind ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT dtype promote_types args Sequence DTypeArg type_promotion_kind Optional ELEMENTWISE_TYPE_PROMOTION_KIND = None dtype_prop_candidates = pyrefly ignore bad-assignment arg args assert isinstance arg str isinstance arg OpsValue arg = arg value assert isinstance arg torch _prims_common Number hasattr arg dtype isinstance arg torch _prims_common Number dtype_prop_candidates append type_to_dtype type arg True continue pyrefly ignore missing-attribute dtype_prop_candidates append arg dtype getattr arg is_scalar False dtype = get_promoted_dtype dtype_prop_candidates type_promotion_kind=type_promotion_kind dtype DtypePropagationOpsHandler Propagate dtype args output Singleton DtypePropagationOpsHandler because we meta program over number op rules Those only defined after other inductor state has run _instance Optional DtypePropagationOpsHandler = None __new__ cls cls _instance None cls _instance = super __new__ cls cls _instance __init__ - None op rule torch _inductor utils op_dtype_propagation_rules items fn = functools partial return_dtype dtype=rule override_return_dtype rule override_return_dtype functools partial op_dtype_rule type_promotion_kind=rule type_promotion_kind setattr op fn Set pointwise operation rules op torch _inductor codegen common pointwise_overrides_data values hasattr op name setattr op name functools partial op_dtype_rule type_promotion_kind=op type_promotion_kind Set boolean operation rules op torch _inductor utils boolean_ops hasattr op setattr op functools partial return_dtype dtype=torch bool unimplemented_ops = OP_NAMES - OrderedSet dir torch _check len unimplemented_ops == lambda f Unimplemented dtype rule ops unimplemented_ops metaprogrammed __init__ staticmethod op_dtype_rule args DTypeArg type_promotion_kind ELEMENTWISE_TYPE_PROMOTION_KIND - torch dtype promote_types args type_promotion_kind=type_promotion_kind staticmethod return_dtype args DTypeArg dtype torch dtype - torch dtype dtype op rules staticmethod constant value torch types Number dtype torch dtype - torch dtype upcast_compute_type dtype staticmethod load_seed name str offset int - torch dtype upcast_compute_type V graph get_dtype name staticmethod randint seed int offset int low int high int - torch dtype torch int staticmethod masked mask DTypeArg body Callable DTypeArg other DTypeArg - torch dtype loop_body LoopBodyBlock assert isinstance body LoopBodyBlock body must LoopBodyBlock TODO - we avoid calling codegen needs work non codegen use cases loads = body graph find_nodes op= call_method target= load len loads = promote_types other upcast_compute_type V graph get_dtype loads - args staticmethod where DTypeArg b DTypeArg c DTypeArg - torch dtype promote_types b c staticmethod index_expr expr sympy Expr dtype torch dtype - torch dtype TODO - TODO - rationalize index_expr The dtype always used we inconsistent about int int lowerings cpp just uses dtype dtype torch int torch int hasattr V kernel index_dtype upcast_compute_type dtype V kernel get_index_dtype_as_torch_dtype staticmethod to_dtype x DTypeArg dtype torch dtype src_dtype Optional torch dtype = None use_compute_types=True - torch dtype upcast_compute_type dtype use_compute_types dtype staticmethod to_dtype_bitcast x DTypeArg dtype torch dtype src_dtype torch dtype - torch dtype upcast_compute_type dtype staticmethod gelu x DTypeArg - torch dtype promote_types x staticmethod mul DTypeArg b DTypeArg - torch dtype promote_types b staticmethod truediv DTypeArg b DTypeArg - torch dtype promote_types b staticmethod pow DTypeArg b DTypeArg - torch dtype promote_types b staticmethod mod DTypeArg b DTypeArg - torch dtype promote_types b staticmethod indirect_indexing x DTypeArg size int check bool = True wrap_neg bool = True - torch dtype torch int staticmethod randn seed int offset int - torch dtype torch float staticmethod rand seed int offset int - torch dtype torch float staticmethod store_reduction name str index value DTypeArg - None None staticmethod reduction dtype torch dtype src_dtype torch dtype reduction_type str value DTypeArg - torch dtype dtype staticmethod store name str index value DTypeArg mode Optional str = None - None None staticmethod partial_accumulate name str reduction_type str value DTypeArg extra_meta dict str Any - None None staticmethod load name str index - torch dtype upcast_compute_type V graph get_dtype name staticmethod floor x DTypeArg - torch dtype promote_types x type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT staticmethod ceil_to_int x DTypeArg dtype torch dtype - torch dtype dtype staticmethod int_truediv x DTypeArg y DTypeArg - torch dtype promote_types x y type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT staticmethod scan dtypes tuple torch dtype combine_fn Callable tuple T tuple T tuple T values tuple T - tuple torch dtype dtypes staticmethod fmod x DTypeArg y DTypeArg - torch dtype promote_types x y staticmethod round_to_int x DTypeArg dtype torch dtype - torch dtype dtype staticmethod identity x DTypeArg - torch dtype promote_types x staticmethod frexp x DTypeArg - tuple torch dtype torch dtype TODO - need handle multiple outputs promote_types x torch int staticmethod sort dtypes tuple torch dtype values tuple T stable bool descending bool - tuple torch dtype dtypes staticmethod trunc x DTypeArg - torch dtype promote_types x staticmethod bucketize values DTypeArg boundaries tuple str sympy Expr sympy Expr sympy Expr boundary_indices DTypeArg indexing_dtype torch dtype right bool sorter Optional tuple str sympy Expr = None sorter_indices Optional T = None - torch dtype indexing_dtype staticmethod rshift x DTypeArg y DTypeArg - torch dtype promote_types x staticmethod round x DTypeArg - torch dtype promote_types x type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT staticmethod trunc_to_int x DTypeArg dtype torch dtype - torch dtype dtype staticmethod floor_to_int x DTypeArg dtype torch dtype - torch dtype dtype staticmethod truncdiv x DTypeArg y DTypeArg - torch dtype promote_types x y staticmethod floordiv x DTypeArg y DTypeArg - torch dtype promote_types x y staticmethod halide_clamp value size check TODO - way registering dtype op backend torch int staticmethod dot x DTypeArg y DTypeArg - torch dtype triton tl dot out_dtype tl float default torch float staticmethod inline_asm_elementwise inputs asm constraints=None dtype=torch float is_pure=True pack= dtype staticmethod lshift x DTypeArg y DTypeArg - torch dtype promote_types x staticmethod check_bounds expr sympy Expr size sympy Expr lower bool upper bool - None None output args DTypeArg - None raise AssertionError f type __name__ ops output should appear here placeholder index int - torch dtype raise AssertionError f type __name__ ops placeholder should appear here staticmethod device_assert_async cond msg str - None None TYPE_CHECKING _typecheck_DtypePropagation DtypePropagationOpsHandler OpsHandler Any pass mypy will error we got any signatures wrong