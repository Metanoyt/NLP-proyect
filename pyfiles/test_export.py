Owner s oncall export ruff noqa F flake noqa contextlib copy dataclasses enum functools logging math operator os re traceback unittest warnings weakref contextlib contextmanager nullcontext dataclasses dataclass re escape typing Dict List Union unittest mock MagicMock patch torch torch _dynamo torchdynamo torch fx traceback fx_traceback torch nn functional F torch utils _pytree pytree functorch experimental control_flow cond map torch Tensor torch _decomp decomposition_table get_decompositions torch _dynamo _trace_wrapped_higher_order_op mod_index torch _dynamo test_case TestCase torch _dynamo testing normalize_gm torch _export config torch _export pass_base _ExportPassBaseDeprecatedDoNotUse torch _export utils get_buffer get_param is_buffer is_param register_dataclass_as_pytree_node torch _functorch aot_autograd aot_export_joint_with_descriptors torch _higher_order_ops associative_scan associative_scan torch _higher_order_ops hints_wrap hints_wrapper torch _higher_order_ops scan scan torch _higher_order_ops while_loop while_loop torch _inductor compile_fx split_const_gm torch _subclasses FakeTensorMode torch export default_decompositions Dim export unflatten torch export _trace _export _export_to_torch_ir DEFAULT_EXPORT_DYNAMO_CONFIG torch export graph_signature ExportGraphSignature InputKind OutputKind OutputSpec TensorArgument torch export passes move_to_device_pass torch fx experimental proxy_tensor make_fx torch fx experimental symbolic_shapes ShapeEnv torch testing FileCheck torch testing _internal common_cuda PLATFORM_SUPPORTS_FLASH_ATTENTION xfailIfDistributedNotSupported torch testing _internal common_utils find_library_location IS_FBCODE IS_MACOS IS_SANDCASTLE IS_WINDOWS run_tests skipIfCrossRef skipIfXpu TEST_TRANSFORMERS TEST_WITH_CROSSREF TestCase TorchTestCase torch testing _internal custom_tensor ConstantExtraMetadataTensor CustomTensorPlainOut torch testing _internal inductor_utils GPU_TYPE HAS_GPU torch testing _internal torchbind_impls load_torchbind_test_lib torch testing _internal triton_utils requires_cuda_and_triton requires_gpu torch testing _internal two_tensor TwoTensor torch utils _pytree register_constant tree_flatten tree_map tree_unflatten TreeSpec treespec_dumps treespec_leaf treespec_loads HAS_GPU triton triton language tl torch _library capture_triton try torchrec sparse jagged_tensor JaggedTensor KeyedJaggedTensor HAS_TORCHREC = True except ImportError HAS_TORCHREC = False try testing except ImportError testing manual=fbcode caffe test test_export-library The following pattern matters ` test_export export ` patched other files like test_export_nonstrict py ` torch export export ` will invalidate patch torch export export torch library define testlib returns_tensor_symint Tensor x - Tensor SymInt torch library define testlib foo Tensor x Tensor b z - Tensor Tensor Tensor tags=torch Tag pt _compliant_tag torch library define testlib foo_mutated Tensor x - Tensor Tensor tags=torch Tag pt _compliant_tag torch library define testlib foo_functional Tensor x - Tensor tags=torch Tag pt _compliant_tag torch library define testlib foo_unbacked Scalar x - Tensor tags=torch Tag pt _compliant_tag torch library impl testlib returns_tensor_symint cpu torch library register_fake testlib returns_tensor_symint returns_tensor_symint_impl x x x shape torch library impl testlib foo cpu torch _dynamo disable foo_impl x z x add_ z add_ x z x + z torch library register_fake testlib foo foo_abstract x z x z x + z torch library impl testlib foo_mutated CompositeImplicitAutograd foo_mutated x b c = torch ops testlib foo x x cos cos torch library impl testlib foo_functional CompositeImplicitAutograd foo_functional x b c = torch ops testlib foo x cos x cos cos torch library impl testlib foo_unbacked CompositeImplicitAutograd foo_unbacked x x torch ones x torch ones torch ones dataclass Inp x Tensor y List Tensor z Dict str Tensor dataclass Inp Tensor b Tensor dataclass Inp f torch Tensor p torch Tensor NON_STRICT_SUFFIX = _nonstrict STRICT_SUFFIX = _strict INLINE_AND_INSTALL_STRICT_SUFFIX = _inline_and_install_strict RETRACEABILITY_STRICT_SUFFIX = _retraceability_strict RETRACEABILITY_NON_STRICT_SUFFIX = _retraceability_nonstrict SERDES_SUFFIX = serdes SERDES_STRICT_SUFFIX = _serdes_strict SERDES_NON_STRICT_SUFFIX = _serdes_nonstrict PREDISPATCH_SUFFIX = _pre_dispatch TRAINING_IR_DECOMP_STRICT_SUFFIX = _training_ir_to_decomp_strict TRAINING_IR_DECOMP_NON_STRICT_SUFFIX = _training_ir_to_decomp_nonstrict CPP_RUNTIME_STRICT_SUFFIX = _cpp_runtime_strict CPP_RUNTIME_NONSTRICT_SUFFIX = _cpp_runtime_nonstrict STRICT_EXPORT_V _SUFFIX = _strict_export_v Now default mode non strict so original unammended test names should treated non-strict is_non_strict_test test_name test_name endswith STRICT_SUFFIX test_name endswith STRICT_EXPORT_V _SUFFIX is_strict_test test_name test_name endswith STRICT_SUFFIX is_strict_v _test test_name test_name endswith STRICT_EXPORT_V _SUFFIX is_inline_and_install_strict_test test_name str - bool test_name endswith INLINE_AND_INSTALL_STRICT_SUFFIX is_retracebility_test test_name test_name endswith RETRACEABILITY_STRICT_SUFFIX test_name endswith RETRACEABILITY_NON_STRICT_SUFFIX is_serdes_test test_name test_name endswith SERDES_STRICT_SUFFIX test_name endswith SERDES_NON_STRICT_SUFFIX need_serdes_test test_name SERDES_SUFFIX test_name is_training_ir_test test_name test_name endswith TRAINING_IR_DECOMP_STRICT_SUFFIX test_name endswith TRAINING_IR_DECOMP_NON_STRICT_SUFFIX is_training_ir_strict_test test_name test_name endswith TRAINING_IR_DECOMP_STRICT_SUFFIX is_cpp_runtime_test test_name test_name endswith CPP_RUNTIME_STRICT_SUFFIX test_name endswith CPP_RUNTIME_NONSTRICT_SUFFIX get_hop_schema ep torch export ExportedProgram hop_node = next node node ep graph nodes isinstance node target torch _ops HigherOrderOperator torch _library utils hop_schema_from_fx_node hop_node unittest skipIf torchdynamo is_dynamo_supported dynamo isn t support TestDynamismExpression TestCase test_export_inline_constraints Module torch nn Module forward x b = x item torch full b f = Module inp = torch tensor ref = f inp gm = export f inp res = gm module inp assertTrue torchdynamo utils same ref res gm = make_fx f tracing_mode= symbolic inp res = gm inp assertTrue torchdynamo utils same ref res test_export_constraints_error_not_in_range InvalidInputConflictWithInputConstraints torch nn Module forward x x + inp = torch zeros dim_x = torch export Dim dim_x min= is_non_strict_test _testMethodName error_type = torch fx experimental symbolic_shapes ConstraintViolationError error_type = torch _dynamo exc UserError assertRaisesRegex error_type range export InvalidInputConflictWithInputConstraints inp dynamic_shapes= x dim_x test_export_slice_maxsize Slice torch nn Module forward args torch ops aten slice Tensor args inp = torch rand dynamic_shapes = Dim dim None None None torch export export Slice inp dynamic_shapes=dynamic_shapes test_no_grad_param_inplace Foo torch nn Module __init__ super __init__ parameter = torch nn Parameter torch ones forward x torch no_grad parameter div_ x + parameter foo_ep = Foo foo_eager = Foo ep = export foo_ep torch rand run_decompositions val = ep graph_signature parameters_to_mutate assertExpectedInline str ep graph strip \ graph p_parameter num_users= = placeholder target=p_parameter x num_users= = placeholder target=x div num_users= = call_function target=torch ops aten div Tensor args = p_parameter kwargs = add num_users= = call_function target=torch ops aten add Tensor args = x div kwargs = div add assertTrue div val keys assertTrue parameter val values test_inp = torch rand res = foo_eager test_inp TODO We almost need make param mutation happen outside graph Or wrap param mutation no_grad HOP Simply overriding gm __call__ doesn t seem work due graph module does something weird __call__ so easy override We inspect module forward bind fake args when retracing assertRaisesRegex RuntimeError leaf res_export = ep module torch rand torch no_grad res_export = ep module test_inp assertTrue torch allclose res res_export test_export_slice_unbacked_dim MySlice torch nn Module forward x seq_len l = seq_len item x = x narrow l x x = torch randn seq_len = torch tensor torch export export MySlice args= x seq_len torch fx experimental _config patch backed_size_oblivious=True test_reshape_view_backed_size_oblivious N = MyModel torch nn Module forward x y = x - s - stacked = torch stack y N dim= N s - reshaped = stacked reshape - N s - N reshaped inps = torch randn spec = x Dim AUTO Dim STATIC ep = export MyModel inps dynamic_shapes=spec test_export_constraints_error ConflictingConstraints torch nn Module forward x b = x item torch _check b = torch _check b = torch _check b = torch _check True torch full b inp = torch tensor ep = export ConflictingConstraints inp assertRaisesRegex RuntimeError r Runtime assertion failed expression u \d+ \ \= ep module torch tensor test_export_assume_static_by_default Module torch nn Module forward x torch Tensor x shape == x + x branch_on_shape = Module inp = torch rand Being able export means shape preserved static export branch_on_shape inp test_export_strict_narrow_unbacked_expr Tests we able handle specialization sizes represented unbacked int expressions transforming them into unbacked int This test only works strict=True since relies dynamo tracing transforming expression into unbacked SymInt identity x x Module torch nn Module __init__ fn super __init__ fn = fn forward x p u = p item torch _check u + = x shape torch _check u = Create tensor size x shape - u - x narrow u + fn x shape - u - inputs = torch arange torch tensor See https github com pytorch pytorch issues Without transforming unbacked int expression we can t export assertRaisesRegex RuntimeError escape Could guard data-dependent expression export Module identity inputs strict=True It works we transform whole unbacked int expression into unbacked int export Module torch sym_fresh_size inputs strict=True InputModule torch nn Module __init__ super __init__ linear = torch nn Linear forward x y linear x y InputModuleWithNestedSubclass torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter CustomTensorPlainOut CustomTensorPlainOut torch Tensor torch Tensor CustomTensorPlainOut torch Tensor torch Tensor forward x = x + p + p sum sum x + unittest skipIf IS_WINDOWS Windows isn t supported case unittest skipIf torchdynamo is_dynamo_supported dynamo isn t support TestExport TestCase _test_export_same_as_eager f args kwargs=None kwargs = kwargs exported_program = export f args kwargs assertEqual exported_program module args kwargs f args kwargs supported module reversed_kwargs = key kwargs key key reversed kwargs assertEqual exported_program module args reversed_kwargs f args reversed_kwargs _check_dynamic_shapes_specs_and_shapes model inputs specs passing_shapes failing_shapes test_serdes=False torch _export serde dynamic_shapes _dump_dynamic_shapes _load_dynamic_shapes torch utils _pytree tree_map _construct_inputs shapes _is_tensor_leaf x isinstance x tuple all isinstance y int y x tree_map lambda x torch randn x _is_tensor_leaf x x shapes is_leaf=_is_tensor_leaf exports list equivalent dynamic shapes specs then tests pass fail list shapes _specs specs ep = export model inputs dynamic_shapes=_specs eps = ep test_serdes test dynamic shapes serialization test behavior remains same when exporting Ser Des specs serialize + deserialize original specs export ep_serdes = export model inputs dynamic_shapes=_load_dynamic_shapes _dump_dynamic_shapes _specs inputs eps append ep_serdes ep eps shapes passing_shapes test_inputs = _construct_inputs shapes ep module test_inputs shapes failing_shapes test_inputs = _construct_inputs shapes assertRaisesRegex AssertionError Guard failed ep module test_inputs test_basic Module torch nn Module forward x y x + y f = Module inp = torch ones torch ones _test_export_same_as_eager f inp skipIfCrossRef test_custom_tag_metadata_re_export Foo torch nn Module __init__ super __init__ w = torch nn Parameter torch rand b = torch nn Parameter torch rand forward x out = torch nn functional linear x w b out f = Foo inputs = torch zeros ep = export f inputs new_gm = copy deepcopy ep graph_module new_gm meta custom = new_gm meta custom f = bar node new_gm graph nodes node op == call_function node target == torch ops aten linear default node meta custom = node meta custom quantization_tag = foo new_ep = ep _update new_gm ep graph_signature new_ep = export new_ep module inputs assertEqual new_ep graph_module meta custom f bar custom field should preserved after re-export should copied other nodes counter = node new_ep graph nodes custom node meta counter += assertTrue node meta custom quantization_tag == foo assertTrue node target == torch ops aten linear default assertEqual counter testing expectedFailureSerDer can t serialize functorch ops testing expectedFailureSerDerNonStrict can t serialize functorch ops test_vmap_to_assert VmapToAssert torch nn Module forward x y f = lambda x y x y cpu memory_format=torch channels_last + sum dim= noqa E vmapped = torch vmap f x y vmapped sum dim= ep = export VmapToAssert torch zeros torch zeros exported = ep module torch ones torch ones eager = VmapToAssert torch ones torch ones assertEqual exported eager test_from_node_metadata_export Foo torch nn Module __init__ - None super __init__ conv d = torch nn Conv d conv d = torch nn Conv d forward x x = conv d x x = x squeeze x = conv d x x example_inputs f = Foo inputs = torch randn ep = export f inputs graph_id = id ep graph gm = ep module torch fx traceback NodeSourceAction node gm graph nodes node op placeholder output call_module continue weight node name bias node name assertTrue node meta from_node - pass_name == ExportedProgram module unlift assertTrue node meta from_node - action == NodeSourceAction CREATE NodeSourceAction REPLACE assertEqual node meta from_node - from_node - graph_id graph_id assertTrue node meta from_node - pass_name == ExportedProgram module assertTrue node meta from_node - action == NodeSourceAction CREATE assertEqual node meta from_node - graph_id graph_id ## re-export ep = export gm inputs gm = ep module graph_id = id ep graph node gm graph nodes node op placeholder output call_module continue weight node name bias node name assertTrue node meta from_node - pass_name == ExportedProgram module unlift assertTrue node meta from_node - action == NodeSourceAction CREATE NodeSourceAction REPLACE assertEqual node meta from_node - from_node - graph_id graph_id assertTrue node meta from_node - pass_name == ExportedProgram module assertTrue node meta from_node - action == NodeSourceAction CREATE assertEqual node meta from_node - graph_id graph_id requires_gpu test_flex_attention_export torch nn attention flex_attention create_block_mask flex_attention MixedFakeModeModel torch nn Module __init__ dim= use_inductor=True super __init__ dim = dim q_proj = torch nn Linear k_proj = torch nn Linear v_proj = torch nn Linear use_inductor = use_inductor forward x batch_size seq_len _ = x shape Process input first - creates fake tensors export s fake mode processed = q_proj x Create some computation depends processed tensor intermediate = processed sum dim=- detach Shape batch seq_len Now call create_block_mask which internally calls torch compile The mask function will capture intermediate which fake tensor export s fake mode create_block_mask will create its own fake mode dynamic_mask_function batch_idx head_idx q_idx kv_idx This captures intermediate tensor outer scope When torch compile called inside create_block_mask tensor will export s fake mode while new tensors created inside will nested fake mode threshold = intermediate batch_idx q_idx seq_len Access captured tensor kv_idx = q_idx threshold Mix fake modes block_mask = create_block_mask mask_mod=dynamic_mask_function B=batch_size H=None Q_LEN=seq_len KV_LEN=seq_len device=x device q = q_proj processed view batch_size seq_len dim k = k_proj processed view batch_size seq_len dim v = v_proj processed view batch_size seq_len dim Use flex_attention problematic block_mask backend = inductor use_inductor eager out = torch compile flex_attention backend=backend q k v block_mask=block_mask out model = MixedFakeModeModel use_inductor=False x = torch randn Inductor doesn t work eager mode flex attention eager_out = model x model use_inductor = True exported_mod = torch export export model x strict=False module assertExpectedInline str exported_mod code strip \ forward x x = fx_pytree tree_flatten_spec x _in_spec q_proj_weight = q_proj weight q_proj_bias = q_proj bias k_proj_weight = k_proj weight k_proj_bias = k_proj bias v_proj_weight = v_proj weight v_proj_bias = v_proj bias _guards_fn = _guards_fn x _guards_fn = None linear = torch ops aten linear default x q_proj_weight q_proj_bias x = None sum_ = torch ops aten sum dim_IntList linear - detach = torch ops aten detach default sum_ sum_ = None arange = torch ops aten arange start device = device type= cpu pin_memory = False arange_ = torch ops aten arange start device = device type= cpu pin_memory = False arange_ = torch ops aten arange start device = device type= cpu pin_memory = False arange_ = torch ops aten arange start device = device type= cpu pin_memory = False lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim = torch _functorch predispatch _add_batch_dim arange arange = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ = torch _functorch predispatch _add_batch_dim arange_ arange_ = _add_batch_dim_ = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ = torch _functorch predispatch _add_batch_dim arange_ arange_ = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ = torch _functorch predispatch _add_batch_dim arange_ arange_ = None remainder = torch ops aten remainder Scalar _add_batch_dim_ torch__dynamo__trace_wrapped_higher_order_op_mod_index = torch__dynamo__trace_wrapped_higher_order_op_ModIndex function_const_func_spec = function_const_func_spec flat_apply = torch ops higher_order flat_apply function_const_func_spec torch__dynamo__trace_wrapped_higher_order_op_mod_index torch _dynamo _trace_wrapped_higher_order_op ModIndex detach _add_batch_dim remainder function_const_func_spec = torch__dynamo__trace_wrapped_higher_order_op_mod_index = _add_batch_dim = remainder = None le = torch ops aten le Tensor _add_batch_dim_ _add_batch_dim_ _add_batch_dim_ = _add_batch_dim_ = None gt = torch ops aten gt Scalar flat_apply flat_apply = None and_ = torch ops aten __and__ Tensor le gt le = gt = None _remove_batch_dim = torch _functorch predispatch _remove_batch_dim and_ and_ = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim_ = torch _functorch predispatch _remove_batch_dim _remove_batch_dim _remove_batch_dim = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None _remove_batch_dim_ = torch _functorch predispatch _remove_batch_dim _remove_batch_dim_ expand = torch ops aten expand default _remove_batch_dim_ _remove_batch_dim_ = expand = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None _remove_batch_dim_ = torch _functorch predispatch _remove_batch_dim _remove_batch_dim_ _remove_batch_dim_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None pad = torch ops aten pad default _remove_batch_dim_ _remove_batch_dim_ = None view = torch ops aten view default pad pad = None permute = torch ops aten permute default view view = None sum_ = torch ops aten sum dim_IntList permute - - permute = None eq = torch ops aten eq Scalar sum_ gt_ = torch ops aten gt Scalar sum_ lt = torch ops aten lt Scalar sum_ sum_ = None and_ = torch ops aten __and__ Tensor gt_ lt gt_ = lt = None _assert_tensor_metadata_default = torch ops aten _assert_tensor_metadata default and_ dtype = torch bool device = device type= cpu layout = torch strided _assert_tensor_metadata_default = None = torch ops aten dtype and_ torch int and_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default eq dtype = torch bool device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype eq torch int eq = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype torch int = None sum_ = torch ops aten sum dim_IntList to_ - argsort = torch ops aten argsort stable to_ stable = True descending = True to_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default sum_ dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype sum_ torch int False False torch contiguous_format sum_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default argsort dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype argsort torch int False False torch contiguous_format argsort = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default to_ dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype to_ torch int to_ = None sum_ = torch ops aten sum dim_IntList to_ - argsort_ = torch ops aten argsort stable to_ stable = True descending = True to_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default sum_ dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype sum_ torch int False False torch contiguous_format sum_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default argsort_ dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype argsort_ torch int False False torch contiguous_format argsort_ = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ = torch _functorch predispatch _add_batch_dim to_ _add_batch_dim_ = torch _functorch predispatch _add_batch_dim to_ lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ = torch _functorch predispatch _add_batch_dim _add_batch_dim_ _add_batch_dim_ = None _add_batch_dim_ = torch _functorch predispatch _add_batch_dim _add_batch_dim_ _add_batch_dim_ = None new_zeros = torch ops aten new_zeros default _add_batch_dim_ dtype = torch int pin_memory = False arange_ = torch ops aten arange default dtype = torch int device = device type= cpu pin_memory = False unsqueeze = torch ops aten unsqueeze default arange_ - arange_ = None arange_ = torch ops aten arange default dtype = torch int device = device type= cpu pin_memory = False unsqueeze_ = torch ops aten unsqueeze default _add_batch_dim_ - _add_batch_dim_ = None lt_ = torch ops aten lt Tensor arange_ unsqueeze_ arange_ = unsqueeze_ = None where = torch ops aten where ScalarOther lt_ _add_batch_dim_ lt_ = _add_batch_dim_ = None new_ones = torch ops aten new_ones default new_zeros pin_memory = False index_put_ = torch ops aten index_put_ default new_zeros unsqueeze where new_ones new_zeros = unsqueeze = where = new_ones = None slice_ = torch ops aten slice Tensor index_put_ index_put_ = None _remove_batch_dim_ = torch _functorch predispatch _remove_batch_dim slice_ slice_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None _remove_batch_dim_ = torch _functorch predispatch _remove_batch_dim _remove_batch_dim_ _remove_batch_dim_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None transpose = torch ops aten transpose int _remove_batch_dim_ - - _remove_batch_dim_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default transpose dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype transpose torch int transpose = None sum_ = torch ops aten sum dim_IntList to_ - argsort_ = torch ops aten argsort stable to_ stable = True descending = True to_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default sum_ dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype sum_ torch int False False torch contiguous_format sum_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default argsort_ dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype argsort_ torch int False False torch contiguous_format argsort_ = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ = torch _functorch predispatch _add_batch_dim to_ _add_batch_dim_ = torch _functorch predispatch _add_batch_dim to_ lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ = torch _functorch predispatch _add_batch_dim _add_batch_dim_ _add_batch_dim_ = None _add_batch_dim_ = torch _functorch predispatch _add_batch_dim _add_batch_dim_ _add_batch_dim_ = None new_zeros_ = torch ops aten new_zeros default _add_batch_dim_ dtype = torch int pin_memory = False arange_ = torch ops aten arange default dtype = torch int device = device type= cpu pin_memory = False unsqueeze_ = torch ops aten unsqueeze default arange_ - arange_ = None arange_ = torch ops aten arange default dtype = torch int device = device type= cpu pin_memory = False unsqueeze_ = torch ops aten unsqueeze default _add_batch_dim_ - _add_batch_dim_ = None lt_ = torch ops aten lt Tensor arange_ unsqueeze_ arange_ = unsqueeze_ = None where_ = torch ops aten where ScalarOther lt_ _add_batch_dim_ lt_ = _add_batch_dim_ = None new_ones_ = torch ops aten new_ones default new_zeros_ pin_memory = False index_put__ = torch ops aten index_put_ default new_zeros_ unsqueeze_ where_ new_ones_ new_zeros_ = unsqueeze_ = where_ = new_ones_ = None slice_ = torch ops aten slice Tensor index_put__ index_put__ = None _remove_batch_dim_ = torch _functorch predispatch _remove_batch_dim slice_ slice_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None _remove_batch_dim_ = torch _functorch predispatch _remove_batch_dim _remove_batch_dim_ _remove_batch_dim_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None transpose_ = torch ops aten transpose int _remove_batch_dim_ - - _remove_batch_dim_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default transpose_ dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype transpose_ torch int transpose_ = None sum_ = torch ops aten sum dim_IntList to_ - argsort_ = torch ops aten argsort stable to_ stable = True descending = True to_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default sum_ dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype sum_ torch int False False torch contiguous_format sum_ = None _assert_tensor_metadata_default_ = torch ops aten _assert_tensor_metadata default argsort_ dtype = torch int device = device type= cpu layout = torch strided _assert_tensor_metadata_default_ = None to_ = torch ops aten dtype argsort_ torch int False False torch contiguous_format argsort_ = None linear_ = torch ops aten linear default linear q_proj_weight q_proj_bias q_proj_weight = q_proj_bias = None view_ = torch ops aten view default linear_ linear_ = None linear_ = torch ops aten linear default linear k_proj_weight k_proj_bias k_proj_weight = k_proj_bias = None view_ = torch ops aten view default linear_ linear_ = None linear_ = torch ops aten linear default linear v_proj_weight v_proj_bias linear = v_proj_weight = v_proj_bias = None view_ = torch ops aten view default linear_ linear_ = None sdpa_score = sdpa_score sdpa_mask = sdpa_mask flex_attention = torch ops higher_order flex_attention view_ view_ view_ sdpa_score to_ to_ to_ to_ to_ to_ to_ to_ sdpa_mask PRESCALE_QK False ROWS_GUARANTEED_SAFE False BLOCKS_ARE_CONTIGUOUS False WRITE_DQ True OUTPUT_LOGSUMEXP False OUTPUT_MAX False detach view_ = view_ = view_ = sdpa_score = to_ = to_ = to_ = to_ = to_ = to_ = to_ = to_ = sdpa_mask = detach = None getitem = flex_attention getitem_ = flex_attention getitem_ = None getitem_ = flex_attention flex_attention = getitem_ = None pytree tree_unflatten getitem _out_spec exported_out = exported_mod x assertEqual exported_out eager_out test_inductor_backend_inside_nonstrict Foo torch nn Module forward x i_want_faster_code inp inp nonlocal x x + inp + inp out = torch compile i_want_faster_code x x x + out foo = Foo assertWarnsRegex UserWarning You calling torch compile inside torch export region ep = export foo torch randn strict=False module assertExpectedInline str ep graph strip \ graph x num_users= = placeholder target=x _guards_fn num_users= = call_module target=_guards_fn args = x kwargs = add num_users= = call_function target=torch ops aten add Tensor args = x x kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add x kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x add_ kwargs = add_ test_bincount M torch nn Module __init__ super __init__ forward x weights = torch linspace steps= bc = x bincount weights bc model = M ep = export model torch randint dtype=torch int inp = torch randint dtype=torch int assertTrue torch allclose ep module inp M inp test_symint_output Foo torch nn Module forward x z y = x size z + y + x z inputs = torch ones dim _x dim _x = torch export dims dim _x dim _x dynamic_shapes = x dim _x dim _x export Foo inputs dynamic_shapes=dynamic_shapes test_no_tensor_computation Module torch nn Module forward x y y f = Module inp = torch ones ep = export f inp assertEqual ep module inp f inp assertExpectedInline str ep graph strip \ graph x_ num_users= = placeholder target=x_ y num_users= = placeholder target=y test_inline_script_function torch jit script _forward x torch Tensor torch jit is_scripting x cos x sin M torch nn Module forward x torch Tensor _forward x x = torch randn ep = torch export export M x FileCheck check_count torch ops aten sin exactly=True run str ep graph FileCheck check_count torch ops aten cos exactly=True run str ep graph res = ep module x We re inlining original _forward function instead scripted function so we get x sin assertEqual res x sin test_nested_module_fake_tensor_leak Bar torch nn Module __init__ super __init__ _tensor_cache = None forward x _tensor_cache None _tensor_cache = x + _tensor_cache sum + x sum Foo torch nn Module __init__ bar super __init__ bar = bar forward x bar x foo = Foo Bar _ = export foo torch ones strict=False assertTrue foo bar _tensor_cache None test_export_leak_compile BaseModule torch nn Module forward args kwargs raise NotImplementedError CacheModule BaseModule __init__ cache torch Tensor super __init__ assert cache ndim == cache = torch nn Parameter cache requires_grad=False forward x torch Tensor - torch Tensor n_tokens = x size rolled_cache = torch roll cache data -n_tokens dims= rolled_cache -n_tokens = x cache data = rolled_cache cache LinearBlock torch nn Module __init__ in_features out_features activation=None super __init__ linear = torch nn Linear in_features out_features activation = activation forward x x = linear x activation x activation x MyModel BaseModule __init__ super __init__ default_cache = torch zeros cache_layer = CacheModule default_cache fc = LinearBlock activation=torch nn ReLU fc = LinearBlock forward x cached = cache_layer x out = fc cached out = fc out out assertRaisesRegex RuntimeError We found fake tensor exported program constant s list This typically means our tracing system encountered op we can t trace through For potential source you can refer following model attribute cache_layer lifted_tensor_ Please file issue github _ = export MyModel torch randn strict=False assertWarnsRegex UserWarning We found fake tensor exported program constant s list This typically means our tracing system encountered op we can t trace through For potential source you can refer following model attribute cache_layer lifted_tensor_ Please file issue github can t trigger all variant export because later will crash good because we warned torch _export config patch error_on_lifted_constant_tensors=False _ = torch export export MyModel torch randn strict=False test_inline_script_class_method M torch nn Module staticmethod torch jit script _forward x torch Tensor torch jit is_scripting x cos x sin forward x torch Tensor M _forward x x = torch randn ep = torch export export M x FileCheck check_count torch ops aten sin exactly=True run str ep graph FileCheck check_count torch ops aten cos exactly=True run str ep graph res = ep module x We re inlining original _forward function instead scripted function so we get x sin assertEqual res x sin test_tag_ac_export ops_to_save = torch ops aten addmm default policy_fn ctx op args wargs op ops_to_save torch utils checkpoint CheckpointPolicy MUST_SAVE torch utils checkpoint CheckpointPolicy PREFER_RECOMPUTE context_fn = functools partial torch utils checkpoint create_selective_checkpoint_contexts policy_fn Block torch nn Module __init__ super __init__ linear = torch nn Linear relu = torch nn ReLU linear = torch nn Linear forward x linear relu linear x Wrap block checkpointing CheckpointedBlock torch nn Module __init__ super __init__ block = Block forward x torch utils checkpoint checkpoint block x context_fn=context_fn model = CheckpointedBlock x = torch randn requires_grad=True ep = torch export export model x strict=True assertExpectedInline str ep graph strip \ graph p_block_linear _weight num_users= = placeholder target=p_block_linear _weight p_block_linear _bias num_users= = placeholder target=p_block_linear _bias p_block_linear _weight num_users= = placeholder target=p_block_linear _weight p_block_linear _bias num_users= = placeholder target=p_block_linear _bias x num_users= = placeholder target=x wrap_body num_users= = get_attr target=wrap_body tag_activation_checkpoint num_users= = call_function target=torch ops higher_order tag_activation_checkpoint args = wrap_body x p_block_linear _weight p_block_linear _bias p_block_linear _weight p_block_linear _bias kwargs = getitem num_users= = call_function target=operator getitem args = tag_activation_checkpoint kwargs = getitem assertExpectedInline str ep graph_module wrap_body graph strip \ graph arg _ num_users= = placeholder target=arg _ arg _ num_users= = placeholder target=arg _ arg _ num_users= = placeholder target=arg _ arg _ num_users= = placeholder target=arg _ arg _ num_users= = placeholder target=arg _ linear num_users= = call_function target=torch ops aten linear default args = arg _ arg _ arg _ kwargs = relu num_users= = call_function target=torch ops aten relu default args = linear kwargs = linear_ num_users= = call_function target=torch ops aten linear default args = relu arg _ arg _ kwargs = linear_ stack = contextlib ExitStack stack jwd = aot_export_joint_with_descriptors stack ep module x node jwd graph_module graph nodes recompute node meta actual = node meta recompute expected = policy_fn None node target None None assertEqual actual expected assertExpectedInline str jwd graph_module code strip \ forward primals tangents primals_ primals_ primals_ primals_ primals_ tangents_ = fx_pytree tree_flatten_spec primals tangents _in_spec t = torch ops aten t default primals_ primals_ = None addmm = torch ops aten addmm default primals_ primals_ t primals_ = None relu = torch ops aten relu default addmm addmm = None detach_ = torch ops aten detach default relu t_ = torch ops aten t default primals_ primals_ = None addmm_ = torch ops aten addmm default primals_ relu t_ primals_ = None t_ = torch ops aten t default t_ t_ = None mm = torch ops aten mm default tangents_ t_ t_ = None t_ = torch ops aten t default tangents_ mm_ = torch ops aten mm default t_ relu t_ = relu = None t_ = torch ops aten t default mm_ mm_ = None sum_ = torch ops aten sum dim_IntList tangents_ True tangents_ = None view = torch ops aten view default sum_ sum_ = None t_ = torch ops aten t default t_ t_ = None detach_ = torch ops aten detach default detach_ detach_ = None threshold_backward = torch ops aten threshold_backward default mm detach_ mm = detach_ = None t_ = torch ops aten t default t t = None mm_ = torch ops aten mm default threshold_backward t_ t_ = None t_ = torch ops aten t default threshold_backward mm_ = torch ops aten mm default t_ primals_ t_ = primals_ = None t_ = torch ops aten t default mm_ mm_ = None sum_ = torch ops aten sum dim_IntList threshold_backward True threshold_backward = None view_ = torch ops aten view default sum_ sum_ = None t_ = torch ops aten t default t_ t_ = None pytree tree_unflatten addmm_ t_ view_ t_ view mm_ _out_spec test_inline_script_class_method_recursive f = i = s = foo torch jit script _inner x torch Tensor y torch Tensor f float i int s_len int x y f i s_len M torch nn Module staticmethod torch jit script _forward x torch Tensor y torch Tensor f float i int s str torch jit is_scripting _inner x cos y cos f i len s _inner x sin y sin f i len s forward x torch Tensor M _forward x y=x f=f i=i s=s x = torch randn ep = torch export export M x FileCheck check_count torch ops aten sin exactly=True run str ep graph FileCheck check_count torch ops aten cos exactly=True run str ep graph res = ep module x We re inlining original _forward function instead scripted function so we get x sin assertEqual res _inner x sin x sin f i len s test_inline_script_method M torch jit ScriptModule torch jit script_method _forward x torch Tensor torch jit is_scripting x cos x sin forward x _forward x Wrapped torch nn Module __init__ mod super __init__ mod = mod forward x mod x x = torch randn ep = torch export export Wrapped M x FileCheck check_count torch ops aten sin exactly=True run str ep graph FileCheck check_count torch ops aten cos exactly=True run str ep graph res = ep module x We re inlining original _forward function instead scripted function so we get x sin assertEqual res x sin test_no_tensor_computation_ Module torch nn Module forward x y x f = Module inp = torch randn ep = export f inp assertEqual ep module inp f inp assertExpectedInline str ep graph strip \ graph x num_users= = placeholder target=x y num_users= = placeholder target=y x test_no_tensor_computation_ Module torch nn Module forward x y f = Module inp = ep = export f inp assertEqual ep module inp f inp assertExpectedInline str ep graph strip \ graph x num_users= = placeholder target=x y num_users= = placeholder target=y test_no_tensor_computation_ Module torch nn Module forward x y x f = Module inp = torch randn ep = export f inp assertEqual ep module inp f inp assertExpectedInline str ep graph strip \ graph x_ num_users= = placeholder target=x_ y num_users= = placeholder target=y x_ test_not_registered_parameter Basic torch nn Module __init__ super __init__ params = foo torch nn Parameter torch ones forward x x + params foo f = Basic args = torch randn strict-mode will error out because foo registered parameter dynamo behavior s different eager We decided follow eager behavior ep = export f args strict=False gm = ep module assertEqual len ep graph_signature lifted_tensor_constants assertEqual len ep graph_signature parameters check foo parameter final graph assertEqual len list gm named_parameters assertEqual gm args f args assertExpectedInline str gm graph strip \ graph lifted_tensor_ num_users= = get_attr target=lifted_tensor_ x num_users= = placeholder target=x _guards_fn num_users= = call_module target=_guards_fn args = x kwargs = add num_users= = call_function target=torch ops aten add Tensor args = x lifted_tensor_ kwargs = add test_int_shape_specialization M torch nn Module forward x ori_size = int x shape - int x shape - x = F interpolate x size=ori_size mode= bilinear x input = torch rand input = torch rand inputs = input input model = M dynamic_shapes = x torch export Dim DYNAMIC torch export Dim DYNAMIC assertRaisesRegex torch fx experimental symbolic_shapes ConstraintViolationError torch _dynamo exc UserError r your code specialized constant \ \ \n r your code specialized constant \ \ \n export model input dynamic_shapes=dynamic_shapes strict=False test_external_call_non_strict_real_tensor ExternalMethod add x x + x Basic torch nn Module __init__ - None super __init__ external_add = ExternalMethod add forward x external_add x f = Basic args = torch randn ep = export f args strict=False assertEqual ep module args f args test_export_statically_known_true Foo torch nn Module forward x y shape = y shape - y shape end = shape x end dynamic_shapes = torch export Dim DYNAMIC torch export Dim DYNAMIC torch export Dim DYNAMIC torch export Dim DYNAMIC m = Foo inp = torch randn torch randn ep = export m inp dynamic_shapes=dynamic_shapes strict=False assertTrue torch allclose ep module inp m inp FileCheck check_count torch ops aten slice Tensor exactly=True run str ep graph FileCheck check_count operator sub exactly=True run str ep graph test_colon_parameter M torch nn Module __init__ - None super __init__ register_parameter foo bar torch nn Parameter torch ones forward x x + getattr foo bar ep = export M torch randn x = torch randn assertEqual ep module x M x test_conv_dynamic Simple module demonstration M torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= padding= relu = torch nn ReLU maxpool = torch nn MaxPool d kernel_size= forward x torch Tensor y torch Tensor - torch Tensor = conv x add_ y maxpool relu example_args = torch randn torch ones dynamic_shapes = x Dim batch y Dim batch m = M exported_program torch export ExportedProgram = export m args=example_args dynamic_shapes=dynamic_shapes args = torch randn torch ones assertEqual exported_program module args m args args = torch randn torch ones assertEqual exported_program module args m args gm torch fx GraphModule = torch export export m args=example_args dynamic_shapes=dynamic_shapes module args = torch randn torch ones assertEqual gm args m args args = torch randn torch ones assertEqual gm args m args stride called undefined tensor testing expectedFailureCppRuntimeNonStrict test_native_multi_attention_head embed_dim = num_heads = bs = sl = device = cpu q = torch rand bs sl embed_dim device=device dtype=torch float - k = q v = q qkv = torch nn Linear embed_dim embed_dim device=device dtype=torch float proj = torch nn Linear embed_dim embed_dim device=device dtype=torch float NativeMHA torch nn Module __init__ embed_dim num_heads qkv proj need_weights average_attn_weights mask_type super __init__ qkv = qkv proj = proj embed_dim = embed_dim num_heads = num_heads need_weights = need_weights average_attn_weights = average_attn_weights mask_type = mask_type forward q k v key_padding_mask torch _native_multi_head_attention q k v embed_dim num_heads qkv weight qkv bias proj weight proj bias key_padding_mask need_weights=False average_attn_weights=False mask_type= mask_type = = src_key_padding_mask mask_type = = src_mask mask_type need_weights True False average_attn_weights True False npt = NativeMHA embed_dim=embed_dim num_heads=num_heads qkv=qkv proj=proj need_weights=need_weights average_attn_weights=average_attn_weights mask_type=mask_type sample_input = q k v None ep = export npt args=sample_input dynamic_shapes= q Dim dim _q max= k Dim dim _k max= v Dim dim _v max= key_padding_mask None assertEqual ep module sample_input npt sample_input test_unused_constant M torch nn Module forward x y = torch tensor x x ep = export M torch ones assertEqual len ep constants M torch nn Module __init__ num_features int = - None super __init__ num_features = num_features forward x torch Tensor - List torch Tensor res = torch Tensor num_features i range num_features res i = x i + res inp = torch ones ep = export M inp assertEqual len ep constants unf = unflatten ep assertTrue torch allclose M inp unf inp test_unbacked_bincount Foo torch nn Module forward xs u u = xs tolist x = torch ones u dtype=torch int y = torch bincount x minlength=u y m = Foo x = torch tensor ep = export m x assertTrue torch allclose ep module x m x y = torch tensor assertTrue torch allclose ep module y m y requires_gpu test_export_custom_triton_kernel triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask torch library triton_op mylib add mutates_args= custom_add x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE capture_triton add_kernel grid x y output n_elements output M torch nn Module forward x y custom_add x y args = torch randn device=GPU_TYPE torch randn device=GPU_TYPE max_len = dynamic_shapes = x Dim dim _x max=max_len y Dim dim _y max=max_len m = M ep = export m args dynamic_shapes=dynamic_shapes FileCheck check_count torch ops mylib add exactly=True run ep graph_module code ep_decomposed = ep run_decompositions decompose_custom_triton_ops=False FileCheck check_count torch ops mylib add exactly=True run ep graph_module code ep_decomposed = ep run_decompositions decompose_custom_triton_ops=True FileCheck check_count torch ops higher_order triton_kernel_wrapper_functional exactly=True run ep_decomposed graph_module code exp_out = m args assertEqual exp_out ep module args requires_gpu test_export_custom_triton_kernel_mutable triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask torch library triton_op mylib add mutates_args= output custom_add_out x torch Tensor y torch Tensor output torch Tensor - torch Tensor n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE capture_triton add_kernel grid x y output n_elements output clone M torch nn Module forward x y out custom_add_out x y out args = torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch zeros device=GPU_TYPE custom_add_out args max_len = dynamic_shapes = x Dim dim _x max=max_len y Dim dim _y max=max_len out Dim dim _z max=max_len m = M ep = export m args dynamic_shapes=dynamic_shapes FileCheck check_count torch ops mylib add exactly=True run ep graph_module code ep_decomposed = ep run_decompositions decompose_custom_triton_ops=False FileCheck check_count torch ops higher_order auto_functionalized exactly=True run ep_decomposed graph_module code ep_decomposed = ep run_decompositions decompose_custom_triton_ops=True is_training_ir_test _testMethodName TODO For training IR test we functionalize custom triton op auto_functionalized The custom op s functional decomposition triggered result It might better decompose custom triton ops Users can workaround unwrapping auto_functionalized order get functional triton hop needed FileCheck check_count torch ops higher_order auto_functionalized exactly=True run ep_decomposed graph_module code FileCheck check_count torch ops higher_order triton_kernel_wrapper_functional exactly=True run ep_decomposed graph_module code x y out = torch randn device=GPU_TYPE torch randn device=GPU_TYPE torch zeros device=GPU_TYPE exp_out = m x y out out_copy = out clone out_copy = out clone out_copy = out clone assertEqual exp_out ep module x y out_copy For non-functional graph module out_copy mutated assertEqual out out_copy assertEqual exp_out ep_decomposed module x y out_copy For non-functional graph module out_copy mutated assertEqual out_copy out_copy test_masked_select_dynamic M torch nn Module __init__ - None super __init__ forward x torch Tensor - torch Tensor mask = x ge torch masked_select x mask example_args = torch randn dim _x_max dim _x_max = dynamic_shapes = x Dim dim _x max=dim _x_max Dim dim _x_max max=dim _x_max m = M exported_program torch export ExportedProgram = export m args=example_args dynamic_shapes=dynamic_shapes Test expected upper bound among range constraints expected_upper_bound = dim _x_max dim _x_max vr_upper_bounds = vr upper vr exported_program range_constraints values assertTrue expected_upper_bound set vr_upper_bounds Test none upper bounds larger vr_upper vr_upper_bounds assertTrue vr_upper = expected_upper_bound test_nonzero_dynamic M torch nn Module __init__ - None super __init__ forward x torch Tensor as_tuple bool - torch Tensor torch nonzero x as_tuple=as_tuple Case as_tuple True as_tuple False as_tuple True False example_args = torch randn as_tuple dim _x_max dim _x_max = dynamic_shapes = x Dim dim _x max=dim _x_max Dim dim _x_max max=dim _x_max as_tuple None m = M exported_program torch export ExportedProgram = export m args=example_args dynamic_shapes=dynamic_shapes Test expected upper bound among range constraints expected_upper_bound = dim _x_max dim _x_max vr_upper_bounds = vr upper vr exported_program range_constraints values assertTrue expected_upper_bound set vr_upper_bounds Test none upper bounds larger vr_upper vr_upper_bounds assertTrue vr_upper = expected_upper_bound Case Test special case when input has zero dimensions nonzero scalar value example_args = torch tensor as_tuple dim _x_max = dynamic_shapes = x None as_tuple None m = M exported_program torch export ExportedProgram = export m args=example_args dynamic_shapes=dynamic_shapes Test expected upper bound equal since our output edge case should always tensor size vr_upper_bounds = vr upper vr exported_program range_constraints values vr_upper vr_upper_bounds assertEqual vr_upper test_detect_leak_strict Foo torch nn Module __init__ super __init__ forward x y x + y global_list = ReferenceControl __init__ mod bank = bank_dict = mod = mod hacked_up_forward self_ x y bank append x clone bank_dict x = x clone global_list append x clone x + y mod forward = hacked_up_forward __get__ mod Foo __call__ x y ep = export mod x y strict=True module out = ep x y out update print bank foo = Foo ref = ReferenceControl foo TODO tmanlaibaatar kinda sucks today there no good way get good source name We should have util post processes dynamo source names more readable assertWarnsRegex UserWarning r L\ \ _modules\ _export_root \ forward\ __func__\ __closure__\ \ \ cell_contents\ bank r &#124; L\ \ _modules\ _export_root \ forward\ __func__\ __closure__\ \ \ cell_contents\ bank_dict r &#124; L\ \ _modules\ _export_root \ forward\ __func__\ __closure__\ \ \ cell_contents ref torch randn torch randn test_mask_nonzero_static TestModule torch nn Module forward seq_embeddings mask exp Instead ` output = seq_embeddings mask ` ` which makes output shape have unbacked symint encode side knowledge output shape exp shape force have backed symint index = torch nonzero_static mask size=exp shape chunked_index = index chunk chunks=mask dim dim= output = seq_embeddings chunked_index squeeze final_output = output final_output m = TestModule seq_embeddings = torch randn mask = torch ones dtype=torch bool exp = torch randn output = m seq_embeddings mask exp batch = torch export Dim batch exp_size = torch export Dim exp_size max= ep = export m seq_embeddings mask exp dynamic_shapes= seq_embeddings batch None mask batch None exp exp_size ep_output = ep module seq_embeddings mask exp assertTrue torch allclose output ep_output seq_embeddings = torch randn mask = torch ones dtype=torch bool exp = torch randn output = m seq_embeddings mask exp ep_output = ep module seq_embeddings mask exp assertTrue torch allclose output ep_output test_setgrad_lifted_tensor M torch nn Module forward x y torch enable_grad c = torch tensor z = c + x + y z z m = M x = torch randn y = torch randn Need surround export no_grad bypass AutogradStateOpsFailSafeguard torch no_grad ep = export m x y assertEqual ep module x y m x y test_subclass_context Foo torch nn Module forward x x + input = TwoTensor TwoTensor torch randn torch rand TwoTensor torch randn torch rand input_test = TwoTensor TwoTensor torch randn torch rand TwoTensor torch randn torch rand strict True False dim = torch export ShapesCollection dim input = Dim STATIC Dim AUTO ep = torch export export Foo input strict=strict dynamic_shapes=dim assertExpectedInline str ep graph strip \ graph x num_users= = placeholder target=x add num_users= = call_function target=torch ops aten add Tensor args = x kwargs = add assertRaisesRegex AssertionError escape Guard failed x size == ep module input_test test_basic_non_strict_real_tensor Basic torch nn Module __init__ - None super __init__ param = torch nn Parameter torch randn forward x y x + y - param f = Basic args = torch randn torch randn ep = export f args strict=False assertEqual ep module args f args test_where_decomp TestModule torch nn Module __init__ super __init__ forward x torch ops aten where default x test_module = TestModule sample_input = torch randn auto_dynamic_shapes_from_args args pyre-ignore This function creates dynamic shapes specification Dim AUTO all dimensions all tensors given argument list isinstance args list auto_dynamic_shapes_from_args arg arg args isinstance args tuple tuple auto_dynamic_shapes_from_args arg arg args isinstance args dict k auto_dynamic_shapes_from_args v k v args items isinstance args torch Tensor j Dim AUTO j range args dim print f args type type args None ep = torch export export test_module sample_input dynamic_shapes=auto_dynamic_shapes_from_args sample_input run_decompositions test_basic_non_strict_fake_tensor Basic torch nn Module __init__ - None super __init__ param = torch nn Parameter torch randn forward x y x + y - param fake_mode = FakeTensorMode shape_env=ShapeEnv tracked_fakes= f = Basic fake_mode args = torch empty torch empty ep = export f args strict=False inputs = torch randn torch randn assertEqual ep module inputs f inputs test_non_strict_dynamic_shapes Foo torch nn Module __init__ - None super __init__ u = torch nn Buffer torch ones v = torch nn Buffer torch ones forward x ys zs c y = ys + ys + zs + zs b v add_ w = u - v x shape c shape = x + w x + y x - w x - y foo = Foo inp = torch ones torch zeros torch ones torch zeros b torch ones torch ones dim = torch export Dim dim min= dynamic_shapes = dim dim dim dim b dim None ep_ns = torch export export foo inp dynamic_shapes=dynamic_shapes strict=False bad_runtime_inp = torch ones torch zeros torch ones torch zeros b torch ones torch ones assertRaisesRegex AssertionError escape Guard failed ys size == x size expected got ep_ns module bad_runtime_inp bad_runtime_inp = torch ones torch zeros torch ones torch zeros b torch ones torch ones assertRaisesRegex AssertionError escape Guard failed c size == expected got ep_ns module bad_runtime_inp good_runtime_inp = torch ones torch zeros torch ones torch zeros b torch ones torch ones ep_ns module good_runtime_inp bad_example_inp = torch ones torch zeros torch ones torch zeros b torch ones torch ones assertRaisesRegex torch fx experimental symbolic_shapes ConstraintViolationError range ep_ns = torch export export foo bad_example_inp dynamic_shapes=dynamic_shapes strict=False test_non_strict_dynamic_shapes_suggested_fixes Foo torch nn Module forward x c x shape = x + c + x - c - foo = Foo bad_example_inp = torch ones torch ones dim = torch export Dim dim min= dynamic_shapes = dim None assertRaisesRegex torch _dynamo exc UserError Constraints violated \\ dim\\ \n Not all values dim satisfy generated guard \n Suggested fixes \n dim = Dim\\ dim min= max= \\ torch export export foo bad_example_inp dynamic_shapes=dynamic_shapes strict=False test_symint_item M torch nn Module forward tensor tensor item input = torch tensor dtype=torch int orig_res = M input ep_res = torch export export M input module input assertEqual orig_res ep_res test_symbool_item M torch nn Module forward tensor tensor item input = torch tensor dtype=torch bool orig_res = M input ep_res = torch export export M input module input assertEqual orig_res ep_res test_symfloat_item M torch nn Module forward tensor tensor item input = torch tensor dtype=torch float orig_res = M input ep_res = torch export export M input module input assertEqual orig_res ep_res test_unbacked_to_cond strict = True M torch nn Module forward az = nonzero true_fn x x + sum false_fn x x + sum r = torch cond az size true_fn false_fn az r M torch randn torch export export M torch randn strict=strict test_unbacked_to_cond_passthrough strict = True M torch nn Module forward az = nonzero true_fn x x + false_fn x x + r = torch cond az size true_fn false_fn az r M torch randn torch export export M torch randn strict=strict test_cond_branches_return_constant_int cpp_runtime_nonstrict id skipTest TODO Unexpected success OSS fbcode M torch nn Module forward x idx = torch cond x sum lambda lambda tuple x idx args = torch randn m = M ep = export M args _testMethodName == test_cond_branches_return_constant_int assertExpectedInline normalize_gm ep module print_readable print_output=False \ GraphModule torch nn Module forward x x f x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None sum_ f = torch ops aten sum default x gt b = torch ops aten gt Scalar sum_ sum_ = None true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond gt true_graph_ false_graph_ gt = true_graph_ = false_graph_ = None getitem_ Sym u = cond cond = None ge_ Sym u = = getitem_ = _assert_scalar_default = torch ops aten _assert_scalar default ge_ Runtime assertion failed expression u = node ge_ ge_ = _assert_scalar_default = None le_ Sym u = = getitem_ = _assert_scalar_default_ = torch ops aten _assert_scalar default le_ Runtime assertion failed expression u = node le_ le_ = _assert_scalar_default_ = None select f = torch ops aten select int x getitem_ x = getitem_ = None pytree tree_unflatten select _out_spec true_graph_ torch nn Module forward false_graph_ torch nn Module forward noqa B assertEqual m args ep module args testing expectedFailureCppRuntimeNonStrict test_cond_access_identical_symint_closure Example torch nn Module forward x trigger target torch cond trigger == lambda x + target lambda x target m = Example x = torch randn trigger = target = args = x trigger target config patch use_new_tracer_experimental=True ep = export m args dynamic_shapes= None Dim DYNAMIC Dim DYNAMIC assertExpectedInline str tuple ep range_constraints values VR int_oo VR int_oo assertEqual m args ep module args test_cond_branches_return_same_int M torch nn Module forward x idx = torch cond x sum lambda lambda tuple x idx args = torch randn m = M ep = export M args Ideally we could remove cond front end directly since s used anyway But we can only do early optimization all outputs same constants which will complicates output check so just keep graph let downstream dce _testMethodName == test_cond_branches_return_same_int assertExpectedInline normalize_gm ep module print_readable print_output=False \ GraphModule torch nn Module forward x x f x = fx_pytree tree_flatten_spec x _in_spec _guards_fn = _guards_fn x _guards_fn = None sum_ f = torch ops aten sum default x gt b = torch ops aten gt Scalar sum_ sum_ = None true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond gt true_graph_ false_graph_ gt = true_graph_ = false_graph_ = None getitem = cond cond = getitem = None select f = torch ops aten select int x x = None pytree tree_unflatten select _out_spec true_graph_ torch nn Module forward false_graph_ torch nn Module forward noqa B assertEqual m args ep module args torch _dynamo config patch capture_scalar_outputs=True test_cond_contains_unbacked_no_escape M torch nn Module forward b b c true_fn x x b item false_fn x x b item r = torch cond true_fn false_fn c r args = torch tensor True torch tensor torch tensor torch randn requires_grad=True torch export export M args test_cond_int_closure M torch nn Module __init__ super __init__ num = forward x true_fn x x num false_fn x x + num r = torch cond true_fn false_fn x r args = torch tensor True torch randn ep = torch export export M args assertEqual ep module args M args test_state_tensors M torch nn Module simple register buffer __init__ - None super __init__ buf = torch nn Buffer torch ones persistent=False forward x x = y = buf y = w = buf + w = buf + w = buf + buf = w z = buf buf = w z = x + y + z + w ep = export M torch randn strict=False run_decompositions assertEqual list ep graph_signature buffers_to_mutate values buf assertTrue torch allclose ep module torch ones + torch ones M torch nn Module simple without register buffer __init__ - None super __init__ buf = torch ones forward x x = y = buf y = buf = buf + z = buf z = x + y + z assertWarnsRegex UserWarning The tensor attribute buf assigned during export export M torch randn strict=False M torch nn Module complex register buffer __init__ - None super __init__ tensors = torch ones torch ones i tensor enumerate tensors register_buffer f buf_ i tensor persistent=False get_tensor i getattr f buf_ i set_tensor i val setattr f buf_ i val forward x x = y = get_tensor + get_tensor y = + set_tensor torch ones + set_tensor torch ones + z = get_tensor + get_tensor z = + x + y + z ep = export M torch randn strict=False run_decompositions assertEqual list ep graph_signature buffers_to_mutate values buf_ buf_ assertTrue torch allclose ep module torch ones + torch ones M torch nn Module complex without register buffer __init__ - None super __init__ tensors = torch ones torch ones get_tensor i tensors i set_tensor i val tensors i = val forward x x = y = get_tensor + get_tensor y = + set_tensor torch ones + set_tensor torch ones + z = get_tensor + get_tensor z = + x + y + z assertWarnsRegex UserWarning The tensor attributes tensors\\ \\ tensors\\ \\ assigned during export export M torch randn strict=False torch _dynamo config patch capture_scalar_outputs=True test_while_loop_tensor_constant_idx while_loop_decomp x y out = torch zeros_like x cond_fn idx out y idx out size body_fn idx out y i = idx item TODO removing those causes PendingUnbackedSymbolNotFound torch _check i = torch _check i x size y = x i + y out = out clone out i = y idx + out y cnt = torch tensor _ out _ = while_loop cond_fn body_fn cnt out y out TestModel torch nn Module forward x y while_loop_decomp x y x y = torch randn torch randn exp_out = TestModel x y ep = export TestModel x y out = ep module x y assertEqual exp_out out test_malformed_fqn_from_source_name See https github com pytorch pytorch issues types MethodType Block torch nn Module __init__ i o super __init__ to_out = torch nn ModuleList to_out append torch nn Linear i o bias=True to_out append torch nn Dropout forward x l to_out x = l x x Problem torch nn Module __init__ super __init__ blocks = torch nn ModuleDict f i Block i range forward x k m blocks items x = m x x Problem torch nn Module __init__ super __init__ blocks = torch nn ModuleList Block i range forward x x = blocks x m blocks x = m x x _split_after_forward args kwargs _orig_forward args kwargs annotate_split_points mod torch nn Module spec qualname split_type spec items atoms = qualname split predecessor_module = mod i atom enumerate atoms - try predecessor_module = getattr predecessor_module atom except AttributeError e raise e mod_to_wrap = getattr predecessor_module atoms - mod_to_wrap _orig_forward = mod_to_wrap forward mod_to_wrap forward = MethodType _split_after_forward mod_to_wrap problem Problem Problem m = problem m torch rand simplified torch distributed pipeline code annotate_split_points m blocks blocks gm = export m torch rand torch export unflatten gm test_unflatten_closure Dummy torch nn Module forward fn x y = x + z = fn y z + N torch nn Module forward x x + M torch nn Module __init__ super __init__ dummy = Dummy n = N forward x y = x + z = dummy lambda k n y + k + y y z + m = M x = torch randn ep = export m x ufm = torch export unflatten ep assertExpectedInline str ufm graph_module code strip \ forward x add = torch ops aten add Tensor x x = None dummy = dummy add add = None add_ = torch ops aten add Tensor dummy dummy = None add_ assertExpectedInline str ufm dummy graph_module code strip \ forward add add_ = torch ops aten add Tensor add add_ = torch ops aten add Tensor add add_ add_ = None add_ = torch ops aten add Tensor add_ add_ = None add_ = torch ops aten add Tensor add_ add add_ = add = None add_ = torch ops aten add Tensor add_ add_ = None add_ test_state_primitives M torch nn Module __init__ - None super __init__ x = y = k z = forward x x = x + y k = y k + z = z + x + x + y k + z ep = export M torch randn assertTrue torch allclose ep module torch zeros torch ones test_state_shape_attribute_assignment TestModule torch nn Module __init__ super __init__ linear = torch nn Linear last_z_shape = linear weight shape forward x last_z_shape = x shape linear x model = TestModule x = torch randn ep_model = export model x strict=False module assertTrue torch allclose model x ep_model x test_output_node_name TestModule torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x model = TestModule x = torch randn ep_model = export model x strict=False module assertEqual list ep_model graph nodes - name output assertTrue torch allclose model x ep_model x test_real_tensor_size_mismatch torch _subclasses fake_tensor MetadataMismatchError M torch nn Module forward b torch ops mylib foo b torch library custom_op mylib foo mutates_args= foo torch Tensor b torch Tensor - torch Tensor + b foo register_fake foo_fake_impl b m n = shape torch empty n m incorrectly permute error_type = MetadataMismatchError is_non_strict_test _testMethodName torch _dynamo exc TorchRuntimeError torch _functorch config patch fake_tensor_propagate_real_tensors=True won t catch anything dims equal export M torch randn torch randn catch concrete inequality assertRaisesRegex error_type r Real tensor propagation found output size mismatch between fake shape real shape r output\ size\ \ func mylib foo default export M torch randn torch randn same test dynamic shapes d = Dim d d = Dim d export M torch randn torch randn dynamic_shapes= d d b d d assertRaisesRegex error_type r Real tensor propagation found output size mismatch between fake shape s\d+ real shape r output\ size\ \ func mylib foo default export M torch randn torch randn dynamic_shapes= d d b d d test_real_tensor_alias_dtype_mismatch torch _subclasses fake_tensor MetadataMismatchError error_type = MetadataMismatchError is_non_strict_test _testMethodName torch _dynamo exc TorchRuntimeError test alias case M torch nn Module forward torch ops mylib foo_alias torch library custom_op mylib foo_alias mutates_args= foo_alias torch Tensor - torch Tensor foo_alias register_fake foo_fake_impl torch _functorch config patch fake_tensor_propagate_real_tensors=True assertRaisesRegex error_type r Real tensor propagation found aliasing mismatch between fake output \n r real output \n func mylib foo_alias default ep = export M torch randn test dtype case N torch nn Module forward torch ops mylib foo_dtype torch library custom_op mylib foo_dtype mutates_args= foo_dtype torch Tensor - torch Tensor foo_dtype register_fake foo_fake_impl m n = shape torch empty m n dtype=torch int torch _functorch config patch fake_tensor_propagate_real_tensors=True assertRaisesRegex error_type r Real tensor propagation found metadata mismatch between fake tensor \n r real tensor \n output func mylib foo_dtype default ep = export N torch randn test_real_tensor_for_max_op Foo torch nn Module forward x y x = x x y = y y max x shape y shape model = Foo inputs = torch zeros torch ones torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs assertEqual ep module inputs model inputs x = torch zeros y = torch ones This seems bug old export because when we pass x x input runtime assertion should fail This because we would create guard y shape x shape somehow old export we dce assertion assertEqual ep module x x model x x assertEqual ep module x y model x y test_draft_export_checks_mutation_with_nan torch library custom_op export foo mutates_args= foo x torch Tensor y torch Tensor - torch Tensor x + y foo register_fake _ x y x + y Foo torch nn Module forward x y foo x y model = Foo inputs = torch full torch nan torch full torch nan torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs test_draft_export_checks_mutation torch library custom_op export foo mutates_args= foo x torch Tensor y torch Tensor - torch Tensor y add_ x clone foo register_fake _ x y x clone Foo torch nn Module forward x y foo x y model = Foo inputs = torch randn torch randn assertRaisesRegex RuntimeError argument y torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs torch library custom_op export foo mutates_args= y foo x torch Tensor y torch Tensor - torch Tensor y add_ x clone foo register_fake _ x y x clone No errors torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs test_draft_export_checks_mutation_list torch library custom_op export foo mutates_args= foo xs List torch Tensor - torch Tensor x y = xs y add_ x clone foo register_fake _ xs x y = xs x clone Foo torch nn Module forward xs foo xs model = Foo inputs = torch randn torch randn assertRaisesRegex RuntimeError argument xs torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs torch library custom_op export foo mutates_args= xs foo xs List torch Tensor - torch Tensor x y = xs y add_ x clone foo register_fake _ xs x y = xs x clone No errors torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs test_draft_export_checks_aliasing torch library custom_op export foo mutates_args= foo x torch Tensor y torch Tensor - torch Tensor x foo register_fake _ x y x clone Foo torch nn Module forward x y foo x y model = Foo inputs = torch randn torch randn assertRaisesRegex RuntimeError may alias torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs torch library custom_op export foo mutates_args= foo x torch Tensor y torch Tensor - torch Tensor x clone foo register_fake _ x y x clone No errors torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs test_draft_export_infers_fake_kernel strict = True torch library _scoped_library export FRAGMENT lib lib define bar Tensor x - Tensor lib impl bar lambda x x clone CPU torch library custom_op export foo mutates_args= foo x torch Tensor y torch Tensor - torch Tensor x y Foo torch nn Module forward x y foo x y torch ops export bar y model = Foo inputs = torch randn torch randn torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs strict=strict expecttest only works base TestExport __class__ = TestExport assertExpectedInline str ep graph_module code strip \ forward x y foo = torch ops export foo default x y x = None sym_size_int = torch ops aten sym_size int foo sym_size_int_ = torch ops aten sym_size int foo sym_constrain_range_for_size_default = torch ops aten sym_constrain_range_for_size default sym_size_int sym_constrain_range_for_size_default = None ge = sym_size_int = sym_size_int = None _assert_scalar_default = torch ops aten _assert_scalar default ge Runtime assertion failed expression u = node ge ge = _assert_scalar_default = None sym_constrain_range_for_size_default_ = torch ops aten sym_constrain_range_for_size default sym_size_int_ sym_constrain_range_for_size_default_ = None ge_ = sym_size_int_ = sym_size_int_ = None _assert_scalar_default_ = torch ops aten _assert_scalar default ge_ Runtime assertion failed expression u = node ge_ ge_ = _assert_scalar_default_ = None bar = torch ops export bar default y y = None sym_size_int_ = torch ops aten sym_size int bar sym_constrain_range_for_size_default_ = torch ops aten sym_constrain_range_for_size default sym_size_int_ sym_constrain_range_for_size_default_ = None ge_ = sym_size_int_ = sym_size_int_ = None _assert_scalar_default_ = torch ops aten _assert_scalar default ge_ Runtime assertion failed expression u = node ge_ ge_ = _assert_scalar_default_ = None foo bar test_draft_export_fake_kernel_inference_errors torch library custom_op export foo mutates_args= foo x torch Tensor y torch Tensor - torch Tensor x expand contiguous Foo torch nn Module forward x y foo x y model = Foo inputs = torch randn torch randn assertRaisesRegex RuntimeError non-zero storage offset torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs torch library custom_op export foo mutates_args= foo x torch Tensor y torch Tensor - torch Tensor torch randn diagonal assertRaisesRegex RuntimeError dense memory torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs test_subclasses_parameterization cpp_runtime_nonstrict id skipTest TODO Unexpected success OSS fbcode Foo torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter CustomTensorPlainOut torch ones torch ones forward x = p + p sum x + m = Foo ref_x = torch randn ref_out = m ref_x ep_training = torch export export m ref_x assertExpectedInline str ep_training graph strip \ graph p_p num_users= = placeholder target=p_p p_p num_users= = placeholder target=p_p x num_users= = placeholder target=x mul num_users= = call_function target=torch ops aten mul Tensor args = p_p kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mul p_p kwargs = sum_ num_users= = call_function target=torch ops aten sum default args = add kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x sum_ kwargs = add_ ep = export m ref_x run_decompositions assertExpectedInline str ep graph strip \ graph p_p num_users= = placeholder target=p_p p_parametrizations_p _original num_users= = placeholder target=p_parametrizations_p _original p_parametrizations_p _original num_users= = placeholder target=p_parametrizations_p _original x num_users= = placeholder target=x mul num_users= = call_function target=torch ops aten mul Tensor args = p_p kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mul p_parametrizations_p _original kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = mul p_parametrizations_p _original kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add add_ kwargs = sum_ num_users= = call_function target=torch ops aten sum default args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x sum_ kwargs = add_ res = ep module ref_x assertEqual res ref_out testing expectedFailureCppRuntimeNonStrict test_subclasses_parameterization_nested Foo torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter CustomTensorPlainOut CustomTensorPlainOut torch Tensor torch Tensor CustomTensorPlainOut torch Tensor torch Tensor forward x = x + p + p sum sum x + m = Foo ref_x = torch randn ref_out = m ref_x ep_training = torch export export m ref_x strict=False assertExpectedInline str ep_training graph strip \ graph p_p num_users= = placeholder target=p_p p_p num_users= = placeholder target=p_p x num_users= = placeholder target=x mul num_users= = call_function target=torch ops aten mul Tensor args = p_p kwargs = add num_users= = call_function target=torch ops aten add Tensor args = x mul kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add p_p kwargs = sum_ num_users= = call_function target=torch ops aten sum default args = add_ kwargs = sum_ num_users= = call_function target=torch ops aten sum default args = sum_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x sum_ kwargs = add_ ep = export m ref_x ep = ep run_decompositions assertExpectedInline str ep graph strip \ graph p_p num_users= = placeholder target=p_p p_parametrizations_p _original num_users= = placeholder target=p_parametrizations_p _original p_parametrizations_p _original num_users= = placeholder target=p_parametrizations_p _original p_parametrizations_p _original num_users= = placeholder target=p_parametrizations_p _original p_parametrizations_p _original num_users= = placeholder target=p_parametrizations_p _original x num_users= = placeholder target=x mul num_users= = call_function target=torch ops aten mul Tensor args = p_p kwargs = add num_users= = call_function target=torch ops aten add Tensor args = x mul kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add p_parametrizations_p _original kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add p_parametrizations_p _original kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add_ add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add p_parametrizations_p _original kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add p_parametrizations_p _original kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add_ add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add_ add_ kwargs = sum_ num_users= = call_function target=torch ops aten sum default args = add_ kwargs = sum_ num_users= = call_function target=torch ops aten sum default args = sum_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x sum_ kwargs = add_ res = ep module ref_x assertEqual res ref_out testing expectedFailureSerDer can t serialize functorch ops testing expectedFailureSerDerNonStrict can t serialize functorch ops testing expectedFailureCppRuntime test_vmap Vmap torch nn Module forward x y f = lambda x y x y + sum dim= noqa E vmapped = torch vmap f x y vmapped sum dim= DYN = torch export Dim DYNAMIC inputs = torch tensor torch tensor dynamic = x DYN y DYN ep = torch export export Vmap inputs dynamic_shapes=dynamic assertExpectedInline str ep graph strip \ graph x num_users= = placeholder target=x y num_users= = placeholder target=y sym_size_int_ num_users= = call_function target=torch ops aten sym_size int args = y kwargs = lazy_load_decompositions num_users= = call_function target=torch _functorch predispatch lazy_load_decompositions args = kwargs = _vmap_increment_nesting num_users= = call_function target=torch _functorch predispatch _vmap_increment_nesting args = sym_size_int_ error kwargs = _add_batch_dim num_users= = call_function target=torch _functorch predispatch _add_batch_dim args = x kwargs = _add_batch_dim_ num_users= = call_function target=torch _functorch predispatch _add_batch_dim args = y kwargs = mul num_users= = call_function target=torch ops aten mul Tensor args = _add_batch_dim _add_batch_dim_ kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mul kwargs = sum_ num_users= = call_function target=torch ops aten sum dim_IntList args = add kwargs = _remove_batch_dim num_users= = call_function target=torch _functorch predispatch _remove_batch_dim args = sum_ sym_size_int_ kwargs = _vmap_decrement_nesting num_users= = call_function target=torch _functorch predispatch _vmap_decrement_nesting args = kwargs = sum_ num_users= = call_function target=torch ops aten sum dim_IntList args = _remove_batch_dim kwargs = sum_ ep = torch export export Vmap inputs dynamic_shapes=dynamic strict=True assertExpectedInline str ep graph strip \ graph x num_users= = placeholder target=x y num_users= = placeholder target=y sym_size_int_ num_users= = call_function target=torch ops aten sym_size int args = y kwargs = lazy_load_decompositions num_users= = call_function target=torch _functorch predispatch lazy_load_decompositions args = kwargs = _vmap_increment_nesting num_users= = call_function target=torch _functorch predispatch _vmap_increment_nesting args = sym_size_int_ error kwargs = _add_batch_dim num_users= = call_function target=torch _functorch predispatch _add_batch_dim args = x kwargs = _add_batch_dim_ num_users= = call_function target=torch _functorch predispatch _add_batch_dim args = y kwargs = mul num_users= = call_function target=torch ops aten mul Tensor args = _add_batch_dim _add_batch_dim_ kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mul kwargs = sum_ num_users= = call_function target=torch ops aten sum dim_IntList args = add kwargs = _remove_batch_dim num_users= = call_function target=torch _functorch predispatch _remove_batch_dim args = sum_ sym_size_int_ kwargs = _vmap_decrement_nesting num_users= = call_function target=torch _functorch predispatch _vmap_decrement_nesting args = kwargs = sum_ num_users= = call_function target=torch ops aten sum dim_IntList args = _remove_batch_dim kwargs = sum_ assertTrue torch allclose ep module inputs Vmap inputs ep = export Vmap inputs dynamic_shapes=dynamic run_decompositions assertTrue torch allclose ep module inputs Vmap inputs testing expectedFailureLegacyExportNonStrict Old export doesn t work subclasses testing expectedFailureLegacyExportStrict Old export doesn t work subclasses test_subclass_nested_attr_access Foo torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter TwoTensor TwoTensor torch ones torch ones TwoTensor torch ones torch ones b = torch nn Buffer TwoTensor TwoTensor torch ones torch ones TwoTensor torch ones torch ones forward x res = p + p + b sum x + res get_elem_a b m = Foo ref_x = torch randn ref_out = m ref_x ep_training = torch export export m ref_x strict=False assertTrue torch allclose ep_training module ref_x ref_out assertExpectedInline str ep_training graph strip \ graph p_p num_users= = placeholder target=p_p p_p num_users= = placeholder target=p_p b_b num_users= = placeholder target=b_b x num_users= = placeholder target=x mul num_users= = call_function target=torch ops aten mul Tensor args = p_p kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mul p_p kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add b_b kwargs = sum_ num_users= = call_function target=torch ops aten sum default args = add_ kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = sum_ kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = access_subclass_inner_tensor_default_ b kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x access_subclass_inner_tensor_default_ kwargs = add_ ep = export m ref_x assertTrue torch allclose ep module ref_x ref_out test_subclass_nested_attr_access_submodule Bar torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter TwoTensor TwoTensor torch ones torch ones TwoTensor torch ones torch ones b = torch nn Buffer TwoTensor TwoTensor torch ones torch ones TwoTensor torch ones torch ones forward x x Foo torch nn Module __init__ super __init__ bar = Bar forward x res = bar p + bar p + bar b sum x + res get_elem_a b m = Foo ref_x = torch randn ref_out = m ref_x ep_training = torch export export m ref_x strict=False assertExpectedInline str ep_training graph strip \ graph p_bar_p num_users= = placeholder target=p_bar_p p_bar_p num_users= = placeholder target=p_bar_p b_bar_b num_users= = placeholder target=b_bar_b x num_users= = placeholder target=x mul num_users= = call_function target=torch ops aten mul Tensor args = p_bar_p kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mul p_bar_p kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add b_bar_b kwargs = sum_ num_users= = call_function target=torch ops aten sum default args = add_ kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = sum_ kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = access_subclass_inner_tensor_default_ b kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x access_subclass_inner_tensor_default_ kwargs = add_ ep = export m ref_x assertTrue torch allclose ep module ref_x ref_out test_subclass_nested_attr_access_const_metadata Foo torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter ConstantExtraMetadataTensor ConstantExtraMetadataTensor torch ones forward x res = p + p res = res + res constant_attribute x + res elem elem m = Foo ref_x = torch randn ref_out = m ref_x ep_training = torch export export m ref_x strict=False assertExpectedInline str ep_training graph strip \ graph p_p num_users= = placeholder target=p_p p_p num_users= = placeholder target=p_p x num_users= = placeholder target=x mul num_users= = call_function target=torch ops aten mul Tensor args = p_p kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mul p_p kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = add_ elem kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = access_subclass_inner_tensor_default_ elem kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x access_subclass_inner_tensor_default_ kwargs = add_ ep = export m ref_x assertTrue torch allclose ep module ref_x ref_out test_subclass_nested_attr_access_const_metadata_not_top_level Foo torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter ConstantExtraMetadataTensor ConstantExtraMetadataTensor torch ones forward x res = p + p res = res + res constant_attribute x + res elem elem m = Foo ref_x = torch randn ref_out = m ref_x ep_training = torch export export m ref_x strict=False assertExpectedInline str ep_training graph strip \ graph p_p num_users= = placeholder target=p_p p_p num_users= = placeholder target=p_p x num_users= = placeholder target=x mul num_users= = call_function target=torch ops aten mul Tensor args = p_p kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mul p_p kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add kwargs = getattr_ num_users= = call_function target=builtins getattr args = add_ elem kwargs = getattr_ num_users= = call_function target=builtins getattr args = getattr_ elem kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x getattr_ kwargs = add_ ep = export m ref_x assertTrue torch allclose ep module ref_x ref_out test_subclass_nested_attr_access_const_metadata_not_top_level Foo torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter TwoTensor ConstantExtraMetadataTensor torch ones ConstantExtraMetadataTensor torch ones forward x res = p + p res = res + res elem + res b constant_attribute x + res elem m = Foo ref_x = torch randn ref_out = m ref_x ep_training = torch export export m ref_x strict=False assertExpectedInline str ep_training graph strip \ graph p_p num_users= = placeholder target=p_p p_p num_users= = placeholder target=p_p x num_users= = placeholder target=x mul num_users= = call_function target=torch ops aten mul Tensor args = p_p kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mul p_p kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = add kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = access_subclass_inner_tensor_default_ elem kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add access_subclass_inner_tensor_default_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add_ kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = add_ kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = access_subclass_inner_tensor_default_ elem kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x access_subclass_inner_tensor_default_ kwargs = add_ ep = export m ref_x assertTrue torch allclose ep module ref_x ref_out test_subclass_nested_attr_access_complicated_metadata Foo torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter ConstantExtraMetadataTensor ConstantExtraMetadataTensor torch ones forward x res = x + p + p res elem elem + p get_complicated_metadata foo m = Foo ref_x = torch randn ref_out = m ref_x ep_training = torch export export m ref_x strict=False assertExpectedInline str ep_training graph strip \ graph p_p num_users= = placeholder target=p_p p_p num_users= = placeholder target=p_p x num_users= = placeholder target=x mul num_users= = call_function target=torch ops aten mul Tensor args = p_p kwargs = add num_users= = call_function target=torch ops aten add Tensor args = x mul kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add p_p kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = add_ elem kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = access_subclass_inner_tensor_default_ elem kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = access_subclass_inner_tensor_default_ kwargs = add_ ep = export m ref_x assertTrue torch allclose ep module ref_x ref_out test_real_tensor_errors_on_aliasing_custom_op torch library custom_op export foo_alias mutates_args= foo x torch Tensor - torch Tensor x Foo torch nn Module forward x torch ops export foo_alias x model = Foo inputs = torch randn error_type = RuntimeError is_non_strict_test _testMethodName torch _dynamo exc TorchRuntimeError assertRaisesRegex error_type r The output custom operator \ \ must also input r custom operator \ \ may alias any inputs r custom operator other returns torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs test_real_tensor_bool_cast Foo torch nn Module forward x bool x eq any model = Foo inputs = torch randn torch _functorch config patch fake_tensor_propagate_real_tensors=True ep = export model inputs strict=False test_is_nonzero Foo torch nn Module forward x torch is_nonzero x _long_tensor nz torch full int nz _float_tensor nz torch full int nz dtype=torch float _bool_tensor nz torch full int nz bool mod = Foo _tensor _long_tensor _float_tensor _bool_tensor local_scalar_dense complex NYI fake tensors torch _functorch config patch fake_tensor_propagate_real_tensors=True nz True False sample_input = _tensor nz=nz ep = export mod sample_input strict=False assertEqual ep module sample_input nz test_export_script_module Foo torch nn Module forward rv torch Tensor t torch Tensor i = t item rv + i foo = Foo foo_script = torch jit script foo inp = torch zeros torch tensor assertRaisesRegex ValueError Exporting ScriptModule supported export foo_script inp torch _export converter TS EPConverter TS EPConverter foo_script inp convert test_dim_auto_and_dim test basic Dims Foo torch nn Module forward x y x - y inputs = torch randn torch randn shapes = x Dim AUTO Dim d min= y Dim d max= Dim DYNAMIC ep = export Foo inputs dynamic_shapes=shapes x y = node node ep graph nodes node op == placeholder assertEqual s = x meta val shape y meta val shape assertEqual s = x meta val shape y meta val shape vr = ep range_constraints s node expr vr = ep range_constraints s node expr assertEqual vr upper vr lower test derived Dims Bar torch nn Module forward x y z x + y + z inputs = torch randn torch randn torch randn dx = Dim dx min= max= shapes = x dx y dx + z Dim AUTO ep = export Bar inputs dynamic_shapes=shapes x y z = node node ep graph nodes node op == placeholder assertEqual s = x meta val shape z meta val shape expr = y meta val shape free_symbols = expr node expr free_symbols assertEqual len free_symbols assertEqual next iter free_symbols s node expr test specialization still complains inputs = torch randn torch randn shapes = x Dim STATIC y Dim dy assertRaisesRegex torch _dynamo exc UserError r You marked your code specialized constant r If you re using Dim DYNAMIC replace either Dim STATIC Dim AUTO export Foo inputs dynamic_shapes=shapes test_issue_ MyModule torch nn Module __init__ super MyModule __init__ forward causal_mask fill_value causal_mask = causal_mask clone mask_length = fill_value shape - causal_mask mask_length = fill_value causal_mask causal_mask = torch randn fill_value = torch randn dynamic_shapes = causal_mask Dim M fill_value Dim N ep = export MyModule causal_mask fill_value dynamic_shapes=dynamic_shapes is_training_ir_test _testMethodName is_retracebility_test _testMethodName assertExpectedInline str ep graph_module code strip \ forward causal_mask fill_value sym_size_int_ = torch ops aten sym_size int fill_value clone = torch ops aten clone default causal_mask causal_mask = None slice_ = torch ops aten slice Tensor clone sym_size_int_ sym_size_int_ = None copy_ = torch ops aten copy_ default slice_ fill_value slice_ = fill_value = copy_ = None clone decomposed_ep = ep run_decompositions assertExpectedInline str decomposed_ep graph_module code strip \ forward causal_mask fill_value sym_size_int_ = torch ops aten sym_size int fill_value clone = torch ops aten clone default causal_mask causal_mask = None slice_ = torch ops aten slice Tensor clone sym_size_int_ copy = torch ops aten copy default slice_ fill_value slice_ = fill_value = None slice_scatter = torch ops aten slice_scatter default clone copy sym_size_int_ clone = copy = sym_size_int_ = None slice_scatter test_dim_dynamic_specialization Foo torch nn Module forward x x + specialization assertRaisesRegex ValueError r Received user-specified dim hint Dim DYNAMIC r export specialized due hint dimension r inputs\ x \ \ shape\ \ \n r Received user-specified dim hint Dim DYNAMIC r export specialized due hint dimension r inputs\ x \ \ shape\ \ export Foo torch randn dynamic_shapes= x Dim DYNAMIC Dim DYNAMIC Bar torch nn Module forward x assert x shape = x + static specialization assertRaisesRegex ValueError r Received user-specified dim hint Dim DYNAMIC r tracing inferred static shape dimension r inputs\ x \ \ shape\ \ \n export Bar torch randn dynamic_shapes= x Dim DYNAMIC min= test_unbacked_slice_forward Foo torch nn Module forward x xs u u = xs tolist out = x u u out x = torch randn idxs = torch tensor mod = Foo ep = export mod x idxs xs idxs torch tensor - - torch tensor - torch tensor - assertTrue torch allclose ep module x xs mod x xs check unbacked bindings should symbols u u output size output storage offset bound_unbacked = set node ep graph nodes bound_unbacked &#124; = node meta get unbacked_bindings keys assertEqual len bound_unbacked test_dim_hint_ranges Foo torch nn Module forward x y x + y inputs = torch randn torch randn shapes = x Dim AUTO min= Dim AUTO y Dim DYNAMIC max= Dim AUTO max= ep = export Foo inputs dynamic_shapes=shapes ep module torch randn torch randn assertRaisesRegex AssertionError escape Guard failed x size = expected = got ep module torch randn torch randn assertRaisesRegex AssertionError escape Guard failed x size = expected = got ep module torch randn torch randn assertRaisesRegex AssertionError escape Guard failed x size = expected = got ep module torch randn torch randn test_dim_hint_range_violations Foo torch nn Module forward xs x y = xs data assert y shape = x y + x y = torch randn torch randn conflict lower bound shapes = torch export ShapesCollection shapes x = Dim DYNAMIC max= assertRaisesRegex ValueError r Received user-specified \ None \ conflicting inferred r \ int_oo\ inputs\ xs \ \ data \ \ \ \ \ \ shape\ \ export Foo data x y dynamic_shapes=shapes conflict upper bound shapes = torch export ShapesCollection shapes y = Dim AUTO min= max= assertRaisesRegex ValueError r Received user-specified \ \ conflicting inferred r \ \ inputs\ xs \ \ data \ \ \ \ \ \ shape\ \ export Foo data x y dynamic_shapes=shapes Bar torch nn Module forward x x + conflict static range shapes = x Dim STATIC min= max= assertRaisesRegex ValueError r Received user-specified \ \ conflicting inferred r \ \ inputs\ x \ shape\ \ export Bar torch randn dynamic_shapes=shapes multiple conflicts Moo torch nn Module forward x y assert x shape = assert y shape = x + y + inps = torch randn torch randn shapes = x Dim DYNAMIC min= y Dim DYNAMIC max= assertRaisesRegex ValueError r Received user-specified \ None\ conflicting inferred r \ \ inputs\ x \ shape\ \ \n r Received user-specified \ None \ conflicting inferred r \ int_oo\ inputs\ y \ shape\ \ export Moo inps dynamic_shapes=shapes test_torch_fn M torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU forward x x = linear x x = linear x x = relu x x = x + x x ep = export M torch randn run_decompositions expected_result = linear_ builtin_function_or_method linear linear_ builtin_function_or_method linear linear_ builtin_function_or_method linear linear_ builtin_function_or_method linear relu_ function relu add_ method_descriptor add actual_result = i node enumerate ep graph nodes node op == call_function actual_result append node meta get torch_fn assertEqual actual_result expected_result M torch nn Module __init__ - None super __init__ forward x weight bias x = torch nn functional linear x weight bias x = torch nn functional relu x x = torch add x x x ep = export M torch randn torch randn torch randn run_decompositions expected_result = linear_ builtin_function_or_method linear linear_ builtin_function_or_method linear relu_ function relu add_ builtin_function_or_method add actual_result = i node enumerate ep graph nodes node op == call_function actual_result append node meta get torch_fn assertEqual actual_result expected_result test_hoo_inline_users_issue This came issue where replace_with_hop passes would inline subgraphs mess up node users nodes present multiple subgraphs e g _x SetGradCase below since s used both set_grad_enabled HOO modules This checks node users node args correspondence check_users_for_graph graph _tuple_contains _tuple val check nested since output node args have format x y any _tuple_contains x val isinstance x tuple x == val x _tuple node graph nodes check node users user node users keys assert _tuple_contains user args node check node args arg node args isinstance arg torch fx Node assert _tuple_contains arg users node check set grad enabled SetGradCase torch nn Module forward x _x = x shape + _xx = _x + torch no_grad y = _x _xx y ep = export SetGradCase torch randn dynamic_shapes= x Dim dx strict=False check_users_for_graph ep graph test_export_custom_op_lib ops_registered_before = set torch ops mylib Assert warning CompositeImplicitAutograd op torch library _scoped_library mylib FRAGMENT lib lib define foo Tensor x - Tensor lib impl foo lambda x x sin CompositeImplicitAutograd ops_registered_after = set torch ops mylib assertEqual ops_registered_after ops_registered_before test_export_preserve_linear_but_not_custom_op table = torch export default_decompositions del table torch ops aten linear default torch library _scoped_library mylib FRAGMENT lib lib define foo Tensor x - Tensor lib impl foo lambda x x sin CompositeImplicitAutograd Bar torch nn Module __init__ super __init__ linear = torch nn Linear forward x lin = linear x torch ops mylib foo lin x = torch randn ep = export Bar x run_decompositions table assertExpectedInline str ep graph_module code strip \ forward p_linear_weight p_linear_bias x linear = torch ops aten linear default x p_linear_weight p_linear_bias x = p_linear_weight = p_linear_bias = None sin = torch ops aten sin default linear linear = None sin test_export_preserve_linear_at_aot_level Foo torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x x = linear x torch ops aten chunk default x ep = torch export export Foo torch randn decomp_table = default_decompositions del decomp_table torch ops aten linear default ep = ep run_decompositions decomp_table gm = ep graph_module linear CompositeImplicitAutograd functional op so we should preserve chunk CompositeImplicitAutograd non-functional op we decompose assertExpectedInline str gm code strip \ forward p_linear_weight p_linear_bias x linear = torch ops aten linear default x p_linear_weight p_linear_bias x = p_linear_weight = p_linear_bias = None split_with_sizes = torch ops aten split_with_sizes default linear linear = None getitem = split_with_sizes getitem_ = split_with_sizes getitem_ = split_with_sizes split_with_sizes = None getitem getitem_ getitem_ test_export_cond_preserve_torch_fn_for_subgraphs MySubModule torch nn Module foo x x cos forward x foo x CondBranchClassMethod torch nn Module __init__ - None super __init__ subm = MySubModule bar x x sin forward x cond x sum = subm forward bar x example_inputs = torch randn m = CondBranchClassMethod m eval gm = export m example_inputs module actual_torch_fns = mod gm modules hasattr mod graph node mod graph nodes node name sin cos torch_fn = node meta get torch_fn print torch_fn actual_torch_fns append torch_fn exp_torch_fns = cos_ method_descriptor cos sin_ method_descriptor sin assertEqual actual_torch_fns exp_torch_fns test_is_exporting Mod torch nn Module forward pred x f x x sin torch compiler is_exporting x cos y = f x true_fn x f x - torch compiler is_exporting f x + false_fn x f x + torch compiler is_exporting f x - torch cond pred true_fn false_fn x y ep = export Mod torch tensor False torch randn FileCheck check_count torch ops aten sin exactly=True run ep graph_module code FileCheck check_count torch ops higher_order cond exactly=True run ep graph_module code True graph should contain sin sub FileCheck check_count torch ops aten sub exactly=True run ep graph_module true_graph_ code FileCheck check_count torch ops aten sin exactly=True run ep graph_module true_graph_ code False graph should contain sin add FileCheck check_count torch ops aten add exactly=True run ep graph_module false_graph_ code FileCheck check_count torch ops aten sin exactly=True run ep graph_module false_graph_ code test_ends_of_bounds_oblivious Foo torch nn Module __init__ super __init__ register_buffer buf torch zeros forward x y buf x shape = x x + y inps = torch randn torch randn dynamic_shapes = x Dim dx min= max= y Dim dy Dim dy torch fx experimental _config patch backed_size_oblivious=True ep = export Foo inps dynamic_shapes=dynamic_shapes ep module torch randn torch randn ep module torch randn torch randn test_colin_unbacked_backed_vr_sub Model torch nn Module forward b c nz = torch nonzero ones = new_ones nz size b size torch _check ones size = equals = torch add ones c equals model = Model example_inputs = torch ones torch randn torch randn dynamic_shapes = None b None c Dim DYNAMIC Dim STATIC torch fx experimental _config patch backed_size_oblivious=True ep = export model example_inputs dynamic_shapes=dynamic_shapes check lower bound sym vr ep range_constraints items str sym u s assertEqual vr lower test_duplicate_modules_with_non_persistent_buffers FooWithBuf torch nn Module __init__ super __init__ register_buffer buf torch randn persistent=False forward x x + buf BarWithFoo torch nn Module __init__ foo super __init__ foo = foo forward x foo x ModWith Bars torch nn Module __init__ super __init__ foo = FooWithBuf b = BarWithFoo foo b = BarWithFoo foo forward x b x + b x mod = ModWith Bars inputs = torch randn ep = export mod inputs assertTrue torch allclose ep module inputs mod inputs test_derived_dim_basic Foo torch nn Module forward x y x + y foo = Foo x y = torch randn torch randn dimx = torch export Dim dimx min= max= dimy = torch export Dim dimy min= max= doesn t work assertRaisesRegex torch _dynamo exc UserError Constraints violated \\ dimy\\ \n The values dimy must always related values dimx \n Suggested fixes \n dimy = dimx \\+ export foo x y dynamic_shapes= dimx dimy dimy = dimx doesn t work assertRaisesRegex torch _dynamo exc UserError Expected input size equal \\ dimx where dimx = got export foo x y dynamic_shapes= dimx dimy dimy = dimx + works ep = export foo x y dynamic_shapes= dimx dimy assertRaisesRegex AssertionError escape Guard failed x size == - + y size expected got ep module torch randn torch randn assertEqual ep module torch randn torch randn size test_derived_dim_nested Foo torch nn Module forward x y x + y foo = Foo x y = torch randn torch randn dimx = torch export Dim dimx min= max= dimy = dimx + works ep = export foo x y dynamic_shapes= dimx dimy assertEqual ep module torch randn torch randn size Foo torch nn Module forward z y z + y foo = Foo z y = torch randn torch randn dimz = dimx dimy = dimx - works ep = export foo z y dynamic_shapes= dimz dimy assertEqual ep module torch randn torch randn size dimz = dimx + dimy = dimx - doesn t work assertRaisesRegex torch _dynamo exc UserError Expected input size equal \\ dimx - where dimx = got export foo z y dynamic_shapes= dimz dimy dimy = dimx + works ep = export foo z y dynamic_shapes= dimz dimy assertRaisesRegex AssertionError escape Guard failed z size = expected = got ep module torch randn torch randn assertRaisesRegex AssertionError escape Guard failed - + z size == y size expected got ep module torch randn torch randn assertEqual ep module torch randn torch randn size test_derived_dim_integer Foo torch nn Module forward w w shape == w w - foo = Foo w = torch randn dimx = torch export Dim dimx min= max= dimw = dimx + doesn t work assertRaisesRegex torch _dynamo exc UserError Expected shape = input Tensor form \\ dimx \\+ where dimx integer export foo w dynamic_shapes= dimw dimw = dimx works ep = export foo w dynamic_shapes= dimw assertRaisesRegex AssertionError escape Guard failed w size == expected got ep module torch randn assertEqual ep module torch randn size assertRaisesRegex AssertionError escape Guard failed w size = expected = got ep module torch randn test_derived_dim_repeat_derived Foo torch nn Module forward u v u + v foo = Foo u v = torch randn torch randn dimx = torch export Dim dimx min= max= dimw = dimx works ep = export foo u v dynamic_shapes= dimw dimw assertEqual ep module torch randn torch randn size test_derived_dim_out_of_order dimy = torch export Dim dimy min= max= dimx = dimy - out order effectively dimy = dimx + dimz = dimy + out order effectively dimz = dimx + Foo torch nn Module forward x y z x + y + z foo = Foo u v w = torch randn torch randn torch randn ep = export foo u v w dynamic_shapes= dimx dimy dimz assertRaisesRegex AssertionError escape Guard failed z size = expected got ep module torch randn torch randn torch randn assertEqual ep module torch randn torch randn torch randn size test_derived_dim_out_of_order_repeat_derived dimy = torch export Dim dimy min= max= dimx = dimy - out order effectively dimy = dimx + dimz = dimy + out order effectively dimz = dimx + dimx = dimx dimx = dimz - works effectively = dimx Foo torch nn Module forward x y z x x x + y + z + x + x foo = Foo u v w u u = torch randn torch randn torch randn torch randn torch randn ep = export foo u v w u u dynamic_shapes= dimx dimy dimz dimx dimx assertRaisesRegex AssertionError escape Guard failed x size == x size expected got ep module torch randn torch randn torch randn torch randn torch randn assertEqual ep module torch randn torch randn torch randn torch randn torch randn size ep = export foo u v w u u reused inputs dynamic_shapes= dimx dimy dimz dimx dimx assertRaisesRegex AssertionError escape Guard failed x size == x size expected got ep module torch randn torch randn torch randn torch randn torch randn assertEqual ep module torch randn torch randn torch randn torch randn torch randn size test_specialize_derived_dim_roots dim derived dim both specialize Foo torch nn Module forward x y x reshape - + y dy = Dim dy min= x y = torch randn torch randn dynamic_shapes = x dy - y dy try export Foo x y dynamic_shapes=dynamic_shapes raise Exception export call should have failed dynamic shapes error except torch _dynamo exc UserError exc expected_error_msg = Specializations unexpectedly required \ dy\ \n solving guards generated dy - resulted specialized value \n Suggested fixes \n dy = \n assertTrue re search expected_error_msg exc args None assertTrue dy - = exc args don t suggest fix non-root dim unittest skip See https github com pytorch pytorch issues test_keep_composite_ops_invalid Foo torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x x = linear x torch ops aten chunk default x _ args kwargs NotImplemented assertWarnsRegex UserWarning The op aten chunk default _ = torch export export Foo torch randn run_decompositions torch ops aten chunk default _ assertWarnsRegex UserWarning The op aten sym_size default _ = torch export export Foo torch randn run_decompositions torch ops aten sym_size default _ assertWarnsRegex UserWarning The op aten native_batch_norm default _ = torch export export Foo torch randn run_decompositions torch ops aten native_batch_norm default _ test_keep_composite_ops_linear_convd MyLinear torch nn Module __init__ - None super __init__ weight = torch randn bias = torch randn forward x torch nn functional linear x weight bias Foo torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv d = torch nn Conv d linear = MyLinear forward x y x_conv = conv x y_conv_ d = conv d y x_linear = linear x_conv x_linear cos + y_conv_ d sum ep = torch export export Foo torch randn torch randn ep_has_linear_convd = ep run_decompositions assertExpectedInline str ep_has_linear_convd graph_module code strip \ forward p_conv_weight p_conv_bias p_conv d_weight p_conv d_bias c_linear_weight c_linear_bias x y conv d = torch ops aten conv d default x p_conv_weight p_conv_bias x = p_conv_weight = p_conv_bias = None conv d = torch ops aten conv d default y p_conv d_weight p_conv d_bias y = p_conv d_weight = p_conv d_bias = None linear = torch ops aten linear default conv d c_linear_weight c_linear_bias conv d = c_linear_weight = c_linear_bias = None cos = torch ops aten cos default linear linear = None sum_ = torch ops aten sum default conv d conv d = None add = torch ops aten add Tensor cos sum_ cos = sum_ = None add decomp_table = default_decompositions del decomp_table torch ops aten conv d default del decomp_table torch ops aten conv d default ep_has_convd = ep run_decompositions decomp_table=decomp_table assertExpectedInline str ep_has_convd graph_module code strip \ forward p_conv_weight p_conv_bias p_conv d_weight p_conv d_bias c_linear_weight c_linear_bias x y conv d = torch ops aten conv d default x p_conv_weight p_conv_bias x = p_conv_weight = p_conv_bias = None conv d = torch ops aten conv d default y p_conv d_weight p_conv d_bias y = p_conv d_weight = p_conv d_bias = None view = torch ops aten view default conv d conv d = None permute = torch ops aten permute default c_linear_weight c_linear_weight = None addmm = torch ops aten addmm default c_linear_bias view permute c_linear_bias = view = permute = None view_ = torch ops aten view default addmm addmm = None cos = torch ops aten cos default view_ view_ = None sum_ = torch ops aten sum dim_IntList conv d conv d = None add = torch ops aten add Tensor cos sum_ cos = sum_ = None add decomp_table = default_decompositions del decomp_table torch ops aten conv d default ep_has_convd = ep_has_convd run_decompositions decomp_table=decomp_table assertExpectedInline str ep_has_convd graph_module code strip \ forward p_conv_weight p_conv_bias p_conv d_weight p_conv d_bias c_linear_weight c_linear_bias x y conv d = torch ops aten conv d default x p_conv_weight p_conv_bias x = p_conv_weight = p_conv_bias = None convolution = torch ops aten convolution default y p_conv d_weight p_conv d_bias False y = p_conv d_weight = p_conv d_bias = None view = torch ops aten view default conv d conv d = None permute = torch ops aten permute default c_linear_weight c_linear_weight = None addmm = torch ops aten addmm default c_linear_bias view permute c_linear_bias = view = permute = None view_ = torch ops aten view default addmm addmm = None cos = torch ops aten cos default view_ view_ = None sum_ = torch ops aten sum dim_IntList convolution convolution = None add = torch ops aten add Tensor cos sum_ cos = sum_ = None add test_keep_composite_ops_linear_convd_for_training_ir MyLinear torch nn Module __init__ - None super __init__ weight = torch nn Buffer torch randn bias = torch nn Buffer torch randn forward x torch nn functional linear x weight bias Foo torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv d = torch nn Conv d linear = MyLinear forward x y x_conv = conv x y_conv_ d = conv d y x_linear = linear x_conv x_linear cos + y_conv_ d sum ep = torch export export Foo torch randn torch randn ep_has_linear_convd = ep run_decompositions decomp_table= assertExpectedInline str ep_has_linear_convd graph_module code strip \ forward p_conv_weight p_conv_bias p_conv d_weight p_conv d_bias b_linear_weight b_linear_bias x y conv d = torch ops aten conv d default x p_conv_weight p_conv_bias x = p_conv_weight = p_conv_bias = None conv d = torch ops aten conv d default y p_conv d_weight p_conv d_bias y = p_conv d_weight = p_conv d_bias = None linear = torch ops aten linear default conv d b_linear_weight b_linear_bias conv d = b_linear_weight = b_linear_bias = None cos = torch ops aten cos default linear linear = None sum_ = torch ops aten sum default conv d conv d = None add = torch ops aten add Tensor cos sum_ cos = sum_ = None add decomp_table = default_decompositions del decomp_table torch ops aten conv d default del decomp_table torch ops aten conv d default ep_has_convd = ep run_decompositions decomp_table=decomp_table assertExpectedInline str ep_has_convd graph_module code strip \ forward p_conv_weight p_conv_bias p_conv d_weight p_conv d_bias b_linear_weight b_linear_bias x y conv d = torch ops aten conv d default x p_conv_weight p_conv_bias x = p_conv_weight = p_conv_bias = None conv d = torch ops aten conv d default y p_conv d_weight p_conv d_bias y = p_conv d_weight = p_conv d_bias = None view = torch ops aten view default conv d conv d = None permute = torch ops aten permute default b_linear_weight b_linear_weight = None addmm = torch ops aten addmm default b_linear_bias view permute b_linear_bias = view = permute = None view_ = torch ops aten view default addmm addmm = None cos = torch ops aten cos default view_ view_ = None sum_ = torch ops aten sum dim_IntList conv d conv d = None add = torch ops aten add Tensor cos sum_ cos = sum_ = None add decomp_table = default_decompositions del decomp_table torch ops aten conv d default ep_has_convd = ep_has_convd run_decompositions decomp_table=decomp_table assertExpectedInline str ep_has_convd graph_module code strip \ forward p_conv_weight p_conv_bias p_conv d_weight p_conv d_bias b_linear_weight b_linear_bias x y conv d = torch ops aten conv d default x p_conv_weight p_conv_bias x = p_conv_weight = p_conv_bias = None convolution = torch ops aten convolution default y p_conv d_weight p_conv d_bias False y = p_conv d_weight = p_conv d_bias = None view = torch ops aten view default conv d conv d = None permute = torch ops aten permute default b_linear_weight b_linear_weight = None addmm = torch ops aten addmm default b_linear_bias view permute b_linear_bias = view = permute = None view_ = torch ops aten view default addmm addmm = None cos = torch ops aten cos default view_ view_ = None sum_ = torch ops aten sum dim_IntList convolution convolution = None add = torch ops aten add Tensor cos sum_ cos = sum_ = None add unittest skip See https github com pytorch pytorch issues test_error_when_passing_mutating_primitive_op Foo torch nn Module forward x x sin ep = export Foo torch ones assertWarnsRegex UserWarning The op aten index_put_ default ep run_decompositions torch ops aten index_put_ default None test_export_cond_warns_constant_pred Mod torch nn Module forward pred x torch cond pred lambda x x sin lambda x x cos x mod = Mod assertWarnsRegex UserWarning Pred Python constant ep = export mod True torch randn nodes = ep module graph find_nodes op= call_function target=torch ops aten sin default assertEqual len nodes test_export_custom_decomp_table_basic_pop torch library _scoped_library mylib FRAGMENT lib lib define foo Tensor x - Tensor lib impl foo lambda x x sin CompositeImplicitAutograd lib define foo Tensor x - Tensor lib impl foo lambda x x sin CompositeImplicitAutograd table = default_decompositions Since table hasn t been materialized yet we shouldn t error val = table pop torch ops mylib foo default assertIsNotNone val assertRaisesRegex KeyError mylib foo default table pop torch ops mylib foo default val = table pop torch ops mylib foo default HELLO assertEqual val HELLO all_ops = set k k v table items assertTrue table has_materialized When we force materialize torch ops mylib foo default should have gone assertFalse torch ops mylib foo default all_ops assertTrue torch ops mylib foo default all_ops test_export_custom_decomp_table_container_methods tests __len__ torch library _scoped_library mylib FRAGMENT lib table = default_decompositions length_before = len table lib define foo Tensor x - Tensor lib impl foo lambda x x sin CompositeImplicitAutograd lib define foo Tensor x - Tensor lib impl foo lambda x x sin CompositeImplicitAutograd table = default_decompositions assertEqual len table - length_before tests __contains__ torch library _scoped_library mylib FRAGMENT lib lib define foo Tensor x - Tensor lib impl foo lambda x x sin CompositeImplicitAutograd table = default_decompositions assertTrue torch ops mylib foo default table del table torch ops mylib foo default assertFalse torch ops mylib foo default table Lot ppl do op all_ops op table del table op torch library _scoped_library mylib FRAGMENT lib lib define foo Tensor x - Tensor lib impl foo lambda x x sin CompositeImplicitAutograd table = default_decompositions torch ops mylib foo default table del table torch ops mylib foo default assertFalse torch ops mylib foo default table table materialize assertFalse torch ops mylib foo default table test_if_post_autograd_op_preserved Foo torch nn Module forward x x sin + x sum ep = export Foo torch ones decomp_table = default_decompositions del decomp_table torch ops aten sum default ep_preserve_sum = ep run_decompositions decomp_table Even though we decomposing core aten which should make sum into sum dim_IntList we explicitly marked do assertExpectedInline str ep_preserve_sum graph_module code strip \ forward x sin = torch ops aten sin default x sum_ = torch ops aten sum default x x = None add = torch ops aten add Tensor sin sum_ sin = sum_ = None add ep_no_preserve_sum = ep run_decompositions assertExpectedInline str ep_no_preserve_sum graph_module code strip \ forward x sin = torch ops aten sin default x sum_ = torch ops aten sum dim_IntList x x = None add = torch ops aten add Tensor sin sum_ sin = sum_ = None add test_set_grad_empty M torch nn Module forward x torch no_grad x = x + x None ep = export M torch ones inp = torch randn assertTrue torch allclose ep module inp inp + test_set_grad_as_side_effect Foo torch nn Module forward x torch _C _set_grad_enabled False x sum before = torch is_grad_enabled ep = torch export export Foo torch randn after = torch is_grad_enabled assertEqual before after test_derived_dim_out_of_order_simplified _dimz = torch export Dim _dimz min= max= dimy = _dimz - dimx = dimy - dimz = torch export Dim dimz min= max= doesn t work should = _dimz Foo torch nn Module forward x y z x + y + z foo = Foo u v w = torch randn torch randn torch randn try export foo u v w dynamic_shapes= dimx dimy dimz except torch _dynamo exc UserError exc expected_error_msg = Constraints violated \ dimz\ \n The values dimz must always related values _dimz - \n Suggested fixes \n dimz = _dimz assertTrue re search expected_error_msg exc args None don t suggest fix non-root dims no need update root here assertTrue _dimz - = Dim exc args assertTrue _dimz - = _dimz - exc args assertTrue _dimz = Dim exc args dimz = dimx + works effectively = _dimz ep = export foo u v w dynamic_shapes= dimx dimy dimz assertRaisesRegex AssertionError escape Guard failed z size = expected got ep module torch randn torch randn torch randn assertEqual ep module torch randn torch randn torch randn size test_simple_export_for_training Foo torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x eager_model = Foo ep_for_training = torch export export eager_model torch ones assertExpectedInline str ep_for_training graph_module code strip \ forward p_linear_weight p_linear_bias x linear = torch ops aten linear default x p_linear_weight p_linear_bias x = p_linear_weight = p_linear_bias = None linear gm = ep_for_training module assertExpectedInline str gm code strip \ forward x x = fx_pytree tree_flatten_spec x _in_spec linear_weight = linear weight linear_bias = linear bias _guards_fn = _guards_fn x _guards_fn = None linear = torch ops aten linear default x linear_weight linear_bias x = linear_weight = linear_bias = None pytree tree_unflatten linear _out_spec assertTrue torch allclose gm torch ones eager_model torch ones test_export_for_training_with_mutation Foo torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones forward x x add_ buffer add_ x + buffer eager_model_for_export = Foo eager_model_for_testing = Foo ep_for_training = torch export export eager_model_for_export torch ones assertExpectedInline str ep_for_training graph_module code strip \ forward b_buffer x add_ = torch ops aten add_ Tensor x x = None add__ = torch ops aten add_ Tensor b_buffer b_buffer = None add = torch ops aten add Tensor add_ add__ add_ = add__ = None add gm = ep_for_training module assertExpectedInline str gm code strip \ forward x x = fx_pytree tree_flatten_spec x _in_spec buffer = buffer _guards_fn = _guards_fn x _guards_fn = None add_ = torch ops aten add_ Tensor x x = None add__ = torch ops aten add_ Tensor buffer buffer = None add = torch ops aten add Tensor add_ add__ add_ = add__ = None pytree tree_unflatten add _out_spec assertTrue torch allclose gm torch ones eager_model_for_testing torch ones test_export_for_training_with_dynamic_shapes Foo torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones forward x x add_ buffer add_ x + buffer sum eager_model_for_export_training = Foo eager_model_for_export_inference = Foo eager_model_for_testing = Foo ep_for_training = torch export export eager_model_for_export_training torch ones dynamic_shapes= Dim x assertTrue torch allclose ep_for_training module torch ones eager_model_for_testing torch ones ep_for_real = export eager_model_for_export_inference torch ones dynamic_shapes= Dim x Since symbol names based hash source names these differ across inference training we do range comparisons instead assertEqual str ep_for_training range_constraints values str ep_for_real range_constraints values test_unbacked_unsqueeze Unsqueeze torch nn Module forward xs u u = xs tolist x = torch zeros u + u contiguous x unsqueeze - mod = Unsqueeze x = torch tensor ep = export mod x strict=False assertTrue torch allclose mod x ep module x x = torch tensor assertTrue torch allclose mod x ep module x test_export_for_training_with_container_type Foo torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones forward container x = container y = container x add_ y add_ x + y + buffer sum eager_model = Foo ep_for_training = torch export export eager_model torch ones torch ones assertTrue torch allclose ep_for_training module torch ones torch ones eager_model torch ones torch ones test_function_holding_tensor global_storage = FunctionClosureLeak torch nn Module forward x fake_tensor = x + In real export would FakeTensor closure fake_tensor shape Captures fake_tensor Store closure globally - creates leak global_storage append closure x sin torch _export config patch detect_non_strict_fake_tensor_leaks=True assertWarnsRegex UserWarning Detected fake tensors still alive after export export FunctionClosureLeak torch randn strict=False test_detect_leak_nonstrict Foo torch nn Module __init__ super __init__ forward x y x + y global_list = ReferenceControl __init__ mod bank = bank_dict = mod = mod hacked_up_forward self_ x y bank append x clone bank_dict x = x clone global_list append x clone x + y mod forward = hacked_up_forward __get__ mod Foo __call__ x y ep = export mod x y strict=False module out = ep x y out update bank foo = Foo ref = ReferenceControl foo ref torch randn torch randn assertTrue isinstance ref bank torch _subclasses fake_tensor FakeTensor torch _export config patch detect_non_strict_fake_tensor_leaks=True assertWarnsRegex UserWarning Detected fake tensors still alive after export ref torch randn torch randn test_detect_leak_nonstrict_with_stacktrace global_list = Foo torch nn Module __init__ super __init__ forward x y nonlocal global_list global_list append x + y x + y foo = Foo ep = export foo torch randn torch randn strict=False assertTrue isinstance global_list torch _subclasses fake_tensor FakeTensor torch _export config patch detect_non_strict_fake_tensor_leaks=True warn_re = re compile r Detected\s+\d+\s+fake\s+tensors r test_export\ py global_list\ append\ x \+ y\ re S assertWarnsRegex UserWarning warn_re ep = export foo torch randn torch randn strict=False test_export_cyclic_reference_leak Node __init__ tag tag = tag ref = None tensor = None bank = LeakyCycle torch nn Module __init__ super __init__ forward x y z = x + y node = Node A node = Node B node ref = node node ref = node node tensor = z Keep cycle alive intentionally - leak nonlocal bank bank append node z sin cos lc = LeakyCycle ep = export lc torch randn torch randn strict=False node _ref = weakref ref bank node _ref = weakref ref bank ref bank clear del bank bank = assertIsNotNone node _ref node should still alive due cycle assertIsNotNone node _ref node should still alive due cycle torch _export config patch detect_non_strict_fake_tensor_leaks=True warn_re = re compile r Detected\s+\d+\s+fake\s+tensors r \\ test_export\ py \s+line\s+\d+ \s+in\s+forward r \\n &#124; \n \s z\s =\s x\s \+\s y re S assertWarnsRegex UserWarning warn_re ep = export lc torch randn torch randn strict=False test_export_for_training_run_decomp Foo torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones linear = torch nn Linear forward x buffer add_ linear x + buffer sum eager_model = Foo ep_for_training = torch export export eager_model torch ones ep_for_inference = ep_for_training run_decompositions assertExpectedInline str ep_for_inference graph_module code strip \ forward p_linear_weight p_linear_bias b_buffer x add = torch ops aten add Tensor b_buffer b_buffer = None permute = torch ops aten permute default p_linear_weight p_linear_weight = None addmm = torch ops aten addmm default p_linear_bias x permute p_linear_bias = x = permute = None sum_ = torch ops aten sum dim_IntList add add_ = torch ops aten add Tensor addmm sum_ addmm = sum_ = None add add_ test_derived_dim_out_of_order_simplified_repeat_non_derived Foo torch nn Module forward x y y z x + y + y + z foo = Foo u v v w = torch randn torch randn torch randn torch randn _dimz = torch export Dim _dimz min= max= dimy = _dimz - dimx = dimy - dimz = dimx + works effectively = _dimz ep = export foo u v v w dynamic_shapes= dimx dimy dimy dimz assertRaisesRegex AssertionError escape Guard failed y size == y size expected got ep module torch randn torch randn torch randn torch randn assertEqual ep module torch randn torch randn torch randn torch randn size test_static_dim_constraints Foo torch nn Module __init__ - None super __init__ l = torch nn Linear forward x y z x = l x + y x z foo = Foo inputs = torch randn torch randn torch randn dx = Dim dx min= max= dy = dx + dz = Dim dz min= max= test tweaking shapes fails wrong_shape_inputs = torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn all these should fine dynamic_shapes dx dy dz dx None dy dz None None None None None None None None Dim STATIC Dim STATIC ep = export foo inputs dynamic_shapes=dynamic_shapes assertEqual foo inputs ep module inputs wrong_inputs wrong_shape_inputs assertRaisesRegex AssertionError Guard failed assertRaises RuntimeError ep module wrong_inputs check range_constraints - static dims shouldn t present ep = export foo inputs dynamic_shapes= dx None dy dz assertEqual len ep range_constraints vr ep range_constraints values assertTrue vr lower vr upper check raised errors assertRaisesRegex torch fx experimental symbolic_shapes ConstraintViolationError torch _dynamo exc UserError Static shape constraint does match input size _ = export foo inputs dynamic_shapes= None None None assertRaisesRegex torch fx experimental symbolic_shapes ConstraintViolationError torch _dynamo exc UserError Static shape constraint does match input size _ = export foo inputs dynamic_shapes= dx dy test_dim_ _ Foo torch nn Module forward x x dx = Dim dx min= max= ep = export Foo torch randn dynamic_shapes= dx None ep module torch randn ep module torch randn assertRaisesRegex AssertionError escape Guard failed x size = expected = got ep module torch randn vr = list ep range_constraints values assertEqual vr lower assertEqual vr upper test_derived_dim_ _ Bar torch nn Module forward x y x + y dx = Dim dx min= max= ep = export Bar torch randn torch randn dynamic_shapes= dx None dx + None assertRaisesRegex AssertionError escape Guard failed - + y size = TODO should error ep module torch randn torch randn range_lower_bounds = sorted vr lower vr ep range_constraints values range_upper_bounds = sorted vr upper vr ep range_constraints values assertEqual range_lower_bounds assertEqual range_upper_bounds test_issue_ Add torch nn Module forward x y x + y m = Add x = torch randn y = torch randn dx = Dim dx min= max= conflicting = x dx Dim STATIC y dx + Dim STATIC assertRaisesRegex torch _dynamo exc UserError r Constraints violated r \n You marked \ dx dynamic your code specialized constant \ \ r \n You marked dx \+ dynamic your code specialized constant \ \ export m x y dynamic_shapes=conflicting test_range_constraints_with_replacement M torch nn Module forward x y x + y m = M inp = torch randn torch randn dynamic_shapes = torch export Dim DYNAMIC torch export Dim DYNAMIC ep = export m inp dynamic_shapes=dynamic_shapes assert len ep range_constraints == vr = next iter ep range_constraints values assertEqual vr lower test_unbacked_linear_layer_norm_input MyModel torch nn Module __init__ super __init__ linear = torch nn Linear bias=True layer_norm = torch nn LayerNorm forward x mask masked_select = x masked_select mask view = masked_select view - linear = linear view layer_norm = layer_norm view linear layer_norm inputs = torch randn dtype=torch float torch randint low= high= size= dtype=torch bool model = MyModel ep = export model inputs ref = model inputs actual = ep module inputs assertTrue torch allclose ref actual assertTrue torch allclose ref actual torch _dynamo config patch capture_scalar_outputs=True test_layer_norm_unbacked_normalized_shape MyModel torch nn Module forward scalar weight bias u = scalar item y = torch ones u torch nn functional layer_norm input=y normalized_shape= u weight=weight bias=bias model = MyModel inputs = torch scalar_tensor dtype=torch int torch randn torch randn ep = export model inputs actual = ep module inputs ref = model inputs assertTrue torch allclose ref actual test_unbacked_ d_matmul Model torch nn Module forward x repeat u = repeat item t = x unsqueeze expand x size u x size - t = torch ones torch matmul t t model = Model inputs = torch randn torch scalar_tensor dtype=torch int exported = export model inputs module assertEqual model inputs exported inputs test_dynamic_shapes_wrapped_with_shape_guards Neuron torch nn Module __init__ n_dims int = n_targets int = super __init__ linear = torch nn Linear n_dims n_targets forward x y torch sigmoid linear x + y args = torch randn torch randn batch = torch export Dim DYNAMIC n = Neuron compiled = export n args dynamic_shapes= batch batch expected = n args mod = compiled module got = mod args assertTrue torch allclose expected got Wrapped Neuron forward args super forward args w = Wrapped is_retracebility_test _testMethodName assertRaisesRegex torch _dynamo exc UserError Detected mismatch between structure ` inputs ` ` dynamic_shapes ` ` inputs ` has elements ` dynamic_shapes ` has elements export w args dynamic_shapes= args batch batch compiled = export w args dynamic_shapes= args batch batch expected = w args mod = compiled module got = mod args assertTrue torch allclose expected got test_dynamic_shapes_builder_basic M torch nn Module forward x y z x + y + z k m = M x = torch randn y = torch randn z = k torch randn args = x y z shapes_collection = torch export ShapesCollection dim = torch export Dim dim max= specify shape tensor shapes_collection x = dim tensor can arbitrarily deep shapes_collection y = dim can also specify some dimension shape tensor shapes_collection z k = dim ep = export m args dynamic_shapes=shapes_collection sym = next iter ep range_constraints keys node ep graph nodes node op == placeholder assertEqual str tuple node meta val shape f sym test_dynamic_shapes_builder_kwargs M torch nn Module forward x y z x + y + z k m = M x = torch randn y = torch randn z = k torch randn args = x kwargs = z z y y shapes_collection = torch export ShapesCollection dim = torch export Dim dim max= shapes_collection x = dim shapes_collection y = dim shapes_collection z k = dim ep = export m args kwargs=kwargs dynamic_shapes=shapes_collection sym = next iter ep range_constraints keys node ep graph nodes node op == placeholder assertEqual str tuple node meta val shape f sym test_dynamic_shapes_builder_pytree torch export register_dataclass Inp serialized_type_name= test_dynamic_shapes_builder_pytree Inp M torch nn Module forward inp Inp inp x + inp y + inp z k m = M x = torch randn y = torch randn z = k torch randn args = Inp x y z shapes_collection = torch export ShapesCollection dim = torch export Dim dim max= shapes_collection x = dim shapes_collection y = dim shapes_collection z k = dim ep = export m args dynamic_shapes=shapes_collection dynamic_shapes m args sym = next iter ep range_constraints keys node ep graph nodes node op == placeholder assertEqual str tuple node meta val shape f sym test_dynamic_shapes_inferred_basic M torch nn Module forward x y z x y must have same dynamic shape say ` dim ` = tmp = x + y z k must have static shape = tmp z k m = M args = torch randn torch randn k torch randn additional_inputs = torch export AdditionalInputs - - - good_args = torch randn torch randn k torch randn additional_inputs add good_args ep = export m args dynamic_shapes=additional_inputs got_shapes = str tuple node meta val shape node ep graph find_nodes op= placeholder dim = next iter ep range_constraints keys expected_shapes = f dim f dim assertEqual got_shapes expected_shapes expect_error bad_args run_time_msg compile_time_msg assertRaisesRegex AssertionError run_time_msg ep module bad_args additional_inputs = torch export AdditionalInputs additional_inputs add bad_args assertRaisesRegex RuntimeError compile_time_msg export m args dynamic_shapes=additional_inputs expect_error - - - bad_args= torch randn torch randn k torch randn run_time_msg=escape Guard failed x size = expected = got compile_time_msg= Expected input = got expect_error - - - bad_args= torch randn torch randn k torch randn run_time_msg=escape Guard failed y size == x size expected got compile_time_msg= Expected input equal got expect_error - - - bad_args= torch randn torch randn k torch randn run_time_msg=escape Guard failed z k size == expected got compile_time_msg=r You marked your code specialized constant If you re using Dim DYNAMIC replace either Dim STATIC Dim AUTO test_additional_inputs_constants dataclass D b bool i int f float t torch Tensor pytree register_dataclass D M torch nn Module forward d D d i + d f + d t input = D True torch ones int tensor change input = D True torch ones ai = torch export AdditionalInputs ai add input ai add input dynamic_shapes = ai dynamic_shapes M input assertEqual dynamic_shapes d None Dim DYNAMIC None Dim DYNAMIC torch export export M input dynamic_shapes=ai float changes error input = D True torch ones ai = torch export AdditionalInputs ai add input ai add input assertRaisesRegex ValueError r they cannot marked dynamic \ \ \ \ \ ai dynamic_shapes M input assertRaisesRegex ValueError r they cannot marked dynamic \ \ \ \ \ torch export export M input dynamic_shapes=ai bool changes error input = D False torch ones ai = torch export AdditionalInputs ai add input ai add input assertRaisesRegex ValueError r they cannot marked dynamic \ True True False\ ai dynamic_shapes M input assertRaisesRegex ValueError r they cannot marked dynamic \ True True False\ torch export export M input dynamic_shapes=ai Differing types input = D True torch ones input = D True False torch ones ai = torch export AdditionalInputs ai add input ai add input assertRaisesRegex ValueError r differing types so they cannot marked dynamic \ False\ print ai dynamic_shapes M input assertRaisesRegex ValueError r differing types so they cannot marked dynamic \ False\ torch export export M input dynamic_shapes=ai test_mismatched_dynamic_shapes AUTO STATIC = Dim AUTO Dim STATIC M torch nn Module forward x x k k + x k k inputs = k k torch rand torch rand dim = torch export Dim dim dynamic_shapes = k k dim dim ValueError Node keys mismatch missing key s x extra key s k assertRaisesRegex torch _dynamo exc UserError re escape When ` dynamic_shapes ` specified dict its top-level keys must arg names x ` inputs ` here they k Since here ` inputs ` list tuple enclosing single dict maybe you just forgot enclose ` dynamic_shapes ` list tuple export M inputs dynamic_shapes=dynamic_shapes dynamic_shapes = k k dim dim torch _dynamo exc UserError Unexpected dynamic_shape dim Tensor try None instead assertRaisesRegex torch _dynamo exc UserError Unexpected input tensor shape dim + re escape specified ` dynamic_shapes k k ` expected either list tuple dimensions dict mapping indices dimensions where each dimension int Dim Dim AUTO Dim STATIC Dim DYNAMIC export M inputs dynamic_shapes=dynamic_shapes dynamic_shapes = k k dim dim ValueError Node type mismatch expected list got tuple assertRaisesRegex torch _dynamo exc UserError re escape Detected mismatch between structure ` inputs ` ` dynamic_shapes ` ` inputs k k ` list ` dynamic_shapes k k ` tuple export M inputs dynamic_shapes=dynamic_shapes dynamic_shapes = k k dim dim ok export M inputs dynamic_shapes=dynamic_shapes dynamic_shapes = k k dim ValueError Node type mismatch expected list got Dim assertRaisesRegex torch _dynamo exc UserError re escape Detected mismatch between structure ` inputs ` ` dynamic_shapes ` ` inputs k k ` list ` dynamic_shapes k k ` export M inputs dynamic_shapes=dynamic_shapes dynamic_shapes = x k dim dim k k dim dim ValueError Node arity mismatch expected got assertRaisesRegex torch _dynamo exc UserError re escape When ` dynamic_shapes ` specified dict its top-level keys must arg names x ` inputs ` here they x k Alternatively you could also ignore arg names entirely specify ` dynamic_shapes ` list tuple matching ` inputs ` export M inputs dynamic_shapes=dynamic_shapes dynamic_shapes = k k dim dim dim ValueError Node arity mismatch expected got assertRaisesRegex torch _dynamo exc UserError re escape Detected mismatch between structure ` inputs ` ` dynamic_shapes ` ` inputs k k ` has elements ` dynamic_shapes k k ` has elements export M inputs dynamic_shapes=dynamic_shapes dynamic_shapes = k K dim dim dim ValueError Node keys mismatch missing key s k extra key s K assertRaisesRegex torch _dynamo exc UserError re escape Detected mismatch between structure ` inputs ` ` dynamic_shapes ` ` inputs k ` has keys k ` dynamic_shapes k ` has keys K export M inputs dynamic_shapes=dynamic_shapes N torch nn Module forward x x k k + x k k inputs = k k torch rand k torch rand dim = torch export Dim dim dynamic_shapes = k k dim k dim ok export N inputs dynamic_shapes=dynamic_shapes O torch nn Module forward x x + inputs = torch randn dynamic_shapes = x dim None assertRaisesRegex torch _dynamo exc UserError r Expected dynamic shape spec ` dynamic_shapes\ x \ ` have same length r actual tensor shape torch\ Size\ \ \ \ \ expected got instead\ export O inputs dynamic_shapes=dynamic_shapes test_unbacked_bindings_for_divisible_u_symint torch _export utils _get_shape_env_from_gm torch utils _sympy symbol prefix_str symbol_is_type SymT M torch nn Module forward b torch ops mylib foo_unbacked b torch library custom_op mylib foo_unbacked mutates_args= foo_unbacked torch Tensor b torch Tensor - torch Tensor b item foo_unbacked register_fake foo_unbacked_fake_impl b ctx = torch library get_ctx u = ctx new_dynamic_size min= max=len torch empty u shape dtype=a dtype check binding path correct ep = export M torch randn torch tensor foo = node node ep graph nodes node name == foo_unbacked unbacked_bindings = foo meta unbacked_bindings assertEqual len unbacked_bindings check binding u path u = next iter unbacked_bindings keys assertEqual type u __name__ Symbol check binding symbol expr path = unbacked_bindings u assertEqual len path check path size DivideByKey assertEqual type path __name__ DivideByKey assertEqual path divisor collect bound symbols bound = set node ep graph nodes bound update node meta get unbacked_bindings check ShapeEnv counters compared binding indices shape_env = _get_shape_env_from_gm ep graph_module next_index = shape_env unbacked_symint_counter shape_env unbacked_symint_counter += symbol bound assertTrue symbol_is_type symbol SymT UNBACKED_INT assertTrue int str symbol len prefix_str SymT UNBACKED_INT next_index test_torch_check_eq_commutativity M torch nn Module forward x x x y z = x item z = x item z = x item instead torch _check z + z == z torch _check z == z + z z + z == z y y + export M torch tensor torch tensor torch tensor torch randn M torch nn Module forward x x x y z = x item z = x item z = x item instead torch _check z + z = z torch _check z = z + z z + z == z y y + export M torch tensor torch tensor torch tensor torch randn test_replaced_unbacked_bindings sympy torch utils _sympy symbol prefix_str symbol_is_type SymT Foo torch nn Module forward x y z m n = x item y item torch _check m == torch _check n == z shape m + n + z inps = torch tensor torch tensor torch randn dynamic_shapes = x None y None z Dim dx max= ep = export Foo inps dynamic_shapes=dynamic_shapes values should have no unbacked symbols bindings should empty node ep graph nodes val = node meta get val bindings = node meta get unbacked_bindings assertTrue isinstance val sympy Symbol symbol_is_type val SymT UNBACKED_INT assertTrue bindings None test_raise_user_error_when_guard_on_data_dependent_operation M torch nn Module forward x y = x nonzero z = y shape z x cos x sin assertRaisesRegex torchdynamo exc UserError torch fx experimental symbolic_shapes GuardOnDataDependentSymNode Could guard data-dependent expression _ = export M torch tensor test_unbacked_infer_size Foo torch nn Module forward x u = x item t = torch empty u - t + t ep = torch export export Foo torch tensor ep module torch tensor ep module torch tensor test_unbacked_pad Foo torch nn Module forward xs pad u u u = xs tolist x = torch ones u u u pl pr pl pr = pad tolist torch nn functional pad x pl pr pl pr x = torch tensor pad = torch tensor - m = Foo ep = export m x pad assertEqual ep module x pad shape m x pad shape don t guard negative positive pad values pad = torch tensor - assertEqual ep module x pad shape m x pad shape test_suggested_fixes_for_data_dependent_errors_basic suggested fixes data-dependent errors only work non-strict mode strict = False error_type = torch fx experimental symbolic_shapes GuardOnDataDependentSymNode Just introduce some indirection N top-level module N calls module M defined next N torch nn Module __init__ - None super __init__ m = M forward t m t + example input t = torch tensor dtype=torch int We define series versions M below Each version has raises data-dependent error next version fixes copy-pasting suggested fix error message The fix always torch check unresolved condition its negation unbacked symints mentioned error message Note suggested fixes terms local variables near location error contain unbacked symints unresolved condition either directly indirectly e g inside list inside shape tensor M_v torch nn Module forward t items = t i item i range t numel r = torch randn items items r view items items M = M_v export N t strict=strict test_suggested_fixes_for_data_dependent_errors_puzzlers suggested fixes data-dependent errors only work non-strict mode strict = False error_type = torch fx experimental symbolic_shapes GuardOnDataDependentSymNode retry_export m inp fixes API applies series fixes retrying export after applying each fix asserting applied fix suggested previous try Using API avoids need define multiple versions same test module ` test_suggested_fixes_for_data_dependent_errors_basic ` above code snippets f join snippets i range len fixes assertRaisesRegex error_type re escape fixes i export m inp code fixes i strict=strict export m inp code fixes strict=strict The following examples lifted ezyang s Data-dependent shape puzzlers notebook https www internalfb com intern anp view id= These test modules written way works well retry_export above Specifically they take extra ` fixes ` argument ` eval ` location expected raise errors cf_implicitsize torch nn Module forward x y fixes i = x item eval fixes instead y i y narrow i squeeze retry_export cf_implicitsize torch tensor torch randn fixes= Could guard data-dependent expression u torch _check i = cf_stacklist torch nn Module forward xs y fixes i = y item eval fixes instead xs i torch stack xs narrow i squeeze retry_export cf_stacklist torch ones i i range torch tensor fixes= Could guard data-dependent expression u torch _check i = cf_tensorsplit torch nn Module forward x offsets_t fixes lengths = torch diff offsets_t tolist rs = start = length lengths eval fixes rs append x narrow start length start += length rs retry_export cf_tensorsplit torch arange torch tensor fixes= nothing fix test_simple_unbacked_view cpp_runtime_nonstrict id skipTest TODO Unexpected success OSS fbcode Foo torch nn Module forward x u = x item y = torch empty u y view u u - u ep = export Foo torch tensor assertEqual ep module torch tensor size assertEqual ep module torch tensor size Foov torch nn Module forward xs xsl = xs tolist b = xsl x = torch zeros x reshape b xs = torch tensor ep = export Foov xs assertEqual ep module xs size assertEqual ep module torch tensor size test_no_suggested_fixes_for_data_dependent_errors suggested fixes data-dependent errors only work non-strict mode strict = False error_type = torch fx experimental symbolic_shapes GuardOnDataDependentSymNode cf_stacklist torch nn Module forward xs y y item local so we can t suggest fix torch stack xs narrow y item squeeze assertRaisesRegex error_type Could guard data-dependent expression u export cf_stacklist torch ones i i range torch tensor strict=strict Box __init__ content content = content torch utils _pytree register_pytree_node register_pytree_node Box lambda box box content None flatten_fn lambda contents _context Box contents unflatten_fn flatten_with_keys_fn=None unflatten_fn serialized_type_name= test_no_suggested_fixes_for_data_dependent_errors Box cf_stacklist_udd torch nn Module forward xs y box = Box y item box content local so we can t suggest fix torch stack xs narrow box content squeeze assertRaisesRegex error_type Could guard data-dependent expression u export cf_stacklist_udd torch ones i i range torch tensor strict=strict test_tolist M torch nn Module forward x x tolist ep = export M torch ones dtype=torch int assertEqual ep module torch tensor test_if_functional Module torch nn Module forward x z = x + z add_ y = z view x shape x cos + y cos foo = Module gm = export foo torch tensor run_decompositions view_count = node gm graph nodes node op == call_function node target == torch ops aten add_ Tensor No more inplace mutation assertNotEqual node target torch ops aten add_ Tensor There shouldn t any inplace mutation node graph node op == call_function node target == torch ops aten view default view_count += There should nonzero view nodes graph assertTrue view_count test_solver_unsupported_sympy_function repro https github com pytorch pytorch issues MyModule torch nn Module __init__ super __init__ forward x y x = torch nn functional interpolate x scale_factor= mode= bilinear x = torch nn functional interpolate x scale_factor= mode= bilinear x = x + y x model = MyModule eval inputs = torch rand torch rand dim = torch export Dim AUTO dynamic_shapes = x dim dim y dim dim exported_program = export model inputs dynamic_shapes=dynamic_shapes assertEqual exported_program module inputs model inputs test_export_max_onnx_reported Model torch nn Module forward x y s = max x shape y shape s = max x shape y shape z = torch zeros s s dtype=x dtype z x shape x shape = x z y shape y shape += y z model = Model x = torch arange reshape y = torch arange reshape DYN = torch export Dim DYNAMIC ep = export model x y dynamic_shapes= DYN DYN DYN DYN strict=True assertTrue torch allclose ep module x y model x y x = torch arange reshape y = torch arange reshape assertRaisesRegex AssertionError escape Guard failed max x size y size == x size is_retracebility_test _testMethodName escape Guard failed max x size y size == x size TODO should error assertTrue torch allclose ep module x y model x y test_export_max_nonstrict FooMax torch nn Module forward x torch ones max x item ep_non_strict_foo_max_symint = export FooMax torch tensor strict=False graph FileCheck check_count torch sym_max count= exactly=True run str ep_non_strict_foo_max_symint FooMaxTensors torch nn Module forward x torch ones max x x + torch ones min x x ep_non_strict_foo_max_symint = export FooMaxTensors torch tensor strict=False graph FileCheck check_count torch ops aten maximum default count= exactly=True run str ep_non_strict_foo_max_symint FileCheck check_count torch ops aten minimum default count= exactly=True run str ep_non_strict_foo_max_symint FooMaxTensorsIter torch nn Module forward x max x x + min x x + max x + min x ep_non_strict_foo_max_symint = export FooMaxTensorsIter torch tensor strict=False graph FileCheck check_count torch ops aten maximum default count= exactly=True run str ep_non_strict_foo_max_symint FileCheck check_count torch ops aten minimum default count= exactly=True run str ep_non_strict_foo_max_symint FileCheck check_count torch ops aten clamp default count= exactly=True run str ep_non_strict_foo_max_symint FooMaxTensorsSymInt torch nn Module forward x y max x shape y shape x shape + min x shape y shape x shape dynamic_shapes = x torch export Dim AUTO y torch export Dim AUTO ep_non_strict_foo_max_symint = export FooMaxTensorsSymInt torch randn torch randn dynamic_shapes=dynamic_shapes strict=False graph FileCheck check_count torch sym_max count= exactly=True run str ep_non_strict_foo_max_symint FileCheck check_count torch sym_min count= exactly=True run str ep_non_strict_foo_max_symint FooMaxTensorsSymShape torch nn Module forward x max x x shape dynamic_shapes = x torch export Dim AUTO assertRaisesRegex RuntimeError Dynamo failed run FX node fake tensors _ = export FooMaxTensorsSymShape torch randn dynamic_shapes=dynamic_shapes strict=True graph assertRaisesRegex RuntimeError Boolean value Tensor more than one value ambiguous _t = export FooMaxTensorsSymShape torch randn dynamic_shapes=dynamic_shapes strict=False graph test_math_pow M torch nn Module forward x y b = x item p = min b p = math pow p y p ep = export M torch tensor torch randn strict=False FileCheck check_count torch sym_min count= exactly=True run str ep graph FileCheck check_count operator pow count= exactly=True run str ep graph test_export_mod_constraints BasicDynamiShapeModel torch nn Module forward x torch Tensor - torch Tensor x view x shape - - m = BasicDynamiShapeModel = torch randn dim _x = torch export Dim dim _x min= dim _x = torch export Dim dim _x max= dynamic_shapes = x dim _x dim _x em = torch export export m dynamic_shapes=dynamic_shapes prefer_deferred_runtime_asserts_over_guards=True em module torch randn assertRaisesRegex RuntimeError r Runtime assertion failed expression Eq\ Mod\ s \ s s \- \ \ em module torch randn dim _x = None dim _x = torch export Dim _dim _x max= dynamic_shapes = x dim _x dim _x em = torch export export m dynamic_shapes=dynamic_shapes x = torch randn assertRaisesRegex AssertionError escape Guard failed x size == expected got em module x test_dont_duck_size_for_auto_dynamic AUTO STATIC = Dim AUTO Dim STATIC Foo torch nn Module forward x y x s s y s + assert y shape == assert x shape == y shape - x y duck sizing would make all static based these sample inputs inputs = torch randn torch randn shapes = x AUTO AUTO y AUTO AUTO ep = export Foo inputs dynamic_shapes=shapes ep module torch randn torch randn test_map cpp_runtime_nonstrict id skipTest TODO Unexpected success OSS fbcode Module torch nn Module forward xs y z body x y z x + y + z map body xs y z list_tensor_map = Module inps = torch ones torch tensor torch tensor _test_export_same_as_eager list_tensor_map inps unittest expectedFailure test_crop_like https fb workplace com groups posts Minimal crop code copied https github com pytorch vision blob main torchvision transforms v functional CropLike torch nn Module forward image crop_height crop_width c image_height image_width = image shape crop_top = int round image_height - crop_height crop_left = int round image_width - crop_width image crop_top crop_top + crop_height crop_left crop_left + crop_width crop = CropLike imagew = Dim width imageh = Dim height dynamic_dims = image None imageh imagew crop_height None crop_width None args = torch rand ecrop = export crop args=args dynamic_shapes=dynamic_dims args = torch rand assertEqual ecrop module args ecrop args test_dim_dynamic_divisibility M torch nn Module forward x x size == x clone x clone input = torch randn model = M dynamic_shapes = x torch export Dim DYNAMIC export model input dynamic_shapes=dynamic_shapes test_export_func_with_kwargs Module torch nn Module forward arg arg kw kw arg + arg kw + kw kw_func = Module args = torch ones torch ones kwargs = kw torch ones kw torch ones _test_export_same_as_eager kw_func args kwargs test_export_func_with_pytree_kwargs Module torch nn Module forward arg arg b arg + kw + b arg + kw + b kw_func = Module args = torch ones torch ones kwargs = kw torch ones kw torch ones b torch ones torch ones _test_export_same_as_eager kw_func args kwargs test_export_func_with_default_kwargs Module torch nn Module forward arg arg b= arg + arg kw + kw + b kw_func = Module Module torch nn Module forward arg arg a= b= arg + arg + b kw_func = Module args = torch ones torch ones kwargs = kw torch ones kw torch ones kwargs = kw torch ones kw torch ones b _test_export_same_as_eager kw_func args kwargs _test_export_same_as_eager kw_func args kwargs kwargs = b _test_export_same_as_eager kw_func args kwargs test_kwargs_reorder M torch nn Module forward x y z x + y + z ep = export M z torch ones y torch ones x torch ones ep module z torch ones y torch ones x torch ones ep module z=torch ones y=torch ones x=torch ones ep module x=torch ones z=torch ones y=torch ones test_set_example_inputs M torch nn Module forward x y z x + y + z inp = torch ones z torch ones y torch ones x torch ones ep = export M inp inp ep module ep example_inputs ep example_inputs ep example_inputs = torch ones x torch ones z torch ones y torch ones ep module ep example_inputs ep example_inputs assertRaisesRegex ValueError Example inputs should tuple ep example_inputs = torch ones assertRaisesRegex ValueError Ran into kwarg keyword mismatch ep example_inputs = torch ones assertRaisesRegex ValueError Trying flatten user inputs ep example_inputs = x torch ones z torch ones y torch ones test_export_func_with_var_postional_args Module torch nn Module forward arg arg args arg + args arg + args kw_func = Module args = torch ones torch ones torch ones torch ones _test_export_same_as_eager kw_func args testing expectedFailureCppRuntime test_export_module Foo torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter CustomTensorPlainOut torch ones torch ones forward x = p + p sum x + model = Foo example_inputs = torch randn ep = export model example_inputs strict=False before = list ep state_dict keys ep run_decompositions after = list ep state_dict keys assertEqual before after test_export_func_with_keyword_only_args Module torch nn Module forward arg arg args kw kw arg + args + kw arg + args + kw kw_func = Module args = torch ones torch ones torch ones torch ones kwargs = kw torch ones kw torch ones _test_export_same_as_eager kw_func args kwargs test_export_func_with_var_keyword_args Module torch nn Module forward arg arg args kw kw kwargs arg + args + kw + kwargs kw arg + args + kw + kwargs kw kw_func = Module args = torch ones torch ones torch ones torch ones kwargs = kw torch ones kw torch ones kw torch ones kw torch ones _test_export_same_as_eager kw_func args kwargs test_unbacked_stack M torch nn Module forward x nz = torch nonzero x nz_size = nz size torch _check nz_size == Create two tensors whose leading dimensions equivalent runtime expressed via different SymInt formulas first = torch zeros nz_size second = torch zeros nz_size torch stack first second dim= inputs = torch ones ep = export M inputs orig_res = M inputs ep_res = ep module inputs assertTrue torch allclose orig_res ep_res test_unbacked_slice_simple M torch nn Module forward scores score_thr topk torch Tensor results=None valid_mask = scores score_thr scores = scores valid_mask valid_idxs = torch nonzero valid_mask scores device num_topk = torch minimum topk torch tensor valid_idxs shape item scores idxs = scores sort descending=True scores = scores num_topk topk_idxs = valid_idxs idxs num_topk keep_idxs labels = topk_idxs unbind dim= scores labels keep_idxs score = torch tensor bbox_pred = torch tensor score_thr = nms_pre = torch tensor inputs = score score_thr nms_pre dict bbox_pred=bbox_pred ep = export M inputs orig_res = M inputs ep_res = ep module inputs assertTrue torch allclose orig_res ep_res assertTrue torch allclose orig_res ep_res assertTrue torch allclose orig_res ep_res test_multidimensional_slicing M torch nn Module forward x y b = x item torch _check b = torch _check b y shape y b is_non_strict_test _testMethodName m = M inp = torch tensor torch ones r = m inp epm = export m inp module er = epm inp assertTrue torch allclose er r testing expectedFailureSerDerNonStrict testing expectedFailureCppRuntimeNonStrict test_more_multidimensional_slicing Inputs d tensor t d tensor x indices into t Output -tuple indices torch library custom_op demo indices d mutates_args= indices d t torch Tensor x torch Tensor - tuple int int int assert t ndim == assert x ndim == x shape == tuple x i item i range The meta-kernel op constrains indices x within bounds t via torch _checks torch library register_fake demo indices d _ t x assert t ndim == assert x ndim == x shape == sizes = tuple torch library get_ctx new_dynamic_size i range i size enumerate sizes torch _check size = torch _check size = t shape i sizes example inputs t = torch randn x = torch tensor test m g debug=False Dynamo does yet support some cases indexing tested here so don t export strict mode is_non_strict_test _testMethodName em = export m t x module debug print em assertTrue torch allclose m t x g t x assertTrue torch allclose em t x m t x In following series test cases M_ corresponds indexing code user might write G_ corresponds equivalent code export might generate rewriting indexing terms sequence lower-level ops indexing ints M_ints torch nn Module forward t x i j k = indices d t x t i j k G_ints torch nn Module forward t x i j k = indices d t x = torch select t i b = torch select j c = torch select b k c test M_ints G_ints indexing slices M_slices torch nn Module forward t x i j k = indices d t x t i j k G_slices torch nn Module forward t x i j k = indices d t x = torch narrow t i b = torch narrow j c = torch narrow b k c test M_slices G_slices indexing ints slices M_ints_slices torch nn Module forward t x i j k = indices d t x t i j k G_ints_slices torch nn Module forward t x i j k = indices d t x = torch narrow t i b = torch select j c = torch narrow b k c test M_ints_slices G_ints_slices indexing ints None M_ints_None torch nn Module forward t x i j k = indices d t x t None i None G_ints_None torch nn Module forward t x i j k = indices d t x = torch unsqueeze t b = torch select i c = torch unsqueeze b c test M_ints_None G_ints_None indexing slices None M_slices_None torch nn Module forward t x i j k = indices d t x t i None j None None k G_slices_None torch nn Module forward t x i j k = indices d t x = torch narrow t i b = torch unsqueeze c = torch narrow b j d = torch unsqueeze c e = torch unsqueeze d f = torch narrow e k f test M_slices_None G_slices_None indexing None Ellipsis int M_None_Ellipsis_int torch nn Module forward t x i j k = indices d t x t None None j G_None_Ellipsis_int torch nn Module forward t x i j k = indices d t x = torch unsqueeze t b = torch unsqueeze c = torch select b j c test M_None_Ellipsis_int G_None_Ellipsis_int indexing slice None Ellipsis int M_slice_None_Ellipsis_int torch nn Module forward t x i j k = indices d t x t i None None j G_slice_None_Ellipsis_int torch nn Module forward t x i j k = indices d t x = torch narrow t i b = torch unsqueeze c = torch unsqueeze b d = torch select c j d test M_slice_None_Ellipsis_int G_slice_None_Ellipsis_int test_sequential_slicing See https github com pytorch pytorch issues TestModule torch nn Module __init__ - None super __init__ seq = torch nn Sequential torch nn Linear torch nn Linear torch nn Linear forward x torch Tensor - torch Tensor seq_last local variable works seq_last = seq seq_last x TestModule torch nn Module __init__ - None super __init__ seq = torch nn Sequential torch nn Linear torch nn Linear torch nn Linear seq_last initialized submodule works seq_last = seq forward x torch Tensor - torch Tensor seq_last x inp = torch randn mod TestModule TestModule epm = export mod inp module assertTrue torch allclose epm inp mod inp test_unflatten_isinstance N torch nn Module forward x b b x + x + M torch nn Module __init__ super __init__ n = N forward x n x + True + n x + False x = torch zeros types = n N ep = export M x preserve_module_call_signature=tuple types keys ufm = torch export unflatten ep assertTrue torch allclose ufm x x + fqn mod ufm named_modules remove_duplicate=False cls = types get fqn ty = f cls __module__ cls __qualname__ assertTrue ty mod type_name test_unflatten_asserts TODO strict-export fails M torch nn Module forward x y b = x item torch _check b = torch _check b y size y b M torch nn Module forward x y b = x item torch _check b = torch _check b y size y b M torch nn Module __init__ - None super __init__ m = M m = M forward x y m x y + m x y inputs = torch tensor torch randn ep = torch export export M inputs dynamic_shapes= x None y Dim moo strict=False orig_res = M inputs ep_res = ep module inputs assertTrue torch allclose orig_res ep_res assertTrue torch allclose orig_res ep_res assertTrue torch allclose orig_res ep_res unflattened = torch export unflatten ep ep_res = unflattened inputs assertTrue torch allclose orig_res ep_res assertTrue torch allclose orig_res ep_res assertTrue torch allclose orig_res ep_res test_unflatten_placeholder_update_child parent_swap Child torch nn Module forward x torch ops aten dropout_ x False Applying dropout inplace x - Foo torch nn Module __init__ super __init__ child = Child forward x f = child x f = x f + f m = Foo inp = torch ones dtype=torch float orig_result = m inp is_retracebility_test _testMethodName inp = torch ones dtype=torch float ep = export m inp preserve_module_call_signature= child unf = unflatten ep unf print_readable inp = torch ones dtype=torch float ep_result = ep module inp assertTrue torch allclose ep_result orig_result unf set_submodule child m child inp = torch ones dtype=torch float unf_result = unf inp assertTrue torch allclose unf_result orig_result test_unflatten_placeholder_update_grandchild cousin_swap Grandchild torch nn Module forward x = x torch float considered mutation x + Child torch nn Module __init__ super __init__ grandchild = Grandchild forward x y = grandchild x y + OtherGrandchild torch nn Module forward x x OtherChild torch nn Module __init__ super __init__ other_grandchild = OtherGrandchild forward x x + other_grandchild x Foo torch nn Module __init__ super __init__ child = Child other_child = OtherChild forward x f = child x f = other_child x f + f inp = torch ones dtype=torch float orig_result = Foo inp assertTrue torch allclose orig_result torch ones is_retracebility_test _testMethodName inp = torch ones dtype=torch float ep = export Foo inp preserve_module_call_signature= child unf = unflatten ep inp = torch ones dtype=torch float ep_result = ep module inp assertTrue torch allclose ep_result orig_result unf set_submodule child Child inp = torch ones dtype=torch float unf_result = unf inp assertTrue torch allclose unf_result orig_result test_unflatten_buffer_update_child parent_swap Child torch nn Module __init__ super __init__ buf = torch nn Buffer torch tensor forward x buf add_ x + Foo torch nn Module __init__ super __init__ child = Child forward x y = child x child buf - + = x + = x = y + child buf y = child x child buf - + = x + = x = y + child buf y = child x child buf - + = x + = x = y + child buf x inp = torch ones dtype=torch float orig_result = Foo inp assertTrue torch allclose orig_result torch ones is_retracebility_test _testMethodName inp = torch ones dtype=torch float ep = export Foo inp preserve_module_call_signature= child unf = unflatten ep inp = torch ones dtype=torch float ep_result = ep module inp assertTrue torch allclose ep_result orig_result unf set_submodule child Child inp = torch ones dtype=torch float unf_result = unf inp assertTrue torch allclose unf_result orig_result test_export_func_with_var_keyword_pytree_args Module torch nn Module forward arg arg args kw kw kwargs arg + arg + args + kw + kwargs kw arg + args + kw + kwargs kw kw_func = Module args = torch ones torch ones torch ones torch ones torch ones kwargs = kw torch ones kw torch ones kw torch ones torch ones kw torch ones _test_export_same_as_eager kw_func args kwargs testing expectedFailureSerDer we don t save placeholder metadata testing expectedFailureCppSerDes we don t save placeholder metadata testing expectedFailureSerDerNonStrict test_linear_conv strict = True MyLinear torch nn Module __init__ - None super __init__ weight = torch randn bias = torch randn forward x torch nn functional linear x weight bias Foo torch nn Module __init__ - None super __init__ conv = torch nn Conv d linear = MyLinear forward x x_conv = conv x x_linear = linear x_conv x_linear cos ep = export Foo torch randn strict=strict node ep graph nodes node op == placeholder node name ep graph_signature inputs_to_buffers node name ep graph_signature inputs_to_parameters assertTrue source_fn_stack node meta test_dynamic_shapes_dataclass torch export register_dataclass Inp serialized_type_name= test_export_api_with_dynamic_shapes Inp Foo torch nn Module forward inputs torch matmul inputs inputs b foo = Foo inputs = Inp a=torch randn b=torch randn batch = Dim batch efoo = export foo inputs dynamic_shapes= inputs batch batch assertEqual First dimension varies across strict non-strict since source names different resulting different symbol names str node meta val shape node efoo graph_module graph nodes node op == placeholder torch Size torch Size testing expectedFailureCppSerDes test_export_method torch _export utils sync_state wrap_method strict = True M torch nn Module __init__ super __init__ t = torch nn Buffer torch tensor forward x foo x bar x foo x t mul_ x + t bar x x - t exporting em = M ex = torch randn foo epm_foo = export wrap_method em foo ex dynamic_shapes= x Dim DYNAMIC strict=strict module bar epm_bar = export wrap_method em bar ex dynamic_shapes= Dim DYNAMIC strict=strict module is_serdes_test _testMethodName sync_state epm_foo epm_bar running m = M rx = torch randn assertTrue torch allclose m t epm_foo t assertTrue torch allclose m t epm_bar t foo assertTrue torch allclose epm_foo rx m foo rx assertTrue torch allclose m t epm_foo t assertTrue torch allclose m t epm_bar t bar assertTrue torch allclose epm_bar rx m bar rx assertTrue torch allclose m t epm_foo t assertTrue torch allclose m t epm_bar t test_export_api_with_dynamic_shapes torch export Dim dims pass dynamic shapes inputs args Foo torch nn Module forward x y torch matmul x y foo = Foo inputs = torch randn torch randn batch = Dim batch efoo = export foo inputs dynamic_shapes= k batch k x y assertEqual efoo module inputs shape foo inputs shape foo = Foo inputs = torch randn kwinputs = y torch randn batch = Dim batch efoo = export foo inputs kwinputs dynamic_shapes= k batch k x y assertEqual efoo module inputs kwinputs shape foo inputs kwinputs shape pass dynamic shapes inputs partial error foo = Foo inputs = torch randn kwinputs = y torch randn batch = Dim batch assertRaisesRegex torch _dynamo exc UserError You marked your code specialized constant If you re using Dim DYNAMIC replace either Dim STATIC Dim AUTO \n Suggested fixes \n batch = export foo inputs kwinputs dynamic_shapes= x batch y None pass dynamic shapes inputs module foo = Foo inputs = torch randn torch randn batch = Dim batch efoo = export foo inputs dynamic_shapes= x batch y batch assertEqual efoo module inputs shape foo inputs shape pass dynamic shapes inputs bounds mostly shared foo = Foo inputs = torch randn torch randn batch = Dim batch min= max= size = Dim size efoo = export foo inputs dynamic_shapes= x batch size size y batch size size node efoo graph_module graph nodes node op == placeholder assertEqual node meta val shape node meta val shape assertEqual efoo module inputs shape foo inputs shape pass dynamic shapes inputs multiple mostly distinct inputs = torch randn torch randn batch M K N = dims batch M K N efoo = export Foo inputs dynamic_shapes= x batch M K y batch K N placeholders = node meta val shape node efoo graph_module graph nodes node op == placeholder assertEqual placeholders placeholders assertEqual efoo module inputs shape foo inputs shape pass dynamic shapes inputs dict Foo torch nn Module forward inputs torch matmul inputs x inputs y foo = Foo inputs = x torch randn y torch randn batch = Dim batch efoo = export foo inputs dynamic_shapes= inputs k batch k x y assertEqual First dimension varies across strict non-strict since source names different resulting different symbol names str node meta val shape node efoo graph_module graph nodes node op == placeholder torch Size torch Size assertEqual efoo module inputs shape foo inputs shape pass dynamic shapes inputs list Foo torch nn Module forward inputs torch matmul inputs inputs foo = Foo inputs = torch randn torch randn batch = Dim batch efoo = export foo inputs dynamic_shapes= inputs batch _ range assertEqual First dimension varies across strict non-strict since source names different resulting different symbol names str node meta val shape node efoo graph_module graph nodes node op == placeholder torch Size torch Size assertEqual efoo module inputs shape foo inputs shape pass dynamic shapes inputs pytree-registered classes HAS_TORCHREC skipping tests torchrec available Foo torch nn Module forward kjt - torch Tensor kjt values + kjt offsets + foo = Foo kjt = KeyedJaggedTensor values=torch Tensor keys= index_ index_ lengths=torch IntTensor offsets=torch IntTensor inputs = kjt dim = Dim dim dim_plus_one = Dim dim_plus_one efoo = torch export export foo inputs dynamic_shapes= kjt dim None dim dim_plus_one None None assertEqual out shape out efoo module inputs out shape out foo inputs pass dynamic shapes inputs distinct error Foo torch nn Module forward x y torch matmul x y foo = Foo inputs = torch randn torch randn batch M K K N = dims batch M K K N assertRaisesRegex torch _dynamo exc UserError Constraints violated \\ K \\ \n K K must always equal \n Suggested fixes \n K = K export foo inputs dynamic_shapes= x batch M K y batch K N pass dynamic shapes inputs specialized error foo = Foo inputs = torch randn torch randn batch M K N = dims batch M K N assertRaisesRegex torch _dynamo exc UserError You marked your code specialized constant If you re using Dim DYNAMIC replace either Dim STATIC Dim AUTO \n Suggested fixes \n K = export foo inputs dynamic_shapes= x batch M K y batch None N pass dynamic shapes inputs guards error Foo torch nn Module forward x y x shape y shape == torch matmul x y x + y foo = Foo inputs = torch randn torch randn batch M K N = dims batch M K N assertRaisesRegex torch _dynamo exc UserError Constraints violated \n Not all values K satisfy generated guard \n Not all values batch satisfy generated guard \n Suggested fixes \n batch = Dim\\ batch max= \\ \n K = \\ _K export foo inputs dynamic_shapes= x batch M K y batch K N test_suggested_fixes_new_roots torch export dims suggested fixes should introduce new root dim modulo guard Foo torch nn Module forward x y z dy = _dx dx = _dx - dz = _dx + suggested fixes results will look something like dx eq _dx- min max dy eq dx+ x shape = x shape = y shape == x + y + z foo = Foo inputs = torch randn torch randn torch randn dx dy dz = dims dx dy dz dynamic_shapes = x dx y dy z dz assertRaisesRegex figure out regex later torch _dynamo exc UserError Constraints violated \n Suggested fixes \n _dx = Dim\ \\ _dx\\ max= \ \n dx = \ _dx - \n dy = \ _dx \n dz = \ _dx \+ export Foo inputs dynamic_shapes=dynamic_shapes retry export _dx = Dim _dx min= max= dynamic_shapes = x _dx - y _dx z _dx + export Foo inputs dynamic_shapes=dynamic_shapes test_refine_dynamic_shapes_from_suggested_fixes torch export dynamic_shapes refine_dynamic_shapes_from_suggested_fixes helper model inputs dynamic_shapes export fail parse refine suggested fixes re-export try export Foo inps dynamic_shapes=dynamic_shapes raise Exception should have raised constraint violation error except torch _dynamo exc UserError exc new_shapes = refine_dynamic_shapes_from_suggested_fixes exc msg dynamic_shapes export Foo inps dynamic_shapes=new_shapes new_shapes specialize dims + derived dims Foo torch nn Module forward x y z x = x + y + z x = x torch randn x x inps = torch randn torch randn torch randn dx = Dim dx max= dynamic_shapes = x dx y dx + z dx + new_shapes = helper Foo inps dynamic_shapes assertEqual new_shapes x assertEqual new_shapes z refine lower upper bound Foo torch nn Module forward x y x shape = y shape = x y + inps = torch randn torch randn dynamic_shapes = x Dim dx y Dim dy new_shapes = helper Foo inps dynamic_shapes assertEqual new_shapes x min assertEqual new_shapes y max divisiblity will introduce new root Foo torch nn Module forward x x shape = x reshape - inps = torch randn dynamic_shapes = Dim dx new_shapes = helper Foo inps dynamic_shapes dim = new_shapes root = dim root assertEqual dim fn assertEqual root min turn dim into derived dim relation Foo torch nn Module forward x y x + y inps = torch randn torch randn dynamic_shapes = x Dim dx Dim dx y Dim dy Dim dy new_shapes = helper Foo inps dynamic_shapes assertEqual new_shapes x new_shapes y root dy = dx + assertEqual new_shapes y fn assertEqual new_shapes x new_shapes y dx = dy nested dynamic shapes spec Foo torch nn Module forward x y x = x data + x + x x = y torch randn x = y b torch randn x x x inps = data torch randn torch randn torch randn torch randn b torch randn dynamic_shapes = x data Dim dx Dim dx Dim dx Dim dx Dim dx Dim dx y Dim dya Dim dya b Dim dyb Dim dyb new_shapes = helper Foo inps dynamic_shapes assertEqual new_shapes x data new_shapes x dx = dx assertEqual new_shapes x root new_shapes x data dx = dx + assertEqual new_shapes x fn assertEqual new_shapes x data new_shapes x dx = dx assertEqual new_shapes y assertEqual new_shapes y b assertEqual new_shapes y b __name__ dyb unchanged test_dynamic_shapes_spec_with_pytree torch export Dim export torch utils _pytree tree_map inputs = tensor torch randn dict_of_tensors k torch randn k A B C D list_of_tensors torch randn _ range batch = Dim batch uniformly specify dynamic shapes all inputs spec = tree_map lambda x batch inputs Foo torch nn Module forward inputs inputs tensor + inputs dict_of_tensors A + inputs list_of_tensors ep = export Foo inputs dynamic_shapes= inputs spec input_shapes = str node meta val shape node ep graph_module graph nodes node op == placeholder assertEqual len input_shapes assertEqual len set input_shapes test_error_does_not_reference_eager_fallback Module torch nn Module forward x y = x nonzero z = y shape z x cos x sin fn_ddo = Module is_non_strict_test _testMethodName error = torch fx experimental symbolic_shapes GuardOnDataDependentSymNode error_msg = r Could guard data-dependent expression error = torchdynamo exc UserError error_msg = r ^ fall back eager assertRaisesRegex error error_msg _ = export fn_ddo torch tensor test_pytree_register_data_class dataclass MyDataClass x int y int z int = None dt = MyDataClass x= y= flat spec = tree_flatten dt assertTrue spec treespec_leaf assertTrue len flat == torch export register_dataclass MyDataClass serialized_type_name= test_pytree_register_data_class MyDataClass flat spec = tree_flatten dt assertEqual spec TreeSpec MyDataClass x y z treespec_leaf treespec_leaf assertEqual flat orig_dt = tree_unflatten flat spec assertTrue isinstance orig_dt MyDataClass assertEqual orig_dt x assertEqual orig_dt y assertEqual orig_dt z None roundtrip_spec = treespec_loads treespec_dumps spec assertEqual roundtrip_spec spec dataclass MyOtherDataClass pytree registration don t allow registering same twice x int y int z int = None Override registration keep none fields register_dataclass_as_pytree_node MyOtherDataClass return_none_fields=True serialized_type_name= test_pytree_regster_data_class MyOtherDataClass dt = MyOtherDataClass x= y= flat spec = tree_flatten dt assertEqual spec TreeSpec MyOtherDataClass x y z treespec_leaf treespec_leaf treespec_leaf assertEqual flat None orig_dt = tree_unflatten flat spec assertTrue isinstance orig_dt MyOtherDataClass assertEqual orig_dt x assertEqual orig_dt y assertEqual orig_dt z None roundtrip_spec = treespec_loads treespec_dumps spec assertEqual roundtrip_spec spec test_pytree_register_nested_data_class dataclass Inner x int y int dataclass Outer xy Inner ab Inner xy = Inner ab = Inner dt = Outer xy ab inp = dt dt dt torch ones dt torch export register_dataclass Inner serialized_type_name= test_pytree_register_nested_data_class Inner torch export register_dataclass Outer serialized_type_name= test_pytree_register_nested_data_class Outer flat spec = tree_flatten inp assertEqual flat torch ones unflat = tree_unflatten flat spec assertEqual unflat inp roundtrip_spec = treespec_loads treespec_dumps spec assertEqual roundtrip_spec spec test_param_util Basic torch nn Module __init__ - None super __init__ lin = torch nn Linear forward x lin x ep = export Basic torch randn num_params = params = node ep graph nodes is_param ep node num_params += params append get_param ep node assertEqual num_params assertEqual params shape weight assertEqual params shape bias test_buffer_util ep = export torch nn BatchNorm d affine=False torch ones num_buffer = buffer = node ep graph nodes is_buffer ep node num_buffer += buffer append get_buffer ep node assertEqual num_buffer The insertion order guaranteed same strict vs non-strict so commenting out assertEqual buffer shape torch Size running_mean assertEqual buffer shape torch Size running_var assertEqual buffer shape torch Size num_batches_tracked test_export_dynamo_config MyModule torch nn Module __init__ - None super __init__ lstm = torch nn LSTM input_size= hidden_size= num_layers= forward inputs torch Tensor - torch Tensor lstm inputs config = DEFAULT_EXPORT_DYNAMO_CONFIG mod = MyModule contextmanager _patch_config kwargs orig_config_dict = dataclasses asdict config try k v kwargs items setattr config k v yield finally k v orig_config_dict items setattr config k v inp = torch rand exported_program = export mod inp strict=True _patch_config allow_rnn False assertRaisesRegex torch _dynamo exc Unsupported Dynamo does support RNN GRU LSTM _ = export mod inp strict=True test_device_to_static Module torch nn Module forward x x cpu ep = export Module torch tensor device= cpu ops = node ep graph nodes node op == call_function ops append node target is_training_ir_test _testMethodName aten will just specialize decomposing no-op assertEqual ops torch ops aten _assert_tensor_metadata default assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten dtype_layout ep = ep run_decompositions ops = node ep graph nodes node op == call_function ops append node target assertEqual len ops test_device_to_dynamic Module torch nn Module forward x x cpu ep = export Module torch tensor device= cpu dynamic_shapes= x Dim i ops = node ep graph nodes node op == call_function ops append node target is_training_ir_test _testMethodName aten will just specialize decomposing no-op assertEqual ops torch ops aten _assert_tensor_metadata default assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten dtype_layout ep = ep run_decompositions ops = node ep graph nodes node op == call_function ops append node target assertEqual len ops test_device_to_mutation Module torch nn Module forward x y = x cpu y add_ y x ep = export Module torch tensor device= cpu ops = node ep graph nodes node op == call_function ops append node target is_training_ir_test _testMethodName aten decomposes no-op add_ decomposes functional variant assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten add Tensor assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten dtype_layout torch ops aten add_ Tensor test mutation x = torch tensor device= cpu y _ = ep module x assertEqual x item assertEqual id y id x test decomp ep ep = ep run_decompositions node ep graph nodes node op == call_function assertNotEqual node target torch ops aten dtype_layout test mutation decomposed program y _ = ep module x assertEqual x item assertEqual id y id x requires_gpu testing expectedFailureCppRuntime test_device_to_gpu Foo torch nn Module forward x x cpu ep = export Foo torch randn GPU_TYPE ops = node ep graph nodes node op == call_function ops append node target is_training_ir_test _testMethodName aten decomposes _to_copy assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten _to_copy default assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten dtype_layout Check device assertion assertRaisesRegex RuntimeError Tensor device mismatch ep module torch randn ep = ep run_decompositions ops = node ep graph nodes node op == call_function ops append node target assertEqual len ops assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten _to_copy default Check device assertion again after decomp assertRaisesRegex RuntimeError Tensor device mismatch ep module torch randn test_tensor_constant_aten_to Module torch nn Module __init__ super Module __init__ t = torch tensor forward x x + t torch float inputs = torch randn model = Module ep = export model inputs run_decompositions ops = node ep graph nodes node op == call_function ops append node target assertGreater len ops assertIn torch ops aten _to_copy default ops assertEqual ep module inputs model inputs test_export_aten_to_unflatten Bar torch nn Module __init__ super __init__ forward x x sum Foo torch nn Module __init__ super __init__ bar = Bar forward x = x torch float bar sum inp = torch randn ep = export Foo inp strict=False preserve_module_call_signature= bar mod = ep module assertTrue torch allclose mod inp Foo inp testing expectedFailureLegacyExportNonStrict testing expectedFailureLegacyExportStrict testing expectedFailureRetraceabilityNonStrict when we retrace ep module hierarchical testing expectedFailureRetraceability when we retrace ep module hierarchical test_export_aten_to_unflatten_subclass Bar torch nn Module __init__ super __init__ forward x x sum Foo torch nn Module __init__ super __init__ bar = Bar param = torch nn Parameter TwoTensor torch ones torch ones forward x = param torch float bar sum + x sum get_elem_a inp = torch randn assertRaisesRegex ValueError It looks like p_param tensor subclass export Foo inp strict=False preserve_module_call_signature= bar run_decompositions test_export_aten_to_unflatten_subclass_pre_dispatch Bar torch nn Module __init__ super __init__ forward x x sum Foo torch nn Module __init__ super __init__ bar = Bar param = torch nn Parameter TwoTensor torch ones torch ones forward x = param torch float bar sum + x sum get_elem_a inp = torch randn ep = torch export export Foo inp strict=False preserve_module_call_signature= bar unflat = unflatten ep bar assertExpectedInline str unflat graph strip \ graph _positional_arg_ num_users= = placeholder target=_positional_arg_ _spec_ num_users= = get_attr target=_spec_ tree_flatten_spec num_users= = call_function target=torch fx _pytree tree_flatten_spec args = _positional_arg_ _spec_ kwargs = num_users= = call_function target=operator getitem args = tree_flatten_spec kwargs = sum_ num_users= = call_function target=torch ops aten sum default args = kwargs = _spec_ num_users= = get_attr target=_spec_ tree_unflatten num_users= = call_function target=torch utils _pytree tree_unflatten args = sum_ _spec_ kwargs = tree_unflatten assertRaisesRegex ValueError It looks like p_param tensor subclass ep run_decompositions test_float_conversion Module torch nn Module forward x x float ep = export Module torch tensor dtype=torch float ops = node ep graph nodes node op == call_function ops append node target is_training_ir_test _testMethodName float decomposes no-op assertEqual ops torch ops aten _assert_tensor_metadata default assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten dtype ep = ep run_decompositions ops = node ep graph nodes node op == call_function ops append node target assertEqual len ops test aliasing x = torch tensor dtype=torch float out = ep module x assertEqual id x id out test_float_conversion_from_int Module torch nn Module forward x x float ep = export Module torch tensor dtype=torch int ops = node ep graph nodes node op == call_function ops append node target is_training_ir_test _testMethodName float decomposes _to_copy assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten _to_copy default assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten dtype Raises error because input dtype same input tensor when exporting assertRaisesRegex RuntimeError Tensor dtype mismatch ep module torch tensor dtype=torch float ep = ep run_decompositions ops = node ep graph nodes node op == call_function ops append node target assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten _to_copy default Check dtype assertion again after decomp assertRaisesRegex RuntimeError Tensor dtype mismatch ep module torch tensor dtype=torch float assertEqual ep module torch tensor dtype=torch int test_device_to_mutation_float Module torch nn Module forward x y = x float y add_ y x ep = export Module torch tensor dtype=torch float ops = node ep graph nodes node op == call_function ops append node target is_training_ir_test _testMethodName aten decomposes no-op add_ decomposes functional variant assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten add Tensor assertEqual ops torch ops aten _assert_tensor_metadata default torch ops aten dtype torch ops aten add_ Tensor test mutation x = torch tensor dtype=torch float y _ = ep module x assertEqual x item assertEqual id y id x test decomp ep ep = ep run_decompositions node ep graph nodes node op == call_function assertNotEqual node target torch ops aten dtype test mutation decomposed program y _ = ep module x assertEqual x item assertEqual id y id x test_module MyLinear torch nn Module __init__ - None super __init__ weight = torch randn bias = torch randn forward x torch nn functional linear x weight bias Foo torch nn Module __init__ - None super __init__ conv = torch nn Conv d linear = MyLinear forward x b = x a_conv = conv a_linear = linear a_conv b_conv = conv b b_linear = linear b_conv a_linear cos + b_linear sin a_linear sin + b_linear cos inp_container = torch randn torch randn ep = export Foo inp_container ep_rexported = export ep module inp_container inp_test = torch randn torch randn assertTrue torch allclose ep module inp_test ep_rexported module inp_test assertTrue torch allclose ep module inp_test ep_rexported module inp_test test_use_embedding_twice Foo torch nn Module __init__ super __init__ embed = torch nn Embedding forward x embed x + embed weight x inputs = torch tensor ep = export Foo inputs test_module_with_dict_container_inp_out MyLinear torch nn Module __init__ - None super __init__ weight = torch randn bias = torch randn forward x torch nn functional linear x weight bias Foo torch nn Module __init__ - None super __init__ conv = torch nn Conv d linear = MyLinear forward x = x b = x b _conv = conv _linear = linear _conv _conv = conv _linear = linear _conv b_conv = conv b b_linear = linear b_conv _linear cos + b_linear sin b _linear sin + b_linear cos inp_container = torch randn torch randn b torch randn ep = export Foo inp_container ep_rexported = export ep module inp_container inp_test = torch randn torch randn b torch randn assertTrue torch allclose ep module inp_test ep_rexported module inp_test assertTrue torch allclose ep module inp_test b ep_rexported module inp_test b test_args_type_checked M torch nn Module forward x x + inp = torch rand assertRaisesRegex torch _dynamo exc UserError tuple Intentionally wrapping ` inp ` tuple trigger error _ = export M inp test_decomp_item_in_prim_before_decomposition M torch nn Module forward x torch ops aten _assert_async msg torch tensor True Fail x ep = export M torch randn FileCheck check_count torch ops aten _assert_async msg exactly=True run ep graph_module code test_decomp_item_in_prim_after_decomposition M torch nn Module forward x torch ops aten _assert_async msg torch tensor True Fail x decomp_table = default_decompositions decomposition_table ep = torch export export M torch randn run_decompositions decomp_table assertExpectedInline str ep graph_module code strip \ forward c_lifted_tensor_ x clone = torch ops prims clone default c_lifted_tensor_ memory_format = torch preserve_format c_lifted_tensor_ = None _assert_async = torch ops aten _assert_async msg clone Fail clone = _assert_async = None x test_decomp_batch_norm_functional_predispatch ConvBatchnorm torch nn Module __init__ - None super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d forward x x = conv x x = bn x x mod = ConvBatchnorm mod eval inp = torch randn gm = torch export export mod inp module assertExpectedInline str gm code strip \ forward x x = fx_pytree tree_flatten_spec x _in_spec conv_weight = conv weight conv_bias = conv bias bn_weight = bn weight bn_bias = bn bias bn_running_mean = bn running_mean bn_running_var = bn running_var bn_num_batches_tracked = bn num_batches_tracked bn_num_batches_tracked = None _guards_fn = _guards_fn x _guards_fn = None conv d = torch ops aten conv d default x conv_weight conv_bias x = conv_weight = conv_bias = None batch_norm = torch ops aten batch_norm default conv d bn_weight bn_bias bn_running_mean bn_running_var False e- True conv d = bn_weight = bn_bias = bn_running_mean = bn_running_var = None pytree tree_unflatten batch_norm _out_spec mod train gm_train = torch export export mod inp module assertExpectedInline str gm_train code strip \ forward x x = fx_pytree tree_flatten_spec x _in_spec conv_weight = conv weight conv_bias = conv bias bn_weight = bn weight bn_bias = bn bias bn_running_mean = bn running_mean bn_running_var = bn running_var bn_num_batches_tracked = bn num_batches_tracked _guards_fn = _guards_fn x _guards_fn = None conv d = torch ops aten conv d default x conv_weight conv_bias x = conv_weight = conv_bias = None add_ = torch ops aten add_ Tensor bn_num_batches_tracked bn_num_batches_tracked = add_ = None batch_norm = torch ops aten batch_norm default conv d bn_weight bn_bias bn_running_mean bn_running_var True e- True conv d = bn_weight = bn_bias = bn_running_mean = bn_running_var = None pytree tree_unflatten batch_norm _out_spec test_constrain_size_in_eager Module torch nn Module forward x y n = x max item torch _check n = y + n fn = Module ep = export fn torch randint torch randint test_inp = torch randint torch randint assertTrue torch allclose ep module test_inp fn test_inp test_constrain_size_with_constrain_value Module torch nn Module forward x y n = x max item torch _check n = torch _check n = y + n fn = Module assertRaisesRegex RuntimeError r Expected cond True got False _ = fn torch randint torch randint ep = export fn torch randint torch randint assertRaisesRegex RuntimeError r Runtime assertion failed expression u \d+ \ \= test_inp = torch randint torch randint _ = ep module test_inp test_while_loop_simple Simple torch nn Module forward ci b cond_fn i x y i body_fn i x y i - x + y y - x torch _higher_order_ops while_loop cond_fn body_fn ci b example_inputs = torch tensor torch randn torch randn ep = export Simple example_inputs assertEqual ep module example_inputs Simple example_inputs test_constrain_size_with_various_cases Module torch nn Module forward x y n = x item torch _check n = y sum + torch ones n sum case = Module Module torch nn Module forward x y n = x item torch _check n = torch _check n = y sum + torch ones n sum case = Module Module torch nn Module forward x y n = x item torch _check n = torch _check n = y sum + torch ones n sum case = Module Module torch nn Module forward x y n = x item torch _check n = y sum + torch ones n sum case = Module Module torch nn Module forward x y n = x item torch _check n = y sum + torch ones n sum case = Module ep = export case torch tensor torch ones assertRaisesRegex RuntimeError r Expected cond True got False _ = case torch tensor - torch randn assertTrue torch allclose ep module torch tensor torch ones case torch tensor torch ones ep = export case torch tensor torch randn assertRaisesRegex RuntimeError r Expected cond True got False _ = case torch tensor torch randn assertRaisesRegex RuntimeError r Expected cond True got False _ = case torch tensor torch randn assertTrue torch allclose ep module torch tensor torch ones case torch tensor torch ones _ = case torch tensor torch randn assertRaisesRegex RuntimeError r Expected cond True got False _ = case torch tensor torch randn ep = export case torch tensor torch randn assertRaisesRegex RuntimeError r Expected cond True got False _ = case torch tensor torch randn assertTrue torch allclose ep module torch tensor torch ones case torch tensor torch ones ep = export case torch tensor torch randn assertRaisesRegex RuntimeError r Expected cond True got False _ = case torch tensor torch randn assertTrue torch allclose ep module torch tensor torch ones case torch tensor torch ones test_automatic_constrain_size M torch nn Module forward x y n = x item y sum + torch ones n sum ep = export M torch tensor torch ones This because we insert sym_constrain_range graph now error_msg = r failed expression u = node assertRaisesRegex RuntimeError error_msg _ = ep module torch tensor - torch randn assertTrue torch allclose ep module torch tensor torch ones M torch tensor torch ones test_cleanup_dynamic_markers - None Foo torch nn Module forward inputs x y = inputs x inputs y x + y inputs = x torch randn y torch randn shapes = inputs x Dim AUTO Dim STATIC y Dim DYNAMIC Dim STATIC ep = export Foo inputs dynamic_shapes=shapes tensor inputs values attr _dynamo_weak_dynamic_indices _dynamo_dynamic_indices _dynamo_dynamic_range _dynamo_static_indices _dynamo_unbacked_indices assertFalse hasattr tensor attr testing expectedFailureCppRuntime test_while_loop_index_assertions torch _higher_order_ops while_loop Foo torch nn Module forward x cond_fn idx acc i = idx item i x size body_fn idx acc check_is_size call needs traced subgraph select call can t cond graph fires fails right before loop termination i = idx item idx + acc + x i acc = torch zeros x size n = torch full dtype=torch int _ out = while_loop cond_fn body_fn n acc out x = torch randn ep = export Foo x strict=False assertTrue torch allclose x sum dim= ep module x testing expectedFailureCppRuntime test_while_loop_assert_separation torch _higher_order_ops while_loop Bar torch nn Module forward idx x i = idx item cond_fn idx x i = idx item torch _check i = i = body_fn idx x i = idx item torch _check i == idx + x + i while_loop cond_fn body_fn idx x + i inps = torch tensor torch zeros ep = export Bar inps strict=False i out = ep module inps assertEqual i assertEqual out item check assertions separate each subgraph assertRaisesRegex RuntimeError r Runtime assertion failed expression Ne\ u \d + \ ep graph_module while_loop_cond_graph_ torch tensor torch zeros assertRaisesRegex RuntimeError r Runtime assertion failed expression Eq\ PythonMod\ u \d + \ \ ep graph_module while_loop_body_graph_ torch tensor torch zeros test_constrain_decomp - None M torch nn Module __init__ - None super __init__ freq = torch ones forward start_pos torch Tensor pos = start_pos item torch _check pos = torch _check pos = freq pos freq pos ep = export M torch tensor FileCheck check_count torch ops aten _assert_scalar default exactly=True run ep graph_module code decompose_ep = ep run_decompositions FileCheck check_count torch ops aten _assert_scalar default exactly=True run ep graph_module code test_mixed_input Module torch nn Module forward b alpha int torch add b alpha=alpha func = Module = torch rand b = torch rand alpha = exported = export func b alpha node exported graph_module graph nodes node op == placeholder assertTrue isinstance node meta val Tensor int testing expectedFailureRetraceability size gets unflattened into tuple test_size_input Model torch nn Module __init__ super Model __init__ forward theta size torch nn functional affine_grid theta size align_corners=None model = Model theta = torch ones size = torch Size inp = theta size eager_result = model inp ep = export model inp epm = ep module ep_result = epm inp assertTrue torch allclose ep_result eager_result args _kwargs = ep example_inputs assertTrue torch allclose arg i arg i zip args inp test_tensor_constant_with_wrapped_method M torch nn Module __init__ super __init__ constant = torch ones forward x x + constant constant Wrapper torch nn Module __init__ fn super __init__ fn = fn forward arg kwargs fn arg kwargs inp = torch zeros test m m_result = m inp ep_result = export m inp module inp m_t ep_t zip m_result ep_result assertTrue torch allclose m_t ep_t test M test Wrapper M forward test_export_with_inline_constraints Module torch nn Module forward x = x item torch _check = torch _check = torch randn f = Module ep = export f torch tensor assertEqual ep module torch tensor shape FileCheck check_count torch ops aten _assert_scalar default exactly=True run ep graph_module code assertRaisesRegex RuntimeError r Runtime assertion failed expression u \d+ \ \= cm ep module torch tensor test_export_with_inline_constraints_complex Module torch nn Module forward x = x item torch _check = torch _check = randn = torch randn torch cat randn transpose torch zeros f = Module ep = export f torch tensor assertEqual ep module torch tensor shape FileCheck check_count torch ops aten _assert_scalar default exactly=True run ep graph_module code test_to_module_with_mutated_buffer Foo torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch zeros forward x buf add_ x sum + buf sum exported = export Foo torch ones stateful_gm = exported module export_return_val = stateful_gm torch ones eager = Foo eager_return_val = eager torch ones assertTrue torch allclose eager_return_val export_return_val name buffer stateful_gm named_buffers assertTrue torch allclose torch ones buffer changed = stateful_gm graph eliminate_dead_code assertFalse changed assertTrue torch allclose stateful_gm torch ones eager torch ones name buffer stateful_gm named_buffers assertTrue torch allclose torch tensor dtype=torch float buffer test_to_module_with_mutated_buffer_multiple Bar torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch ones forward x buf add_ x sum + buf sum Foo torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch zeros bar = Bar forward x buf add_ bar buf add_ bar = bar x bar sum + buf sum exported = export Foo torch ones stateful_gm = exported module export_return_val = stateful_gm torch ones eager = Foo eager_return_val = eager torch ones assertTrue torch allclose eager_return_val export_return_val name buffer stateful_gm named_buffers name == L__self___buf assertTrue torch allclose torch ones buffer name == L__self___bar_buf assertTrue torch allclose torch tensor dtype=torch float buffer changed = stateful_gm graph eliminate_dead_code assertFalse changed assertTrue torch allclose stateful_gm torch ones eager torch ones name buffer stateful_gm named_buffers name == L__self___buf assertTrue torch allclose torch tensor dtype=torch float buffer name == L__self___bar_buf assertTrue torch allclose torch tensor dtype=torch float buffer test_module_input Foo torch nn Module forward x y m m x y + x + y i = InputModule f = Foo ep = export f torch randn torch randn i strict=False m = InputModule inputs = torch randn torch randn m assertEqual f inputs ep module inputs test_module_input_subclasses_parameterization_nested Module torch nn Module forward x m m x mod = InputModuleWithNestedSubclass f = Module ref_x = torch randn ref_out = f ref_x mod ep = torch export export f torch randn mod strict=False assertEqual ref_out ep module ref_x mod test_unbacked_noncontig_lin cpp_runtime_nonstrict id skipTest TODO Unexpected success OSS fbcode Foo torch nn Module __init__ super __init__ lin = torch nn Linear forward x n = x item y = torch empty x view - lin y mod = Foo x = torch tensor ep = export mod x assertEqual mod x shape ep module x shape x = torch tensor assertEqual mod x shape ep module x shape test_runtime_assert_for_prim Foo torch nn Module forward x y x + y foo = Foo tensor_inp = torch ones dim _x = torch export Dim dim _x min= dynamic_shapes = x dim _x y None exported = torch export export foo tensor_inp dynamic_shapes=dynamic_shapes assertTrue torch allclose exported module torch ones foo torch ones assertRaisesRegex AssertionError escape Guard failed y == expected got _ = exported module torch ones exported = torch export export foo tensor_inp dynamic_shapes=dynamic_shapes assertRaisesRegex AssertionError escape Guard failed y == expected got _ = exported module torch ones test_runtime_assert_for_prm_str Foo torch nn Module forward b mode torch div b rounding_mode=mode foo = Foo inps = torch randn torch randn trunc exported = export foo inps assertRaisesRegex AssertionError escape Guard failed mode == trunc expected trunc got floor _ = exported module torch randn torch randn floor assertTrue torch allclose exported module inps foo inps test_sym_or_sym_and cpp_runtime_nonstrict id skipTest TODO Unexpected success OSS fbcode torch fx experimental symbolic_shapes sym_and sym_or Foo torch nn Module forward xs u u u = xs tolist torch _check sym_or u == u == u == torch _check sym_and u = u = u == u + u + u ep = export Foo torch tensor strict=False ep module torch tensor ep module torch tensor ep module torch tensor assertRaisesRegex RuntimeError r expression Eq\ u \ \ &#124; Eq\ u \ \ &#124; Eq\ u \ ep module torch tensor assertRaisesRegex RuntimeError r expression u \d + = ep module torch tensor test_redundant_assert_max_upper_bound M torch nn Module forward x b = x nonzero torch _check b shape = b m = M inp = torch tensor dim = torch export Dim dim ep = export m inp dynamic_shapes= dim FileCheck check_count torch ops aten _assert_scalar default exactly=True run ep graph_module code test_to_module_with_mutated_buffer_multiple_update_sub_later Bar torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch ones forward x buf add_ x sum + buf sum Foo torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch zeros bar = Bar forward x buf add_ bar = bar x bar buf add_ bar sum + buf sum exported = export Foo torch ones stateful_gm = exported module export_return_val = stateful_gm torch ones eager = Foo eager_return_val = eager torch ones assertTrue torch allclose eager_return_val export_return_val name buffer stateful_gm named_buffers name == L__self___buf assertTrue torch allclose torch ones buffer name == L__self___bar_buf assertTrue torch allclose torch tensor dtype=torch float buffer changed = stateful_gm graph eliminate_dead_code assertFalse changed assertTrue torch allclose stateful_gm torch ones eager torch ones name buffer stateful_gm named_buffers name == L__self___buf assertTrue torch allclose torch tensor dtype=torch float buffer name == L__self___bar_buf assertTrue torch allclose torch tensor dtype=torch float buffer test_retracable_ep Bar torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch ones forward x buf add_ x sum + buf sum Foo torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch zeros bar = Bar forward x buf add_ bar = bar x bar buf add_ bar sum + buf sum inp = torch ones exported = torch export export Foo inp reexported = torch export export exported module inp assertTrue torch allclose Foo inp reexported module inp dim _x = torch export Dim dim _x exported = torch export export Foo inp dynamic_shapes= dim _x reexported = torch export export exported module inp assertRaisesRegex AssertionError escape Guard failed x size == expected got reexported module torch ones reexported = torch export export exported module inp dynamic_shapes= dim _x assertTrue torch allclose Foo torch ones reexported module torch ones can t retrace invalid inputs respect original ExportedProgram dim _x_v = torch export Dim dim _x_v min= exported_v = torch export export Foo inp dynamic_shapes= x dim _x_v assertRaisesRegex AssertionError escape Guard failed x size = expected = got torch export export exported_v module torch randn test_export_cond_symbool_pred A torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones forward buffer cos Foo torch nn Module __init__ - None super __init__ = A forward x true_fn x x cos + sum false_fn x x sin cond x shape true_fn false_fn x dim = torch export Dim dim min= inp = torch ones ep = export Foo inp dynamic_shapes= x dim schema = get_hop_schema ep assertExpectedInline str schema cond SymBool pred GraphModule true_fn GraphModule false_fn Tensor operands - Tensor serdes deserializes tuple list need_serdes_test _testMethodName assertExpectedInline ep graph_module code strip \ forward b_a_buffer x sym_size_int_ = torch ops aten sym_size int x gt = sym_size_int_ sym_size_int_ = None true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond gt true_graph_ false_graph_ x b_a_buffer gt = true_graph_ = false_graph_ = x = b_a_buffer = None getitem = cond cond = None getitem assertExpectedInline ep graph_module code strip \ forward b_a_buffer x sym_size_int_ = torch ops aten sym_size int x gt = sym_size_int_ sym_size_int_ = None true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond gt true_graph_ false_graph_ x b_a_buffer gt = true_graph_ = false_graph_ = x = b_a_buffer = None getitem = cond cond = None getitem assertTrue torch allclose ep module torch ones Foo torch ones test_ccode_python_mod sympy torch utils _sympy functions PythonMod Foo torch nn Module forward xs u u = xs tolist u u ep = export Foo torch tensor strict=False u _node u _node = list ep graph nodes - args u = u _node meta val u = u _node meta val assertExpectedInline sympy ccode PythonMod u u u + u assertExpectedInline sympy ccode PythonMod u u u u u u + abs u u u test_aten_lift_fresh_copy M torch nn Module forward x torch ops aten lift_fresh_copy x ep = export M torch ones run_decompositions found = False op = torch ops aten clone default FileCheck check_count op exactly=True run ep graph_module code test_cond_buffers M torch nn Module __init__ - None super __init__ register_parameter param torch nn Parameter torch ones requires_grad=False buffer = torch nn Buffer torch ones + true_fn x x + param false_fn x x + buffer forward x cond x shape == true_fn false_fn x inp = torch ones ep = torch export export M inp inp = torch randn epm = ep module assertTrue torch allclose epm inp M inp gm epm named_modules isinstance gm torch fx GraphModule continue assertEqual len node node gm graph nodes node op == placeholder requires_cuda_and_triton testing expectedFailureCppRuntime test_export_associative_scan_symbol_dim device = torch device cuda combine_mode = pointwise dim = torch export Dim dim min= max= xs = torch ones device=device Foo torch nn Module __init__ - None super __init__ combine_fn x y x + y forward x associative_scan combine_fn x combine_mode=combine_mode ep = export Foo xs dynamic_shapes= x dim module_out = Foo xs assertTrue torch allclose ep module xs module_out requires_cuda_and_triton testing expectedFailureCppRuntime test_export_associative_scan_symbol_scandim device = torch device cuda combine_mode = pointwise dim = torch export Dim dim min= max= xs = torch ones device=device Foo torch nn Module __init__ - None super __init__ combine_fn x y x + y forward x associative_scan combine_fn x combine_mode=combine_mode ep = export Foo xs dynamic_shapes= x dim module_out = Foo xs assertTrue torch allclose ep module xs module_out requires_cuda_and_triton test_export_associative_scan_lifted_buffers cpp_runtime_nonstrict id skipTest TODO Unexpected success OSS fbcode device = torch device cuda combine_mode = pointwise A torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones device=device forward buffer cos M torch nn Module __init__ - None super __init__ = A combine_fn x y x + y forward x associative_scan combine_fn x combine_mode=combine_mode inp = torch ones device=device ep = export M inp epm = ep module assertTrue torch allclose epm inp M inp gm epm named_modules isinstance gm torch fx GraphModule continue assertEqual len node node gm graph nodes node op == placeholder scan supported sigmoid yet testing expectedFailureCppRuntime test_export_scan_pytree_output add carry accum carry + carry accum moo + accum moo + M torch nn Module forward init accum scan add init accum inp = torch randn init xs = torch ones moo torch ones moo torch ones ep = export M init xs assertEqual ep module init xs M init xs test_map_buffers M torch nn Module __init__ - None super __init__ register_parameter param torch nn Parameter torch tensor requires_grad=False buffer = torch nn Buffer torch tensor + m = M map_fn x y z = x + y + m param + m buffer z add_ z M torch nn Module forward xs y map map_fn xs y example_inputs = torch ones torch tensor ep = torch export export M example_inputs example_inputs = torch randn torch tensor epm = ep module assertTrue torch allclose epm example_inputs M example_inputs gm epm named_modules isinstance gm torch fx GraphModule continue assertEqual len node node gm graph nodes node op == placeholder test_no_check_is_size_error Module torch nn Module forward x = x item torch randn view f = Module ep = export f torch tensor ep module torch tensor assertRaisesRegex RuntimeError r Runtime assertion failed u ep module torch tensor test_suggest_torch_checks_with_non_negative_check unittest mock patch sympy torch export dynamic_shapes defaultdict torch fx experimental symbolic_shapes _suggest_torch_checks u = sympy Symbol u cond = u = mock_exception = MagicMock spec=torch fx experimental symbolic_shapes GuardOnDataDependentSymNode mock_exception args = Test error message mock_exception cond = cond mock_printer = MagicMock mock_printer doprint side_effect = lambda expr str cond expr == cond u Simulating condition patch torch fx experimental symbolic_shapes _PythonMsgPrinter return_value=mock_printer src_map = defaultdict list src_map u = u _suggest_torch_checks mock_exception src_map error_msg = mock_exception args assertIn torch _check u error_msg test_suggest_torch_checks_with_regular_check sympy torch export dynamic_shapes defaultdict torch fx experimental symbolic_shapes _suggest_torch_checks mock_exception = MagicMock spec=torch fx experimental symbolic_shapes GuardOnDataDependentSymNode mock_exception args = Test error message mock_cond = MagicMock mock_cond free_symbols = sympy Symbol u mock_exception cond = mock_cond mock_printer = MagicMock mock_printer doprint side_effect = lambda expr u expr == mock_cond u = patch torch fx experimental symbolic_shapes _PythonMsgPrinter return_value=mock_printer src_map = defaultdict list src_map u = u _suggest_torch_checks mock_exception src_map error_msg = mock_exception args assertIn torch _check u error_msg assertIn torch _check u = error_msg test_train_eval_on_exported_preautograd_module Foo torch nn Module __init__ - None super __init__ forward x x shape x cos x sin graph_module = _export Foo torch ones pre_dispatch=True module assertRaisesRegex NotImplementedError r Calling train\ \ supported yet graph_module train assertRaisesRegex NotImplementedError r Calling eval\ \ supported yet graph_module eval test_lifted_constants - None Module torch nn Module forward x x + torch tensor f = Module ep = export f torch tensor assertEqual len ep graph_signature input_specs assertEqual len ep constants Foo torch nn Module __init__ - None super __init__ = torch tensor forward x list_tensor = torch tensor torch tensor x + + list_tensor + list_tensor ep = export Foo torch tensor assertEqual len ep graph_signature input_specs assertEqual len ep state_dict assertEqual len ep constants inp = torch tensor assertTrue torch allclose ep module inp Foo inp transform = ep run_decompositions assertEqual len ep graph_signature input_specs assertTrue torch allclose ep module inp transform module inp Boo torch nn Module __init__ - None super __init__ = torch tensor True forward x list_tensor = torch tensor False torch tensor True x + + list_tensor + list_tensor ep = export Boo torch tensor False assertEqual len ep graph_signature input_specs assertEqual len ep state_dict assertEqual len ep constants inp = torch tensor True assertTrue torch allclose ep module inp Boo inp transform = ep run_decompositions assertEqual len ep graph_signature input_specs assertTrue torch allclose ep module inp transform module inp test_tensor_attribute_zero_args Foo torch nn Module __init__ value super __init__ x = torch tensor value forward x clone m = Foo ep = export m assertEqual ep graph_signature lifted_tensor_constants x test_preserve_shape_dynamism_for_unused_inputs torch export register_dataclass Inp serialized_type_name= test_preserve_shape_dynamism_for_unused_inputs Inp Module torch nn Module forward x Inp x f + mod = Module example_inputs = Inp f=torch ones p=torch zeros ep_static = export mod example_inputs node ep_static graph nodes node op == placeholder s node meta val shape assertIsInstance s int dim _x_f dim _x_p = torch export dims dim _x_f dim _x_p dynamic_shapes = x dim _x_f dim _x_p ep_dynamic = export mod example_inputs dynamic_shapes=dynamic_shapes node ep_dynamic graph nodes node op == placeholder i s enumerate node meta val shape i == assertIsInstance s torch SymInt assertIsInstance s int test_multiple_definitions_same_name_dim Foo torch nn Module forward x y torch matmul x y A = torch export Dim C min= B = torch export Dim C max= assertRaisesRegex torch _dynamo exc UserError Found different definitions Dim\\ min= \\ Dim\\ max= \\ same symbolic dimension torch export export Foo torch randn torch randn dynamic_shapes= x A B y B A test_multinomial_dynamic Model torch nn Module forward x y torch multinomial x y shape model = Model DYNAMIC = torch export Dim DYNAMIC exported_module inputs dynamic_shapes = tuple tuple DYNAMIC _ inp shape inp inputs ep = export model inputs dynamic_shapes=dynamic_shapes ep module check inputs epm eager_result = model inputs ep_result = epm inputs assertEqual ep_result shape eager_result shape inputs = torch tensor dtype=torch float torch ones dtype=torch int epm = exported_module inputs output shape where n_sample = dist_size check inputs epm inputs = torch tensor dtype=torch float torch ones dtype=torch int output shape n_sample = dist_size check inputs epm inputs = torch tensor dtype=torch float torch ones dtype=torch int assertRaisesRegex RuntimeError cannot sample n_sample dist_size epm inputs inputs = torch tensor dtype=torch float torch ones dtype=torch int epm = exported_module inputs output shape n_row n_sample = dist_size check inputs epm inputs = torch tensor dtype=torch float torch ones dtype=torch int epm = exported_module inputs assertRaisesRegex RuntimeError cannot sample n_sample dist_size epm inputs test_export_with_wrong_inputs MyModule torch nn Module forward x x + x exported_program = export MyModule torch rand assertRaisesRegex ValueError Trying flatten user inputs exported_program module torch rand torch rand test_export_decomps_simple M torch nn Module __init__ - None super __init__ lin = torch nn Linear forward x lin x inp = torch randn m = M ep = export m inp state_dict = ep state_dict assertTrue torch allclose ep module inp m inp core_aten_ep = ep run_decompositions FileCheck check_count torch ops aten permute default exactly=True run core_aten_ep graph_module code FileCheck check_count torch ops aten t default exactly=True run core_aten_ep graph_module code assertTrue torch allclose core_aten_ep module inp m inp assertEqual id state_dict id ep state_dict unittest skipIf IS_FBCODE We can t customize decomp fbcode test_export_decomp_torture_case_ M torch nn Module __init__ - None super __init__ lin = torch nn Linear forward x lin x inp = torch randn m = M ep = export m inp custom_decomp_callable x weight bias x + bias decomp_table = default_decompositions decomp_table torch ops aten linear default = custom_decomp_callable core_aten_ep = ep run_decompositions decomp_table assertExpectedInline str core_aten_ep graph_module code strip \ forward p_lin_weight p_lin_bias x add = torch ops aten add Tensor x p_lin_bias x = p_lin_bias = None add unittest skipIf IS_FBCODE We can t customize decomp fbcode test_export_decomp_torture_case_ MyLinear torch nn Module __init__ - None super __init__ weight = torch randn bias = torch randn forward x torch nn functional linear x weight bias Foo torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv d = torch nn Conv d linear = MyLinear forward x y x_conv = conv x y_conv_ d = conv d y x_linear = linear x_conv x_linear cos + y_conv_ d sum ep = export Foo torch randn torch randn ep_has_linear_convd = ep run_decompositions decomp_table= _decompose_linear_custom x weight bias torch matmul x weight T + bias ep_decompose_linear = ep_has_linear_convd run_decompositions decomp_table= torch ops aten linear default _decompose_linear_custom assertExpectedInline str ep_decompose_linear graph_module code strip \ forward p_conv_weight p_conv_bias p_conv d_weight p_conv d_bias c_linear_weight c_linear_bias x y conv d = torch ops aten conv d default x p_conv_weight p_conv_bias x = p_conv_weight = p_conv_bias = None conv d = torch ops aten conv d default y p_conv d_weight p_conv d_bias y = p_conv d_weight = p_conv d_bias = None permute = torch ops aten permute default c_linear_weight c_linear_weight = None matmul = torch ops aten matmul default conv d permute conv d = permute = None mul = torch ops aten mul Tensor c_linear_bias c_linear_bias = None add = torch ops aten add Tensor matmul mul matmul = mul = None cos = torch ops aten cos default add add = None sum_ = torch ops aten sum default conv d conv d = None add_ = torch ops aten add Tensor cos sum_ cos = sum_ = None add_ test_export_decomps_dynamic M torch nn Module __init__ - None super __init__ lin = torch nn Linear forward x lin x inp = torch randn m = M ep = export m inp dynamic_shapes= x Dim batch core_aten_ep = ep run_decompositions input_node = node node core_aten_ep graph nodes node op == placeholder - assertTrue isinstance input_node meta val shape torch SymInt FileCheck check_count torch ops aten permute default exactly=True run core_aten_ep graph_module code FileCheck check_count torch ops aten t default exactly=True run core_aten_ep graph_module code assertTrue torch allclose core_aten_ep module inp m inp test_nonzero_ Module torch nn Module forward x torch nonzero x f = Module ep = export f torch ones inp = torch randn assertTrue torch allclose ep module inp torch nonzero inp test_redundant_asserts Foo torch nn Module forward x y = x item torch zeros y f = Foo ep = export f torch tensor FileCheck check_count torch ops aten _assert_scalar default exactly=True run ep graph_module code ep = ep run_decompositions FileCheck check_count torch ops aten _assert_scalar default exactly=True run ep graph_module code test_non_arg_name_dynamic_shapes_api Foo torch nn Module forward b sum + b sum foo = Foo dim = torch export Dim dim ep = torch export export foo torch randn torch randn dynamic_shapes= None dim test_inp = torch randn torch randn assertEqual ep module test_inp foo test_inp ep_v = torch export export foo torch randn torch randn dynamic_shapes= None None assertRaisesRegex AssertionError escape Guard failed b size == expected got ep_v module test_inp test_constant_output ModuleConstant torch nn Module __init__ - None super __init__ b = torch randn forward b ModuleNestedConstant torch nn Module __init__ - None super __init__ bff = torch randn forward x y prediction x + y bff mod = ModuleConstant ep = export mod assertEqual ep module mod args = torch randn torch randn mod = ModuleNestedConstant ep = export mod args assertEqual ep module args mod args test_non_arg_name_dynamic_shapes_api_with_kwarg Foo torch nn Module forward b kw kw sum + b sum + kw sum - kw sum foo = Foo dim = torch export Dim dim dim_for_kw = torch export Dim dim_for_kw ep = torch export export foo torch randn torch randn kw torch ones kw torch zeros We specifying dynamism first kwarg even though user passed different order dynamic_shapes= None dim dim_for_kw None test_inp = torch randn torch randn test_kwargs = kw torch ones kw torch zeros This should work even kwarg order flipped assertEqual ep module test_inp test_kwargs foo test_inp test_kwargs test_non_arg_name_dynamic_shapes_api_with_container_type Foo torch nn Module forward b sum + sum + b sum inp_a = torch randn torch randn inp_b = torch randn inp = inp_a inp_b count = dynamify_inp x Mark second input dynamic nonlocal count count == dim = torch export Dim dim min= count += dim count += None dynamic_shapes = tree_map dynamify_inp inp foo = Foo ep = torch export export foo inp dynamic_shapes=dynamic_shapes test_inp = torch randn torch randn torch randn assertRaisesRegex AssertionError escape Guard failed size = expected = got ep module test_inp test_nested_module M torch nn Module forward x x + x M torch nn Module forward x m = M m x x inps = torch randn ep = export M inps assertTrue torch allclose ep module inps M inps add_nodes = node node ep graph nodes node op == call_function node target == torch ops aten add Tensor assertEqual len add_nodes add_node = add_nodes assertEqual len add_node meta nn_module_stack assertTrue M list add_node meta nn_module_stack values assertExpectedInline str ep graph strip \ graph x num_users= = placeholder target=x add num_users= = call_function target=torch ops aten add Tensor args = x x kwargs = mul num_users= = call_function target=torch ops aten mul Tensor args = add x kwargs = mul unflattened = unflatten ep assertTrue torch allclose unflattened inps M inps test_nested_module_with_init_buffer M torch nn Module __init__ - None super __init__ b = torch ones forward x x + b M torch nn Module forward x m = M m x x inps = torch randn ep = export M inps assertTrue torch allclose ep module inps M inps assertEqual len ep state_dict assertEqual len ep constants assertExpectedInline str ep graph strip \ graph x num_users= = placeholder target=x ones num_users= = call_function target=torch ops aten ones default args = kwargs = device cpu pin_memory False add num_users= = call_function target=torch ops aten add Tensor args = x ones kwargs = mul num_users= = call_function target=torch ops aten mul Tensor args = add x kwargs = mul unflattened = unflatten ep assertTrue torch allclose unflattened inps M inps test_nested_module_with_constant_buffer M torch nn Module __init__ - None super __init__ b = torch tensor forward x x + b M torch nn Module forward x m = M m x x inps = torch randn ep = torch export export M inps run_decompositions assertTrue torch allclose ep module inps M inps assertEqual len ep state_dict assertEqual len ep constants assertExpectedInline str ep graph strip \ graph c_lifted_tensor_ num_users= = placeholder target=c_lifted_tensor_ x num_users= = placeholder target=x clone num_users= = call_function target=torch ops aten clone default args = c_lifted_tensor_ kwargs = add num_users= = call_function target=torch ops aten add Tensor args = x clone kwargs = mul num_users= = call_function target=torch ops aten mul Tensor args = add x kwargs = mul unflattened = unflatten ep assertTrue torch allclose unflattened inps M inps test_nested_module_with_parameter M torch nn Module __init__ - None super __init__ = torch nn Parameter torch ones b = torch nn Parameter torch tensor forward x x + b M torch nn Module forward x m = M m x x inps = torch randn Strict export segfaults Issue ep = torch export export M inps strict=False run_decompositions assertTrue torch allclose ep module inps M inps assertEqual len ep state_dict assertEqual len ep constants assertExpectedInline str ep graph strip \ graph c_lifted_tensor_ num_users= = placeholder target=c_lifted_tensor_ x num_users= = placeholder target=x ones num_users= = call_function target=torch ops aten ones default args = kwargs = device cpu pin_memory False detach num_users= = call_function target=torch ops aten detach default args = ones kwargs = clone num_users= = call_function target=torch ops aten clone default args = c_lifted_tensor_ kwargs = detach_ num_users= = call_function target=torch ops aten detach default args = clone kwargs = mul num_users= = call_function target=torch ops aten mul Tensor args = detach detach_ kwargs = add num_users= = call_function target=torch ops aten add Tensor args = x mul kwargs = mul_ num_users= = call_function target=torch ops aten mul Tensor args = add x kwargs = mul_ unflattened = unflatten ep assertTrue torch allclose unflattened inps M inps test_module_dict_key Module torch nn Module __init__ super __init__ mod = torch nn Linear forward x d d = m d name name m named_children x + d mod m = Module sample_inputs = torch randn mod torch randn ep = export m sample_inputs assertEqual ep module sample_inputs m sample_inputs test_lazy_module_kwargs LazyModule torch nn modules lazy LazyModuleMixin torch nn Module initialize_parameters args kwargs pass forward x y x + y m = LazyModule ep = export m x torch randn y torch randn inputs = x torch randn y torch randn assertEqual ep module inputs m inputs test_retrace_pre_autograd Foo torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones forward x buffer add_ x sum + buffer sum inp = torch randn gm = export Foo inp dynamic_shapes= torch export Dim dim min= module assertRaisesRegex AssertionError escape Guard failed x size = expected = got gm torch randn assertRaisesRegex AssertionError escape Guard failed x size = expected = got export gm torch randn ep = export gm torch randn dynamic_shapes= torch export Dim dim min= test_inp = torch ones assertTrue torch allclose ep module test_inp Foo forward test_inp test_runtime_assert_with_size M torch nn Module forward x y = x item y ep = export M torch tensor torch ones dynamic_shapes= x None y torch export Dim t inp = torch tensor torch randn assertTrue torch allclose ep module inp M inp unittest skip Test only supposed work non-strict mode test_issue_ TestModule torch nn Module __init__ - None super __init__ = torch tensor forward x torch Tensor - torch Tensor x + forward_hook module torch nn Module inputs output - torch Tensor output seq = torch nn Sequential TestModule eval seq b = torch tensor handle = seq register_forward_hook forward_hook M torch nn Module __init__ - None super __init__ seq = seq forward x seq x + seq b inp = torch randn ep = export M inp This errors because dynamo adds extra input test_export_with_fake_tensor_inputs fake_mode = torch _subclasses fake_tensor FakeTensorMode Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x out = linear x out Put inputs device fake_mode torch device meta x = torch rand model = Model exported_program = torch export export model x export_res = exported_program module x exp_res = model x all_meta_val = node meta val node exported_program graph_module graph nodes val node meta assertTrue export_res size == exp_res size assertTrue all val device == x device val all_meta_val assertTrue all val fake_mode all_meta_val fake_mode val all_meta_val decomposed_ep = exported_program run_decompositions export_res = decomposed_ep module x assertTrue export_res size == exp_res size skipIfXpu test_export_with_fake_tensor_inputs_on_cuda_devices fake_mode = torch _subclasses fake_tensor FakeTensorMode Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x out = linear x out Put inputs device fake_mode torch device meta x = torch rand model = Model Manually set fake_device fake tensors x fake_device = torch device cuda n p model named_parameters p fake_device = torch device cuda Need set all requires_grad tensors False because fake_tensor CUDA device doesn t quite work well aot_autograd right now due some logic fails check call getDeviceGuardImpl InputMetadata x requires_grad = False n p model named_parameters p requires_grad = False check_device_and_fake_mode exported_program = torch export export model x export_res = exported_program module x exp_res = model x all_meta_val = node meta val node exported_program graph_module graph nodes val node meta assertTrue export_res size == exp_res size assertTrue all val device == x device val all_meta_val assertTrue all val fake_mode all_meta_val fake_mode val all_meta_val check_device_and_fake_mode test_run_decomposition_supports_user_input_mutation SingleOp torch nn Module __init__ - None super __init__ op = torch ops aten native_batch_norm forward input weight bias running_mean running_var training momentum eps kwargs op input weight bias running_mean running_var training momentum eps kwargs input = torch randn weight = torch randn bias = torch randn running_mean = torch randn running_var = torch randn training = True momentum = eps = model = SingleOp output = model input weight bias running_mean running_var training momentum eps ep = torch export export model args= input weight bias running_mean running_var training momentum eps ep run_decompositions assertEqual ep module input weight bias running_mean running_var training momentum eps output test_export_graph_with_no_inputs We saw pattern when users want export graph initlizes states model Module torch nn Module forward torch randn torch randn f = Module ep = torch export export f b = ep module assertEqual size torch Size assertEqual b size torch Size Contains unbacked symint M torch nn Module forward full = torch full i = full item torch full i f = M ep = export f = ep module assertEqual size torch Size assertEqual torch zeros test_pad_sequence Module torch nn Module forward x torch _C _nn pad_sequence x m = Module inputs = torch randn ep = torch export export m inputs dynamic_shapes= x Dim batch_size assertEqual ep module inputs m inputs ModuleBatchFirst torch nn Module forward x torch _C _nn pad_sequence x batch_first=True m = ModuleBatchFirst inputs = torch randn ep = torch export export m inputs dynamic_shapes= x Dim batch_size assertEqual ep module inputs m inputs ModuleMulti torch nn Module forward x y z torch _C _nn pad_sequence x y z m = ModuleMulti inputs = torch randn torch randn torch randn ep = torch export export m inputs dynamic_shapes= x Dim batch_size y Dim y z Dim z assertEqual ep module inputs m inputs ModuleMultiBatchFirst torch nn Module forward x y z torch _C _nn pad_sequence x y z batch_first=True m = ModuleMulti inputs = torch randn torch randn torch randn ep = torch export export m inputs dynamic_shapes= x Dim batch_size y Dim y z Dim z assertEqual ep module inputs m inputs test_operator_aten_tensor_mode_variant Module torch nn Module forward x torch ops aten div Tensor_mode x rounding_mode= floor m = Module args = torch randn ep = export m args assertEqual ep module args m args test_cdist_forward_compute_mode_zero_export CDistModel torch nn Module __init__ super CDistModel __init__ forward x y compute_mode torch ops aten _cdist_forward x y p= compute_mode=compute_mode x = torch ones y = torch ones model = CDistModel expected_none = model x y None ep_none = torch export export model x y None assertTrue torch equal ep_none module x y None expected_none expected_ = model x y ep_ = torch export export model x y assertTrue torch equal ep_ module x y expected_ test_export_then_compile_tensor_ctor M torch nn Module forward scores mask scores = scores masked_fill mask torch tensor torch finfo scores dtype min bs n_heads q_length k_length scores tensor_cpu = torch randn mask_cpu = torch BoolTensor False True False False False False False False m = M eval res_ref = m tensor_cpu mask_cpu print res_ref format res_ref flush=True exported_model = _export m tensor_cpu mask_cpu pre_dispatch=True module optimized_model = torch compile exported_model optimized_model tensor_cpu mask_cpu test_export_input_mutation_static_shape MutationModel torch nn Module forward x y x view - add_ y x inputs = torch randn torch tensor model = MutationModel ep = export model inputs inputs_export = copy deepcopy inputs inputs_model = copy deepcopy inputs assertEqual ep module inputs_export model inputs_model assertEqual inputs + torch tensor inputs_model assertEqual inputs + torch tensor inputs_export test_export_input_mutation_dynamic_shape MutationModel torch nn Module forward x y x mul_ y x inputs = torch randn torch randn model = MutationModel ep = torch export export model inputs dynamic_shapes= x torch export Dim dim None y None nodes = list ep graph nodes assertEqual nodes op placeholder assertIsInstance nodes meta val torch Tensor assertIsInstance nodes meta val shape torch SymInt inputs_export = copy deepcopy inputs inputs_model = copy deepcopy inputs assertEqual ep module inputs_export model inputs_model assertEqual inputs inputs_model assertEqual inputs inputs_export test_export_input_mutation_bug M torch nn Module forward x x = x + x inputs = torch ones ep = torch export export M inputs m = ep module Make name conflict placeholder name we get aot_export i node enumerate m graph nodes node op == placeholder node name = f arg _ i + m recompile ep = torch export export m inputs inputs = torch randn assertEqual ep module copy deepcopy inputs M copy deepcopy inputs test__scaled_dot_product_flash_attention Module torch nn Module forward q k v res = torch nn functional scaled_dot_product_attention q k v res m = Module inputs = torch randn torch randn torch randn ep = export m inputs assertEqual ep module inputs m inputs test_sym_sqrt math M torch nn Module forward x x torch sym_sqrt x shape ep = export M torch ones dynamic_shapes= x Dim dim _ExportPassBaseDeprecatedDoNotUse ep graph_module FileCheck check_count torch _sym_sqrt exactly=True run ep graph_module code test_check_specialized_int SingleOp torch nn Module __init__ - None super __init__ op = torch ops aten scatter_add forward t dim index src kwargs op t dim index src kwargs t = torch randn dim = - index = torch tensor src = torch randn model = SingleOp output = model t dim index src ep = torch export export model args= t dim index src ep = ep run_decompositions assertEqual ep module t dim index src output test_fqn NestedChild torch nn Module forward x x x Child torch nn Module __init__ - None super __init__ nested = NestedChild register_parameter child param torch nn Parameter torch ones forward x x = nested x x + child param Child torch nn Module __init__ - None super __init__ child buffer = torch nn Buffer torch ones forward x x - child buffer MyModule torch nn Module __init__ - None super __init__ foo = Child bar = Child register_parameter rootparam torch nn Parameter torch ones forward x x = x rootparam x = foo x x = bar x x orig_eager = MyModule test_inp = torch randn torch_gm = _export_to_torch_ir orig_eager torch rand torch_gm state_dict keys k v orig_eager state_dict items assertIn k torch_gm state_dict assertEqual v torch_gm state_dict k assertTrue torch allclose torch_gm test_inp orig_eager test_inp pre_autograd_gm = torch export _trace _export orig_eager torch rand pre_dispatch=True module k v orig_eager state_dict items assertIn k pre_autograd_gm state_dict assertEqual v pre_autograd_gm state_dict k assertTrue torch allclose pre_autograd_gm test_inp orig_eager test_inp ep = export orig_eager torch rand k v orig_eager state_dict items We do need normalize key here because exported program s state dict able contain module information assertIn k ep state_dict assertEqual v ep state_dict k assertTrue torch allclose ep module test_inp orig_eager test_inp assertTrue torch_gm state_dict keys orig_eager state_dict keys test_nn_module_stack Leaf torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x Bar torch nn Module __init__ - None super __init__ leaf = Leaf buffer = torch nn Buffer torch randn forward x buffer sum + leaf x sum Foo torch nn Module __init__ - None super __init__ bar = Bar forward x y = bar buffer + x bar x + y sum inp = torch randn mod = Foo ep_strict = torch export export mod inp run_decompositions ep_non_strict = torch export export mod inp strict=False run_decompositions gm_unflat_non_strict = unflatten ep_non_strict assertTrue hasattr gm_unflat_non_strict bar assertTrue hasattr gm_unflat_non_strict bar buffer assertTrue hasattr gm_unflat_non_strict bar leaf gm_unflat_strict = unflatten ep_strict assertEqual gm_unflat_non_strict inp gm_unflat_strict inp assertExpectedInline str gm_unflat_non_strict bar leaf linear graph strip \ graph x num_users= = placeholder target=x weight num_users= = get_attr target=weight bias num_users= = get_attr target=bias permute num_users= = call_function target=torch ops aten permute default args = weight kwargs = addmm num_users= = call_function target=torch ops aten addmm default args = bias x permute kwargs = addmm gm_flat_non_strict = ep_non_strict module gm_flat_strict = ep_strict module assertEqual gm_flat_non_strict inp gm_flat_strict inp test_nn_module_stack_shared_submodule Leaf torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x Bar torch nn Module __init__ - None super __init__ leaf = Leaf buffer = torch nn Buffer torch randn forward x buffer sum + leaf x sum BarDifferent torch nn Module __init__ - None super __init__ leaf = Leaf forward x = leaf x sum b = leaf x sum + b Foo torch nn Module __init__ - None super __init__ bar = Bar bar_different = BarDifferent forward x y = bar buffer + x bar x + bar_different x + y sum inp = torch randn mod = Foo ep_strict = export mod inp ep_non_strict = export mod inp strict=False gm_unflat_non_strict = unflatten ep_non_strict assertTrue hasattr gm_unflat_non_strict bar assertTrue hasattr gm_unflat_non_strict bar buffer assertTrue hasattr gm_unflat_non_strict bar leaf assertTrue hasattr gm_unflat_non_strict bar_different leaf gm_unflat_strict = unflatten ep_strict assertEqual gm_unflat_non_strict inp gm_unflat_strict inp assertExpectedInline str gm_unflat_non_strict bar leaf linear graph strip \ graph x num_users= = placeholder target=x weight num_users= = get_attr target=weight bias num_users= = get_attr target=bias linear num_users= = call_function target=torch ops aten linear default args = x weight bias kwargs = linear assertExpectedInline str gm_unflat_non_strict bar_different leaf linear graph strip \ graph add_ num_users= = placeholder target=add_ weight num_users= = get_attr target=weight bias num_users= = get_attr target=bias linear_ num_users= = call_function target=torch ops aten linear default args = add_ weight bias kwargs = linear_ gm_flat_non_strict = ep_non_strict module gm_flat_strict = ep_strict module assertEqual gm_flat_non_strict inp gm_flat_strict inp test_unflatten_random_dag_ dag N torch nn Module __init__ super __init__ forward x x + N torch nn Module __init__ super __init__ n = N forward x x + N torch nn Module __init__ super __init__ n = N forward x x = n n x + x + N torch nn Module __init__ super __init__ n = N forward x x = n x + x = n n n x + x + N torch nn Module __init__ super __init__ n = N forward x x = n x + x = n n x + x = n n n x + x + n = N inp = torch ones eager = n inp ep = export n inp epm = ep module ufm = torch export unflatten ep assertTrue torch allclose epm inp eager assertTrue torch allclose ufm inp eager test_unflatten_random_dag_ dag N torch nn Module __init__ super __init__ forward x x + N torch nn Module __init__ super __init__ n = N forward x x = n x + x + N torch nn Module __init__ super __init__ n = N forward x x + N torch nn Module __init__ super __init__ n = N forward x x = n n x + x = n n n x + x + N torch nn Module __init__ super __init__ n = N forward x x = n n x + x = n n n n x + x + N torch nn Module __init__ super __init__ n = N forward x x = n x + x = n n x + x = n n n n x + x = n n n n n x + x + n = N inp = torch ones eager = n inp ep = export n inp epm = ep module ufm = torch export unflatten ep assertTrue torch allclose epm inp eager assertTrue torch allclose ufm inp eager test_unflatten_random_dag_buf_ N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones forward x x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = n x + x = n n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n n n buf x = n x + x = n n n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n n n n buf x = n x + x = n n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n n n buf x = x + n n n n n n buf x = n x + x = n n n x + x = n n n n x + x = n n n n n n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = x + n n n n buf x = n x + x = n n x + x = n n n n x + x = n n n n n n n x + x + n = N inp = torch ones eager = n inp ep = export n inp epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_random_dag_mutating_buf_ N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones forward x x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = n x + x = n n x + n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n n buf x = n x + x = n n x + x = n n n x + n buf add_ n n buf add_ x + n = N inp = torch ones eager = n inp ep = export N inp epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_random_dag_mutating_buf_ N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones forward x x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n n buf x = n x + x = n n x + x = n n n x + n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n n buf x = n n x + x = n n n x + x = n n n n x + n buf add_ n n buf add_ n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = x + n n n buf x = x + n n n n buf x = n x + x = n n x + x = n n n x + x = n n n n x + x = n n n n n x + n n buf add_ n n n n buf add_ x + n = N inp = torch ones eager = n inp ep = export N inp epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_random_dag_mutating_buf_ N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones forward x x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = n x + n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n buf x = n n x + n buf add_ n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = n n x + n buf add_ n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n buf x = n x + x = n n n x + x = n n n n x + n n n buf add_ n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n n n buf x = x + n n n n n buf x = n x + x = n n n x + n n n n buf add_ n n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n n n buf x = x + n n n n n buf x = n x + x = n n x + x = n n n x + x = n n n n n n x + n n buf add_ n n n buf add_ n n n n buf add_ n n n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = x + n n n n buf x = x + n n n n n buf x = x + n n n n n n n buf x = n x + x = n n n x + n buf add_ n n n n n buf add_ n n n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n n n buf x = x + n n n n n n n buf x = n x + x = n n x + x = n n n x + x = n n n n x + x = n n n n n n n x + n n n buf add_ n n n n n n buf add_ n n n n n n n n buf add_ x + n = N inp = torch ones eager = n inp ep = export N inp epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_random_dag_preserving_ N torch nn Module __init__ super __init__ forward x x + N torch nn Module __init__ super __init__ n = N forward x x + N torch nn Module __init__ super __init__ n = N forward x x = n x + x + N torch nn Module __init__ super __init__ n = N forward x x = n x + x = n n x + x = n n n x + x + inp = torch ones eager = N inp fqns = n n n n n n ep = export N inp preserve_module_call_signature=fqns epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_random_dag_mutating_buf_preserving_ N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones forward x x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = n x + n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = n x + n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = x + n n n buf x = n n x + x = n n n x + x + inp = torch ones eager = N inp fqns = n n n n n n ep = export N inp preserve_module_call_signature=fqns epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_random_dag_mutating_buf_preserving_ _ N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones forward x x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = n n x + n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = n x + n n buf add_ n n n buf add_ x + inp = torch ones eager = N inp fqns = n n n n n n ep = export N inp preserve_module_call_signature=fqns epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_random_dag_mutating_buf_preserving_ N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones forward x x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n buf x = n x + x = n n x + n buf add_ n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = x + n n n buf x = n x + x = n n x + n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n buf x = x + n n n buf x = n x + x = n n x + n n n buf add_ x + inp = torch ones eager = N inp fqns = n n n n n n n n n n ep = export N inp preserve_module_call_signature=fqns epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_random_dag_mutating_buf_preserving_ N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones forward x x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = n x + x = n n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = x + n n n buf x = n x + x = n n x + x = n n n x + n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n buf x = x + n n n buf x = n n x + x = n n n x + x = n n n n x + n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n buf x = x + n n n buf x = x + n n n n buf x = x + n n n n n buf x = n x + x = n n x + x = n n n x + n n buf add_ n n n buf add_ n n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n n buf x = x + n n n n buf x = x + n n n n n buf x = x + n n n n n n buf x = n x + x = n n x + x = n n n x + x = n n n n x + x = n n n n n x + x = n n n n n n x + n n buf add_ n n n n buf add_ n n n n n buf add_ n n n n n n buf add_ x + inp = torch ones eager = N inp fqns = n n n n n n n n n n n n n n n n n n n n n ep = export N inp preserve_module_call_signature=fqns epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_random_dag_mutating_buf_preserving_ N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones forward x x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = n x + x = n n x + n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = n x + x = n n n x + x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n buf x = x + n n n buf x = n x + x = n n n n x + n n buf add_ n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = x + n n n n buf x = x + n n n n n buf x = n x + x = n n x + x = n n n n x + x = n n n n n x + n buf add_ n n buf add_ n n n buf add_ n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n n buf x = x + n n n n buf x = x + n n n n n n buf x = n x + x = n n x + x = n n n x + x = n n n n n x + x = n n n n n n x + n n n buf add_ n n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n buf x = x + n n n n n n buf x = n x + x = n n x + x = n n n x + x = n n n n x + x = n n n n n n x + x = n n n n n n n x + n buf add_ n n buf add_ n n n buf add_ n n n n buf add_ n n n n n buf add_ n n n n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n buf x = x + n n buf x = x + n n n n n buf x = x + n n n n n n n buf x = n x + x = n n x + x = n n n x + x = n n n n x + x = n n n n n x + x = n n n n n n x + n buf add_ n n n n n n n buf add_ n n n n n n n n buf add_ x + N torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones n = N forward x x = x + n n buf x = x + n n n buf x = x + n n n n n buf x = x + n n n n n n buf x = x + n n n n n n n buf x = n x + x = n n x + x = n n n n x + x = n n n n n n n x + x = n n n n n n n n x + x = n n n n n n n n n x + n n n buf add_ n n n n n n n n buf add_ n n n n n n n n n buf add_ x + inp = torch ones eager = N inp fqns = n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n ep = export N inp strict=False strict export slow large random dags preserve_module_call_signature=fqns epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_random_dag_const_preserving_ N torch nn Module __init__ super __init__ const = torch ones forward x x + N torch nn Module __init__ super __init__ const = torch ones n = N forward x x = x + n const x = n x + x + N torch nn Module __init__ super __init__ const = torch ones n = N forward x x = x + n n const x = n x + x = n n x + x + inp = torch ones eager = N inp fqns = n n n ep = export N inp preserve_module_call_signature=fqns epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_none_buffers mod = torch nn InstanceNorm d args = torch randn ep = torch export export mod args strict=False assertTrue torch allclose ep module args mod args test_partial_patched_forward Foo torch nn Module forward x x + fancy_forward x y x + + y Foo forward = functools partial fancy_forward y=torch randn x = torch randn strict unsupported Tracing through optional input ep = export Foo x strict=False ep module x Bar torch nn Module forward x y z x + y + z mod = Bar mod forward = functools partial mod forward z= mod forward = functools partial mod forward y=torch randn ep = export mod x strict=False ep module x testing expectedFailureCppRuntime test_symint_input_basic M torch nn Module forward x y x y ep = export M assertEqual ep module assertRaisesRegex AssertionError escape Guard failed x == expected got assertEqual ep module ep = export M dynamic_shapes= x Dim DYNAMIC y Dim AUTO assertEqual ep module assertEqual ep module ep = export M dynamic_shapes= x Dim DYNAMIC y Dim AUTO assertEqual ep module assertEqual ep module ep = export M dynamic_shapes= x None y Dim AUTO assertEqual ep module assertRaisesRegex AssertionError escape Guard failed x == expected got assertEqual ep module M torch nn Module forward x y x moo y ep = export M moo torch ones dynamic_shapes= x moo Dim DYNAMIC y Dim DYNAMIC inp = moo torch ones assertTrue torch allclose ep module inp M inp testing expectedFailureCppRuntime test_symint_input_specialization M torch nn Module forward x y assert x == assert y shape == x y inp = torch randn assertRaisesRegex torch _dynamo exc UserError r You marked your code specialized constant r If you re using Dim DYNAMIC replace either Dim STATIC Dim AUTO ep = export M inp dynamic_shapes= Dim DYNAMIC None ep = export M inp dynamic_shapes= Dim AUTO None assertRaisesRegex AssertionError escape Guard failed x == expected got ep module torch randn testing expectedFailureCppRuntime test_symint_input_ranges M torch nn Module forward x y x y inp = torch randn ep = export M inp dynamic_shapes= Dim DYNAMIC min= max= None ep module torch randn assertRaisesRegex AssertionError escape Guard failed x = expected = got ep module torch randn assertRaisesRegex AssertionError escape Guard failed x = expected = got ep module torch randn While tracing range found subset original range M torch nn Module forward x y assert x assert x = x y inp = torch randn ep = export M inp dynamic_shapes= Dim DYNAMIC min= max= None constraints = list ep range_constraints values constraint = constraints assertEqual constraint lower assertEqual constraint upper While tracing range found bigger than original range M torch nn Module forward x y assert x assert x x y inp = torch randn ep = export M inp dynamic_shapes= Dim DYNAMIC min= max= None constraints = list ep range_constraints values constraint = constraints assertEqual constraint lower assertEqual constraint upper While tracing range found outside original range M torch nn Module forward x y assert x assert x x y inp = torch randn assertRaisesRegex ValueError r \ \ conflicting \ \ ep = export M inp dynamic_shapes= Dim DYNAMIC min= max= None testing expectedFailureCppRuntime test_symint_input_additional_inputs M torch nn Module forward x y x + y additional_inputs = torch export AdditionalInputs additional_inputs add additional_inputs add additional_inputs add ep = torch export export M dynamic_shapes=additional_inputs assertEqual ep module assertEqual ep module assertEqual ep module testing expectedFailureCppRuntime test_symint_input_shapes_collection M torch nn Module forward x y x + y torch utils _pytree pytree torch export dynamic_shapes _IntWrapper args = _IntWrapper _IntWrapper shapes_collection = torch export ShapesCollection shapes_collection args = Dim DYNAMIC shapes_collection args = Dim DYNAMIC ep = torch export export M args dynamic_shapes=shapes_collection assertEqual ep module assertEqual ep module assertEqual ep module test_dynamic_shapes_bounds M torch nn Module Example bounds dynamic shapes forward x torch Tensor y torch Tensor zs list torch Tensor x + y torch cat zs m = M x = torch randn y = torch randn zs = torch randn torch randn torch export Dim ShapesCollection dynamic_shapes = ShapesCollection dynamic_shapes x = Dim DYNAMIC Dim DYNAMIC dynamic_shapes y = Dim DYNAMIC Dim DYNAMIC z zs dynamic_shapes z = Dim DYNAMIC Dim DYNAMIC assertRaisesRegex torch _dynamo exc UserError r Constraints violated \n r You marked L\ y \ size\ \ \ \ dynamic your code specialized constant \ \ r If you re using Dim DYNAMIC replace either Dim STATIC Dim AUTO export m x y zs dynamic_shapes=dynamic_shapes test_unflatten_random_dag_const_preserving_ _ N torch nn Module __init__ super __init__ const = torch ones forward x x + N torch nn Module __init__ super __init__ const = torch ones n = N forward x x = x + n const x = n x + x + N torch nn Module __init__ super __init__ const = torch ones n = N forward x x = x + n const x = n x + x = n n x + x + inp = torch ones eager = N inp fqns = n n n ep = export N inp preserve_module_call_signature=fqns epm = ep module ufm = torch export unflatten ep assert torch allclose epm inp eager assert torch allclose ufm inp eager test_unflatten_no_unroll inp = torch ones N torch nn Module __init__ super __init__ const = torch ones buf = torch nn Buffer torch ones forward x b b x + const + x + buf + - const K torch nn Module __init__ super __init__ n = N forward x n x True P torch nn Module __init__ super __init__ n = N forward x x = x + x = n x True x = n x False x + x Q torch nn Module __init__ super __init__ k = K forward x x = x + x = k n x True x = k n x False x + x R torch nn Module __init__ super __init__ k = K forward x x = x + x = k x x = k n x False x + x _N torch nn Module forward x x + _N_ torch nn Module forward x x + Mod path_n P n Q k n R k n m = Mod eager_result = m inp test ep swap epm = ep module ufm = torch export unflatten ep exported_result = epm inp assertTrue torch allclose exported_result eager_result unflattened_result = ufm inp assertTrue torch allclose unflattened_result eager_result fqn mod swap items ufm set_submodule fqn mod unflattened_result = ufm inp assertTrue torch allclose unflattened_result eager_result is_retracebility_test _testMethodName swapping will work retrace test export Mod inp preserve_module_call_signature= path_n swap= path_n N test export Mod inp swap= path_n _N path_n + _N_ test_preserve_module_call_signature_unflatten_specialization N torch nn Module forward x b b x + x + M torch nn Module __init__ super __init__ n = N forward x x = x + x = n x True x + inp = torch ones m = M eager_result = m inp is_retracebility_test _testMethodName swapping will work retrace ep = export M inp preserve_module_call_signature= n epm = ep module ufm = torch export unflatten ep exported_result = epm inp assertTrue torch allclose exported_result eager_result unflattened_result = ufm inp assertTrue torch allclose unflattened_result eager_result ufm set_submodule n N unflattened_result = ufm inp assertTrue torch allclose unflattened_result eager_result test_unflatten_multiple_graphs_dispatch N torch nn Module forward x b b x + x + M torch nn Module __init__ super __init__ n = N forward x x = x + x = n x True x = x + x = n x True x = x + x = n x False x = x + x inp = torch ones m = M eager_result = m inp test ep epm = ep module ufm = torch export unflatten ep exported_result = epm inp assertTrue torch allclose exported_result eager_result unflattened_result = ufm inp assertTrue torch allclose unflattened_result eager_result is_training_ir_test _testMethodName test torch export export M inp strict=not is_non_strict_test _testMethodName preserve_module_call_signature= n test export M inp preserve_module_call_signature= n test_unflatten_multiple_graphs_preserve_signature_no_error N torch nn Module forward x b b x + x + M torch nn Module __init__ super __init__ n = N forward x x = x + x = n x True x = x + x = n x False x = x + x inp = torch ones m = M eager_result = m inp test ep swap=None epm = ep module ufm = torch export unflatten ep exported_result = epm inp assertTrue torch allclose exported_result eager_result unflattened_result = ufm inp assertTrue torch allclose unflattened_result eager_result swap fqn mod swap items ufm set_submodule fqn mod unflattened_result = ufm inp assertTrue torch allclose unflattened_result eager_result is_retracebility_test _testMethodName swapping will work retrace test export M inp preserve_module_call_signature= n swap= n N test export M inp test_unflatten_multiple_graphs_state N torch nn Module __init__ super __init__ register_buffer buf torch ones persistent=False forward x b b buf add_ buf add_ x + buf M torch nn Module __init__ super __init__ n = N forward x x = n x True x = x + x = n x False x = x + x = n x True x = x + x = n x False x inp = torch ones m = M eager_result = m inp test ep swap=None epm = ep module ufm = torch export unflatten ep exported_result = epm inp assertTrue torch allclose exported_result eager_result unflattened_result = ufm inp assertTrue torch allclose unflattened_result eager_result swap fqn mod swap items ufm set_submodule fqn mod unflattened_result = ufm inp assertTrue torch allclose unflattened_result eager_result is_retracebility_test _testMethodName swapping will work retrace test export M inp preserve_module_call_signature= n swap= n N running decompositions again should work all IRs ep = export M inp preserve_module_call_signature= n test ep run_decompositions swap= n N test export M inp strict = is_non_strict_test _testMethodName ept = torch export export M inp strict=strict preserve_module_call_signature= n test ept test_set_grad_unflatten M torch nn Module forward b torch no_grad + b M torch nn Module __init__ super __init__ m = M forward b m b inp = torch ones torch ones ep = export M inp epm = ep module ufm = torch export unflatten ep assertTrue torch allclose ufm inp epm inp test_placeholder_update_preserving Child torch nn Module forward x = x add_ - Foo torch nn Module __init__ super __init__ child = Child forward x f = child x x - + = x - = f = x x = f + f inp = torch ones dtype=torch float ep = export Foo inp inp = torch ones dtype=torch float ep = export Foo inp preserve_module_call_signature= child inp = torch ones dtype=torch float orig_result = Foo inp inp = torch ones dtype=torch float ep _result = ep module inp assertTrue torch allclose ep _result orig_result inp = torch ones dtype=torch float ep _result = ep module inp assertTrue torch allclose ep _result orig_result test_constant_tensor_with_non_functional TestModel torch nn Module __init__ super __init__ params = torch ones forward x ff = params + ff = params + buf = torch ops aten sub_ Tensor ff ff buf sum + x sum model = TestModel x = torch zeros ep_training = torch export export model x strict=False state_dict_before = ep_training state_dict ep = export model x strict=False run_decompositions state_dict_after = ep state_dict assertEqual state_dict_before keys state_dict_after keys assertExpectedInline str ep graph_module code strip \ forward c_params x add = torch ops aten add Tensor c_params add_ = torch ops aten add Tensor c_params c_params = None sub = torch ops aten sub Tensor add add_ add = add_ = None sum_ = torch ops aten sum dim_IntList sub sub = None sum_ = torch ops aten sum dim_IntList x x = None add_ = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add_ test_constant_tensor_with_non_functional_nested SubMod torch nn Module __init__ super __init__ params = torch ones forward x x TestModel torch nn Module __init__ super __init__ submod = SubMod forward x ff = submod params + ff = submod params + buf = torch ops aten sub_ Tensor ff ff buf sum + x sum model = TestModel x = torch zeros ep_training = torch export export model x strict=False state_dict_before = ep_training state_dict ep = export model x strict=False run_decompositions state_dict_after = ep state_dict assertEqual state_dict_before keys state_dict_after keys assertExpectedInline str ep graph_module code strip \ forward c_submod_params x add = torch ops aten add Tensor c_submod_params add_ = torch ops aten add Tensor c_submod_params c_submod_params = None sub = torch ops aten sub Tensor add add_ add = add_ = None sum_ = torch ops aten sum dim_IntList sub sub = None sum_ = torch ops aten sum dim_IntList x x = None add_ = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add_ test_cond_unflatten M torch nn Module forward p b true_fn x y x + y false_fn x y x - y torch cond p true_fn false_fn b M torch nn Module __init__ super __init__ m = M forward p b m p b inp = torch tensor False torch ones torch ones ep = export M inp epm = ep module ufm = torch export unflatten ep assertTrue torch allclose ufm inp epm inp test_unflatten_multiple_graphs_shared_submodule N torch nn Module forward x b b x + x + gen_m n n_ p p_ Create module instance where n p share same submodule instance The booleans n n_ p p_ passed two calls each n p they determine which path through shared submodule instance taken during export M torch nn Module __init__ super __init__ n = N p = n forward x x = x + x = n x n x = x + x = n x n_ x = x + x = p x p x = x + x = p x p_ x + M inp = torch ones test m expected_graph expected_fqns expected_duplicates eager_result = m inp ep = export m inp exported_result = ep module inp exported eager results should match baseline assertTrue torch allclose exported_result eager_result unflattened = torch export unflatten ep unflattened_result = unflattened inp unflattened eager results should match needs multiple specialized graphs shared submodule instance assertTrue torch allclose unflattened_result eager_result expected graph should call minimal number specialized submodules assertExpectedInline str unflattened graph strip expected_graph expected graph should contain minimal number specialized submodule fqns assertEqual sorted fqn fqn _ unflattened named_modules remove_duplicate=False fqn = _guards_fn expected_fqns expected graph should contain minimal number specialized submodule instances b expected_duplicates is_non_strict_test _testMethodName NOTE non-strict does de-duplicate shared submodules through different fqns In particular we use different module ids n p calls non-strict strict we use same module id which enables additional reuse This pre-existing behavior might need fixed orthogonally assertNotEqual id getattr unflattened id getattr unflattened b assertEqual id getattr unflattened id getattr unflattened b ep = export m inp preserve_module_call_signature= n p exported_result = ep module inp assertTrue torch allclose exported_result eager_result unflattened = torch export unflatten ep unflattened_result = unflattened inp assertTrue torch allclose unflattened_result eager_result test gen_m n=True n_ =False p=False p_ =False p should share n_ graph p_ should optimized away \ graph x num_users= = placeholder target=x add num_users= = call_function target=torch ops aten add Tensor args = x kwargs = n num_users= = call_module target=n args = add kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = n kwargs = n_ num_users= = call_module target=n args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = n_ kwargs = p num_users= = call_module target=p args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = p kwargs = p_ num_users= = call_module target=p args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = p_ kwargs = add_ n n p n p test gen_m n=True n_ =False p=True p_ =False p should reuse n graph p_ should reuse n_ graph \ graph x num_users= = placeholder target=x add num_users= = call_function target=torch ops aten add Tensor args = x kwargs = n num_users= = call_module target=n args = add kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = n kwargs = n_ num_users= = call_module target=n args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = n_ kwargs = p num_users= = call_module target=p args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = p kwargs = p_ num_users= = call_module target=p args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = p_ kwargs = add_ n n p p n p n p test gen_m n=True n_ =True p=True p_ =False n_ should optimized away p should reuse n graph \ graph x num_users= = placeholder target=x add num_users= = call_function target=torch ops aten add Tensor args = x kwargs = n num_users= = call_module target=n args = add kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = n kwargs = n_ num_users= = call_module target=n args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = n_ kwargs = p num_users= = call_module target=p args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = p kwargs = p_ num_users= = call_module target=p args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = p_ kwargs = add_ n p p n p test gen_m n=True n_ =False p=False p_ =True p should reuse n_ graph p_ should reuse n graph \ graph x num_users= = placeholder target=x add num_users= = call_function target=torch ops aten add Tensor args = x kwargs = n num_users= = call_module target=n args = add kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = n kwargs = n_ num_users= = call_module target=n args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = n_ kwargs = p num_users= = call_module target=p args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = p kwargs = p_ num_users= = call_module target=p args = add_ kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = p_ kwargs = add_ n n p p n p p n test_stack_trace Foo torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x x = linear x x = x ep = export Foo torch randn run_decompositions check correct lines stack trace trace_mul = node node ep graph nodes node name == mul meta get stack_trace assertTrue re search r test_export py forward\n x \ = trace_mul trace_addmm = node node ep graph nodes node name addmm linear meta get stack_trace assertTrue re search r test_export py forward\n x = linear\ x\ trace_addmm test_stack_trace_make_fx Foo torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x x = linear x x = x inp = torch randn gm = torch fx experimental proxy_tensor make_fx Foo record_stack_traces=True inp check correct lines stack trace trace_mul = node node gm graph nodes node name == mul_ meta get stack_trace assertTrue re search r test_export py forward\n x \ = trace_mul trace_addmm = node node gm graph nodes node name addmm t meta get stack_trace assertTrue re search r test_export py forward\n x = linear\ x\ trace_addmm check correct lines still stack trace after export ep = export gm torch randn run_decompositions check correct lines stack trace trace_mul = node node ep graph nodes node name == mul meta get stack_trace assertTrue re search r test_export py forward\n x \ = trace_mul trace_addmm = node node ep graph nodes node name addmm linear meta get stack_trace assertTrue re search r test_export py forward\n x = linear\ x\ trace_addmm test_filter_traceback_frames TestTracer torch fx Tracer __init__ - None super __init__ record_stack_traces = True _filter_traceback_frames user_stack_summary traceback StackSummary - traceback StackSummary Keep last frame user_frames = user_stack_summary - traceback StackSummary from_list user_frames Foo torch nn Module forward x x = x graph = TestTracer trace Foo trace_x = node node graph nodes node name == x stack_trace assertTrue re search r proxy py create_node\n trace_x testing expectedFailureSerDerNonStrict register_constant needs handle serialization testing expectedFailureSerDer register_constant needs handle serialization test_register_constant dataclass frozen=True MyInput int_ int int_ int Foo torch nn Module __init__ super __init__ forward x f x + f int_ + f int_ register_constant MyInput ep = export Foo torch randn MyInput strict=False inp = torch ones assertEqual ep module inp MyInput Foo inp MyInput test_cond_with_module_stack_export_with Bar torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x true_fn x linear x cos false_fn x linear x sin torch cond x sum true_fn false_fn x CondExport torch nn Module __init__ - None super __init__ bar = Bar forward x x cos + bar x inp = torch randn ep = torch export export CondExport inp strict=False assertExpectedInline ep graph_module code strip \ forward p_bar_linear_weight p_bar_linear_bias x cos = torch ops aten cos default x sum_ = torch ops aten sum default x gt = torch ops aten gt Scalar sum_ sum_ = None true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond gt true_graph_ false_graph_ p_bar_linear_bias p_bar_linear_weight x gt = true_graph_ = false_graph_ = p_bar_linear_bias = p_bar_linear_weight = x = None getitem = cond cond = None add = torch ops aten add Tensor cos getitem cos = getitem = None add schema = get_hop_schema ep assertExpectedInline str schema cond Tensor pred GraphModule true_fn GraphModule false_fn Tensor operands - Tensor cond_top_level_nn_module_stack = node meta nn_module_stack node ep graph nodes node name == true_graph_ assertTrue test_cond_with_module_stack_export_with locals Bar str cond_top_level_nn_module_stack TODO See https github com pytorch pytorch issues unittest expectedFailure test_cond_with_module_stack_export_with_unflatten Bar torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x true_fn x linear x cos false_fn x linear x sin torch cond x shape true_fn false_fn x CondExport torch nn Module __init__ - None super __init__ bar = Bar forward x x cos + bar x inp = torch randn ep = torch export export CondExport inp strict=False cond_top_level_nn_module_stack = node meta nn_module_stack node ep graph nodes node name == true_graph_ we can t preserve nn_module_stack subgraphs now node ep graph_module true_graph_ graph nodes assertEqual node meta nn_module_stack cond_top_level_nn_module_stack doesn t work today gm_unflat_strict = unflatten ep test_modules_access_for_deleted_submodule Foo torch nn Module __init__ super __init__ linear = torch nn Linear foo = torch nn Linear forward x name mod _modules items mod None continue pass linear x mod = Foo mod foo = None mod torch randn export mod torch randn strict=False test_profiling_code Foo torch nn Module forward x torch profiler record_function foo x sin ep = export Foo torch randn strict=True FileCheck check_count torch ops profiler _record_function_enter_new default exactly=True run ep graph_module code test_replace_unbacked_with_very_large_upperbound strict = True beyond ^ where python floats lose precision VERY_LARGE_INT = Model torch nn Module forward x t unbacked = t item torch _check unbacked = VERY_LARGE_INT y = torch ones unbacked x reshape - + y inp = torch randn torch tensor spec = x Dim AUTO Dim STATIC t Dim STATIC ep = export Model inp dynamic_shapes=spec strict=strict assertTrue torch allclose Model inp ep module inp test_predispatch_cond Model torch nn Module __init__ - None super __init__ pred = torch nn Buffer torch tensor False t = torch nn Buffer torch tensor forward x y true_fn x y torch enable_grad x - + t + y torch cond pred true_fn lambda x y x + - t + y x y model = Model torch no_grad exported_program = torch export export model torch tensor torch tensor dynamic_shapes=None strict=False schema = get_hop_schema exported_program assertExpectedInline str schema cond Tensor pred GraphModule true_fn GraphModule false_fn Tensor operands - Tensor noqa B assertExpectedInline str exported_program graph_module code strip \ forward b_pred b_t x y true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond b_pred true_graph_ false_graph_ b_t x y b_pred = true_graph_ = false_graph_ = b_t = x = y = None getitem = cond cond = None getitem noqa B assertExpectedInline str exported_program graph_module true_graph_ code strip \ forward b_t x y submod_ = submod_ add_ = torch ops higher_order wrap_with_set_grad_enabled True submod_ x b_t y submod_ = x = b_t = y = None getitem = add_ add_ = None getitem assertExpectedInline str exported_program graph_module true_graph_ submod_ code strip \ forward x b_t y sub = torch ops aten sub Tensor x x = None add = torch ops aten add Tensor sub b_t sub = b_t = None add_ = torch ops aten add Tensor add y add = y = None add_ test_python_asserts_with_sym_int Model torch nn Module forward x y = x + assert y max item y model = Model ep = torch export export model torch zeros dtype=torch int Graph should look like GraphModule torch nn Module forward x i add i = torch ops aten add Tensor x x = None max_ i = torch ops aten max default add item Sym u = torch ops aten item default max_ max_ = None ge Sym u = = item = _assert_scalar_default = torch ops aten _assert_scalar default ge Runtime assertion failed expression u = node ge ge = _assert_scalar_default = None gt_ Sym u = item item = None _assert_scalar_default_ = torch ops aten _assert_scalar default gt_ Runtime assertion failed expression u node gt_ gt_ = _assert_scalar_default_ = None add inputs = torch ones dtype=torch int assertEqual ep module inputs model inputs inputs = -torch ones dtype=torch int assertRaisesRegex RuntimeError Runtime assertion failed expression ep module inputs test_predispatch_grad_wrappers Model torch nn Module forward x y torch enable_grad x = x - y torch no_grad x = x + y x no grad model = Model torch no_grad ep_nograd = torch export export model torch tensor torch tensor dynamic_shapes=None strict=False check only sub op wrapped grad_enabled getattr_nodes = node node ep_nograd graph nodes node op == get_attr assertEqual len getattr_nodes grad_subgraph = getattr ep_nograd graph_module getattr_nodes target op_node = node node grad_subgraph graph nodes node op == call_function assertEqual op_node target _name aten sub Tensor enable grad model = Model ep_grad = torch export export model torch tensor torch tensor dynamic_shapes=None strict=False check only add op wrapped grad_enabled getattr_nodes = node node ep_grad graph nodes node op == get_attr assertEqual len getattr_nodes grad_subgraph = getattr ep_grad graph_module getattr_nodes target op_node = node node grad_subgraph graph nodes node op == call_function assertEqual op_node target _name aten add Tensor testing expectedFailureRetraceability test_layer_sharing N C H W = Module torch nn Module __init__ - None super __init__ layer = torch nn LayerNorm C H W norms = torch nn ModuleList layer layer forward x norm norms x = norm x x m = Module copied_m = copy deepcopy m ep = export copied_m torch randn N C H W assertEqual copied_m state_dict m state_dict assertEqual ep state_dict m state_dict test_module_list_slice ModuleListTruncated torch nn Module __init__ super __init__ fcs = torch nn ModuleList torch nn Linear _ range forward x fc fcs x = fc x x x = torch rand mod = ModuleListTruncated epm = export mod x module assertTrue torch allclose mod x epm x test_non_persistent_buffer MyModule torch nn Module __init__ - None super __init__ foo = torch nn Buffer torch rand persistent=False forward x foo + x MyOuterModule torch nn Module __init__ - None super __init__ inner = MyModule forward x inner x inp = torch rand _test m non_persistent_buffer ep = export m inp assertEqual ep module inp m inp Non-persistent buffers should show up state dict assertNotIn non_persistent_buffer ep state_dict named_buffers = name buffer name buffer ep named_buffers But they should show up named_buffers assertIn non_persistent_buffer named_buffers assertIn non_persistent_buffer ep constants assertEqual len ep constants Check same properties unlifted module mod = ep module assertNotIn non_persistent_buffer mod state_dict mod_named_buffers = name buffer name buffer mod named_buffers assertIn non_persistent_buffer mod_named_buffers assertIn non_persistent_buffer ep constants assertEqual len ep constants assertEqual mod inp m inp _test MyModule foo _test MyOuterModule inner foo testing expectedFailureTrainingIRToRunDecomp set_grad disappears after decomp testing expectedFailureTrainingIRToRunDecompNonStrict set_grad disappears after decomp test_export_with_set_grad_enabled Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x torch no_grad linear x model = Model ep = export model torch randn FileCheck check_count torch ops higher_order wrap_with_set_grad_enabled exactly=True run ep graph_module code test_export_with_autocast Model torch nn Module forward x torch autocast device_type= cuda dtype=torch int enabled=True y = x sin sum torch autocast device_type= cpu dtype=torch float enabled=True z = y sin sum z model = Model ep = export model torch randn autocast nodes do exist after run_decomposition is_training_ir_test _testMethodName assertIn torch ops higher_order wrap_with_autocast ep graph_module code _export_for_traininig using pre_dispatch=False Therefore autocast calls replaced hop gm = torch export export model torch randn module assertIn autocast gm code test_export_as_backend f x y x + y my_custom_backend gm example_inputs gm = torch export export gm tuple example_inputs strict=False run_decompositions module gm inp = torch randn torch randn new_res = torch compile f backend=my_custom_backend inp assertTrue torch allclose f inp new_res test_nonstrict_retrace_preserves_metadata MyModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x inp = torch randn m = MyModule ep = torch export export m inp strict=False retrace ep = torch export export ep module inp strict=False n n zip list ep graph nodes list ep graph nodes assertEqual n meta get stack_trace n meta get stack_trace test_fake_weights MyModule torch nn Module __init__ - None super __init__ foo = torch nn Parameter torch randn bar = torch nn Buffer torch randn persistent=False baz = torch nn Buffer torch randn persistent=True forward x foo + x + bar + baz fake_mode = torch _subclasses FakeTensorMode shape_env=ShapeEnv tracked_fakes= fake_mode m = MyModule inp = torch randn ep = export m inp Can t compare outputs because module has fake weights test_fake_inputs MyModule torch nn Module __init__ - None super __init__ foo = torch nn Parameter torch randn forward x foo + x fake_mode = torch _subclasses FakeTensorMode shape_env=ShapeEnv tracked_fakes= m = MyModule fake_mode inp = torch randn ep = export m inp assertEqual ep module torch ones m torch ones test_double_lifted_constants EmptyM torch nn Module __init__ super __init__ forward torch tensor torch tensor m = EmptyM ep = torch export export m out real_out zip ep module m assertTrue torch allclose out real_out test_trace_under_fake MyModule torch nn Module __init__ - None super __init__ foo = torch nn Parameter torch randn forward x foo + x fake_mode = torch _subclasses FakeTensorMode shape_env=ShapeEnv tracked_fakes= fake_mode m = MyModule inp = torch randn Can t use unqualified export will attempt deserialize under new FakeTensorMode ep = torch export export m inp test_constant_no_user_inp Bar torch nn Module __init__ super __init__ = torch ones forward x x sin = torch ones Foo torch nn Module __init__ super __init__ bar = Bar register_buffer buf torch ones forward bar bar + + bar + buf export Foo strict=False test_compiling_state TestModule torch nn Module forward x torch _dynamo is_compiling x x TestModule torch nn Module forward x torch _utils is_compiling x x TestModule torch nn Module forward x torch compiler is_compiling x x m TestModule TestModule TestModule input = torch randn ep_strict = export m input strict=True ep_non_strict = export m input strict=False assertTrue torch allclose input m input assertTrue torch allclose input ep_strict module input assertTrue torch allclose input ep_non_strict module input test_user_input_and_buffer_mutation MyModule torch nn Module __init__ - None super __init__ foo = torch nn Buffer torch randn forward x foo add_ x add_ foo + x mod = MyModule mod_copy = copy deepcopy mod ep = export mod_copy torch rand assertEqual mod foo ep module foo assertEqual mod torch ones ep module torch ones test_unbacked_scalar_constructor Foo torch nn Module forward u zuf b torch tensor u item torch tensor zuf item torch tensor b item mod = Foo inps = torch tensor torch tensor torch tensor True ep = torch export export mod inps eager_out ep_out zip mod inps ep module inps assertTrue torch allclose eager_out ep_out test other inputs inps = torch tensor torch tensor - torch tensor False eager_out ep_out zip mod inps ep module inps assertTrue torch allclose eager_out ep_out test_symint_tensor_return Module torch nn Module forward x b = torch ops testlib returns_tensor_symint x b _test_export_same_as_eager Module torch randn test_custom_op_auto_functionalize M torch nn Module __init__ - None super __init__ forward x z torch ops testlib foo x z inps = torch ones torch ones inps_for_export = torch ones torch ones inps_for_export_with_decomp = torch ones torch ones ep = torch export export M inps_for_export x_new_eager z_new_eager legit_eager = M inps x_new_export z_new_export legit_export = ep module inps_for_export assertTrue torch allclose x_new_eager x_new_export assertTrue torch allclose z_new_eager z_new_export assertTrue torch allclose legit_eager legit_export ep = ep run_decompositions x_new_export z_new_export legit_export = ep module inps_for_export_with_decomp assertTrue torch allclose x_new_eager x_new_export assertTrue torch allclose z_new_eager z_new_export assertTrue torch allclose legit_eager legit_export test_custom_op_auto_functionalize_pre_dispatch M torch nn Module __init__ - None super __init__ forward x torch ops testlib foo_mutated x inps = torch ones ep = torch export export M inps run_decompositions IS_FBCODE assertExpectedInline str ep graph_module code strip \ forward x cos = torch ops aten cos default x auto_functionalized = torch ops higher_order auto_functionalized torch ops testlib foo default x = x z = cos x = cos = None getitem_ = auto_functionalized auto_functionalized = None cos_ = torch ops aten cos default getitem_ getitem_ getitem_ cos_ assertExpectedInline str ep graph_module code strip \ forward x cos = torch ops aten cos default x auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops testlib foo default _x_base_index = _z_base_index = _all_bases = x cos x = cos = None getitem_ = auto_functionalized_v auto_functionalized_v = None cos_ = torch ops aten cos default getitem_ getitem_ getitem_ cos_ test_custom_op_auto_warn_pre_dispatch M torch nn Module __init__ - None super __init__ forward x torch ops testlib foo_functional x inps = torch ones ep = torch export export M inps run_decompositions IS_FBCODE assertExpectedInline str ep graph_module code strip \ forward x cos = torch ops aten cos default x cos_ = torch ops aten cos default x x = None auto_functionalized = torch ops higher_order auto_functionalized torch ops testlib foo default x = cos z = cos_ cos = cos_ = None getitem_ = auto_functionalized auto_functionalized = None cos_ = torch ops aten cos default getitem_ getitem_ = None cos_ assertExpectedInline str ep graph_module code strip \ forward x cos = torch ops aten cos default x cos_ = torch ops aten cos default x x = None auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops testlib foo default _x_base_index = _z_base_index = _all_bases = cos cos_ cos = cos_ = None getitem_ = auto_functionalized_v auto_functionalized_v = None cos_ = torch ops aten cos default getitem_ getitem_ = None cos_ ep = torch export _trace _export M inps pre_dispatch=True assertExpectedInline str ep graph_module code strip \ forward x foo_functional = torch ops testlib foo_functional default x x = None foo_functional test_placeholder_naming_order See https github com pytorch pytorch issues Mod torch nn Module __init__ super __init__ layer = torch nn Linear layer = torch nn Linear forward x x flag=True x o = layer x x o = layer x torch cat x o x o dim= mod = Mod args = torch rand kwargs = flag False x torch rand ep = export mod args kwargs check graph behaviorally correct assertTrue torch allclose ep module args kwargs mod args kwargs check graph input names expected assertEqual ep graph_signature user_inputs x False x test_kwarg_dynamic_shapes_diff_order DummyModel torch nn Module __init__ super __init__ = torch ones forward baba start end baba sum + start sum + end sum f = DummyModel kwargs = end torch ones start torch ones dynamic_shapes = baba torch export Dim end_dim end torch export Dim end_dim start torch export Dim end_dim torch export Dim end_dim ep = torch export export f torch ones kwargs dynamic_shapes=dynamic_shapes run_decompositions ep module torch ones kwargs test_placeholder_naming_order_variadic Mod torch nn Module forward b c kwargs - b + c kwargs d mod = Mod args = torch randn kwargs = c torch randn b torch randn d torch randn ep = export mod args kwargs assertTrue torch allclose ep module args kwargs mod args kwargs assertEqual ep graph_signature user_inputs c b d test_isnonzero Foo torch nn Module forward x torch ops aten is_nonzero x assertRaisesRegex RuntimeError Boolean value Tensor more than export Foo torch randn strict=False test_placeholder_naming_collisions test collisions between nested user inputs Foo torch nn Module forward x x_foo x_foo_ x foo + x_foo + x_foo_ inputs = foo torch randn torch randn torch randn ep = export Foo inputs expected_names = x_foo_ x_foo_ _ x_foo_ _ real_names = spec arg name spec ep graph_signature input_specs assertEqual expected_names real_names test collisions between user inputs params buffers constants Foo torch nn Module __init__ - None super __init__ param = torch nn Parameter torch randn alpha = torch nn Buffer torch randn persistent=True beta = torch nn Buffer torch randn persistent=False gamma = torch randn forward p b_alpha b c_gamma p = p param + param b = alpha + beta + b_alpha + b beta c = gamma + c_gamma p b c inputs = param torch randn torch randn beta torch randn torch randn ep = export Foo inputs expected_names = user inputs should prioritized unprefixed p_param_ InputKind PARAMETER b_alpha_ InputKind BUFFER b_beta_ InputKind BUFFER c_gamma_ InputKind CONSTANT_TENSOR p_param InputKind USER_INPUT b_alpha InputKind USER_INPUT b_beta InputKind USER_INPUT c_gamma InputKind USER_INPUT real_names = spec arg name spec kind spec ep graph_signature input_specs assertEqual expected_names real_names test collisions between user inputs call_function nodes Foo torch nn Module forward mul add add_ mul mul + add add_ ep = export Foo torch randn torch randn torch randn expected_names_and_ops = mul placeholder add placeholder add_ placeholder mul_ call_function mul_ call_function add_ call_function output output real_names_and_ops = node name node op node ep graph nodes assertEqual expected_names_and_ops real_names_and_ops skipIfCrossRef Dynamo changes order ops under Torch function modes test_placeholder_naming_collisions_hoo_subgraphs test collisions between user inputs top-level nodes HOO subgraph nodes Foo torch nn Module forward x mul mul_ _mul = x x y = cond _mul sum lambda x y z x y z lambda x y z x + y + z _mul mul mul_ torch enable_grad y = y y y torch no_grad ep = torch export _trace _export Foo torch randn torch randn torch randn pre_dispatch=True schema = get_hop_schema ep assertExpectedInline str schema cond Tensor pred GraphModule true_fn GraphModule false_fn Tensor operands - Tensor test cond subgraph expected_names_and_ops = mul_ placeholder mul placeholder mul_ placeholder mul_ call_function mul_ call_function output output real_names_and_ops = node name node op node ep graph_module true_graph_ graph nodes assertEqual expected_names_and_ops real_names_and_ops test set_grad_enabled subgraph expected_names_and_ops = getitem placeholder mul_ call_function output output real_names_and_ops = node name node op node ep graph_module submod_ graph nodes assertEqual expected_names_and_ops real_names_and_ops test collisions between user inputs higher order op subgraphs please never do Foo torch nn Module forward input true_graph body_graph x = input + true_graph + true_graph x = cond x sum lambda x x lambda x x + x x = cond x sum lambda x x lambda x x + x x inputs = torch randn torch randn torch randn torch randn ep = export Foo inputs expected_getattr_names = true_graph_ false_graph_ true_graph_ false_graph_ real_getattr_names = node name node ep graph nodes node op == get_attr assertEqual expected_getattr_names real_getattr_names test_constant_input_naming Foo torch nn Module forward x y div= floor torch div x y rounding_mode=div f = Foo inputs = torch randn torch randn floor ep = export f inputs div_spec = ep graph_signature input_specs assertEqual div_spec arg name div assertEqual div_spec arg value floor test_attr_assignment_extra Foo torch nn Module __init__ super __init__ forward x bar = x sum x + assertWarnsRegex UserWarning The tensor attribute bar assigned during export _ = export Foo torch randn strict=False test_vmap_custom_autograd_function torch _dynamo _trace_wrapped_higher_order_op TransformGetItemToIndex IndexingModule torch nn Module __init__ base_size int = super __init__ register_buffer base torch arange base_size forward indices torch Tensor - torch Tensor TransformGetItemToIndex Each element ` indices ` scalar tensor so our override kicks torch vmap lambda i base i indices m = IndexingModule idxs = torch tensor ep = torch export export m idxs strict=False assertExpectedInline ep graph \ graph b_base num_users= = placeholder target=b_base indices num_users= = placeholder target=indices lazy_load_decompositions num_users= = call_function target=torch _functorch predispatch lazy_load_decompositions args = kwargs = _vmap_increment_nesting num_users= = call_function target=torch _functorch predispatch _vmap_increment_nesting args = error kwargs = _add_batch_dim num_users= = call_function target=torch _functorch predispatch _add_batch_dim args = indices kwargs = torch__dynamo__trace_wrapped_higher_order_op_mod_index num_users= = get_attr target=torch__dynamo__trace_wrapped_higher_order_op_ModIndex function_const_func_spec num_users= = get_attr target=function_const_func_spec flat_apply num_users= = call_function target=torch ops higher_order flat_apply args = function_const_func_spec torch__dynamo__trace_wrapped_higher_order_op_mod_index torch _dynamo _trace_wrapped_higher_order_op ModIndex b_base _add_batch_dim kwargs = _remove_batch_dim num_users= = call_function target=torch _functorch predispatch _remove_batch_dim args = flat_apply kwargs = _vmap_decrement_nesting num_users= = call_function target=torch _functorch predispatch _vmap_decrement_nesting args = kwargs = _remove_batch_dim assertEqual m idxs ep module idxs ep = ep run_decompositions assertExpectedInline ep graph \ graph b_base num_users= = placeholder target=b_base indices num_users= = placeholder target=indices index num_users= = call_function target=torch ops aten index Tensor args = b_base indices kwargs = index assertEqual m idxs ep module idxs test_unbacked_deferred_runtime_retrace Foo torch nn Module forward x y y_sum = y sin sum torch no_grad = x item torch _check torch _check unbacked_shape = torch ops testlib foo_unbacked y + y_sum + unbacked_shape sum inps = torch tensor torch randn ep_pre = torch export export Foo inps strict=False assertExpectedInline str ep_pre graph_module submod_ code strip \ forward x item = torch ops aten item default x x = None ge = item = _assert_scalar_default = torch ops aten _assert_scalar default ge Runtime assertion failed expression u = node ge ge = _assert_scalar_default = None le = item = _assert_scalar_default_ = torch ops aten _assert_scalar default le Runtime assertion failed expression u = node le le = _assert_scalar_default_ = None gt_ = item _assert_scalar_default_ = torch ops aten _assert_scalar default gt_ Runtime assertion failed expression u node gt_ gt_ = _assert_scalar_default_ = None lt_ = item _assert_scalar_default_ = torch ops aten _assert_scalar default lt_ Runtime assertion failed expression u node lt_ lt_ = _assert_scalar_default_ = None foo_unbacked = torch ops testlib foo_unbacked default item item = None foo_unbacked ep_aot = ep_pre run_decompositions assertExpectedInline str ep_aot graph_module code strip \ forward x y sin = torch ops aten sin default y sum_ = torch ops aten sum dim_IntList sin sin = None _local_scalar_dense = torch ops aten _local_scalar_dense default x x = None ge_ = _local_scalar_dense = _assert_scalar_default = torch ops aten _assert_scalar default ge_ Runtime assertion failed expression u = node ge_ ge_ = _assert_scalar_default = None le_ = _local_scalar_dense = _assert_scalar_default_ = torch ops aten _assert_scalar default le_ Runtime assertion failed expression u = node le_ le_ = _assert_scalar_default_ = None gt = _local_scalar_dense _assert_scalar_ = torch ops aten _assert_scalar default gt Runtime assertion failed expression u node gt_ gt = _assert_scalar_ = None lt = _local_scalar_dense _local_scalar_dense = None _assert_scalar_ = torch ops aten _assert_scalar default lt Runtime assertion failed expression u node lt_ lt = _assert_scalar_ = None full = torch ops aten full default dtype = torch float layout = torch strided device = device type= cpu pin_memory = False add = torch ops aten add Tensor y sum_ y = sum_ = None sum_ = torch ops aten sum dim_IntList full full = None add_ = torch ops aten add Tensor add sum_ add = sum_ = None add_ test_nested_dynamic_shapes_spec Foo torch nn Module forward x b b c c c = x + + b + b + c + c + c f = Foo inputs = torch randn torch randn torch randn torch randn torch randn make sure gets parsed correctly individual inputs tensors dynamic_shapes = x None None None None None None None export f inputs dynamic_shapes=dynamic_shapes test_disable_forced_specializations_ok check we don t force specialization defer runtime asserts prefer_deferred_runtime_asserts_over_guards=True successfully export case modulo guards torch export dims Mod Reshape torch nn Module forward x x reshape x shape - - Mod s s s - = inputs = torch randn dx dy = dims dx dy use_new_tracer True False torch _export config patch use_new_tracer_experimental=use_new_tracer ep = torch export _trace _export Mod Reshape inputs dynamic_shapes= x dx dy prefer_deferred_runtime_asserts_over_guards=True pre_dispatch=True out = ep module torch randn assertEqual out shape torch ones shape out = ep module torch randn assertEqual out shape torch ones shape assertRaisesRegex RuntimeError r ^Runtime assertion failed expression Eq\ Mod\ s\d+\ s\d+ \ s\d+\s -\s \ \ node eq ^ $ ep module torch randn fail case d reshape FreeReshape torch nn Module forward x y z x reshape - + y reshape - + z s s = s s = s inputs = torch randn torch randn torch randn dynamic_shapes = x Dim f dx i min= i range y Dim f dy i min= i range z Dim f dz i min= i range private_api True False private_api ep = torch export export FreeReshape inputs dynamic_shapes=dynamic_shapes prefer_deferred_runtime_asserts_over_guards=True ep = export FreeReshape inputs dynamic_shapes=dynamic_shapes out = ep module torch randn torch randn torch randn assertEqual out shape torch ones shape out = ep module torch randn torch randn torch randn assertEqual out shape torch ones shape private_api assertRaisesRegex RuntimeError r Runtime assertion failed expression Eq\ \ node fail only runtime ep module torch randn torch randn torch randn fail no runtime assert exported module fails anyway wrong inputs assertRaisesRegex AssertionError escape Guard failed x size x size == y size y size expected got ep module torch randn torch randn torch randn case d reshape previously failing different issue Reshape d torch nn Module forward x y x reshape - + y s s s = s inputs = torch randn torch randn dynamic_shapes = x Dim dx min= Dim dx min= Dim dx min= y Dim dy min= ep = torch export export Reshape d inputs dynamic_shapes=dynamic_shapes prefer_deferred_runtime_asserts_over_guards=True out = ep module torch randn torch randn assertEqual out shape torch ones shape assertRaisesRegex RuntimeError r Runtime assertion failed expression Eq\ \ node fail only runtime ep module torch randn torch randn fail test_disable_forced_specializations_errors check error messages hybrid symints Foo torch nn Module forward w x y z w reshape - + x y + z simple s s = s s = s inputs = torch randn torch randn torch randn torch randn dynamic_shapes = w Dim f dw i i range x Dim f dx i i range y Dim dy y z incorrect export supposed fail z Dim dz suggested fix should match these up assertRaisesRegex disable=True suggested fixes should specialize torch _dynamo exc UserError r Constraints violated \n r Suggested fixes \n r dz = dy \n msg export Foo inputs dynamic_shapes=dynamic_shapes strict=False test_preserve_requires_grad_placeholders Module torch nn Module __init__ - None super __init__ p = torch nn Parameter torch randn forward x y p + x + y m = Module ep = export m torch randn torch randn requires_grad=True placeholders = node node ep graph_module graph nodes node op == placeholder assertTrue placeholders meta val requires_grad assertFalse placeholders meta val requires_grad assertTrue placeholders meta val requires_grad test_expand_copy_export_handles_implicit_true ExpandModel torch nn Module __init__ super __init__ forward x implicit torch expand_copy x implicit=implicit model = ExpandModel x = torch ones model x False model x True export model x False export model x True test_unbacked_expand cpp_runtime_nonstrict id skipTest TODO Unexpected success OSS fbcode Foo torch nn Module forward xs u u u = xs tolist x = torch empty u u x expand - u u ep = export Foo torch tensor assertEqual list ep module torch tensor shape assertEqual list ep module torch tensor shape Bar torch nn Module forward xs u u = xs tolist x = torch empty u x expand u ep = export Bar torch tensor assertEqual ep module torch tensor shape assertEqual ep module torch tensor shape assertRaisesRegex RuntimeError r Runtime assertion failed expression Eq\ u u \ ep module torch tensor test_reshape_view_helper see https github com pytorch pytorch issues Model torch nn Module __init__ - None super __init__ forward x x = x view x size - torch _refs __init__ _reshape_view_helper will generate guards reshape kernel Ne s so reshape isn t no-op Ne Mod s so reshape needs first flatten s - s then split_dim - s check these show up graph torch nn functional softmax x dim= don t think softmax actually creates any issues just part original test model = Model x = torch rand dynamic_shapes = x Dim batch ep = torch export export model x dynamic_shapes=dynamic_shapes prefer_deferred_runtime_asserts_over_guards=True assertRaisesRegex RuntimeError r Runtime assertion failed expression Ne\ s \ ep module torch randn assertRaisesRegex RuntimeError r Runtime assertion failed expression Ne\ Mod\ s \ \ ep module torch randn ep module torch randn test_full_on_scalar_tensor Foo torch nn Module forward val torch full val dtype=torch float export Foo args= torch tensor test_custom_pytree Foo __init__ attr attr attr None raise ValueError Shouldn t None attr = attr attr = attr FooModel torch nn Module __init__ super __init__ foo_attr = Foo torch ones torch ones forward foo foo attr sum + foo attr sum + foo_attr attr sum flat foo torch utils _pytree _list_flatten foo attr foo attr flat_with_keys foo torch utils _pytree _list_flatten_with_keys foo attr foo attr unflat val context l = torch utils _pytree _list_unflatten val context Foo l l torch utils _pytree register_pytree_node Foo flat unflat flatten_with_keys_fn=flat_with_keys serialized_type_name=f Foo __module__ Foo __name__ torch export export FooModel Foo torch ones torch ones strict=False test_allow_explicit_guards_as_runtime_asserts check explicit guards treated runtime assertions Foo torch nn Module forward x y check negation first guard also shows up runtime assertion x shape == y shape False x + y x shape == y shape False x + y + x shape == y shape True x y inputs = torch randn torch randn dynamic_shapes = x Dim dx min= y Dim dy min= ep = torch export export Foo inputs dynamic_shapes=dynamic_shapes prefer_deferred_runtime_asserts_over_guards=True check forward pass out out = ep module torch randn torch randn assertEqual out shape torch ones shape assertEqual out shape torch ones shape assertRaisesRegex RuntimeError r Runtime assertion failed expression Ne\ s s \ fail only runtime ep module torch randn torch randn fail assertRaisesRegex RuntimeError r Runtime assertion failed expression Ne\ s s \ \ ep module torch randn torch randn fail assertRaisesRegex RuntimeError r Runtime assertion failed expression Eq\ s \ \ s \ ep module torch randn torch randn fail should set command line flag TORCH_DYNAMO_DO_NOT_EMIT_RUNTIME_ASSERTS= dynamo checks torch time so setting os environ makes no difference instead manually patch dynamo config test test setting flag removes runtime asserts torch _dynamo config _dynamo_config _dynamo_config patch do_not_emit_runtime_asserts=True ep = torch export _trace _export Foo inputs dynamic_shapes=dynamic_shapes prefer_deferred_runtime_asserts_over_guards=True run_decompositions assertEqual node target == torch ops aten _assert_scalar default node ep graph nodes count True test_unbacked_kth_value Foo torch nn Module forward x y n = y item k = min n x kthvalue k dim= keepdim=True values inps = torch arange torch tensor ep = export Foo inps assertEqual ep module inps item test_constant_output_dup M torch nn Module __init__ super __init__ constant = torch ones forward x x + constant constant ep = export M torch ones run_decompositions mod = ep module b = mod torch zeros assertTrue torch allclose torch ones assertTrue torch allclose b torch ones test_constant_tensor_mutation M torch nn Module __init__ super __init__ foo = torch randn forward x foo add_ foo + x assertRaisesRegex RuntimeError Constant foo _ = export M torch ones strict=False run_decompositions graph test_constant_return M torch nn Module __init__ super __init__ foo = torch randn forward x foo foo + x graph = export M torch ones strict=False run_decompositions graph assertExpectedInline str graph strip \ graph c_foo num_users= = placeholder target=c_foo x num_users= = placeholder target=x add num_users= = call_function target=torch ops aten add Tensor args = c_foo x kwargs = c_foo add test_constant_requires_grad_const M torch nn Module __init__ super __init__ foo = torch randn requires_grad=True forward x x cos + foo sum gm = export M torch ones module assertFalse gm foo requires_grad test_constant_aliasing M torch nn Module __init__ m foo super __init__ m = m foo = foo forward x x + foo + m x M torch nn Module __init__ - None super __init__ foo = torch ones requires_grad=True forward x x + foo m = M m = M m m foo inps = torch ones ep = export m inps strict=False check both constants appear list assertEqual sorted list ep constants foo m foo check only one input spec exists num_constant_inputs = spec kind == InputKind CONSTANT_TENSOR spec ep graph_signature input_specs count True assertEqual num_constant_inputs unflatten unflattened = unflatten ep assertTrue torch allclose m inps unflattened inps testing expectedFailureRetraceability test_unused_aliases Foo torch nn Module __init__ - None super __init__ param alpha = torch nn Parameter torch randn beta = alpha gamma = alpha forward x x + gamma inps = torch randn ep = export Foo inps placeholder nodes will deduplicated strict-mode check all params still appear state dict param alpha beta gamma assertTrue param ep state_dict check they also appear unflattened state dict unep = unflatten ep param alpha beta gamma assertTrue param unep state_dict test_intermediate_shape_comp Foo torch nn Module forward x y z = torch cat x x dim= w = z repeat y shape w shape + x shape inputs = torch randn torch randn shapes = x Dim dx y Dim dy ep = export Foo inputs dynamic_shapes=shapes run_decompositions test shape size compute sym_size call add_node = node node ep graph nodes node target == operator add assertTrue add_node args target == operator mul test sym_size calls only happen placeholders sym_size_nodes = node node ep graph nodes node target == torch ops aten sym_size int assertEqual len sym_size_nodes assertTrue all node args op == placeholder node sym_size_nodes dynamo will DCE repeat node AOTAutograd will leave training IR will also DCE due retracing repeat_nodes = node node ep graph nodes node target == torch ops aten repeat default assertEqual len repeat_nodes test_checks_to_constrain_range Foo torch nn Module forward x y n = y item m = y item torch _check m = torch _check n = torch _check -m = - m = torch _check n = n has range x n inputs = torch randn torch tensor ep = export Foo inputs FileCheck check_count torch ops aten _assert_scalar default exactly=True run ep graph_module code ep = ep run_decompositions FileCheck check_count torch ops aten _assert_scalar default exactly=True run ep graph_module code check runtime ep module torch randn torch tensor assertRaisesRegex RuntimeError r Runtime assertion failed expression u \d+ \ \= ep module torch randn torch tensor torch fx experimental _config patch backed_size_oblivious=True test_baddbmm M torch nn Module __init__ super __init__ weight = torch nn Parameter torch randn dtype=torch float bias = torch nn Parameter torch randn dtype=torch float forward x torch ops aten baddbmm default bias x weight x = torch randn dtype=torch float x = torch randn dtype=torch float m = M ep = export m x dynamic_shapes= Dim batch assertTrue torch allclose m x ep module x assertTrue torch allclose m x ep module x testing expectedFailureSerDerNonStrict constructor serialized today testing expectedFailureSerDer constructor serialized today testing expectedFailureRetraceability dynamo doesn t work FlatApply op test_capture_subclass_constructor Foo torch nn Module __init__ super __init__ buffer = torch nn Buffer TwoTensor torch randn torch randn forward x two_tensor = TwoTensor x TwoTensor x x + buffer val = x + two_tensor val b mod = Foo ep = torch export export mod torch randn strict=False assertExpectedInline str ep graph strip \ graph b_buffer num_users= = placeholder target=b_buffer x num_users= = placeholder target=x twotensor___init__ num_users= = get_attr target=twotensor___init__ twotensor_const_func_spec num_users= = get_attr target=twotensor_const_func_spec flat_apply num_users= = call_function target=torch ops higher_order flat_apply args = twotensor_const_func_spec twotensor___init__ x x kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = flat_apply b kwargs = twotensor___init__ num_users= = get_attr target=twotensor___init__ twotensor_const_func_spec _ num_users= = get_attr target=twotensor_const_func_spec flat_apply_ num_users= = call_function target=torch ops higher_order flat_apply args = twotensor_const_func_spec _ twotensor___init__ access_subclass_inner_tensor_default_ flat_apply kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = flat_apply_ b kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = access_subclass_inner_tensor_default_ b kwargs = add num_users= = call_function target=torch ops aten add Tensor args = flat_apply_ b_buffer kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = access_subclass_inner_tensor_default_ add kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = add_ b kwargs = access_subclass_inner_tensor_default_ num_users= = call_function target=torch ops export access_subclass_inner_tensor default args = access_subclass_inner_tensor_default_ kwargs = access_subclass_inner_tensor_default_ inp = torch randn assertEqual ep module inp mod inp torch inference_mode ep = ep run_decompositions There should no subclases assertExpectedInline str ep graph strip \ graph b_parametrizations_buffer_original num_users= = placeholder target=b_parametrizations_buffer_original b_parametrizations_buffer_original num_users= = placeholder target=b_parametrizations_buffer_original x num_users= = placeholder target=x add_ num_users= = call_function target=torch ops aten add Tensor args = x b_parametrizations_buffer_original kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x add_ kwargs = add_ assertEqual ep module inp mod inp mod = Foo ep = export mod torch randn run_decompositions assertEqual ep module inp mod inp is_training_ir_test _testMethodName assertExpectedInline str ep graph strip \ graph b_parametrizations_buffer_original num_users= = placeholder target=b_parametrizations_buffer_original b_parametrizations_buffer_original num_users= = placeholder target=b_parametrizations_buffer_original x num_users= = placeholder target=x add num_users= = call_function target=torch ops aten add Tensor args = x b_parametrizations_buffer_original kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x add kwargs = add_ assertExpectedInline str ep graph strip \ graph b_parametrizations_buffer_original num_users= = placeholder target=b_parametrizations_buffer_original b_parametrizations_buffer_original num_users= = placeholder target=b_parametrizations_buffer_original x num_users= = placeholder target=x add_ num_users= = call_function target=torch ops aten add Tensor args = x b_parametrizations_buffer_original kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = x add_ kwargs = add_ test_capture_subclass_wrong torch _export wrappers mark_subclass_constructor_exportable_experimental assertRaisesRegex RuntimeError fn which supported If torch _disable_dynamo mark_subclass_constructor_exportable_experimental fn b + b Foo torch nn Module torch _disable_dynamo mark_subclass_constructor_exportable_experimental __init__ super __init__ forward x x cos export Foo torch randn test_capture_subclass_constructor_torch_ir Foo torch nn Module __init__ super __init__ buffer = torch nn Buffer TwoTensor torch randn torch randn forward x two_tensor = TwoTensor x TwoTensor x x + buffer val = x + two_tensor val b mod = Foo gm_torch_ir = _export_to_torch_ir mod torch randn FileCheck check_count torch testing _internal two_tensor TwoTensor exactly=True run gm_torch_ir code test_sym_float_operators Module torch nn Module forward x - x max item + x m = Module args = torch ones ep = export m args assertEqual ep module args m args test_cse_for_symint Foo torch nn Module check sym ops only get computed once forward x y x shape - y shape = x shape - y shape = x shape - y shape = x y inputs = torch randn torch randn shapes = x Dim dx y Dim dy ep = torch export export Foo inputs dynamic_shapes=shapes prefer_deferred_runtime_asserts_over_guards=True count pow nodes sym_size int nodes assertEqual node target node ep graph nodes count operator pow FileCheck check_count torch ops aten sym_size int exactly=True run ep graph_module code ep = ep run_decompositions assertEqual node target node ep graph nodes count operator pow FileCheck check_count torch ops aten sym_size int exactly=True run ep graph_module code test_shared_submodule_nn_module_stack Shared torch nn Module __init__ - None super __init__ layernorm = torch nn LayerNorm sub_net = torch nn Sequential layernorm torch nn ReLU layernorm torch nn ReLU forward x sub_net x eager_module = Shared inps = torch rand export_module = export eager_module inps nn_module_stacks = node meta get nn_module_stack node export_module graph nodes node op == call_function norm str node target assertEqual len nn_module_stacks filtered_nn_module_stack = list nn_module_stack values - nn_module_stack nn_module_stacks assertEqual filtered_nn_module_stack sub_net assertEqual filtered_nn_module_stack sub_net test_slice_nn_module_stack N torch nn Module forward x y x + y M torch nn Module __init__ super __init__ n = N mod_list_ = torch nn Sequential tuple n _ range mod_list_ = torch nn ModuleList n _ range forward x y m mod_list_ x = m x y m mod_list_ x = m x y x export_module = export M torch randn torch randn nn_module_stacks = node meta get nn_module_stack node export_module graph nodes node op == call_function add str node target assertEqual len nn_module_stacks filtered_nn_module_stack = list nn_module_stack values - nn_module_stack nn_module_stacks is_strict_test _testMethodName is_strict_v _test _testMethodName assertEqual filtered_nn_module_stack mod_list_ assertEqual filtered_nn_module_stack mod_list_ assertEqual filtered_nn_module_stack mod_list_ slice None assertEqual filtered_nn_module_stack mod_list_ slice None test_invalid_pytree_dynamo_graph_capture Block __init__ b = b = b Foo torch nn Module forward block block + block b torch _dynamo functional_export _dynamo_graph_capture_for_export assertRaisesRegex torch _dynamo exc UserError It looks like one inputs type _dynamo_graph_capture_for_export Foo Block torch randn torch randn test_enum_str TensorDim str enum Enum DDP = ddp FSDP = fsdp CP = cp TP = tp Foo torch nn Module __init__ super __init__ forward x val = x sin TensorDim DDP ddp val += x cos ddp TensorDim DDP val += x cos val torch _dynamo functional_export _dynamo_graph_capture_for_export inp = torch randn gm = export Foo inp run_decompositions module assertExpectedInline str gm graph strip \ graph x num_users= = placeholder target=x _guards_fn num_users= = call_module target=_guards_fn args = x kwargs = sin num_users= = call_function target=torch ops aten sin default args = x kwargs = cos num_users= = call_function target=torch ops aten cos default args = x kwargs = add num_users= = call_function target=torch ops aten add Tensor args = sin cos kwargs = cos_ num_users= = call_function target=torch ops aten cos default args = x kwargs = add_ num_users= = call_function target=torch ops aten add Tensor args = add cos_ kwargs = add_ assertEqual gm inp Foo inp test_split_const_gm_with_lifted_constants Model torch nn Module __init__ - None super __init__ w_pre = torch randn b = torch randn forward x w_transpose = torch transpose w_pre w_relu = torch nn functional relu w_transpose w = w_relu + b torch matmul x w + b + torch arange dtype=torch float example_inputs = torch randn mod = Model ep = torch export export mod example_inputs new_gm = copy deepcopy ep graph_module new_sig = copy deepcopy ep graph_signature placeholder_nodes = node node new_gm graph nodes node op == placeholder constants = ep state_dict ep constants lifted_constants = n name constants spec target n spec zip placeholder_nodes new_sig input_specs spec target None w_pre b lifted_constant_names = list lifted_constants lifted_constant_values = lifted_constants n n lifted_constant_names const_gm _ = split_const_gm new_gm False lifted_constant_names counter = node const_gm graph nodes node op == call_function counter += assertTrue counter == counter = n new_gm graph nodes n op == placeholder counter += expect existing placeholders folded constant assertTrue counter == b folded_const folded_const const_folded_value = const_gm lifted_constant_values test_input = torch randn new_gm c_w_pre b x folded_const folded_const actual = new_gm lifted_constant_values const_folded_value test_input const_folded_value const_folded_value expected = mod test_input assertEqual actual expected const_gm _ = split_const_gm ep graph_module False lifted_constant_names lambda x True counter = node const_gm graph nodes node op == call_function assertTrue False test_istft_op istft_class torch nn Module forward spec window = torch hann_window type torch FloatTensor torch istft spec n_fft= hop_length= window=window length= model = istft_class real_part = torch randn dtype=torch float imaginary_part = torch randn dtype=torch float spec = torch complex real_part imaginary_part export model spec test_custom_op_preserve M torch nn Module forward x y = torch ops testlib foo_functional default x torch ops testlib foo_mutated default y decomp_table = torch export default_decompositions del decomp_table torch ops testlib foo_functional default ep = torch export export M torch randn run_decompositions decomp_table IS_FBCODE assertExpectedInline str ep graph_module code strip \ forward x foo_functional = torch ops testlib foo_functional default x x = None cos = torch ops aten cos default foo_functional auto_functionalized = torch ops higher_order auto_functionalized torch ops testlib foo default x = foo_functional z = cos foo_functional = cos = None getitem_ = auto_functionalized auto_functionalized = None cos_ = torch ops aten cos default getitem_ getitem_ cos_ assertExpectedInline str ep graph_module code strip \ forward x foo_functional = torch ops testlib foo_functional default x x = None cos = torch ops aten cos default foo_functional auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops testlib foo default _x_base_index = _z_base_index = _all_bases = foo_functional cos foo_functional = cos = None getitem_ = auto_functionalized_v auto_functionalized_v = None cos_ = torch ops aten cos default getitem_ getitem_ cos_ test_run_decompositions_keep_metadata Make sure metadata kept after exported program run_decompositions torch library custom_op mylib add mutates_args= add x torch Tensor y torch Tensor - torch Tensor torch library register_fake mylib add _ x torch Tensor y torch Tensor - torch Tensor torch empty_like x TestModel torch nn Module forward x y torch ops mylib add x y model = TestModel x_example = torch randn y_example = torch randn exported_program = export model x_example y_example node exported_program graph nodes node meta custom = my_field dummy node exported_program graph nodes assertEqual node meta custom my_field dummy decomposed_program = exported_program run_decompositions node decomposed_program graph nodes assertEqual node meta custom my_field dummy test_run_decompositions_keep_tensor_constant_metadata Make sure metadata tensor constants kept after run_decompositions M torch nn Module __init__ super __init__ b = torch ones linear = torch nn Linear forward x b + linear x ep = export M torch ones node ep graph nodes node meta custom = my_field dummy node ep graph nodes assertEqual node meta custom my_field dummy decomp_ep = ep run_decompositions node decomp_ep graph nodes assertEqual node meta custom my_field dummy test_export_linear_preserve_dynamic_shape M torch nn Module __init__ super __init__ lin = torch nn Linear forward x lin x mod = M ep = export mod torch randn dynamic_shapes= x Dim x table = torch export default_decompositions del table torch ops aten linear default ep = ep run_decompositions table comp_mod = ep module inp = torch randn inp = torch randn assertTrue torch allclose comp_mod inp mod inp assertTrue torch allclose comp_mod inp mod inp torch fx experimental _config patch backed_size_oblivious=True test_repeat_interleave M torch nn Module forward values batch_sizes torch repeat_interleave torch arange values shape batch_sizes inp = torch randint torch randint ep = torch export export M inp dynamic_shapes= Dim dim Dim dim assertTrue torch allclose M inp ep module inp inp = torch randint torch randint assertTrue torch allclose M inp ep module inp test_automatic_dynamic_shapes_simple_equality The next test cases tests automatic dynamic shapes specs verifying automatic dynamism leads replacement symbols being set equalities inferred relationships being checked runtime asserts Check we specialize static values when program says so AUTO STATIC = Dim AUTO Dim STATIC case direct equality between symbols SimpleEquality torch nn Module forward x y z all inputs should have shape s s x + y + z inputs = tuple torch randn _ range fully dynamic _check_dynamic_shapes_specs_and_shapes SimpleEquality inputs specs= AUTO AUTO AUTO AUTO AUTO AUTO AUTO AUTO AUTO AUTO AUTO AUTO x AUTO AUTO y AUTO AUTO z AUTO AUTO passing_shapes= failing_shapes= test_serdes=True static s _check_dynamic_shapes_specs_and_shapes specifying just one dimension static should enough specialize all s SimpleEquality inputs specs= AUTO AUTO AUTO AUTO AUTO None x AUTO AUTO y AUTO AUTO z AUTO None passing_shapes= failing_shapes= test_serdes=True fully static _check_dynamic_shapes_specs_and_shapes should specialize all SimpleEquality inputs specs= x None AUTO y AUTO AUTO z AUTO None passing_shapes= failing_shapes= test_serdes=True test_automatic_dynamic_shapes_constant_relation AUTO STATIC = Dim AUTO Dim STATIC case related constant s + = s OffBy torch nn Module forward x y x + y inputs = torch randn torch randn fully dynamic _check_dynamic_shapes_specs_and_shapes OffBy inputs specs= AUTO AUTO x AUTO y AUTO passing_shapes= failing_shapes= test_serdes=True static s should specialize s _check_dynamic_shapes_specs_and_shapes OffBy inputs specs= x AUTO y None passing_shapes= failing_shapes= test_serdes=True test_automatic_dynamic_shapes_linear_relation AUTO STATIC = Dim AUTO Dim STATIC case linear relation LinearRel torch nn Module forward x y x s y s relation seems s + == s x + y inputs = torch randn torch randn fully dynamic _check_dynamic_shapes_specs_and_shapes LinearRel inputs specs= AUTO AUTO x AUTO y AUTO passing_shapes= failing_shapes= test_serdes=False static s shouldn t actually specialize s guard s + == _check_dynamic_shapes_specs_and_shapes LinearRel inputs specs= AUTO None x AUTO y None passing_shapes= failing_shapes= test_serdes=False static s will definitely specialize s guard + == s - == s _check_dynamic_shapes_specs_and_shapes LinearRel inputs specs= None AUTO passing_shapes= failing_shapes= test_serdes=True test_preserve_annotation M torch nn Module forward x fx_traceback annotate pp_stage fx_traceback annotate fdsp_bucket x = x + x = x - fx_traceback annotate cuda_stream fsdp_bucket x = x x = x x m = M fx_traceback preserve_node_meta ep = export m torch randn node ep graph nodes node op placeholder output continue node target == torch ops aten add Tensor assertTrue node meta custom pp_stage fdsp_bucket node target == torch ops aten sub Tensor assertTrue node meta custom pp_stage node target == torch ops aten mul Tensor assertTrue node meta custom pp_stage cuda_stream fsdp_bucket node target == torch ops aten div Tensor custom node meta assertTrue node meta custom raise AssertionError f Node checked node node target test_dynamic_shapes_serdes_generic torch _export serde dynamic_shapes _dump_dynamic_shapes _load_dynamic_shapes Foo torch nn Module forward b c d d == hello x = + b = torch cat b b dim= reshape - x + b c test de serialization some generic specs dz = Dim dz min= max= dx = dz dy = dx + inputs = torch randn torch randn torch randn torch randn hello dynamic_shapes = dx dy b dz c None d None ep = export Foo inputs dynamic_shapes=dynamic_shapes _check_dynamic_shapes_specs_and_shapes Foo inputs dynamic_shapes hello hello hello test_serdes=True assertExpectedInline _dump_dynamic_shapes dynamic_shapes inputs DynamicShapesSpec dynamic_shapes= dz dz + dz _DimHint STATIC _DimHint STATIC None dims= dz RootDim min= max= derived= dz dz + assertExpectedInline _dump_dynamic_shapes dynamic_shapes inputs to_dict=True dynamic_shapes dz dz + dz _DimHint STATIC _DimHint STATIC None dims dz min max derived dz dz + dx _ dy _ dz _ _ _ = _load_dynamic_shapes _dump_dynamic_shapes dynamic_shapes inputs assertEqual dx root dz assertEqual dy root dz test_dynamic_shapes_serdes_various serialization dataclass inputs Dim AUTO STATIC kwargs torch _export serde dynamic_shapes _dump_dynamic_shapes _load_dynamic_shapes auto static = Dim AUTO Dim STATIC dataclass Input Tensor b Tensor torch export register_dataclass Input serialized_type_name= test_dynamic_shapes_serdes_various Input Foo torch nn Module forward x y z x - torch randn y + y b + z args = torch randn kwargs = y Input a=torch randn b=torch randn z torch randn dynamic_shapes = x auto static y auto auto auto auto z auto dump dynamic_shapes assertExpectedInline _dump_dynamic_shapes dynamic_shapes args kwargs DynamicShapesSpec dynamic_shapes= _DimHint AUTO _DimHint STATIC _DimHint AUTO _DimHint AUTO _DimHint AUTO _DimHint AUTO _DimHint AUTO dims= assertExpectedInline _dump_dynamic_shapes dynamic_shapes args kwargs to_dict=True dynamic_shapes _DimHint AUTO _DimHint STATIC _DimHint AUTO _DimHint AUTO _DimHint AUTO _DimHint AUTO _DimHint AUTO dims test_dynamic_shapes_serdes_user_errors check error messages dynamic shapes de serialization torch _export serde dynamic_shapes _dump_dynamic_shapes _load_dynamic_shapes DynamicShapesSpec RootDim torch _export serde serialize _dataclass_to_dict stuff should well tested ` test_mismatched_dynamic_shapes ` assertRaisesRegex torch _dynamo exc UserError re escape Detected mismatch between structure ` inputs ` ` dynamic_shapes ` ` inputs k ` list ` dynamic_shapes k ` tuple dynamic_shapes = x k Dim dx Dim dy _dump_dynamic_shapes dynamic_shapes k torch randn loading from_dict=True False spec = DynamicShapesSpec dynamic_shapes= dx dims= dx RootDim min= max= derived= spec_dict = _dataclass_to_dict spec assertRaisesRegex torch _dynamo exc UserError re escape With from_dict=True expected ` spec ` dict got torch _export serde dynamic_shapes DynamicShapesSpec _load_dynamic_shapes spec from_dict=True assertRaisesRegex torch _dynamo exc UserError re escape Expected ` spec ` DynamicShapesSpec got dict _load_dynamic_shapes spec_dict from_dict=False assertExpectedInline _load_dynamic_shapes spec from_dict=False Dim dx min= max= check incorrect info dims assertRaisesRegex torch _dynamo exc UserError re escape Expected dims ` spec dims ` map ` min ` int got dx None spec = dynamic_shapes dx dims dx min None max derived _load_dynamic_shapes spec from_dict=True assertRaisesRegex torch _dynamo exc UserError re escape Expected dims ` spec dynamic_shapes ` tracked ` spec dims ` got dx which dict_keys dy spec = dynamic_shapes dx dims dy min max derived _load_dynamic_shapes spec from_dict=True assertRaisesRegex torch _dynamo exc UserError re escape Expected derived expressions linear expressions got dx + spec = dynamic_shapes dx dims dx min max derived dx + _load_dynamic_shapes spec from_dict=True Previously export run_decomp would dispatch sdpa math backend which doesn t guarantee contiguous tensor As result downstream view op would fail In eager normal export sdpa decomps flash_attention which has correct handling non-contiguous output Since normal export we dispatch flash_attention we also force run_decomp follow flash_attention test_attention Foo torch nn Module __init__ - None super __init__ embed_dim = num_heads = dropout = batch_first = True self_attention = torch nn MultiheadAttention embed_dim num_heads dropout=self dropout batch_first=self batch_first forward input torch Tensor x _ = self_attention input input input need_weights=False x inps = torch randn device= cpu export Foo inps test_dim_dynamic dynamic = Dim DYNAMIC dynamic should infer equalities relations Relations torch nn Module forward u w x y z = u + w + x s == s + == s + b = y flatten + z s s == s b inputs = torch randn torch randn torch randn torch randn torch randn ep = export Relations inputs dynamic_shapes= u dynamic w dynamic x dynamic y dynamic dynamic z dynamic ep module torch randn torch randn torch randn torch randn torch randn dynamic should complain when force specialized Specialize torch nn Module forward x torch _check x shape == x + assertRaisesRegex torch _dynamo exc UserError r You marked your code specialized constant r If you re using Dim DYNAMIC replace either Dim STATIC Dim AUTO ep = export Specialize torch randn dynamic_shapes= x dynamic dynamic dynamic should handle complex guards same way auto ModConstraint torch nn Module forward x torch Tensor - torch Tensor x view x shape - - private_api True False private_api ep = torch export export ModConstraint torch randn dynamic_shapes= x dynamic dynamic prefer_deferred_runtime_asserts_over_guards=True ep = export ModConstraint torch randn dynamic_shapes= x dynamic dynamic ep module torch randn num_asserts = node target == torch ops aten _assert_scalar default node ep graph nodes count True private_api assertEqual num_asserts assertRaisesRegex RuntimeError r Runtime assertion failed expression Eq\ Mod\ s \ s s - \ \ ep module torch randn no runtime assert exported module assertEqual num_asserts fails anyway wrong inputs assertRaisesRegex AssertionError escape Guard failed x size x size - + x size == expected got ep module torch randn testing expectedFailureSerDer T testing expectedFailureSerDerNonStrict test_hints_wrapper strict = True M torch nn Module __init__ - None super __init__ forward x y x = x + y inner_body_fn x y x = torch relu x x = x + y x outer_body_fn x y x = hints_wrapper inner_body_fn x y hints= inner_body True x = torch abs x x res = hints_wrapper outer_body_fn x y hints= outer_body True res x = torch randn y = torch ones ep_for_training = torch export export M x y strict=strict assertExpectedInline normalize_gm ep_for_training graph_module print_readable print_output=False \ GraphModule torch nn Module forward x f y f add f = torch ops aten add Tensor x y x = None hints_wrapper_body_graph_ = hints_wrapper_body_graph_ hints_wrapper = torch ops higher_order hints_wrapper hints_wrapper_body_graph_ add y hints = outer_body True hints_wrapper_body_graph_ = add = y = None getitem f = hints_wrapper hints_wrapper = None getitem hints_wrapper_body_graph_ torch nn Module forward arg _ f arg _ f hints_wrapper_body_graph_ = hints_wrapper_body_graph_ hints_wrapper = torch ops higher_order hints_wrapper hints_wrapper_body_graph_ arg _ arg _ hints = inner_body True hints_wrapper_body_graph_ = arg _ = arg _ = None getitem f = hints_wrapper hints_wrapper = None abs_ f = torch ops aten abs default getitem getitem = None abs_ hints_wrapper_body_graph_ torch nn Module forward arg _ f arg _ f relu f = torch ops aten relu default arg _ arg _ = None add f = torch ops aten add Tensor relu arg _ relu = arg _ = None add ignore_empty_lines=True ep = export M x y strict=strict run_decompositions export_res = ep module x y ref_res = M x y assertEqual export_res ref_res assertExpectedInline normalize_gm ep graph_module print_readable print_output=False \ GraphModule torch nn Module forward x f y f add f = torch ops aten add Tensor x y x = None hints_wrapper_body_graph_ = hints_wrapper_body_graph_ hints_wrapper = torch ops higher_order hints_wrapper hints_wrapper_body_graph_ add y hints = outer_body True hints_wrapper_body_graph_ = add = y = None getitem f = hints_wrapper hints_wrapper = None getitem hints_wrapper_body_graph_ torch nn Module forward arg _ f arg _ f hints_wrapper_body_graph_ = hints_wrapper_body_graph_ hints_wrapper = torch ops higher_order hints_wrapper hints_wrapper_body_graph_ arg _ arg _ hints = inner_body True hints_wrapper_body_graph_ = arg _ = arg _ = None getitem f = hints_wrapper hints_wrapper = None abs_ f = torch ops aten abs default getitem getitem = None abs_ hints_wrapper_body_graph_ torch nn Module forward arg _ f arg _ f relu f = torch ops aten relu default arg _ arg _ = None add f = torch ops aten add Tensor relu arg _ relu = arg _ = None add ignore_empty_lines=True testing expectedFailureStrict test_hop doesn t have dynamo implementation testing expectedFailureStrictV test_hop doesn t have dynamo implementation testing expectedFailureRetraceability test_hop doesn t have dynamo implementation testing expectedFailureTrainingIRToRunDecomp test_hop doesn t have dynamo implementation testing expectedFailureSerDerNonStrict TODO serde torch FunctionSchema implemented yet testing expectedFailureSerDer TODO serde torch FunctionSchema implemented yet test_export_function_schema torch utils _pytree pytree torch _higher_order_ops utils _maybe_run_with_interpreter autograd_not_implemented reenter_make_fx unique_graph_id torch _ops HigherOrderOperator torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor ProxyTorchDispatchMode track_tensor_tree pytree register_constant torch FunctionSchema TestFunctionSchemaHop HigherOrderOperator __init__ super __init__ test_function_schema __call__ fn x torch Tensor schema Union torch FunctionSchema pytree TreeSpec isinstance schema torch FunctionSchema _ schema = pytree tree_flatten schema super __call__ fn x schema trace_hop proxy_mode fn x schema sub_gm = reenter_make_fx fn x i gm_name = unique_graph_id proxy_mode prefix= _sub_gm proxy_mode tracer root register_module gm_name sub_gm out_proxy = proxy_mode tracer create_proxy call_function test_hop tuple proxy_mode tracer unwrap_proxy arg arg sub_gm x schema example_out = test_hop sub_gm x schema track_tensor_tree example_out out_proxy constant=None tracer=proxy_mode tracer dense_hop fn x schema assert isinstance schema pytree TreeSpec schema = pytree tree_unflatten schema assert isinstance schema torch FunctionSchema schema == torch ops aten sin default _schema fn x fake_hop mode fn x schema mode dense_hop fn x schema func_hop ctx fn x schema unwrapped_x = ctx unwrap_tensors x functional_fn = ctx functionalize _maybe_run_with_interpreter fn ctx wrap_tensors test_hop functional_fn unwrapped_x schema test_hop = TestFunctionSchemaHop test_hop py_impl ProxyTorchDispatchMode trace_hop test_hop py_impl torch _C DispatchKey CompositeExplicitAutograd dense_hop test_hop py_impl FakeTensorMode fake_hop test_hop py_autograd_impl autograd_not_implemented test_hop deferred_error=True test_hop py_functionalize_impl func_hop Model torch nn Module forward x fn x x sin test_hop fn x torch ops aten sin default _schema mod = Model x = torch randn ep = export mod x assertEqual x sin ep module x pytree _deregister_pytree_node torch FunctionSchema unittest skipIf torch cuda is_available Test requires CUDA test_exception Model torch nn Module __init__ super __init__ embedding = torch nn Embedding num_embeddings= embedding_dim= register_buffer buffer torch ones register_buffer param torch ones forward x token_ids = torch randint device=x device embedded = embedding token_ids sum buffer sum + param sum + x sum + embedded BarModel torch nn Module __init__ super __init__ mod = Model forward x cuda str x device mod = mod x device mod x x sum BarBar torch nn Module __init__ super __init__ mod = BarModel forward x torch amp autocast device_type= cuda y = mod x y torch no_grad assertRaisesRegex RuntimeError Couldn t swap Embedding weight _ = torch export export BarBar x torch randn device= cuda strict=False module test_export_for_training_with_state_dict_hooks _state_dict_pre_hook mod prefix keep_vars mod _buffers test = torch Tensor _state_dict_hook mod state_dict prefix args kwargs keys = list state_dict keys key keys local_key = key len prefix local_key startswith layer new_key = prefix + local_key replace layer state_dict new_key = state_dict key new_key = key del state_dict key Layer torch nn Module __init__ super __init__ linear = torch nn Linear linear = torch nn Linear forward x x = linear x x = torch relu x x = linear x x CustomModule torch nn Module __init__ super __init__ _register_state_dict_hook _state_dict_hook register_state_dict_pre_hook _state_dict_pre_hook non-persistent buffer named_buffers foo = torch nn Buffer torch rand persistent=False non-persistent buffer named_buffers register_buffer buf None persistent=False layer = Layer forward x x = layer x x M = CustomModule inp = torch randn ep = export M inp export_res = ep module inp ref_res = M inp assertEqual export_res ref_res we want store unprocessed keys assertTrue layer linear weight layer linear bias layer linear weight layer linear bias issubset spec target spec ep graph_signature input_specs unflattened = torch export unflatten ep export_res = unflattened inp assertEqual export_res ref_res torch _export utils _disable_load_state_dict_hooks M state_dict = M state_dict assertEqual layer linear weight layer linear bias layer linear weight layer linear bias state_dict keys state_dict = M state_dict assertEqual linear weight linear bias linear weight linear bias test state_dict keys testing expectedFailureSerDer T testing expectedFailureSerDerNonStrict test_dynamic_sym_round ModuleWithSymRound torch nn Module forward x out_size = round x shape x out_size dim_min = dim_max = dynamic_shapes = x Dim n min=dim_min max=dim_max module = ModuleWithSymRound inp = torch randn ep = export module inp dynamic_shapes=dynamic_shapes Expect builtin round export graph round_nodes = n n ep graph nodes n op == call_function n target == round assertEqual len round_nodes Check pre post-export equality i range dim_min dim_max + dyn_inp = torch randn i export_res = ep module dyn_inp ref_res = module dyn_inp assertEqual export_res ref_res testing expectedFailureSerDer testing expectedFailureSerDerNonStrict test_dynamic_lr_shift Module torch nn Module forward x rshift = x shape lshift = x shape x rshift x lshift dynamic_shapes = x Dim N min= max= inp = torch randn ep = export Module inp dynamic_shapes=dynamic_shapes op operator lshift operator rshift shift_op = n n ep graph nodes n op == call_function n target == op assertEqual len shift_op test_export_rnn_variants_with_warning Test when exporting RNN LSTM GRU models non-strict mode Produces expected warnings about tensor attributes being assigned during export Does leak fake tensors model s flat weights Does produce extra tensor constants graph signature rnn_types = torch nn RNN RNN torch nn LSTM LSTM torch nn GRU GRU rnn_class rnn_name rnn_types subTest rnn_type=rnn_name m = rnn_class input_size= hidden_size= num_layers= batch_first=True sample_inputs = torch randn eager_out = m sample_inputs Verify export produces expected warning about tensor attributes assertWarnsRegex UserWarning r The tensor attributes self\ _flat_weights\ \ self\ _flat_weights\ \ r self\ _flat_weights\ \ self\ _flat_weights\ \ assigned during export ep = torch export export m sample_inputs strict=False ep_out = ep module sample_inputs assertEqual eager_out ep_out Verify no fake tensor leakage flat weights should real tensors flat_weight m _flat_weights assertTrue isinstance flat_weight torch _subclasses fake_tensor FakeTensor Verify no tensor constants graph signature assertEqual len ep graph_signature lifted_tensor_constants contextmanager distributed_env world_size try torch distributed init_process_group backend= fake world_size=world_size rank= yield finally torch distributed destroy_process_group xfailIfDistributedNotSupported test_distributed_all_reduce Foo torch nn Module __init__ super __init__ linear = torch nn Linear forward x y = linear x abs clamp max= torch distributed all_reduce y y distributed_env world_size= m = Foo ep = export m torch randn inp = torch randn assertTrue torch allclose ep module inp m inp xfailIfDistributedNotSupported test_distributed_all_gather Foo torch nn Module forward x ys = torch empty_like x _ range torch distributed all_gather ys x ys distributed_env world_size= m = Foo ep = export m torch randn inp = torch randn assertTrue torch allclose b b zip ep module inp m inp xfailIfDistributedNotSupported test_distributed_all_gather_into_tensor Foo torch nn Module forward x y = torch empty torch distributed all_gather_into_tensor y x y distributed_env world_size= m = Foo ep = export m torch randn inp = torch randn assertTrue torch allclose ep module inp m inp xfailIfDistributedNotSupported testing expectedFailureCppRuntime test_distributed_all_to_all_single Foo torch nn Module forward x y = torch empty torch distributed all_to_all_single y x y distributed_env world_size= m = Foo ep = export m torch randn nodes = ep graph find_nodes op= call_function target=torch ops _c d_functional all_to_all_single default assertEqual len nodes xfailIfDistributedNotSupported testing expectedFailureCppRuntime test_distributed_reduce_scatter_tensor Foo torch nn Module forward x y = torch empty torch distributed reduce_scatter_tensor y x y distributed_env world_size= m = Foo ep = export m torch randn nodes = ep graph find_nodes op= call_function target=torch ops _c d_functional reduce_scatter_tensor default assertEqual len nodes test_default_decomposition_core_cia_ops Verify core ATen ops Composite Implicit Autograd dispatch decomposed default TODO Add avg_pool d adaptive_avg_pool d when ready See issue core_cia_ops = torch ops aten upsample_bilinear d vec torch ops aten upsample_bilinear d vec align_corners False scale_factors output_size None torch ops aten upsample_nearest d vec torch ops aten upsample_nearest d vec scale_factors output_size None op_name op kwargs core_cia_ops items M torch nn Module forward x op x kwargs ep = export M torch randn FileCheck check_count op_name exactly=True run ep graph_module code decomp_table = default_decompositions ep = ep run_decompositions decomp_table=decomp_table FileCheck check_count op_name exactly=True run ep graph_module code test_wrapper_module f x torch abs x torch export _wrapper_utils model = _wrapper_utils _WrapperModule f ep = export model torch randn assertExpectedInline str ep graph_module code strip \ forward args_ abs_ = torch ops aten abs default args_ args_ = None abs_ test_sdpa_gqa torch nn attention sdpa_kernel SDPBackend Foo torch nn Module forward q k v F scaled_dot_product_attention q k v enable_gqa=True q = torch randn k = torch randn v = torch randn sdpa_kernel SDPBackend MATH ep_math = export Foo q k v ep_math = ep_math run_decompositions assertExpectedInline ep_math graph_module code strip \ forward q k v mul = torch ops aten mul Scalar q q = None unsqueeze = torch ops aten unsqueeze default k k = None expand = torch ops aten expand default unsqueeze unsqueeze = None clone = torch ops aten clone default expand memory_format = torch contiguous_format expand = None view = torch ops aten view default clone clone = None unsqueeze_ = torch ops aten unsqueeze default v v = None expand_ = torch ops aten expand default unsqueeze_ unsqueeze_ = None clone_ = torch ops aten clone default expand_ memory_format = torch contiguous_format expand_ = None view_ = torch ops aten view default clone_ clone_ = None permute = torch ops aten permute default view view = None mul_ = torch ops aten mul Scalar permute permute = None expand_ = torch ops aten expand default mul mul = None view_ = torch ops aten view default expand_ expand_ = None expand_ = torch ops aten expand default mul_ mul_ = None view_ = torch ops aten view default expand_ expand_ = None bmm = torch ops aten bmm default view_ view_ view_ = view_ = None view_ = torch ops aten view default bmm bmm = None _softmax = torch ops aten _softmax default view_ - False eq = torch ops aten eq Scalar view_ -inf view_ = None logical_not = torch ops aten logical_not default eq eq = None any_ = torch ops aten any dim logical_not - True logical_not = None logical_not_ = torch ops aten logical_not default any_ any_ = None full_like = torch ops aten full_like default _softmax pin_memory = False memory_format = torch preserve_format where = torch ops aten where logical_not_ full_like _softmax logical_not_ = full_like = _softmax = None expand_ = torch ops aten expand default where where = None view_ = torch ops aten view default expand_ expand_ = None expand_ = torch ops aten expand default view_ view_ = None view_ = torch ops aten view default expand_ expand_ = None bmm_ = torch ops aten bmm default view_ view_ view_ = view_ = None view_ = torch ops aten view default bmm_ bmm_ = None view_ sdpa_kernel SDPBackend FLASH_ATTENTION ep_flash = export Foo q k v ep_flash = ep_flash run_decompositions assertExpectedInline ep_flash graph_module code strip \ forward q k v mul = torch ops aten mul Scalar q q = None unsqueeze = torch ops aten unsqueeze default k k = None expand = torch ops aten expand default unsqueeze unsqueeze = None clone = torch ops aten clone default expand memory_format = torch contiguous_format expand = None view = torch ops aten view default clone clone = None unsqueeze_ = torch ops aten unsqueeze default v v = None expand_ = torch ops aten expand default unsqueeze_ unsqueeze_ = None clone_ = torch ops aten clone default expand_ memory_format = torch contiguous_format expand_ = None view_ = torch ops aten view default clone_ clone_ = None permute = torch ops aten permute default view view = None mul_ = torch ops aten mul Scalar permute permute = None expand_ = torch ops aten expand default mul mul = None view_ = torch ops aten view default expand_ expand_ = None expand_ = torch ops aten expand default mul_ mul_ = None view_ = torch ops aten view default expand_ expand_ = None bmm = torch ops aten bmm default view_ view_ view_ = view_ = None view_ = torch ops aten view default bmm bmm = None _softmax = torch ops aten _softmax default view_ - False eq = torch ops aten eq Scalar view_ -inf view_ = None logical_not = torch ops aten logical_not default eq eq = None any_ = torch ops aten any dim logical_not - True logical_not = None logical_not_ = torch ops aten logical_not default any_ any_ = None full_like = torch ops aten full_like default _softmax pin_memory = False memory_format = torch preserve_format where = torch ops aten where logical_not_ full_like _softmax logical_not_ = full_like = _softmax = None expand_ = torch ops aten expand default where where = None view_ = torch ops aten view default expand_ expand_ = None expand_ = torch ops aten expand default view_ view_ = None view_ = torch ops aten view default expand_ expand_ = None bmm_ = torch ops aten bmm default view_ view_ view_ = view_ = None view_ = torch ops aten view default bmm_ bmm_ = None permute_ = torch ops aten permute default view_ view_ = None clone_ = torch ops aten clone default permute_ memory_format = torch contiguous_format permute_ = None permute_ = torch ops aten permute default clone_ clone_ = None permute_ test backend check invalid inputs error_type = RuntimeError is_non_strict_test _testMethodName torch _dynamo exc TorchRuntimeError assertRaisesRegex error_type r Number heads key value must divide number heads export Foo torch randn k v test_namedtuple_input_export test NamedTuple inputs both strict non-strict export modes collections namedtuple PointNT = namedtuple PointNT x y M torch nn Module forward x y x + y inp = PointNT torch ones torch ones ep_non_strict = export M inp result_non_strict = ep_non_strict module inp ep_strict = export M inp strict=True result_strict = ep_strict module inp assertEqual result_non_strict result_strict test_tril_dynamic_diagonal Module torch nn Module forward x y x_len = x shape y_len = y shape mask = torch ones x_len y_len dtype=torch bool device=x device mask = mask tril diagonal=y_len - x_len mask x = torch randn y = torch randn x_len = Dim x_len min= max= y_len = Dim y_len min= max= ep = export Module x y dynamic_shapes= x x_len y y_len eager_out = Module x y exported_out = ep module x y assertEqual eager_out exported_out assertEqual exported_out shape x = torch randn y = torch randn eager_out = Module x y exported_out = ep module x y assertEqual eager_out exported_out assertEqual exported_out shape expected_mask = torch ones dtype=torch bool tril diagonal= assertEqual eager_out expected_mask test_triu_dynamic_diagonal Module torch nn Module forward x y x_len = x shape y_len = y shape mask = torch ones x_len y_len dtype=torch bool device=x device mask = mask triu diagonal=y_len - x_len mask x = torch randn y = torch randn x_len = Dim x_len min= max= y_len = Dim y_len min= max= ep = export Module x y dynamic_shapes= x x_len y y_len eager_out = Module x y exported_out = ep module x y assertEqual eager_out exported_out assertEqual exported_out shape x = torch randn y = torch randn eager_out = Module x y exported_out = ep module x y assertEqual eager_out exported_out assertEqual exported_out shape expected_mask = torch ones dtype=torch bool triu diagonal= assertEqual eager_out expected_mask unittest skipIf torchdynamo is_dynamo_supported dynamo isn t support TestOneOffModelExportResult TestCase test_scaled_dot_product_attention_cpu This test makes sure we always getting same decomposition result SDPA As now _scaled_dot_product_flash_attention_for_cpu expected show up export result Some downstream backend then further decompose into core ATen ops torch _decomp decompositions py search _scaled_dot_product_flash_attention_for_cpu Export decomposing based CompositeImplicitAutograd kernel implementation SDPA If test fails means kernel being modified In case we strongly encourage you change decomposition rule under torch _decomp decompositions py along kernel changes so all downstream backends being affected ScaledDotProductAttention torch nn Module __init__ - None super __init__ forward q k v attn_output = F scaled_dot_product_attention q k v None dropout_p= is_causal=True attn_output q = torch randn device= cpu k = torch randn device= cpu v = torch randn device= cpu torch nn attention SDPBackend torch nn attention sdpa_kernel SDPBackend MATH ep = torch export export ScaledDotProductAttention q k v ep run_decompositions skipIfCrossRef unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Can t run fused SDPA platform test_scaled_dot_product_attention_cuda This test makes sure we always getting same decomposition result SDPA As now _scaled_dot_product_flash_attention expected show up export result GPU tensors given Currently there s no downstream backend relies export result so test fails feel free change latest export result ScaledDotProductAttention torch nn Module __init__ - None super __init__ forward q k v attn_output = F scaled_dot_product_attention q k v None dropout_p= is_causal=True attn_output q = torch randn dtype=torch bfloat device= cuda k = torch randn dtype=torch bfloat device= cuda v = torch randn dtype=torch bfloat device= cuda ep = torch export export ScaledDotProductAttention q k v run_decompositions code_str = \ forward q k v _scaled_dot_product_flash_attention = torch ops aten _scaled_dot_product_flash_attention default q k v True scale = q = k = v = None getitem = _scaled_dot_product_flash_attention _scaled_dot_product_flash_attention = None getitem try assertExpectedInline ep graph_module code strip code_str except AssertionError code_str = \ forward q k v _scaled_dot_product_cudnn_attention = torch ops aten _scaled_dot_product_cudnn_attention default q k v None False True q = k = v = None getitem = _scaled_dot_product_cudnn_attention _scaled_dot_product_cudnn_attention = None getitem assertExpectedInline ep graph_module code strip code_str test_int_list_output M torch nn Module forward x x + x x x ep = torch export export M torch ones res = ep module torch ones assertEqual res test_primitive_constant_output Z torch nn Module forward x y torch no_grad y x moo ep = torch export export Z torch tensor res = ep module torch tensor assertEqual res torch tensor assertEqual res moo B torch nn Module forward x y y x y ep = torch export export B torch tensor res = ep module torch tensor assertEqual res torch tensor assertEqual res assertRaisesRegex AssertionError escape Guard failed y == expected got res = ep module torch tensor F torch nn Module forward x constant primitive type y = y x y ep = torch export export F torch tensor res = ep module torch tensor assertEqual res torch tensor assertEqual res Q torch nn Module forward x y y x y - ep = torch export export Q torch tensor res = ep module torch tensor assertEqual res torch tensor assertEqual res test_unbacked_sdpa torch torch nn attention sdpa_kernel SDPBackend torch nn functional scaled_dot_product_attention Module torch nn Module forward query torch Tensor cache torch Tensor start_pos torch Tensor - torch Tensor x sizes sp = start_pos item Checks needed slicing torch _check sp = torch _check sp = key = cache sp + sp+ value = cache sp + sp+ query = query transpose bs n_local_heads seqlen head_dim key = key transpose value = value transpose https github com pytorch pytorch blob main aten src ATen native transformers attention cpp#L scaled_dot_product_attention query key value cache = torch randn dtype=torch float query = torch randn dtype=torch float start_pos = torch tensor sdpa_kernel SDPBackend MATH torch no_grad ep = torch export export Module query cache start_pos args = query cache start_pos assertEqual ep module args Module args args = query cache torch tensor assertEqual ep module args Module args args = query cache torch tensor assertEqual ep module args Module args test_none_input_output Z torch nn Module forward x y x x ep = torch export export Z torch tensor None res = ep module torch tensor None assertEqual res torch tensor B torch nn Module forward x y x x y ep = torch export export B torch tensor None res = ep module torch tensor None assertEqual res torch tensor assertEqual res None decomp = ep run_decompositions gm = decomp module res = gm torch tensor None assertEqual res torch tensor assertEqual res None test_print M torch nn Module forward x print start x = x + x print x x = x x print x = x + x x x gm = export M torch randn graph_module assertExpectedInline gm code strip \ forward x add = torch ops aten add Tensor x x x = None mul = torch ops aten mul Tensor add add add_ = torch ops aten add Tensor mul mul mul = None add add_ test_print_graph_signature M torch nn Module __init__ super __init__ buf = torch nn Buffer torch ones forward x x add_ buf add_ buf + x ep = export M torch ones assertExpectedInline str ep graph_signature strip \ inputs b_buf BUFFER target= buf persistent=True x USER_INPUT outputs add USER_OUTPUT ep = ep run_decompositions assertExpectedInline str ep graph_signature strip \ inputs b_buf BUFFER target= buf persistent=True x USER_INPUT outputs add_ BUFFER_MUTATION target= buf add USER_INPUT_MUTATION target= x add_ USER_OUTPUT unittest skipIf TEST_TRANSFORMERS No transformers test_hf_logging_logger transformers logger = transformers utils logging get_logger __name__ M torch nn Module forward x logger warning_once start x = x + x x = x x x = x + x x x gm = export M torch randn graph_module assertExpectedInline gm code strip \ forward x add = torch ops aten add Tensor x x x = None mul = torch ops aten mul Tensor add add add_ = torch ops aten add Tensor mul mul mul = None add add_ test_warning M torch nn Module forward x warnings warn moo res = x + x warnings warn f res res gm = export M torch randn graph_module assertExpectedInline gm code strip \ forward x add = torch ops aten add Tensor x x x = None add test_logging_logger strict = True logger = logging getLogger __name__ M torch nn Module forward x logger log start x = x + x logger debug x x = x x logger info x = x + x x x gm = export M torch randn strict=strict graph_module assertExpectedInline gm code strip \ forward x add = torch ops aten add Tensor x x x = None mul = torch ops aten mul Tensor add add add_ = torch ops aten add Tensor mul mul mul = None add add_ test_constant_fqn Nested torch nn Module __init__ - None super __init__ constant = torch rand parameter = torch nn Parameter torch rand forward x x + constant Mod torch nn Module __init__ - None super __init__ nested = Nested forward x nested x + nested constant + nested parameter m = Mod ep = export m torch rand strict=True assertEqual ep constants nested constant m nested constant assertEqual ep module torch ones m torch ones test_constant_name Nested torch nn Module __init__ - None super __init__ constant = torch rand parameter = torch nn Parameter torch rand forward x x + constant Mod torch nn Module __init__ - None super __init__ nested_ = Nested nested_ = Nested forward x nested_ x + nested_ x + nested_ constant + nested_ constant + nested_ parameter + nested_ parameter m = Mod ep = export m torch rand strict=False assertEqual ep module torch ones m torch ones check constant fqn when there multiple instances same assertEqual ep constants nested_ constant m nested_ constant assertEqual ep constants nested_ constant m nested_ constant check constant_name graph placeholders = node node ep graph_module graph nodes node op == placeholder assertEqual len placeholders assertTrue all ph name == ph target ph placeholders suffix should added duplicated constant_name assertEqual placeholders name c_nested_ _constant assertEqual placeholders name c_nested_ _constant test_nested_retrace Nested torch nn Module __init__ - None super __init__ param = torch nn Parameter torch randn forward x x + param Foo torch nn Module __init__ - None super __init__ nested = Nested forward x x + nested x first export foo = Foo meta inputs = torch ones device= meta foo inputs ep = torch export export foo inputs strict=False second export foo_ = ep module ep_ = torch export export foo_ inputs strict=False node node zip ep graph nodes ep_ graph nodes nn_module_stack_ = node meta get nn_module_stack None nn_module_stack_ = node meta get nn_module_stack None nn_module_stack_ None assertTrue nn_module_stack_ None v v zip nn_module_stack_ values nn_module_stack_ values assertEqual v v test_duplicated_getitem Foo torch nn Module forward x torch topk x foo = Foo inputs = torch randn ep = torch export export foo inputs strict=False graph_module = copy deepcopy ep graph_module call_function_node = None num_getitems = node graph_module graph nodes node op == call_function node target == torch ops aten topk default call_function_node = node node op == call_function node target == operator getitem assertIs node args call_function_node num_getitems += assertIsNotNone call_function_node assertEqual num_getitems output_node = list graph_module graph nodes - nodes = graph_module graph inserting_before output_node nodes append graph_module graph call_function operator getitem call_function_node nodes append graph_module graph call_function operator getitem call_function_node nodes append graph_module graph call_function operator getitem call_function_node nodes append graph_module graph call_function operator getitem call_function_node signature = ExportGraphSignature input_specs=ep graph_signature input_specs output_specs=ep graph_signature output_specs + OutputSpec kind=OutputKind USER_OUTPUT arg=TensorArgument name=node name target=None node nodes output_node args = output_node args + tuple nodes graph_module recompile new_ep = ep _update graph_module signature new_num_getitems = node new_ep graph nodes node op == call_function node target == torch ops aten topk default call_function_node = node node op == call_function node target == operator getitem assertIs node args call_function_node new_num_getitems += assertEqual num_getitems new_num_getitems assertEqual len list new_ep graph nodes - args len signature output_specs requires_cuda_and_triton test_assert_tensor_metadata_device_index N torch nn Module __init__ super __init__ forward x y x = x float y = y float x + y inp = torch randn device= cuda torch randn device= cuda ep = export N inp ep = move_to_device_pass ep cuda cuda ep module torch randn device= cuda torch randn device= cuda unittest skipIf HAS_TORCHREC only run when there torchrec imported test_torchrec_jagged_tensor Foo torch nn Module forward jt - torch Tensor vals = jt lengths view - long vals + foo = Foo jt = JaggedTensor values=torch Tensor lengths=torch IntTensor offsets=torch IntTensor TODO tmanlaibaatar because we call unflatten flat tracer creates new JaggedTensor gets pruned reachable Not sure what right way fix since just warning probably ok xfail now assertWarnsRegex UserWarning While exporting we found certain side effects happened model forward Here list potential sources you can double check \ \ L\ jt \ \ \ torch _export config patch use_new_tracer_experimental=False _ = torch export export foo jt strict=True test_input_output_no_stacktrace M torch nn Module forward x x + x pyt_model = M example_inputs = torch ones Wrapper __init__ model example_inputs model = model example_inputs = example_inputs compile exp_program = torch export export model args=self example_inputs exp_program = exp_program run_decompositions get_decompositions torch ops aten new_full forward args kwargs compile wrapper = Wrapper pyt_model example_inputs wrapper forward test_export_with_dict_input_nested_in_args Test export dictionary input nested args MyModel torch nn Module __init__ super MyModel __init__ linear = torch nn Linear forward data_batch h = linear data_batch h = linear data_batch h + h Create model example inputs model = MyModel = torch randn = torch randn original_input = example_args_forward = original_input Export model exported_model = export model example_args_forward Run both models compare results reordered_input = original_output = exported_model module reordered_input loaded_output = model original_input Verify outputs close allowing floating point differences torch testing assert_close original_output loaded_output test_strict_export_with_shared_parameters Test parameter names preserved when there shared parameters same name M torch nn Module __init__ super __init__ n = torch nn Parameter torch ones n = n forward x res = x n res = x n res + res m = M ep = torch export export m torch ones strict=True gm = ep module Check named_parameters preserved original_param_names = name name _ m named_parameters exported_param_names = name name _ gm named_parameters assertEqual original_param_names exported_param_names unittest skipIf torchdynamo is_dynamo_supported dynamo doesn t support TestExportCustomClass TorchTestCase setUp load_torchbind_test_lib test_lift_custom_obj TODO fix test once custom tracing implemented custom_obj = torch classes _TorchScriptTesting _PickleTester Foo torch nn Module forward x x + x f = Foo inputs = torch zeros ep = export f inputs Replace one values instance our custom node ep graph nodes node op == call_function node target == torch ops aten add Tensor ep graph inserting_before node setattr ep graph_module custom_obj custom_obj getattr_node = ep graph get_attr custom_obj Copy over nn_module_stack they required getattr_node meta nn_module_stack = node meta nn_module_stack custom_node = ep graph call_function torch ops _TorchScriptTesting take_an_instance default getattr_node custom_node meta val = torch ones Copy over nn_module_stack they required custom_node meta nn_module_stack = node meta nn_module_stack custom_node meta torch_fn = custom_op torch ops _TorchScriptTesting take_an_instance default arg _ = node args node args = arg custom_node torch _export passes lift_constants_pass lift_constants_pass torch _export serde serialize deserialize serialize constants = lift_constants_pass ep graph_module ep graph_signature k v constants items assert k ep constants ep _constants k = v serialized_vals = serialize ep deserialized_ep = deserialize serialized_vals node deserialized_ep graph nodes node op == call_function node target == torch ops _TorchScriptTesting take_an_instance default arg = node args assertTrue arg op == placeholder test_int_lift_constant M torch nn Module forward x + torch tensor + x ep = export M torch ones dynamic_shapes= Dim DYNAMIC Dim DYNAMIC inp = torch randn assertTrue torch allclose M inp ep module inp test_export_script_module Add torch nn Module forward x y x + y Mod torch nn Module __init__ super __init__ add_mod = torch jit script Add _c forward x y add_mod forward x y x y = torch randn torch randn mod = Mod is_non_strict_test _testMethodName ep = export mod x y assertEqual ep module x y mod x y FileCheck check_count torch ops aten add Tensor exactly=True run ep graph_module code TODO strict mode doesn t work because dynamo add_mod treated user defined variable We might need add CustomModule variable support assertRaisesRegex torch _dynamo exc Unsupported UserDefined non-function ep = export mod x y test_preserve_non_cia_op M torch nn Module forward x torch nn functional elu x ep = export M torch randn FileCheck check_count torch ops aten elu default exactly=True run ep graph_module code decomp_table = default_decompositions ep = ep run_decompositions decomp_table=decomp_table FileCheck check_count torch ops aten elu default exactly=True run ep graph_module code test_preserve_cia_op StaticResizeTrilinear dModule torch nn Module forward x = torch nn functional interpolate x size= x shape x shape x shape mode= trilinear align_corners=False antialias=False ep = export StaticResizeTrilinear dModule torch randn FileCheck check_count torch ops aten upsample_trilinear d vec exactly=True run ep graph_module code decomp_table = default_decompositions del decomp_table torch ops aten upsample_trilinear d vec ep = ep run_decompositions decomp_table=decomp_table FileCheck check_count torch ops aten upsample_trilinear d vec exactly=True run ep graph_module code test_export_unbacked_lt MyModel torch nn Module forward x ranks first_k = ranks max item narrow = x narrow dim= start= length=first_k lt = narrow narrow size lt inps = torch randn torch arange dtype=torch int spec = x Dim AUTO Dim AUTO ranks Dim AUTO traced = export MyModel inps dynamic_shapes=spec strict=True run_decompositions test_unbacked_contiguous MyModel torch nn Module forward x mask masked_select = x masked_select mask view = masked_select view - contig = view contiguous contig + example_inputs = torch randn dtype=torch bfloat torch randint low= high= size= dtype=torch bool spec = x Dim STATIC Dim STATIC mask Dim STATIC Dim STATIC traced = export MyModel example_inputs strict=True assertExpectedInline traced graph_module code \ forward x mask masked_select = torch ops aten masked_select default x mask x = mask = None sym_size_int_ = torch ops aten sym_size int masked_select sym_constrain_range_for_size_default = torch ops aten sym_constrain_range_for_size default sym_size_int_ sym_constrain_range_for_size_default = None ge = sym_size_int_ = _assert_scalar_default = torch ops aten _assert_scalar default ge Runtime assertion failed expression u = node ge ge = _assert_scalar_default = None le = sym_size_int_ = _assert_scalar_default_ = torch ops aten _assert_scalar default le Runtime assertion failed expression u = node le le = _assert_scalar_default_ = None mod = sym_size_int_ eq_ = mod == mod = None _assert_scalar_default_ = torch ops aten _assert_scalar default eq_ Runtime assertion failed expression Eq Mod u node eq_ eq_ = _assert_scalar_default_ = None floordiv = sym_size_int_ mul_ = floordiv floordiv = None eq_ = sym_size_int_ == mul_ sym_size_int_ = mul_ = None _assert_scalar_default_ = torch ops aten _assert_scalar default eq_ Runtime assertion failed expression Eq u u node eq_ eq_ = _assert_scalar_default_ = None view = torch ops aten view default masked_select - masked_select = None add = torch ops aten add Tensor view view = None add ignore_empty_lines=True test_unbacked_select_index MyModel torch nn Module forward x y u = y item x select u example_inputs = torch randn dtype=torch bfloat torch tensor traced = export MyModel example_inputs run_decompositions assertExpectedInline traced graph_module code \ forward x y item = torch ops aten item default y y = None select = torch ops aten select int x item x = item = None select ignore_empty_lines=True test_is_fx_tracing M torch nn Module forward x y torch fx _symbolic_trace is_fx_tracing x + y x y inp = torch randn torch randn ep = export M inp FileCheck check_count torch ops aten add exactly=True run str ep graph M torch nn Module forward x y torch fx _symbolic_trace is_fx_symbolic_tracing x + y x y inp = torch randn torch randn ep = export M inp FileCheck check_count torch ops aten mul exactly=True run str ep graph test_item M torch nn Module __init__ super __init__ = b = forward y = torch tensor This becomes = item bt = torch tensor b This becomes b = bt item b y ep = export M torch ones FileCheck check_count torch ops aten mul Tensor exactly=True run str ep graph __name__ == __main__ run_tests