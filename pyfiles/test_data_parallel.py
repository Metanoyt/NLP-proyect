Owner s oncall jit os sys unittest torch torch nn nn torch nn parallel dp Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir torch testing _internal common_utils raise_on_run_directly torch testing _internal jit_utils JitTestCase RUN_CUDA_MULTI_GPU TestDataParallel JitTestCase Mpy torch nn Module __init__ - None super TestDataParallel Mpy __init__ m = nn Sequential nn Linear nn BatchNorm d nn ReLU nn Linear torch jit ignore forward input m input Mpy torch nn Module __init__ block super TestDataParallel Mpy __init__ m = block torch jit ignore forward input m forward input Mpy torch nn Module __init__ block block super TestDataParallel Mpy __init__ m = block m = block torch jit ignore forward input x = m forward input m x Msm torch jit ScriptModule __constants__ = m __init__ - None super TestDataParallel Msm __init__ m = nn Sequential nn Linear nn BatchNorm d nn ReLU nn Linear torch jit script_method forward input m input Msm torch jit ScriptModule __init__ block super TestDataParallel Msm __init__ block = block torch jit script_method forward input x = block input x check_replicas module replicas input_shape= input = torch randn input_shape cuda expected_output = module input data i replica enumerate replicas p replica parameters assertEqual p get_device i b replica buffers assertEqual b get_device i replica_input = input cuda i assertEqual replica replica_input data expected_output unittest skipIf RUN_CUDA_MULTI_GPU multi-GPU supported test_python_submodule_script module = Mpy Msm cuda replicas = dp replicate module check_replicas module replicas unittest skipIf RUN_CUDA_MULTI_GPU multi-GPU supported test_shared_module s = Msm p = Mpy s module = Mpy p s cuda replicas = dp replicate module check_replicas module replicas unittest skipIf RUN_CUDA_MULTI_GPU multi-GPU supported test_traced_module module = torch jit trace Mpy Mpy torch ones cuda replicas = dp replicate module check_replicas module replicas unittest skipIf RUN_CUDA_MULTI_GPU multi-GPU supported test_tensor_sharing module = Msm Msm cuda replica = dp replicate module assert_share_data t t Only checks they point same memory same device t device == t device t storage data_ptr == t storage data_ptr p p zip module parameters replica parameters assertTrue assert_share_data p p p p zip module buffers replica buffers assertTrue assert_share_data p p p p zip module parameters replica parameters assertFalse assert_share_data p p p p zip module buffers replica buffers assertFalse assert_share_data p p unittest skipIf RUN_CUDA_MULTI_GPU multi-GPU supported test_tensor_sharing_with_forward module = Msm Msm cuda replica = dp replicate module x = torch ones requires_grad=True cuda first_forward = module x first_forward sum backward torch no_grad p module parameters Use data here avoid version counter bump The graph created following forward will wrong we never backward through them so s fine p data -= p grad second_forward = module x replica which same GPU has shallow copy original params buffers r _forward = replica x assertEqual second_forward r _forward replica which different GPU has deep copy original params buffers x = torch ones requires_grad=True cuda device= r _forward = replica x assertEqual first_forward r _forward __name__ == __main__ raise_on_run_directly test test_jit py