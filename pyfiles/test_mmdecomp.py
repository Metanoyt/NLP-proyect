Owner s module nn math unittest typing Union torch torch _inductor config torch _inductor decomposition mm torch _subclasses fake_tensor FakeTensorMode torch fx experimental symbolic_shapes DimDynamic ShapeEnv StatelessSymbolicContext torch testing _internal common_cuda SM OrLater torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_nn NNTestCase torch testing _internal common_utils IS_WINDOWS parametrize run_tests torch testing _internal inductor_utils GPU_TYPE HAS_GPU default_atol = torch float e- torch bfloat float infinity torch float e- default_rtol = torch float e- torch bfloat float infinity torch float e- rand_math_tensor shape tuple Union int list int device str dtype torch dtype requires_grad bool = False packed bool = False - torch Tensor Creates rand dense nested tensor given shape type Args shape Tuple int Shape Tensor construct device str which device create tensor dtype torch dtype Tensors dtype requires_grad bool optional Tensors grad status Defaults False packed bool optional Whether create single QKV packed Defaults False Returns torch Tensor A new tensor torch randn shape device=device dtype=dtype requires_grad=requires_grad init_tensor tensor_list kwargs - torch Tensor torch Tensor tensor_list kwargs run_comp_nocomp function inputs kwargs c_function = torch compile function f_res = function inputs cf_res = c_function inputs math isinf kwargs get atol math isinf kwargs get rtol torch testing assert_close f_res cf_res kwargs The test functions used several tests torch_mm b torch mm b torch_addmm add b c torch addmm add b c torch_bmm b torch bmm b torch_baddbmm add b c alpha beta torch baddbmm add b c alpha=alpha beta=beta create_fake_tensor_with_dynamic_size x fake_mode fake_mode dynamic_sizes = DimDynamic DYNAMIC _ range x dim dynamic_strides = DimDynamic INFER_STRIDE _ range x dim fake_mode from_tensor x symbolic_context=StatelessSymbolicContext dynamic_sizes=dynamic_sizes dynamic_strides=dynamic_strides The shapes we test ts_list = TestDecomp NNTestCase _do_cuda_memory_leak_check = GPU_TYPE == cuda _do_cuda_non_default_stream = GPU_TYPE == cuda unittest skipIf HAS_GPU GPU tests require triton parametrize dtype torch float torch bfloat test_simple_mm device dtype fudge = rtol = default_rtol dtype fudge atol = default_atol dtype fudge t_size ts_list _ _ _ _ = t_size t = rand_math_tensor _ _ dtype=dtype device=device t = rand_math_tensor _ _ dtype=dtype device=device tadd = rand_math_tensor _ _ dtype=dtype device=device run_comp_nocomp torch_mm t t rtol=rtol atol=atol run_comp_nocomp torch_addmm tadd t t rtol=rtol atol=atol unittest skipIf HAS_GPU GPU tests require triton parametrize dtype torch float torch bfloat SM OrLater torch float parametrize bs test_batched_mm device dtype bs fudge = rtol = default_rtol dtype fudge atol = default_atol dtype fudge t_size ts_list _ _ _ _ = t_size t = rand_math_tensor bs _ _ dtype=dtype device=device t = rand_math_tensor bs _ _ dtype=dtype device=device tadd = rand_math_tensor bs _ _ dtype=dtype device=device run_comp_nocomp torch_bmm t t rtol=rtol atol=atol alpha - - beta - - run_comp_nocomp torch_baddbmm tadd t t alpha beta rtol=rtol atol=atol unittest skipIf HAS_GPU GPU tests require triton config patch coordinate_descent_tuning=True test_bmm_batch _last_dim_size_is_one device fudge = rtol = default_rtol torch float fudge atol = default_atol torch float fudge t = torch randn device=device t = torch randn device=device run_comp_nocomp torch_bmm t t rtol=rtol atol=atol unittest skipIf HAS_GPU GPU tests require triton parametrize dtype torch float torch bfloat torch int test_some device dtype Pytorch data type fully supported cuda today - unfortunately we can t skipIf because we don t see actual params skipIf device startswith GPU_TYPE dtype == torch int run_comp_nocomp torch_mm init_tensor dtype=dtype device=device init_tensor dtype=dtype device=device run_comp_nocomp torch_mm init_tensor dtype=dtype device=device init_tensor dtype=dtype device=device unittest skipIf HAS_GPU GPU tests require triton parametrize dtype torch float torch bfloat torch int parametrize bs test_some_batched device dtype bs Pytorch data type fully supported cuda today - unfortunately we can t skipIf because we don t see actual params skipIf device startswith GPU_TYPE dtype == torch int run_comp_nocomp torch_bmm init_tensor bs dtype=dtype device=device init_tensor bs dtype=dtype device=device run_comp_nocomp torch_bmm init_tensor bs dtype=dtype device=device init_tensor bs dtype=dtype device=device parametrize dtype torch float torch bfloat test_dynamic_shape_mm device dtype Test mm decomp does evaluate expressions dynamic shapes shape_env = ShapeEnv fake_mode = FakeTensorMode shape_env=shape_env Only test decomp cpu match fake tensors dynamo device = cpu t_size ts_list _ _ _ _ = t_size Create fake tensors t = create_fake_tensor_with_dynamic_size rand_math_tensor _ _ dtype=dtype device=device fake_mode t = create_fake_tensor_with_dynamic_size rand_math_tensor _ _ dtype=dtype device=device fake_mode Save expression types check any symints evaluated og_t _expr_types = type d node expr type d torch SymInt int d t size og_t _expr_types = type d node expr type d torch SymInt int d t size r = mm t t Make sure all symints evaluated new_t _expr_types = type d node expr type d torch SymInt int d t size new_t _expr_types = type d node expr type d torch SymInt int d t size assertTrue all og_t _expr_types i == new_t _expr_types i i range len og_t _expr_types assertTrue all og_t _expr_types i == new_t _expr_types i i range len og_t _expr_types r NotImplemented Check output well formed assertEqual t size r size assertEqual t size r size r_expr_types = type d node expr type d torch SymInt int d r size assertTrue r_expr_types == og_t _expr_types assertTrue r_expr_types == og_t _expr_types device_types = cpu GPU_TYPE instantiate_device_type_tests TestDecomp globals only_for=device_types __name__ == __main__ We don t support torch compile Windows IS_WINDOWS run_tests