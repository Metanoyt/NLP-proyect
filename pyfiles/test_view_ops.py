Copyright c Meta Platforms Inc affiliates Owner s oncall distributed itertools typing cast torch torch distributed dist torch rand randn Tensor torch distributed tensor DeviceMesh distribute_tensor DTensor init_device_mesh Partial Replicate Shard torch distributed tensor _ops _view_ops Broadcast dim_maps Flatten InputDim Repeat Singleton Split view_groups torch distributed tensor debug CommDebugMode torch distributed tensor placement_types _StridedShard Placement torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor create_local_tensor_test_class DTensorTestBase with_comms torch utils _pytree pytree TestViewOps DTensorTestBase property world_size - int test_view_groups assertEqual view_groups Split Flatten InputDim InputDim Split Flatten InputDim InputDim assertEqual view_groups Flatten InputDim InputDim InputDim assertEqual view_groups Split Flatten InputDim InputDim InputDim InputDim InputDim Split Flatten InputDim InputDim InputDim InputDim InputDim assertEqual view_groups Split Flatten InputDim InputDim InputDim Split Flatten InputDim InputDim InputDim Split Flatten InputDim InputDim Split Flatten InputDim InputDim assertEqual view_groups Flatten InputDim InputDim Split InputDim Split InputDim InputDim assertEqual view_groups Singleton InputDim Split InputDim Split InputDim Singleton Split InputDim Singleton assertEqual view_groups Flatten InputDim InputDim InputDim InputDim Singleton assertEqual view_groups Split InputDim Split InputDim InputDim Flatten InputDim InputDim assertEqual view_groups - InputDim InputDim InputDim call_dt_test op args kwargs device_mesh DeviceMesh dim_map = dim_maps op rules = dim_map args kwargs outputs = op args kwargs flat_args = pytree arg_tree_leaves args in_shape = flat_args shape no_shard_dims = set rule rules isinstance rule Repeat isinstance rule input_dim InputDim no_shard_dims add rule input_dim input_dim isinstance rule Flatten dim rule input_dims isinstance dim InputDim no_shard_dims add dim input_dim isinstance rule Split isinstance rule input_dim Flatten dim rule input_dim input_dims isinstance dim InputDim no_shard_dims add dim input_dim op == torch unbind no_shard_dims add kwargs get dim sharding_choices = cast list Placement Replicate + Shard i i s enumerate in_shape s i no_shard_dims all_sharding_choices = itertools product device_mesh ndim sharding_choices outer_mesh = device_mesh outer inner_mesh = device_mesh inner inner_mesh_size = inner_mesh size strided_sharding_choices = _StridedShard i split_factor=inner_mesh_size Shard i i s enumerate in_shape s i no_shard_dims in_shard itertools chain all_sharding_choices strided_sharding_choices isinstance in_shard _StridedShard op = Tensor view continue cannot produce DTensor using ` ` distribute_tensor ` ` ` ` _StridedShard ` ` Need distribute input over inner mesh dim first then distribute _local_tensor over outer mesh dim in_dt = distribute_tensor args inner_mesh in_shard in_dt = distribute_tensor in_dt _local_tensor outer_mesh Shard in_shard dim in_dt = DTensor from_local in_dt _local_tensor device_mesh in_shard in_dt = distribute_tensor args device_mesh in_shard comm_mode = CommDebugMode comm_mode out_dt = op in_dt args kwargs assertEqual comm_mode get_total_counts Expected no redistribution full_out = out_dt full_tensor dist get_rank == assertEqual outputs full_out dimmap_test op args expected_rule_output rules = dim_maps op args assertEqual rules expected_rule_output call_dt_test op args device_mesh with_comms test_illegal_views device_mesh = build_device_mesh D mesh see above tensor = torch randn dtensor = distribute_tensor tensor device_mesh Replicate shard = dtensor redistribute device_mesh=device_mesh placements= Shard dim= view should legal since sharding even flatten includes only one sharded dim shard view - shard = dtensor redistribute device_mesh=device_mesh placements= Shard dim= assertRaisesRegex RuntimeError Sharding propagation failed shard view - uneven case since mesh dim tensor = torch randn dtensor = distribute_tensor tensor device_mesh Replicate shard = dtensor redistribute device_mesh=device_mesh placements= Shard dim= assertRaisesRegex RuntimeError Sharding propagation failed shard view - assuming world size + tensor shardable dim size viewable when resulting dim has size tensor = torch randn dtensor = distribute_tensor tensor device_mesh Replicate shard = dtensor redistribute device_mesh=device_mesh placements= Shard dim= assertRaisesRegex RuntimeError Sharding propagation failed shard view - with_comms test_view_ops mesh_shape = dist get_world_size device_mesh = init_device_mesh device_type mesh_shape=mesh_shape mesh_dim_names= outer inner dimmap_test torch atleast_ d randn Singleton dimmap_test torch atleast_ d randn InputDim dimmap_test torch atleast_ d randn InputDim InputDim dimmap_test torch atleast_ d randn Singleton Singleton dimmap_test torch atleast_ d randn Singleton InputDim dimmap_test torch atleast_ d randn InputDim InputDim dimmap_test torch atleast_ d randn InputDim InputDim InputDim dimmap_test torch atleast_ d randn Singleton Singleton Singleton dimmap_test torch atleast_ d randn Singleton InputDim Singleton dimmap_test torch atleast_ d randn InputDim InputDim Singleton dimmap_test torch atleast_ d randn InputDim InputDim InputDim dimmap_test torch atleast_ d randn InputDim InputDim InputDim InputDim assertRaises AssertionError dim_maps torch broadcast_to randn dimmap_test torch broadcast_to rand Singleton InputDim InputDim dimmap_test torch broadcast_to rand Broadcast Singleton InputDim InputDim dimmap_test torch broadcast_to rand Broadcast Singleton InputDim Broadcast InputDim InputDim dimmap_test torch broadcast_to rand - InputDim InputDim dimmap_test torch broadcast_to rand - InputDim InputDim InputDim dimmap_test torch broadcast_to randn Broadcast Singleton InputDim Broadcast InputDim InputDim dimmap_test Tensor expand randn - Broadcast Singleton InputDim Broadcast InputDim InputDim Broadcast InputDim dimmap_test Tensor expand randn - Broadcast Singleton InputDim Broadcast InputDim InputDim Broadcast InputDim dimmap_test torch flatten randn Flatten InputDim InputDim dimmap_test torch flatten randn InputDim dimmap_test torch flatten randn Singleton dimmap_test torch movedim randn InputDim InputDim InputDim InputDim dimmap_test torch movedim randn InputDim InputDim InputDim dimmap_test torch movedim randn InputDim InputDim InputDim dimmap_test torch movedim randn InputDim InputDim InputDim dimmap_test torch movedim randn InputDim InputDim dimmap_test torch movedim randn InputDim InputDim InputDim dimmap_test torch movedim randn - - InputDim InputDim InputDim dimmap_test torch permute randn InputDim InputDim InputDim dimmap_test torch permute randn - - - InputDim InputDim InputDim dimmap_test torch ravel randn Flatten InputDim InputDim dimmap_test torch ravel randn InputDim dimmap_test torch ravel randn Singleton dimmap_test Tensor repeat randn Singleton Broadcast Singleton Singleton InputDim Repeat InputDim dimmap_test torch reshape randn Flatten InputDim InputDim InputDim dimmap_test torch tile randn Singleton Broadcast Singleton Singleton InputDim Repeat InputDim dimmap_test torch tile randn InputDim InputDim Repeat InputDim dimmap_test torch transpose randn InputDim InputDim InputDim InputDim dimmap_test torch transpose randn - InputDim InputDim InputDim InputDim dimmap_test torch unsqueeze randn InputDim Singleton InputDim InputDim dimmap_test Tensor view randn Flatten InputDim InputDim InputDim dimmap_test Tensor view randn - InputDim dimmap_test Tensor view randn - Flatten InputDim InputDim dimmap_test Tensor view randn - Flatten InputDim InputDim input_dim= InputDim dimmap_test Tensor view randn Split Flatten input_dims= InputDim InputDim InputDim group_shape= split_id= Split Flatten input_dims= InputDim InputDim InputDim group_shape= split_id= Split Flatten input_dims= InputDim InputDim InputDim group_shape= split_id= Split Flatten input_dims= InputDim InputDim InputDim group_shape= split_id= TODO Currently functional collectives complex numbers fully supported so we having standalone test view_as_complex view_as_real combined Once complex numbers supported we can add following dim_map test dimmap_test torch view_as_complex randn InputDim Flatten InputDim InputDim dimmap_test torch view_as_real torch randn dtype=torch cfloat InputDim Split InputDim Split InputDim with_comms test_complex_view_ops device_mesh = DeviceMesh device_type torch arange dist get_world_size view - inp = randn intermediate = torch view_as_complex inp out = torch view_as_real intermediate test dim_map correctness expected_view_as_complex_rule = InputDim Flatten InputDim InputDim view_as_complex_rule = dim_maps torch view_as_complex inp assertEqual view_as_complex_rule expected_view_as_complex_rule expected_view_as_real_rule = InputDim Split InputDim Split InputDim view_as_real_rule = dim_maps torch view_as_real intermediate assertEqual view_as_real_rule expected_view_as_real_rule test sharded computation correctness NOTE For input torch view_as_complex sharding last two dimensions supported sharding_choices list Placement = Replicate Shard all_sharding_choices = itertools product device_mesh ndim sharding_choices inp_shard all_sharding_choices inp_dt = distribute_tensor inp device_mesh inp_shard comm_mode = CommDebugMode comm_mode intermediate_dt = torch view_as_complex inp_dt out_dt = torch view_as_real intermediate_dt assertEqual comm_mode get_total_counts Expected no redistribution assertEqual out out_dt full_tensor with_comms test_dtensor_view_op_uneven When sharded dimension unchanged view op should trigger any communication And behavior should same operating under single-device Test two uneven cases view op sharded tensor dim so only first rank has non-empty shard sharded tensor dim uneven such some ranks have full shards smaller non-empty shards empty shards dim _sizes = world_size + dim _size dim _sizes p = torch randn dim _size mesh = init_device_mesh device_type world_size dtensor = distribute_tensor p mesh Shard CommDebugMode comm_mode view = dtensor view dim _size assertEqual len comm_mode get_comm_counts when no communication happens data pointer should same assertEqual view to_local data_ptr dtensor to_local data_ptr view = dtensor view dim _size assertEqual view to_local data_ptr dtensor to_local data_ptr assertEqual len comm_mode get_comm_counts view = dtensor view dim _size assertEqual view to_local data_ptr dtensor to_local data_ptr assertEqual len comm_mode get_comm_counts view = dtensor view dtensor shape assertEqual view to_local data_ptr dtensor to_local data_ptr assertEqual len comm_mode get_comm_counts with_comms test_view_redistribution This test added demonstrate incorrect view ops behavior redistribution happens x = torch randn mesh = init_device_mesh device_type world_size dtensor_x = distribute_tensor x mesh Shard assertRaisesRegex RuntimeError Sharding propagation failed dtensor_x view - with_comms test_squeeze_ mesh_ d = init_device_mesh device_type mesh_dim_names= b init_manual_seed_for_rank x = torch randn device=self device_type dist_x = DTensor from_local x mesh_ d Partial Shard _test_op_on_dtensor torch ops aten squeeze_ dim dist_x check DTensor subclass metadata well placements assertEqual dist_x shape torch Size assertEqual dist_x stride assertEqual dist_x placements Partial Shard TestViewOpsWithLocalTensor = create_local_tensor_test_class TestViewOps skipped_tests= Comparing data pointers supported local tensor test_dtensor_view_op_uneven __name__ == __main__ run_tests