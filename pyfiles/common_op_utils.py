mypy allow-untyped-defs typing Optional torch torch utils _pytree pytree _basic_validation op args= kwargs=None Common validation across all ops go here torch distributed _shard sharded_tensor ShardedTensor len args == kwargs None len kwargs == raise ValueError f No input op __name__ Validate types has_distributed_tensor = False is_distributed_tensor e nonlocal has_distributed_tensor isinstance e ShardedTensor has_distributed_tensor = True pytree tree_map_ is_distributed_tensor args pytree tree_map_ is_distributed_tensor kwargs has_distributed_tensor raise TypeError f torch function op __name__ args args f kwargs kwargs called without any distributed tensor Validate all distributed tensors use same PG cur_pg Optional torch distributed ProcessGroup = None validate_pg e nonlocal cur_pg isinstance e ShardedTensor cur_pg None e _process_group cur_pg raise RuntimeError All distributed tensors should use same ProcessGroup used together op cur_pg = e _process_group pytree tree_map_ validate_pg args pytree tree_map_ validate_pg kwargs _register_default_op op decorator decorator op tensor_default_op types args= kwargs=None pg=None Handles ` ` __torch_function__ ` ` dispatch default tensor ops behave same ` ` torch Tensor ` ` such ` ` torch Tensor shape ` ` ` ` torch Tensor dtype ` ` We simply lower real op call DisableTorchFunctionSubclass context like ` ` torch Tensor __torch_function__ ` ` avoid recursions kwargs None kwargs = torch _C DisableTorchFunctionSubclass op args kwargs