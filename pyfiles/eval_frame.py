mypy disable-error-code= method-assign This module implements core frame evaluation handler TorchDynamo s compilation system The eval frame handler intercepts Python bytecode execution runtime enable dynamic compilation optimization PyTorch code Key components defined here - Frame evaluation handlers intercept analyze Python execution frames - Guards management tracking dependencies invalidating compiled code - Optimization contexts decorators optimize run_once disable etc - Export functionality saving optimized graphs - Backend compiler integrations callback management Functions file responsible modifying eval frame handler RUNTIME Therefore all functions file hot performance-critical Functions only execute compile time should placed torch _dynamo convert_frame The eval frame handler core mechanism enables TorchDynamo dynamically intercept analyze optimize PyTorch code during execution It works registering custom frame evaluation function gets called every Python frame allowing us detect PyTorch operations trigger compilation needed __future__ annotations atexit contextlib functools inspect logging os sys sysconfig textwrap threading traceback types unittest warnings weakref dataclasses dataclass enum Enum os path dirname join typing Any NamedTuple Optional Sized TYPE_CHECKING Union unittest mock patch sympy torch torch fx torch utils _pytree pytree torch utils checkpoint torch _guards see discussion https github com pytorch pytorch issues torch _C _dynamo eval_frame noqa F reset_code set_code_exec_strategy set_eval_frame set_guard_complete_hook set_guard_error_hook set_skip_guard_eval_unsafe unsupported torch _dispatch python enable_python_dispatcher torch _dynamo types ConvertFrameReturn FrameAction FrameExecStrategy torch _export utils _compiling_state_context torch _subclasses fake_tensor unset_fake_temporarily torch _utils_internal DISABLE_JUSTKNOBS justknobs_check log_export_usage torch export dynamic_shapes _combine_args _DimHint _DimHintType _IntWrapper _process_dynamic_shapes _RelaxedConstraint Constraint torch fx GraphModule torch fx experimental _dynamism clone_and_convert_to_meta track_dynamism_across_examples torch fx experimental proxy_tensor make_fx torch fx experimental symbolic_shapes ConstraintViolationError DimDynamic ShapeEnv StatelessSymbolicContext torch fx graph _PyTreeCodeGen _PyTreeInfo config convert_frame distributed external_utils trace_rules utils backends registry CompilerFn lookup_backend code_context code_context exc CondOpArgsMismatchError ShortenTraceback Unsupported UserError UserErrorType hooks Hooks mutation_guard install_generation_tagging_init utils _get_error_on_graph_break _set_error_on_graph_break common_constant_types compile_times TYPE_CHECKING collections abc Callable Iterable Sequence torch _dynamo package CompilePackage torch _dynamo repro after_dynamo WrapBackendDebug torch _subclasses fake_tensor torch fx node Argument Node Target types CacheEntry DynamoCallback DynamoFrameType GuardFail GuardFilterEntry log = logging getLogger __name__ always_optimize_code_objects = utils ExactWeakKeyDictionary null_context = contextlib nullcontext See https github com python typing pull Unset Enum token = cached_backends dict int CompilerFn = unset = Unset token DISABLE_JUSTKNOBS _maybe_set_eval_frame = set_eval_frame _maybe_set_eval_frame callback DynamoCallback - DynamoCallback A wrapper set_eval_frame guarded Justknob Users can disable torchDynamo setting JK False justknobs_check pytorch compiler enable_compiler_set_eval_frame torch _dynamo utils warn_once Dynamo disabled Justknob enable_compiler_set_eval_frame skipping set_eval_frame callback set_eval_frame callback dataclass DynamoStance stance str = default skip_guard_eval_unsafe bool = False backend Union str Callable Any None = None _stance = DynamoStance _set_stance stance DynamoStance - DynamoStance global _stance torch _C _dynamo eval_frame get_eval_frame_callback callback = get_eval_frame_callback callback False callback None raise RuntimeError attempted set_stance torch compile region prior = _stance _stance = stance prior _set_stance _dynamo_forbidden = True type ignore attr-defined _EXAMPLE_INPUTS Optional dict str list Any = None get_example_inputs key str - list Any global _EXAMPLE_INPUTS _EXAMPLE_INPUTS None _EXAMPLE_INPUTS = key _EXAMPLE_INPUTS _EXAMPLE_INPUTS key = _EXAMPLE_INPUTS key _callback_from_stance callback DynamoCallback - DynamoCallback _stance stance == default force_backend _stance backend None callback False None callback = _create_wrapped_callback get_compiler_fn _stance backend callback _stance stance == eager_then_compile callback False None _create_delayed_compile_callback callback _stance stance callback _stance stance == aot_eager_then_compile callback False None _create_delayed_compile_callback callback _stance stance callback _stance stance == force_eager disable None _stance stance == eager_on_recompile run mode False _stance stance == fail_on_recompile callback False None callback fail_callback frame DynamoFrameType args Any kwargs Any - ConvertFrameReturn trace_rules check frame f_code ConvertFrameReturn convert_frame has_tensor_in_frame frame ConvertFrameReturn torch _C _dynamo eval_frame _debug_get_cache_entry_list _debug_get_precompile_entries torch _dynamo guards get_and_maybe_log_recompilation_reasons message = Detected recompile when torch compile stance fail_on_recompile + f filename frame f_code co_filename + f function name frame f_code co_name + f line number frame f_lineno cache_entries = _debug_get_cache_entry_list frame f_code cache_entries reasons = get_and_maybe_log_recompilation_reasons cache_entries frame skip_logging=True reasons failures = textwrap indent \n join reasons - guard_failure_details = f triggered following guard failure s \n failures message += f \n textwrap indent guard_failure_details precompile_entries = _debug_get_precompile_entries frame f_code len precompile_entries message += \nFailed following precompiled guards entry precompile_entries message += f \n entry guard_manager entry guard_manager check_verbose frame f_locals type ignore attr-defined raise RuntimeError message prevent cache miss due different backend fail_callback _torchdynamo_orig_backend = callback type ignore attr-defined fail_callback raise RuntimeError f invalid torch compile stance _stance _create_wrapped_callback compiler_fn CompilerFn - convert_frame CatchErrorsWrapper hooks = Hooks convert_frame catch_errors_wrapper convert_frame convert_frame type ignore arg-type compiler_fn hooks hooks _get_or_add_example_inputs frame DynamoFrameType - list Any key = frame f_code co_filename + str frame f_code co_firstlineno example_inputs = get_example_inputs key len example_inputs example_inputs append clone_and_convert_to_meta frame f_locals example_inputs _create_delayed_compile_callback callback DynamoCallback stance str - Callable Any callback_fn args Any kwargs Any - convert_frame ConvertFrameReturn frame = args example_inputs = _get_or_add_example_inputs frame len example_inputs == stance == eager_then_compile ConvertFrameReturn frame_exec_strategy=FrameExecStrategy FrameAction DEFAULT FrameAction DEFAULT stance == aot_eager_then_compile aot_eager_fn = get_compiler_fn aot_eager _create_wrapped_callback aot_eager_fn args kwargs dynamism = track_dynamism_across_examples example_inputs code_context get_context frame f_code dynamism = dynamism compiler_fn = callback _torchdynamo_orig_backend _torchdynamo_orig_backend type ignore union-attr _create_wrapped_callback compiler_fn args kwargs prevent cache miss due different backend callback_fn _torchdynamo_orig_backend = callback type ignore attr-defined callback_fn _is_skip_guard_eval_unsafe_stance - bool _stance skip_guard_eval_unsafe _reset_guarded_backend_cache - None global cached_backends backend cached_backends values hasattr backend reset backend reset cached_backends clear DONT_WRAP_FILES = For tracing into fx modules inspect getsourcefile GraphModule join dirname dirname __file__ onnx _internal fx dynamo_graph_extractor py _debug_get_cache_entry_list code Union types CodeType Callable Any - list CacheEntry Given code object callable object retrieve cache entries stored code callable code code = code __code__ torch _C _dynamo eval_frame _debug_get_cache_entry_list code OptimizedModule torch nn Module Wraps original nn Module object later patches its forward method optimized forward method _torchdynamo_orig_callable Callable Any get_compiler_config Callable Any _opt_mod_attributes = _orig_mod dynamo_ctx _torchdynamo_orig_callable get_compiler_config forward _forward __dict__ named_children_walk _super_module_initialized __init__ mod torch nn Module dynamo_ctx _TorchDynamoContext - None NOTE must go first because attribute reads writes ` ` uses ` _orig_mod ` sometimes users override ` Module __init__ ` do attribute reads writes ` ` We also can t use regular setattr because ` super __setattr__ ` will complain module value before ` super __init__ ` object __setattr__ _orig_mod mod _super_module_initialized = False super __init__ _super_module_initialized = True Installs params buffer _orig_mod = mod ` super __setattr__ ` will register module dynamo_ctx = dynamo_ctx _initialize training = _orig_mod training __len__ - int Proxy len call original module isinstance _orig_mod Sized len _orig_mod Mimic python s default behavior objects without length raise TypeError f type _orig_mod __name__ does support len _initialize - None Do stuff constructor lower overhead slightly isinstance dynamo_ctx DisableContext No need check trace rules forward = dynamo_ctx _orig_mod __call__ config wrap_top_frame isinstance _orig_mod forward types MethodType trace_rules check _orig_mod forward getattr _orig_mod _is_fsdp_managed_module False This may torch nn instance trace_rules py which won t trigger frame evaluation workaround add extra frame we can capture forward = dynamo_ctx external_utils wrap_inline _orig_mod Invoke hooks outside dynamo then pickup inner frame forward = dynamo_ctx _orig_mod __call__ hasattr _orig_mod _initialize_hook _forward = forward forward = _call_lazy_check __call__ args Any kwargs Any - Any torch nn modules module _has_any_global_hook warnings warn Using ` torch compile module ` when there global hooks modules e g ` register_module_forward_hook ` will cause hooks fire extra time ` OptimizedModule ` created ` torch compile module ` If causes undesired behavior please try using ` module compile ` use per-module hooks instead stacklevel= super __call__ args kwargs _aot_compile inputs list torch _dynamo aot_compile ModelInput - None Experimental AOT Compile set inputs use forward function model = _orig_mod hooks = dynamo_ctx _hooks assert hooks None config enable_aot_compile raise RuntimeError AOT Compile enabled please set torch _dynamo config enable_aot_config=True dynamo_ctx fullgraph raise RuntimeError Graph breaks supported aot compile Please use torch compile fullgraph=True callable dynamo_ctx callback raise RuntimeError aot compile requires callable dynamo callback backend = innermost_fn dynamo_ctx callback unaltered_fn_attr= _torchdynamo_orig_backend torch _dynamo aot_compile aot_compile_module forward = aot_compile_module model inputs hooks backend _save_aot_compiled_module path Optional str = None - bytes config enable_aot_compile raise RuntimeError AOT Compile enabled please set torch _dynamo config enable_aot_config=True torch _dynamo aot_compile AOTCompiledModel assert isinstance forward AOTCompiledModel result bytes = forward serialize path None open path wb f f write result result _load_aot_compiled_module data bytes - None config enable_aot_compile raise RuntimeError AOT Compile enabled please set torch _dynamo config enable_aot_config=True torch _dynamo aot_compile AOTCompiledModel compiled_forward = AOTCompiledModel deserialize _orig_mod data assert isinstance compiled_forward AOTCompiledModel forward = compiled_forward __reduce__ - tuple type OptimizedModule tuple torch nn Module _TorchDynamoContext __class__ _orig_mod dynamo_ctx __getstate__ - dict str Any state = dict __dict__ state pop forward None state pop __call__ None state __setstate__ state dict str Any - None __dict__ = state _initialize property pyrefly ignore bad-override training - bool _orig_mod training training setter training value bool - None Ignore ` training ` mutation ` super __init__ ` since s setting default ` nn Module ` we mirroring ` training ` attr ` _orig_mod ` _super_module_initialized _orig_mod training = value __getattr__ name str - Any name == _orig_mod _modules _orig_mod getattr _orig_mod name __setattr__ name str val Any - None Allow patching over attributes hasattr type name super __setattr__ name val name OptimizedModule _opt_mod_attributes super __setattr__ name val setattr _orig_mod name val __delattr__ name str - None This mirrors ` __setattr__ ` hasattr type name super __delattr__ name name OptimizedModule _opt_mod_attributes super __delattr__ name delattr _orig_mod name _call_lazy_check args Any kwargs Any - Any hasattr _orig_mod _initialize_hook hasattr _orig_mod _infer_parameters callable _orig_mod _infer_parameters In case lazy module we want run pre-hooks which initialize Afterwards lazy module deletes its pre-hooks avoid treating lazy subsequent recompile _orig_mod _infer_parameters _orig_mod args kwargs _forward args kwargs __dir__ - list str orig_mod_attrs = _orig_mod __dir__ orig_mod_attrs + attr attr super __dir__ attr orig_mod_attrs remove_from_cache f Any - None Make sure f __code__ cached force recompile isinstance f types CodeType reset_code f hasattr f __code__ reset_code f __code__ hasattr getattr f forward None __code__ reset_code f forward __code__ reset type ignore attr-defined reset log warning could determine __code__ s f nothing - None pass always_false - bool False innermost_fn fn Callable Any unaltered_fn_attr str = _torchdynamo_orig_callable - Callable Any In case nesting _TorchDynamoContext calls find innermost function TorchDynamo caches fn __code__ object so its necessary find innermost function pass optimize run disable etc unaltered_fn = fn while hasattr unaltered_fn unaltered_fn_attr unaltered_fn = getattr unaltered_fn unaltered_fn_attr assert callable unaltered_fn f A callable function expected type unaltered_fn provided unaltered_fn make_set_enable_dynamic enable bool - Any assert isinstance enable bool enable Assume everything dynamic default config _make_closure_patcher assume_static_by_default=False config _make_closure_patcher automatic_dynamic_shapes=False assume_static_by_default=True A thread local storage serves store information Dynamo traces through user provided function DynamoTLS threading local Each string summary frame Dynamo attempted trace stored temporal order traced_frame_infos list str = dynamo_tls = DynamoTLS clear_dynamo_tls - None dynamo_tls traced_frame_infos clear atexit register _log_traced_frames - None At program exit log all frames Dynamo has attempted trace excluding continuation frames generated Dynamo msg = \n join dynamo_tls traced_frame_infos msg = textwrap indent msg msg = f TorchDynamo attempted trace following frames \n msg \n log info msg guard_collectives_hook guard_eval_result bool - bool torch distributed dist torch _dynamo utils dynamo_timed guard_eval_result == True == cache hit pg = distributed get_guard_pg dynamo_timed guard_collective log_pt _compile_event=False log_waitcounter=True log debug guard_collective s guard_eval_result TODO bit awkward time isn t inside dynamo compile region all_results = None pg size dist all_gather_object all_results guard_eval_result group=pg True = everyone hit OK run False = someone missed force recompile everywhere res = all all_results log debug guard_collective s - s guard_eval_result res res guard_eval_result _not_set = object _TorchDynamoContext __init__ callback DynamoCallback on_enter Callable Any = nothing backend_ctx_ctor Callable contextlib AbstractContextManager Any = null_context patch_fn Callable Any = nothing first_ctx bool = False fullgraph bool = False error_on_graph_break Optional bool = None export bool = False dynamic Optional bool = None compiler_config Optional Any = None package Optional CompilePackage = None hooks Optional Hooks = None - None super __init__ assert callable callback callback False callback None callback DynamoCallback = callback _backend_ctx_ctor = backend_ctx_ctor prior Union Unset DynamoCallback = unset first_ctx = first_ctx fullgraph = fullgraph error_on_graph_break = error_on_graph_break export = export _dynamic = dynamic compiler_config = compiler_config cleanup_fns list Callable Any = enter_exit_hooks = _package = package _hooks = hooks patch_fn Save backends so we can reset them during torch _dynamo reset backend = innermost_fn callback unaltered_fn_attr= _torchdynamo_orig_backend type ignore arg-type cached_backends setdefault id backend backend type ignore arg-type dynamic None enter_exit_hooks append make_set_enable_dynamic dynamic on_enter nothing case common call_on_enter - Callable None on_enter nothing enter_exit_hooks append call_on_enter backend_ctx_ctor contextlib nullcontext case common call_backend_ctx - functools partial Optional bool ctx = backend_ctx_ctor ctx __enter__ functools partial ctx __exit__ None None None enter_exit_hooks append call_backend_ctx __enter__ - None config raise_on_ctx_manager_usage raise RuntimeError torch _dynamo optimize used context manager Please refer https pytorch org tutorials intermediate torch_compile_tutorial html use torch _dynamo optimize annotation decorator prior = set_eval_frame None cleanup_fns = enter enter enter_exit_hooks prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe _is_skip_guard_eval_unsafe_stance _maybe_set_eval_frame _callback_from_stance callback __exit__ exc_type Optional type BaseException exc_val Optional BaseException exc_tb Optional types TracebackType - Optional bool assert prior unset set_eval_frame None set_skip_guard_eval_unsafe prior_skip_guard_eval_unsafe cleanup cleanup_fns cleanup cleanup_fns clear _maybe_set_eval_frame _callback_from_stance prior prior = unset None __call__ fn Any - Any public api compiler config options get_compiler_config - Any compiler_config package DynamoCache If _package lazily initialized we should check dynamo cache now config caching_precompile _package None _package is_initialized result = DynamoCache load fn result None Create fresh CompilePackage _package initialize fn None ignore_inlined_sources=False try _package initialize fn result dynamo ignore_inlined_sources=False _package install result backends except RuntimeError log warning Failed load entry dynamo cache exc_info=True _package initialize fn None ignore_inlined_sources=False fn = innermost_fn fn aot_compile example_inputs tuple tuple Any dict str Any - Any torch _dynamo aot_compile aot_compile_fullgraph fullgraph raise RuntimeError Graph breaks supported aot compile Please use torch compile fullgraph=True callable callback raise RuntimeError aot compile requires callable dynamo callback assert _hooks None aot_compile_fullgraph fn example_inputs hooks=self _hooks backend=innermost_fn callback unaltered_fn_attr= _torchdynamo_orig_backend add context containing GraphModule any GraphModule forward functions isinstance fn GraphModule add context containing GraphModule any GraphModule forward functions code_context get_context fn forward __code__ orig_graphmodule = weakref ref fn Optimize forward method torch nn Module object isinstance fn torch nn Module mod = fn new_mod = OptimizedModule mod Save function pointer find original callable while nesting decorators new_mod _torchdynamo_orig_callable = mod forward when compiling torch nn Module provide public api OptimizedModule get_compiler_config assert hasattr new_mod get_compiler_config new_mod get_compiler_config = get_compiler_config new_mod inspect isclass fn User has wrapped compile disable decorator Apply disable init call method cls_obj = fn cls_obj __call__ = cls_obj __call__ issubclass cls_obj torch nn Module NN module variable tracker directly inlines _call_impl cls_obj _call_impl = cls_obj _call_impl cls_obj assert callable fn f A callable function expected type fn provided try filename = inspect getsourcefile fn except TypeError filename = None config debug_force_nested_calls fn = external_utils wrap_inline fn config wrap_top_frame filename None trace_rules check fn getattr fn __name__ _call_impl _wrapped_call_impl _lazy_forward filename DONT_WRAP_FILES call builtin without frame us capture fn = external_utils wrap_inline fn do_nothing arg Any kwargs Any - None pass callback Callable Any = do_nothing hasattr callback callback = callback type ignore assignment is_jit_tracing = torch _C _is_tracing is_fx_symbolic_tracing = torch fx _symbolic_trace is_fx_symbolic_tracing functools wraps fn compile_wrapper args Any kwargs Any - Any prior = set_eval_frame None try We shouldn t compile inside kernel invocation tracing_context = torch _guards TracingContext try_get tracing_context fake_mode None tracing_context fake_mode in_kernel_invocation fn args kwargs Skip nested compile - just inline function is_fx_symbolic_tracing config error_on_nested_fx_trace raise RuntimeError Detected you using FX symbolically trace dynamo-optimized function This supported moment fn args kwargs is_jit_tracing raise RuntimeError Detected you using FX torch jit trace dynamo-optimized function This supported moment cleanups = enter enter enter_exit_hooks prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe _is_skip_guard_eval_unsafe_stance prior_error_on_graph_break = None fullgraph error_on_graph_break None prior_error_on_graph_break = _get_error_on_graph_break _set_error_on_graph_break error_on_graph_break Ensure assertion occurs after graph pushes something onto DynamicLayerStack then we pop off constructed graph code isn t guarded try finally This used context putting ` ` here noticeable perf regression saved_dynamic_layer_stack_depth = torch _C _functorch get_dynamic_layer_stack_depth _maybe_set_eval_frame _callback_from_stance callback try fn args kwargs except Unsupported e config verbose raise strip internal tracebacks causes cur_exn BaseException = e while cur_exn __cause__ None cur_exn __cause__ with_traceback None cur_exn = cur_exn __cause__ pyrefly ignore invalid-inheritance raise e with_traceback None e __cause__ User compiler error except ShortenTraceback e Failures backend likely don t have useful data TorchDynamo frames so we strip them out raise e remove_dynamo_frames None see TORCHDYNAMO_VERBOSE= finally Restore dynamic layer stack depth necessary set_eval_frame None prior_error_on_graph_break None _set_error_on_graph_break prior_error_on_graph_break torch _C _functorch pop_dynamic_layer_stack_and_undo_to_depth saved_dynamic_layer_stack_depth set_skip_guard_eval_unsafe prior_skip_guard_eval_unsafe cleanup cleanups cleanup finally _maybe_set_eval_frame prior hooks properly handle inlining error_on_graph_break None compile_wrapper _torchdynamo_inline = type ignore attr-defined external_utils wrap_inline_with_error_on_graph_break fn error_on_graph_break compile_wrapper _torchdynamo_inline = fn type ignore attr-defined Save function pointer find original callable while nesting decorators compile_wrapper _torchdynamo_orig_callable = fn type ignore attr-defined when compiling user function instead nn Module provide public api _fn get_compiler_config assert hasattr compile_wrapper get_compiler_config compile_wrapper get_compiler_config = get_compiler_config type ignore attr-defined torch _dynamo config enable_aot_compile compile_wrapper aot_compile = aot_compile type ignore attr-defined If function called using torch _dynamo optimize decorator we should prevent any type skipping callback None False hasattr fn __code__ raise RuntimeError textwrap dedent torch _dynamo optimize called non function object If callable please wrap relevant code into function optimize wrapper function CallableClass __init__ - None super __init__ relu = torch nn ReLU __call__ x relu torch sin x print_hello print Hello world mod = CallableClass If you want optimize __call__ function other code wrap up function wrapper_fn x y = mod x y sum then optimize wrapper_fn opt_wrapper_fn = torch _dynamo optimize wrapper_fn always_optimize_code_objects fn __code__ = True compile_wrapper OptimizeContext _TorchDynamoContext __init__ callback DynamoCallback backend_ctx_ctor Callable contextlib AbstractContextManager Any first_ctx bool = False fullgraph bool = False error_on_graph_break Optional bool = None export bool = False dynamic Optional bool = None compiler_config Optional Any = None rebuild_ctx Optional Callable Union OptimizeContext _NullDecorator = None package Optional CompilePackage = None hooks Optional Hooks = None - None on_enter - None install_generation_tagging_init super __init__ callback=callback on_enter=on_enter backend_ctx_ctor=backend_ctx_ctor patch_fn=TorchPatcher patch first_ctx=first_ctx fullgraph=fullgraph error_on_graph_break=error_on_graph_break export=export dynamic=dynamic compiler_config=compiler_config package=package hooks=hooks config compiled_autograd _dynamic = _dynamic _dynamic None _dynamic = torch _dynamo config assume_static_by_default call_compiled_autograd - functools partial Optional bool assert rebuild_ctx None compiler_fn = rebuild_ctx ctx = torch _dynamo compiled_autograd _enable compiler_fn pyrefly ignore bad-argument-type dynamic=_dynamic ignore_active_disable_ctx=False ctx __enter__ functools partial ctx __exit__ None None None enter_exit_hooks append call_compiled_autograd __reduce__ - tuple type OptimizeContext tuple Any dict str Any __class__ callback _backend_ctx_ctor first_ctx export export dynamic _dynamic compiler_config compiler_config RunOnlyContext _TorchDynamoContext __init__ - None cudagraph trees relies generation increment on_enter - None torch _dynamo mutation_guard GenerationTracker generation += super __init__ callback=False on_enter=on_enter __reduce__ - tuple type RunOnlyContext tuple Any __class__ DisableContext _TorchDynamoContext __init__ msg Optional str = None wrapping bool = True - None super __init__ callback=None msg = msg wrapping = wrapping __call__ fn Callable Any - Callable Any Earlier code base _TorchDynamoContext But we moved here have better code organization For disable we just want callback None We don t have check trace_rules create any wrapper fn = innermost_fn fn isinstance fn torch nn Module mod = fn new_mod = OptimizedModule mod new_mod _torchdynamo_orig_callable = mod forward new_mod isinstance fn type User has wrapped compile disable decorator Apply disable init call method cls_obj = fn Disable init useful reconstruction bytecodes where we want prevent Dynamo tracing into init function Check test_reconstruction test_model_output py cls_obj __init__ = cls_obj __init__ type ignore misc cls_obj __call__ = cls_obj __call__ issubclass cls_obj torch nn Module NN module variable tracker directly inlines _call_impl Disable pyrefly ignore missing-attribute cls_obj _call_impl = cls_obj _call_impl cls_obj assert callable fn f A callable function expected type fn provided _fn args Any kwargs Any - Any prior = set_eval_frame None try _maybe_set_eval_frame _callback_from_stance callback try fn args kwargs finally set_eval_frame None finally _maybe_set_eval_frame prior Under some circumstances e g precompile we can end up calling disable decorator generated bytecode trigger recompile This due fact old callback torch compile still active under circumstance we will trigger failure set_stance fail_on_recompile Therefore we want skip calling into any frame case wrapping _fn = functools wraps fn _fn _fn _torchdynamo_disable = True type ignore attr-defined _fn _torchdynamo_disable_msg = msg type ignore attr-defined Save function pointer find original callable while nesting decorators _fn _torchdynamo_orig_callable = fn type ignore attr-defined _fn __reduce__ - tuple type DisableContext tuple Any __class__ _optimize_catch_errors compile_fn convert_frame ConvertFrameProtocol hooks Hooks backend_ctx_ctor Callable contextlib AbstractContextManager Any = null_context fullgraph bool = False error_on_graph_break Optional bool = None export bool = False dynamic Optional bool = None compiler_config Optional Any = None rebuild_ctx Optional Callable Union OptimizeContext _NullDecorator = None package Optional CompilePackage = None - OptimizeContext OptimizeContext convert_frame catch_errors_wrapper compile_fn hooks backend_ctx_ctor=backend_ctx_ctor first_ctx=True fullgraph=fullgraph error_on_graph_break=error_on_graph_break export=export dynamic=dynamic compiler_config=compiler_config rebuild_ctx=rebuild_ctx package=package hooks=hooks get_compiler_fn compiler_fn Union str Callable Any None - WrapBackendDebug repro after_dynamo wrap_backend_debug compiler_fn None Special case None avoid crashing hasattr compiler_str = None hasattr compiler_fn compiler_name compiler_str = compiler_fn compiler_name type ignore union-attr assert isinstance compiler_str str isinstance compiler_fn str compiler_str = compiler_fn compiler_str = None compiler_fn = lookup_backend compiler_fn type ignore arg-type wrap_backend_debug compiler_fn compiler_str _NullDecorator contextlib nullcontext type ignore type-arg __call__ fn Callable Any - Callable Any assert callable fn f A callable function expected type fn provided fn Make dynamo graph have same input output spec user code argument_names f_sig inspect Signature args Union list Any tuple Any kwargs dict str Any - list str signature_to_fullargspec sig inspect Signature - inspect FullArgSpec Get list Parameter objects Signature object params = list sig parameters values Separate positional arguments keyword-only arguments varargs varkw args = p name p params p kind == inspect Parameter POSITIONAL_OR_KEYWORD kwonlyargs = p name p params p kind == inspect Parameter KEYWORD_ONLY varargs = next p name p params p kind == inspect Parameter VAR_POSITIONAL None varkw = next p name p params p kind == inspect Parameter VAR_KEYWORD None Get default values positional arguments keyword-only arguments defaults = tuple p default p params p kind == inspect Parameter POSITIONAL_OR_KEYWORD p default inspect Parameter empty kwonlydefaults = p name p default p params p kind == inspect Parameter KEYWORD_ONLY p default inspect Parameter empty Get annotations parameters value annotations = sig return_annotation annotations = sig return_annotation parameter params annotations parameter name = parameter annotation Return FullArgSpec object extracted attributes inspect FullArgSpec args varargs varkw defaults kwonlyargs kwonlydefaults annotations fullargspec = signature_to_fullargspec f_sig Map ` args ` -to- positional arguments original signature input_strs = fullargspec args len args len args len fullargspec args If there more arguments left ` args ` they map varargs original signature Assign names varargs _ varargs _ assert fullargspec varargs None More arguments than expected input_strs += f fullargspec varargs _ i i range len args - len input_strs len args len fullargspec args If there fewer arguments ` args ` than ` fullargspec args ` implies these arguments either default values provided ` kwargs ` The former can safely ignored Because Dynamo export does export them part function signature The latter will handled next step unprovided_arg fullargspec args len args -len fullargspec defaults assert unprovided_arg kwargs f Missing argument unprovided_arg Keyword arguments provided ` kwargs ` input_strs += list kwargs keys Keyword-only arguments default values provided exported part function signature kwonly_arg fullargspec kwonlyargs kwonlydefaults = fullargspec kwonlydefaults assert kwonly_arg kwargs kwonly_arg kwonlydefaults f Missing keyword only argument kwonly_arg input_strs check_if_dynamo_supported - None sys version_info = raise RuntimeError Python + yet supported torch compile sysconfig get_config_var Py_GIL_DISABLED == sys version_info raise RuntimeError torch compile supported Python built GIL disabled Please use Python + is_dynamo_supported - bool try check_if_dynamo_supported True except Exception False check_if_inductor_supported - None check_if_dynamo_supported is_inductor_supported - bool try check_if_inductor_supported True except Exception False check_for_incompatible_configs - None Some configs should mutually exclusive assert config suppress_errors config fail_on_recompile_limit_hit Dynamo configs suppress_error fail_on_recompile_limit_hit can both active same time optimize args Any kwargs Any - Union OptimizeContext _NullDecorator rebuild_ctx - Union OptimizeContext _NullDecorator ca_kwargs_override = config compiled_autograd_kwargs_override ca_kwargs_override NOTE The process translating other ` torch compile ` kwargs ` torch _dynamo optimize ` kwargs more complicated we will add future when needed assert set ca_kwargs_override keys == fullgraph f Only ` fullgraph ` kwarg override supported now got ca_kwargs_override keys kwargs nopython = ca_kwargs_override fullgraph optimize args kwargs _optimize rebuild_ctx args kwargs _optimize rebuild_ctx Callable Union OptimizeContext _NullDecorator backend Union str Callable Any = inductor nopython bool = False error_on_graph_break Optional bool = None guard_export_fn Optional Callable _guards GuardsSet None = None guard_fail_fn Optional Callable GuardFail None = None guard_filter_fn Optional Callable list GuardFilterEntry list bool = None disable bool = False dynamic Optional bool = None package Optional CompilePackage = None - Union OptimizeContext _NullDecorator The main entrypoint TorchDynamo Do graph capture call backend optimize extracted graphs Args backend One two things - Either function callable taking torch fx GraphModule example_inputs returning python callable runs graph faster One can also provide additional context backend like torch jit fuser fuser setting backend_ctx_ctor attribute See AOTAutogradMemoryEfficientFusionWithContext usage - Or string backend name ` torch _dynamo list_backends ` nopython If True graph breaks will errors there will single whole-program graph error_on_graph_break If None current ` error_on_graph_break ` setting set given value See ` torch _dynamo error_on_graph_break ` more details what ` error_on_graph_break ` means Unlike ` nopython=True ` i e ` fullgraph=True ` there no guarantee single whole-program graph If ` nopython ` True ` error_on_graph_break ` does nothing disable If True turn decorator into no-op dynamic If True upfront compile dynamic kernel possible If False disable all dynamic shapes support always specialize If None automatically detect when sizes vary generate dynamic kernels upon recompile Example Usage torch _dynamo optimize toy_example b check_if_dynamo_supported check_for_incompatible_configs Note The hooks object could global instead passed around however would make confusing API usage plumbing story wherein we nest multiple optimize calls There some prior art around w r t nesting backend calls enforced same compiler however feels onerous callback hooks feels better give our users easier understand UX cost little more plumbing our end hooks = Hooks guard_export_fn=guard_export_fn guard_fail_fn=guard_fail_fn guard_filter_fn=guard_filter_fn torch _C _log_api_usage_once torch _dynamo optimize disable os environ get TORCHDYNAMO_DISABLE == justknobs_check pytorch compiler enable_dynamo _NullDecorator nopython config debug_force_graph_break_on_leaf_return optimize_assert backend dynamic=dynamic hooks=hooks rebuild_ctx=rebuild_ctx package=package backend = get_compiler_fn backend Find backend has any extra context manager backend_ctx_ctor = getattr backend backend_ctx_ctor null_context The backend function stashed callable returned _optimize_catch_errors field _torchdynamo_orig_backend This can used eval_frame c insert guard backend With CachingPrecompile instantiate uninitialized CompilePackage which gets initialized _optimize_catch_errors __call__ once we have function config caching_precompile package None package CompilePackage package = CompilePackage fn=None dynamo=None ignore_inlined_sources=False _optimize_catch_errors convert_frame convert_frame backend hooks package=package hooks backend_ctx_ctor fullgraph=False error_on_graph_break=error_on_graph_break config debug_force_graph_break_on_leaf_return dynamic=dynamic compiler_config= backend get_compiler_config hasattr backend get_compiler_config None rebuild_ctx=rebuild_ctx package=package TODO voz Consider making explain output alongside run part run patch torch _dynamo symbolic_convert explain True explain f Callable Any extra_args Any extra_kwargs Any - Any backends debugging ExplainOutput inner args Any kwargs Any - ExplainOutput TODO voz Do we want decorator reset type ignore attr-defined reset graphs list torch fx GraphModule = break_reasons list Any = op_count int = ops_per_graph list list Target = out_guards list _guards Guard = dynamo_graph_accumulating_compiler gm torch fx GraphModule example_inputs Any - Callable Any backends debugging _explain_graph_detail nonlocal graphs nonlocal op_count nonlocal ops_per_graph nonlocal break_reasons gm graphs op_count ops_per_graph break_reasons = _explain_graph_detail gm graphs op_count ops_per_graph break_reasons gm forward guard_export_print guards Iterable _guards Guard - None nonlocal out_guards out_guards extend guards opt_f = optimize dynamo_graph_accumulating_compiler nopython=False guard_export_fn=guard_export_print f TODO voz We may have instances ` f ` mutate inputs we should track sideeffects reject opt_f args kwargs graph_count = len graphs graph_break_count = graph_count - compile_time = compile_times repr= str TODO voz Do we want decorator reset ExplainOutput graphs graph_count graph_break_count break_reasons op_count ops_per_graph out_guards compile_time extra_args extra_kwargs warnings warn explain f args kwargs deprecated use explain f args kwargs instead If you don t migrate we may break your explain call future your user defined kwargs conflict future kwargs added explain f FutureWarning stacklevel= inner extra_args extra_kwargs inner FlattenInputOutputSignature torch fx Transformer __init__ m torch fx GraphModule flat_args list Any matched_input_elements_positions list int flat_results Sequence Any matched_output_elements_positions list int example_fake_inputs list torch Tensor flat_args_dynamic_dims list set int fake_mode Optional fake_tensor FakeTensorMode = None - None super __init__ m assert len flat_args_dynamic_dims == len flat_args matched_input_elements_to_fake = val example_fake_inputs ix ix val enumerate matched_input_elements_positions new_args = i range len flat_args arg = super placeholder f arg i i matched_input_elements_to_fake arg node meta val = matched_input_elements_to_fake i Fill node meta val faketensor input s found matched_input_elements_positions fake_mode None isinstance flat_args i torch Tensor TODO zhxchen Also preserve all user constraints here arg node meta val = fake_mode from_tensor flat_args i symbolic_context=StatelessSymbolicContext dynamic_sizes= DimDynamic DYNAMIC d flat_args_dynamic_dims i DimDynamic STATIC d range len flat_args i shape constraint_sizes= None len flat_args i shape isinstance flat_args i _IntWrapper arg node meta val = flat_args i val arg node meta val = flat_args i new_args append arg old_args_gen = new_args i i matched_input_elements_positions matched_output_elements_positions = matched_output_elements_positions flat_results = flat_results placeholder target Target args tuple Argument kwargs dict str Any - Any arg = next old_args_gen val current_node meta arg node meta val = current_node meta val tensor_dict current_node meta arg node meta tensor_dict = current_node meta tensor_dict example_value current_node meta NB intentionally do use set_example_value arg node meta example_value = current_node meta example_value unbacked_bindings current_node meta arg node meta unbacked_bindings = current_node meta unbacked_bindings arg output target Target args tuple Argument kwargs dict str Any - Any dynamo_result_flat = args lookup = dynamo_result_flat new_args type ignore misc new_results_flat = i range len flat_results matched_output_elements_positions i None new_results_flat append lookup matched_output_elements_positions i const_val = flat_results i assert isinstance const_val tuple common_constant_types new_results_flat append const_val super output target new_results_flat run_node n Node - Any current_node = n result_proxy = super run_node n val current_node meta result_proxy node meta val = current_node meta val example_value current_node meta NB intentionally do use set_example_value result_proxy node meta example_value = current_node meta example_value unbacked_bindings current_node meta result_proxy node meta unbacked_bindings = current_node meta unbacked_bindings current_node op = output result_proxy node _rename getattr current_node name result_proxy node name result_proxy transform - torch fx GraphModule result_gm = super transform dynamo_flat_name_to_original_fqn module meta type ignore operator result_gm meta dynamo_flat_name_to_original_fqn = module meta type ignore index dynamo_flat_name_to_original_fqn type ignore index dynamo_compile_id module meta type ignore operator result_gm meta dynamo_compile_id = module meta dynamo_compile_id type ignore index result_gm ExportResult NamedTuple graph_module torch fx GraphModule guards _guards GuardsSet NB Do add new fields without overriding __iter__ people destructuring so BC-breaking NOTE function only supports graphs created Dynamo s OutputGraph module check_signature_rewritable graph torch fx GraphModule - None input_errors = node graph graph find_nodes op= placeholder set OutputGraph _call_user_compiler assert hasattr node _dynamo_source assert hasattr graph _source_to_user_stacks NOTE We can safely ignore these type warnings only function made OutputGraph checked assertions source = node _dynamo_source type ignore attr-defined user_stacks = graph _source_to_user_stacks get source type ignore operator union-attr user_stacks None continue assert len user_stacks In some cases we may have useful stack Look useful stack stack = None s user_stacks len s == continue stack = s break stack None msg = f source name closed over free variable tb = join traceback format_list stack extra = len user_stacks extra = f elided len user_stacks - more accesses msg = f source name accessed \n tb extra TODO option print ALL stack traces once input_errors append msg input_errors raise UserError UserErrorType INVALID_INPUT Cannot export model which references tensors neither buffers parameters constants nor direct inputs For each tensor you d like tensor explicit input add dummy argument top-level model definition you exporting you would like its value embedded exported constant wrap its access function marked assume_constant_result \n\n + \n\n join input_errors check_user_input_output flat_values list Any error_type UserErrorType - None supported_types = torch Tensor torch SymInt torch SymFloat torch SymBool torch _C ScriptObject _IntWrapper + list common_constant_types is_supported_type val Any - bool isinstance val tuple supported_types value_type = input error_type == UserErrorType INVALID_INPUT output We only check outputs None Inputs can None v flat_values is_supported_type v error_type == UserErrorType INVALID_INPUT v None continue raise UserError error_type f It looks like one value_type s type ` type v ` supported pytree-flattenable \n f Exported graphs value_type s can only contain f following supported types supported_types \n If you using custom object please register pytree_flatten unflatten function using ` torch utils _pytree register_pytree_node ` ` torch export register_dataclass ` rewrite_signature f_sig inspect Signature graph torch fx GraphModule fake_mode Optional fake_tensor FakeTensorMode flat_args list Any in_spec pytree TreeSpec example_fake_inputs list Any graph_captured_input Iterable Any graph_captured_output Optional Iterable Any dynamo_traced_result Any flat_args_dynamic_dims list set int - torch fx GraphModule orig_args orig_kwargs = pytree tree_unflatten flat_args in_spec check_user_input_output flat_args UserErrorType INVALID_INPUT flat_results_traced out_spec_traced = pytree tree_flatten dynamo_traced_result check_user_input_output flat_results_traced UserErrorType INVALID_OUTPUT check_optional_input_and_error f_sig inspect Signature - None Check function has optional input name param f_sig parameters items param default inspect Parameter empty torch _dynamo exc Unsupported log error Parameter s optional default value s name param default raise Unsupported Tracing through optional input supported yet case_name= optional_input produce_matching debug_type str sources Iterable Any candidates Iterable Any - list Optional int matched_elements_positions list Optional int = dict_of_source_vals = i val enumerate sources dict_of_source_vals id val = i val candidates isinstance val tuple common_constant_types matched_elements_positions append None id val dict_of_source_vals debug_type == inputs check_optional_input_and_error f_sig raise AssertionError f Unexpectedly found type val debug_type \n Please file issue along paste logs TORCH_LOGS= +export matched_elements_positions append dict_of_source_vals id val matched_elements_positions matched_input_elements_positions = produce_matching inputs flat_args graph_captured_input assert graph_captured_output None matched_output_elements_positions = produce_matching outputs list graph_captured_output + flat_args flat_results_traced new_graph = FlattenInputOutputSignature graph flat_args matched_input_elements_positions type ignore arg-type flat_results_traced matched_output_elements_positions type ignore arg-type example_fake_inputs flat_args_dynamic_dims fake_mode transform new_graph graph _codegen = _PyTreeCodeGen _PyTreeInfo argument_names f_sig orig_args orig_kwargs in_spec out_spec_traced new_graph recompile new_graph export f Callable Any extra_args Any aten_graph bool = False pre_dispatch bool = False decomposition_table Optional dict torch _ops OpOverload Callable Any = None tracing_mode str = symbolic dynamic_shapes Optional Union dict str Any tuple Any list Any = None specialize_float bool = True assume_static_by_default bool = False same_signature bool = True disable_constraint_solver bool = False prefer_deferred_runtime_asserts_over_guards bool = False _log_export_usage bool = True constraints Optional list Constraint = None extra_kwargs Any - Callable ExportResult Export input function f format can executed outside PyTorch using FX graph Args f callable A PyTorch function exported aten_graph bool If True exports graph ATen operators If False exports graph Python operators Default False pre_dispatch bool If True exports graph ATen operators before any logic PyTorch dispatcher has run This can useful you want apply further transformations graph before running through autograd autocast any other functionalities integrated into dispatcher This flag only valid aten_graph=True set Default False decomposition_table dict A dictionary maps operators their decomposition functions Required aten_graph tracing_mode specified Default None tracing_mode str If symbolic turn dynamic shapes support Default symbolic dynamic_shapes An optional argument where type should either dict argument names ` ` f ` ` their dynamic shape specifications tuple specifies dynamic shape specifications each input original order If you specifying dynamism keyword args you will need pass them order defined original function signature The dynamic shape tensor argument can specified either dict dynamic dimension indices func ` Dim ` types where required include static dimension indices dict when they they should mapped None tuple list func ` Dim ` types None where func ` Dim ` types correspond dynamic dimensions static dimensions denoted None Arguments dicts tuples lists tensors recursively specified using mappings sequences contained specifications same_signature bool If True rewrite returned graph s signature same f disable_constraint_solver bool Whether dim constraint solver must disabled Returns A function given args kwargs returns tuple graph guards Graph An FX graph representing execution input PyTorch function provided arguments options Guards The guards we accumulated during tracing f above Raises AssertionError If decomposition_table specified without setting aten_graph=True graph breaks during tracing export AssertionError If Dynamo input output consistent traced input output Note - headerdoc authored ChatGPT slight modifications author config debug_force_graph_break_on_leaf_return raise unittest SkipTest Cannot force graph break export _log_export_usage log_export_usage event= export private_api flags= _dynamo Deal local variable referenced before assignment _f = f _specialize_float = specialize_float _assume_static_by_default = assume_static_by_default _constraints = constraints inner args Any kwargs Any - ExportResult _constraints combined_args = _combine_args _f args kwargs constraints = _process_dynamic_shapes combined_args dynamic_shapes constraints = _constraints f = _f specialize_float = _specialize_float assume_static_by_default = _assume_static_by_default check_if_dynamo_supported torch _C _log_api_usage_once torch _dynamo export decomposition_table None assert aten_graph Specifying decomposition_table table tracing mode illegal without setting aten_graph=True pre_dispatch assert aten_graph pre_dispatch=True can only used when aten_graph=True f = innermost_fn f call_to_inspect = f forward isinstance f torch nn Module f original_signature = inspect signature call_to_inspect type ignore arg-type graph = None out_guards = None graph_captured_input = None graph_captured_result Optional tuple torch Tensor = None fake_mode = None result_traced = None guard_export_print guards _guards GuardsSet - None nonlocal out_guards assert out_guards None whole graph export entails exactly one guard export out_guards = guards example_inputs list Any = dynamo_normalization_capturing_compiler gm torch fx GraphModule inner_example_inputs list Any - Callable Any nonlocal graph assert graph None Tried emit second graph during export Tracing through f must produce single graph graph = gm nonlocal fake_mode example_inputs NB do NOT pass inner_example_inputs here we detecting Dynamo allocated fake mode which should DISTINCT potential outer ambient fake mode which user provided example_inputs always user specified inputs so they would have wrong fake mode attached them fake_mode = _guards detect_fake_mode example_inputs = inner_example_inputs result_capturing_wrapper graph_inputs Any - Any nonlocal graph_captured_result nonlocal graph_captured_input graph_captured_input = graph_inputs assert graph None named_parameters = dict graph named_parameters remove_duplicate=False named_buffers = dict graph named_buffers remove_duplicate=False ambient_fake_mode = _guards detect_fake_mode graph_inputs _guards detect_fake_mode graph_inputs None fake_mode We reran fake tensor propagation we didn t do anything resulting unbacked SymInts Drop them pending list NB wrong graph_captured_result has data-dependent output size ignore_fresh_unbacked = null_context assert ambient_fake_mode None shape_env = ambient_fake_mode shape_env ignore_fresh_unbacked = shape_env ignore_fresh_unbacked_symbols type ignore assignment ambient_fake_mode enable_python_dispatcher ignore_fresh_unbacked params_and_buffers = named_parameters named_buffers fake_params_buffers = name value params_and_buffers items fake_params_buffers name = ambient_fake_mode from_tensor value static_shapes=True torch _export non_strict_utils key_path_to_source KeyPath fakify_with_ambient path KeyPath t Union torch Tensor _IntWrapper Any - Any isinstance t torch Tensor pyrefly ignore missing-attribute ambient_fake_mode from_tensor t static_shapes=True isinstance t _IntWrapper t dynamism None isinstance t dynamism _DimHint t dynamism type _DimHintType DYNAMIC _DimHintType AUTO type ignore union-attr source = key_path_to_source path symint = ambient_fake_mode shape_env create_unspecified_symint_and_symbol type ignore union-attr t val source DimDynamic DYNAMIC symint t val t fake_graph_inputs = pytree tree_map_with_path fakify_with_ambient graph_inputs graph_captured_result = torch func functional_call graph fake_params_buffers type ignore arg-type fake_graph_inputs type ignore arg-type graph_captured_result result_capturing_wrapper Note This needed rewrite_signature We need put before optimize_assert since user program may mutate inputs flat_args in_spec = pytree tree_flatten args kwargs remove_from_cache f constraint_violation_error = None tracing_mode = symbolic assume_static_by_default = True config patch specialize_int=True specialize_float=specialize_float assume_static_by_default=assume_static_by_default automatic_dynamic_shapes=False capture_dynamic_output_shape_ops=True capture_scalar_outputs=True constant_fold_autograd_profiler_enabled=True prefer_deferred_runtime_asserts_over_guards=prefer_deferred_runtime_asserts_over_guards install_free_tensors ensures params buffers still added graph attributes makes Dynamo emits graphs follow export pytree-able input requirements install_free_tensors=config install_free_tensors_for_export _compiling_state_context opt_f = optimize_assert dynamo_normalization_capturing_compiler hooks=Hooks guard_export_fn=guard_export_print guard_fail_fn=None export=True export_constraints=constraints f TODO voz We may have instances ` f ` mutate inputs we should track sideeffects reject try result_traced = opt_f args kwargs except ConstraintViolationError e constraint_violation_error = e remove_from_cache f disable_constraint_solver shape_env = getattr fake_mode shape_env None None dim_constraints = shape_env dim_constraints None isinstance call_to_inspect torch _ops OpOverloadPacket torch _ops OpOverload trace_rules check call_to_inspect dim_constraints solve forced_specializations = dim_constraints forced_specializations msg = dim_constraints prettify_results original_signature dynamic_shapes constraint_violation_error forced_specializations constraint_violation_error constraint_violation_error args = constraint_violation_error args + msg forced_specializations constraint_violation_error = ConstraintViolationError msg log info Summary dimension constraints s msg Error we have any constraints static values k shape_env var_to_range keys isinstance k sympy Integer constraint_violation_error = ConstraintViolationError f join traceback format_list shape_env var_to_stack k \n It appears you re trying set constraint f value which we evaluated have static value k Set TORCH_LOGS= +export more information constraint_violation_error raise constraint_violation_error graph None assert same_signature Failed produce graph during tracing no tensor operations found same_signature False If module does contain any tensor computation we would create graph inputs outputs To consistent graph traced dynano ` graph ` will have only tensor inputs placeholders tensor outputs output nodes non-tensor inputs outputs will added when rewriting signature We will also construct ` example_inputs ` ` graph_captured_input ` ` graph_captured_result ` corresponding ` graph ` example_inputs = graph_captured_input = graph_captured_result = fake_mode = torch _subclasses FakeTensorMode shape_env=ShapeEnv export=True out_guards None out_guards = _guards GuardsSet assert out_guards None suppress mypy error parameter_names = list original_signature parameters keys fx_graph = torch fx Graph i name enumerate parameter_names torch is_tensor flat_args i node = fx_graph placeholder name node meta val = fake_mode from_tensor flat_args i static_shapes=True graph_captured_input = graph_captured_input + flat_args i example_inputs append flat_args i fx_graph output graph_captured_result module = torch nn Module graph = torch fx GraphModule module fx_graph log info Failed capture graph during tracing no tensor operations found \n\n s graph print_readable print_output=False colored=True assert out_guards None Failed produce guards during tracing assert fake_mode None log info Dynamo captured graph \n\n s graph print_readable print_output=False colored=True This check need happened before aten_graph because placeholder s _source_node attribute preserved make_fx same_signature check_signature_rewritable graph NB This mostly hitting cache Dynamo already converted these example_fake_inputs = fake_mode from_tensor t isinstance t torch Tensor t t example_inputs aten_graph Running graph interpreter needed propagating stack_trace graph_with_interpreter args Any - Any torch fx traceback preserve_node_meta torch fx Interpreter graph run args type ignore arg-type unset_fake_temporarily enable_python_dispatcher fake_mode try graph = make_fx graph_with_interpreter decomposition_table=decomposition_table tracing_mode= real _allow_non_fake_inputs=True pre_dispatch=pre_dispatch _allow_fake_constant=False example_fake_inputs except CondOpArgsMismatchError e Wrap internal error user-facing error raise UserError noqa B UserErrorType DYNAMIC_CONTROL_FLOW str e case_name= cond_operands assert graph None node graph graph find_nodes op= get_attr isinstance getattr graph node target torch Tensor type ignore arg-type node meta val = fake_mode from_tensor getattr graph node target type ignore arg-type static_shapes=True same_signature flat_args_dynamic_dims = c dim c constraints c t_id == id x isinstance c _RelaxedConstraint c constraint_range vr lower = c constraint_range vr upper x flat_args graph = rewrite_signature original_signature graph fake_mode flat_args in_spec example_fake_inputs graph_captured_input type ignore arg-type graph_captured_result result_traced type ignore possibly-undefined flat_args_dynamic_dims ExportResult graph out_guards extra_args extra_kwargs warnings warn export f args kwargs deprecated use export f args kwargs instead If you don t migrate we may break your export call future your user defined kwargs conflict future kwargs added export f FutureWarning stacklevel= inner extra_args extra_kwargs type ignore return-value inner optimize_assert args Any kwargs Any - OptimizeContext rebuild_ctx kwargs kwargs rebuild_ctx None called optimize rebuild_ctx = kwargs rebuild_ctx del kwargs rebuild_ctx rebuild_ctx - OptimizeContext optimize_assert args kwargs _optimize_assert rebuild_ctx args kwargs _optimize_assert rebuild_ctx Callable OptimizeContext backend Union str Callable Any None hooks Hooks = Hooks None None None export bool = False export_constraints Optional Any = None dynamic Optional bool = None package Optional CompilePackage = None - OptimizeContext Guarantees single-graph capture The same ` torch _dynamo optimize backend ` ignores symbolic_convert error_on_graph_break setting Used fullgraph=True export since we must always error graph breaks ignore symbolic_convert error_on_graph_break Can also used testing backend = get_compiler_fn backend Find backend has any extra context manager backend_ctx_ctor = getattr backend backend_ctx_ctor null_context config caching_precompile package None Create uninitialized package will set filled _OptimizeContext __call__ We need instantiate object here because same CompilePackage needs shared between convert_frame_assert OptimizeContext package CompilePackage package = CompilePackage fn=None dynamo=None ignore_inlined_sources=False _optimize_catch_errors convert_frame convert_frame_assert backend export=export export_constraints=export_constraints package=package hooks backend_ctx_ctor fullgraph=True export=export dynamic=dynamic rebuild_ctx=rebuild_ctx package=package TorchPatcher staticmethod functools cache patch - None A better way disable following would decorate source functions torch _disable_dynamo However causes issues torch deploy internally decorators disable torch jit trace = disable torch jit trace reason= tracing into TorchScript fully supported torch jit trace_module = disable torch jit trace_module reason= tracing into TorchScript fully supported torch jit _get_trace_graph = disable torch jit _get_trace_graph reason= tracing into TorchScript fully supported torch fx _symbolic_trace Tracer trace = disable torch fx _symbolic_trace Tracer trace reason= tracing into FX fully supported torch distributions Distribution set_default_validate_args False torch optim adadelta adagrad adam adamax adamw asgd lbfgs nadam radam rmsprop rprop sgd sparse_adam optimizer_modules = adadelta adagrad adam adamax adamw asgd lbfgs nadam radam rmsprop rprop sgd sparse_adam opt_mod optimizer_modules opt_name = opt_mod __name__ split - fused_fn_name = f _fused_ opt_name hasattr opt_mod fused_fn_name setattr opt_mod fused_fn_name disable getattr opt_mod fused_fn_name reason= don t trace into fused optimizer optimizer_classes = opt opt torch optim __dict__ values inspect isclass opt issubclass opt torch optim Optimizer Note we don t support sparsity tracing through backwards excluded_optimizer_classes = torch optim SparseAdam torch optim LBFGS opt optimizer_classes opt excluded_optimizer_classes opt step = disable opt step reason=f optimizer opt step supported hasattr opt _init_group opt _init_group = disable opt _init_group reason=f optimizer opt _init_group supported staticmethod suppress_torch_distributed_warnings fn Callable Any - Callable Any inner_fn args Any kwargs Any - Any torch _logging hide_warnings torch _logging _internal user_warning_filter fn args kwargs inner_fn skip_code code types CodeType - None set_code_exec_strategy code FrameExecStrategy FrameAction SKIP FrameAction DEFAULT