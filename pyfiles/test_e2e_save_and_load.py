Owner s oncall distributed time concurrent futures Future dataclasses dataclass field enum auto Enum functools partial io BytesIO typing Any torch torch distributed dist torch distributed checkpoint DCP torch distributed checkpoint state_dict_saver saver torch nn nn torch nn functional F torch distributed checkpoint staging DefaultStager StagingOptions torch distributed checkpoint state_dict _patch_model_state_dict _patch_optimizer_state_dict get_model_state_dict get_optimizer_state_dict get_state_dict set_state_dict torch distributed checkpoint state_dict_loader _load_state_dict_from_keys torch distributed checkpoint state_dict_saver AsyncCheckpointerType AsyncSaveResponse torch distributed checkpoint stateful Stateful torch distributed checkpoint utils CheckpointException torch distributed device_mesh init_device_mesh torch distributed distributed_c d ReduceOp torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp api ShardingStrategy torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch nn parallel DistributedDataParallel torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase skip_if_lt_x_gpu with_comms torch testing _internal distributed checkpoint_utils with_temp_dir torch testing _internal distributed common_state_dict VerifyStateDictMixin device_type = acc type acc = torch accelerator current_accelerator cpu Simple boring model TestDummyModel torch nn Module __init__ - None super __init__ torch manual_seed net = nn Linear net = nn Linear net = nn Linear net = nn Linear forward x x = F relu net x x = F relu net x x = F relu net x x = F relu net x x get_input torch rand device=device_type TestStatefulObj __init__ - None data = torch rand device=device_type state_dict data data load_state_dict state_dict data = state_dict data __eq__ other torch equal data other data ModelType Enum FSDP = auto HSDP = auto FSDP_TP = auto DDP = auto NONE = auto no parallelization dataclass TestTrainState step int = current_loss float = - losses list float = field default_factory=list state_dict - dict str Any loss_bytes = BytesIO torch save losses loss_bytes step torch tensor step dtype=torch int current_loss torch tensor current_loss dtype=torch float losses loss_bytes load_state_dict state_dict - None step = state_dict step item current_loss = state_dict current_loss item state_dict losses seek losses = torch load state_dict losses __eq__ other step == other step current_loss == other current_loss losses == other losses _train model optim train_steps= torch manual_seed loss = None train_state = TestTrainState _ range train_steps loss = model model get_input sum loss backward We usually sync loss across dp ranks real training This just simulating testing purpose train_state step += train_state current_loss = torch rand item train_state losses append train_state current_loss optim step optim zero_grad loss train_state TestE ESaveAndLoad DTensorTestBase VerifyStateDictMixin property backend curr_backend = dist get_default_backend_for_device device_type f cpu gloo device_type curr_backend _create_model compile model_type state_dict_options=None dummy_model = TestDummyModel device_type assert model_type ModelType f model_type supported model_type == ModelType FSDP device_mesh = init_device_mesh device_type world_size model = FSDP dummy_model device_mesh=device_mesh use_orig_params=True model_type == ModelType HSDP device_mesh = init_device_mesh device_type world_size model = FSDP dummy_model device_mesh=device_mesh use_orig_params=True sharding_strategy=ShardingStrategy HYBRID_SHARD model_type == ModelType FSDP_TP mesh_ d = init_device_mesh device_type world_size mesh_dim_names= dp tp tp_mesh = mesh_ d tp dp_mesh = mesh_ d dp parallelize_plan = net ColwiseParallel net RowwiseParallel model = parallelize_module dummy_model tp_mesh parallelize_plan model = FSDP model device_mesh=dp_mesh use_orig_params=True model_type == ModelType DDP model = DistributedDataParallel dummy_model model get_input = partial TestDummyModel get_input model model = dummy_model compile TODO enable dynamic=True when dynamic shape support enabled model = torch compile model model = torch compile model dynamic=False optim = _optim model model_type ModelType NONE _patch_model_state_dict model options=state_dict_options _patch_optimizer_state_dict model optimizers=optim options=state_dict_options model optim _optim model torch optim Adam model parameters lr= skip_if_lt_x_gpu with_comms with_temp_dir parametrize compile True False TODO Previously PairwiseParallel does shard properly passing ModelType FSDP_TP test where should have failed Disabling failed test temporarily unblock deprecation PairwiseParallel parametrize model_type ModelType FSDP ModelType HSDP ModelType DDP test_e e compile model_type _run_e e_test compile model_type skip_if_lt_x_gpu with_comms with_temp_dir parametrize cache_staged_state_dict async_checkpointer_type zoc False AsyncCheckpointerType THREAD False True AsyncCheckpointerType THREAD False False AsyncCheckpointerType PROCESS False True AsyncCheckpointerType PROCESS False False AsyncCheckpointerType PROCESS True False AsyncCheckpointerType THREAD True test_e e_async_cached cache_staged_state_dict async_checkpointer_type zoc _run_e e_test compile=False model_type=ModelType FSDP async_op=True cache_staged_state_dict=cache_staged_state_dict async_checkpointer_type=async_checkpointer_type zoc=zoc _run_e e_test compile model_type async_op=False cache_staged_state_dict=False async_checkpointer_type=None zoc=False model optim = _create_model compile ModelType NONE _train model optim train_steps= dist_model dist_optim = _create_model compile model_type _ original_train_state = _train dist_model dist_optim train_steps= original_stateful_obj = TestStatefulObj tests arbitrary saving loading sd = model dist_model optimizer dist_optim s original_stateful_obj train_state original_train_state async_op writer = DCP FileSystemWriter temp_dir cache_staged_state_dict=cache_staged_state_dict stager = None cache_staged_state_dict use_shared_memory = async_checkpointer_type == AsyncCheckpointerType PROCESS staging_options = StagingOptions use_async_staging=zoc use_shared_memory=use_shared_memory use_pinned_memory=zoc use_non_blocking_copy=zoc stager = DefaultStager staging_options async_save_response_or_future = saver async_save sd storage_writer=writer async_checkpointer_type= async_checkpointer_type async_checkpointer_type AsyncCheckpointerType THREAD async_stager=stager isinstance async_save_response_or_future Future save_future = async_save_response_or_future assert isinstance async_save_response_or_future AsyncSaveResponse save_future = async_save_response_or_future upload_completion wait future complete t = time monotonic while save_future done time sleep print f still waiting time monotonic - t save_future result DCP save sd checkpoint_id=self temp_dir loaded_stateful_obj = TestStatefulObj loaded_train_state = TestTrainState dist_model dist_optim = _create_model compile model_type DCP load state_dict= model dist_model optimizer dist_optim s loaded_stateful_obj train_state loaded_train_state checkpoint_id=self temp_dir assertEqual original_stateful_obj loaded_stateful_obj assertEqual original_train_state loaded_train_state train one more step both models loss _ = _train model optim train_steps= dist_loss _ = _train dist_model dist_optim train_steps= assertEqual loss dist_loss dist_msd dist_osd = get_state_dict dist_model optimizers=dist_optim model_sd _ = get_state_dict model optimizers=optim _verify_msd model_sd dist_msd _verify_osd_by_load model optim _optim model dist_osd with_temp_dir test_stateful_and_non_stateful_loads - None StateDict dict __init__ set_sd_item_called = False __setitem__ item value set_sd_item_called = True super __setitem__ item value Foo Stateful __init__ load_state_dict_called = False state_dict load_state_dict state_dict load_state_dict_called = True stateful_foo = Foo sd = StateDict sd foo = stateful_foo sd set_sd_item_called = False DCP save sd checkpoint_id=self temp_dir DCP load sd checkpoint_id=self temp_dir Validate stateful object loaded in-place assertTrue stateful_foo load_state_dict_called Validate stateful object NOT replaced state dict assertFalse sd set_sd_item_called sd = StateDict sd foo = replicated torch rand bytes sd set_sd_item_called = False DCP save sd checkpoint_id=self temp_dir DCP load sd checkpoint_id=self temp_dir Validate non-stateful state dict replaced loaded state dict assertTrue sd set_sd_item_called skip_if_lt_x_gpu with_comms with_temp_dir test_different_ordered_state_dict_keys Tests order keys state dict does matter when loading If order accounted following test would cause deadlock world_size = world_size Foo state_dict load_state_dict state_dict tl = torch ones dtype=torch int device=device_type _ range world_size t = torch arange dtype=torch int device=device_type + + dist get_rank dist all_gather tl t async_op=False Bar state_dict load_state_dict state_dict tensor = torch arange dtype=torch int device=device_type + + dist get_rank dist all_reduce tensor op=ReduceOp SUM rank == sd = A Foo B Bar sd = B Bar A Foo DCP save sd checkpoint_id=self temp_dir DCP load sd checkpoint_id=self temp_dir with_temp_dir test_no_dist since comm s initialized method ` no_dist ` assumed False DCP save checkpoint_id=self temp_dir DCP load checkpoint_id=self temp_dir skip_if_lt_x_gpu with_comms with_temp_dir test_partial_load model optim = _create_model compile=False model_type=ModelType NONE _train model optim train_steps= dist_model dist_optim = _create_model compile=False model_type=ModelType FSDP _train dist_model dist_optim train_steps= DCP save model dist_model optimizer dist_optim checkpoint_id=self temp_dir dist_model _ = _create_model compile=False model_type=ModelType FSDP DCP load model dist_model checkpoint_id=self temp_dir dist_msd = get_model_state_dict dist_model model_sd = get_model_state_dict model _verify_msd model_sd dist_msd another way loaded_model_sd = _load_state_dict_from_keys model checkpoint_id=self temp_dir model _verify_msd model_sd loaded_model_sd offload_to_cpu=True loaded_optim_state = _load_state_dict_from_keys optimizer state checkpoint_id=self temp_dir optimizer state assertNotIn param_groups loaded_optim_state k v dist_optim state_dict state items optim_key exp_avg exp_avg_sq step _compare_tensor loaded_optim_state k optim_key v optim_key offload_to_cpu=True skip_if_lt_x_gpu with_comms with_temp_dir test_overwrite t t = torch randn torch randn DCP save random t checkpoint_id=self temp_dir DCP save random t storage_writer=DCP FileSystemWriter temp_dir overwrite=True sd = random torch zeros DCP load sd checkpoint_id=self temp_dir assertTrue torch allclose sd random t assertRaisesRegex CheckpointException Checkpoint already exists DCP save random t storage_writer=DCP FileSystemWriter temp_dir overwrite=False TestNoCPU DTensorTestBase property backend nccl with_comms test_no_cpu assertRaisesRegex AssertionError r A CPU backend must enabled async save f = saver async_save f result TestInitStateDict DTensorTestBase with_temp_dir test_init_state_dict temp_dir = temp_dir model = TestDummyModel optim = torch optim Adam model parameters lr= state_dict_to_save = model get_model_state_dict model optimizer get_optimizer_state_dict model optim DCP save state_dict_to_save checkpoint_id=temp_dir torch manual_seed model_ = TestDummyModel Changing learning rate optimizer which tensor optim_ = torch optim Adam model_ parameters lr= msd = get_model_state_dict model_ osd = get_optimizer_state_dict model_ optim_ state_dict_to_load = model msd optimizer osd DCP load state_dict_to_load checkpoint_id=temp_dir We need check two variables point same object memory since we claim DCP in-place loading assertTrue msd state_dict_to_load model assertTrue osd state_dict_to_load optimizer set_state_dict calls load_state_dict model optimizer so we should see optim_ param_groups learning rate instead now set_state_dict model_ optim_ model_state_dict=state_dict_to_load model optim_state_dict=state_dict_to_load optimizer assertEqual msd get_model_state_dict model_ assertEqual osd get_optimizer_state_dict model_ optim_ assertEqual optim_ param_groups lr instantiate_parametrized_tests TestE ESaveAndLoad __name__ == __main__ run_tests