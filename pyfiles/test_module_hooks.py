Owner s module nn gc math pickle unittest warnings weakref collections namedtuple OrderedDict copy deepcopy functools partial tempfile NamedTemporaryFile typing Any torch torch nn nn torch testing _internal common_nn _create_basic_net NNTestCase torch testing _internal common_utils instantiate_parametrized_tests IS_WINDOWS parametrize parametrize_test run_tests skipIfTorchDynamo swap TestCase Net nn Module __init__ - None super __init__ seq = nn Sequential nn Linear _ range seq = nn Sequential nn Linear _ range forward x torch Tensor - torch Tensor seq seq x ToyNamedTuple = namedtuple ToyNamedTuple content ToyModel nn Module __init__ with_named_tuple=False - None super __init__ net = Net net = Net with_named_tuple = with_named_tuple forward x torch Tensor - torch Tensor res = net net x with_named_tuple ToyNamedTuple res res forward_hook TestCase fired_hooks list int expected_module nn Module hook_id int module nn Module inp tuple torch Tensor out torch Tensor - None fired_hooks append hook_id assertEqual id module id expected_module assertEqual len inp forward_pre_hook TestCase fired_hooks list int expected_module nn Module hook_id int module nn Module inp tuple torch Tensor - None fired_hooks append hook_id assertEqual id module id expected_module assertEqual len inp full_backward_hook TestCase fired_hooks list int expected_module nn Module hook_id int module nn Module grad_input tuple torch Tensor grad_output tuple torch Tensor - None fired_hooks append hook_id assertEqual id module id expected_module assertEqual len grad_input assertEqual len grad_output full_backward_pre_hook TestCase fired_hooks list int expected_module nn Module hook_id int module nn Module grad_input tuple torch Tensor - None fired_hooks append hook_id assertEqual id module id expected_module assertEqual len grad_input KwargModel nn Module __init__ - None super __init__ net = Net net = Net forward x torch Tensor bias torch Tensor = None - torch Tensor bias None x = x + bias x internal_forward_hook module nn Module args tuple torch Tensor kwargs dict str Any out torch Tensor out + kwargs bias FailsInForwardModel nn Module __init__ - None super __init__ net = Net forward x torch Tensor fail bool = True - torch Tensor fail raise RuntimeError failing forward net x kwarg_forward_pre_hook TestCase fired_hooks list int expected_module nn Module hook_id int module nn Module args tuple torch Tensor kwargs dict str Any - tuple Any Any fired_hooks append hook_id assertEqual id module id expected_module assertEqual len args kwargs bias = kwargs bias args kwargs kwarg_forward_hook TestCase fired_hooks list int expected_module nn Module hook_id int module nn Module args tuple torch Tensor kwargs dict str Any out torch Tensor - Any fired_hooks append hook_id assertEqual id module id expected_module assertEqual len args out = out + kwargs bias out DummyContextManager __init__ inp input = inp __enter__ args kwargs input append __exit__ args kwargs input append - TestModuleHooks TestCase parametrize_test named_tuple True False test_forward_hooks named_tuple fired_hooks list int = model = ToyModel named_tuple x = torch randn hook = partial forward_hook fired_hooks model net seq model net seq register_forward_hook partial hook model net seq register_forward_hook partial hook prepend=True model net seq register_forward_hook partial hook model net seq register_forward_hook partial hook model net seq register_forward_hook partial hook prepend=True expected = assertEqual fired_hooks out = model x assertEqual fired_hooks expected assertIsInstance out ToyNamedTuple named_tuple tuple out sum backward assertEqual fired_hooks expected model x sum backward assertEqual fired_hooks expected + expected parametrize_test named_tuple True False test_forward_pre_hooks named_tuple fired_hooks list int = model = ToyModel named_tuple x = torch randn hook = partial forward_pre_hook fired_hooks model net seq model net seq register_forward_pre_hook partial hook prepend=True model net seq register_forward_pre_hook partial hook model net seq register_forward_pre_hook partial hook model net seq register_forward_pre_hook partial hook model net seq register_forward_pre_hook partial hook prepend=True expected = assertEqual fired_hooks out = model x assertEqual fired_hooks expected assertIsInstance out ToyNamedTuple named_tuple tuple out sum backward assertEqual fired_hooks expected model x sum backward assertEqual fired_hooks expected + expected parametrize_test named_tuple True False test_full_backward_hooks named_tuple fired_hooks list int = model = ToyModel named_tuple x = torch randn hook = partial full_backward_hook fired_hooks model net model net register_full_backward_hook partial hook model net register_full_backward_hook partial hook model net register_full_backward_hook partial hook model net register_full_backward_hook partial hook prepend=True model net register_full_backward_hook partial hook prepend=True expected = assertEqual fired_hooks out = model x assertEqual fired_hooks assertIsInstance out ToyNamedTuple named_tuple tuple out sum backward assertEqual fired_hooks expected model x sum backward assertEqual fired_hooks expected + expected parametrize_test named_tuple True False test_full_backward_pre_hooks named_tuple fired_hooks list int = model = ToyModel named_tuple x = torch randn hook = partial full_backward_pre_hook fired_hooks model net model net register_full_backward_pre_hook partial hook prepend=True model net register_full_backward_pre_hook partial hook prepend=True model net register_full_backward_pre_hook partial hook model net register_full_backward_pre_hook partial hook model net register_full_backward_pre_hook partial hook expected = assertEqual fired_hooks out = model x assertEqual fired_hooks assertIsInstance out ToyNamedTuple named_tuple tuple out sum backward assertEqual fired_hooks expected model x sum backward assertEqual fired_hooks expected + expected Backward pre hook can affect subsequent gradient computation rg True False = torch ones requires_grad=rg model = nn Linear fn _unused_module grad_output grad_output model register_full_backward_pre_hook fn out = model out sum backward assertEqual model weight grad torch zeros rg assertEqual grad torch zeros_like assertIsNone grad parametrize_test named_tuple True False test_mixed_hooks named_tuple fired_hooks list int = model = ToyModel named_tuple x = torch randn model register_forward_pre_hook partial forward_pre_hook fired_hooks model model register_forward_hook partial forward_hook fired_hooks model model register_full_backward_pre_hook partial full_backward_pre_hook fired_hooks model model register_full_backward_hook partial full_backward_hook fired_hooks model assertEqual fired_hooks out = model x assertEqual fired_hooks assertIsInstance out ToyNamedTuple named_tuple tuple out sum backward assertEqual fired_hooks model x sum backward assertEqual fired_hooks test_kwarg_hooks test forward pre hook fired_hooks list int = x torch Tensor = torch ones bias torch Tensor = torch ones model = KwargModel model register_forward_pre_hook partial kwarg_forward_pre_hook fired_hooks model with_kwargs=True forward-pre bias = bias So out = x + bias assertEqual fired_hooks out = model x bias=bias assertEqual fired_hooks assertEqual out x + bias rtol= atol= e- test forward pre forward hooks fired_hooks list int = x torch Tensor = torch ones bias torch Tensor = torch ones model = KwargModel model register_forward_hook partial kwarg_forward_hook fired_hooks model with_kwargs=True model register_forward_pre_hook partial kwarg_forward_pre_hook fired_hooks model with_kwargs=True forward-pre bias = bias forward out = x + bias forward-post out = out + bias So out = x + bias assertEqual fired_hooks out = model x bias=bias assertEqual fired_hooks assertEqual out x + bias rtol= atol= e- test nn Module member method forward-post hook x torch Tensor = torch ones bias torch Tensor = torch ones model = KwargModel model register_forward_hook model internal_forward_hook with_kwargs=True forward out = x + bias forward-post out = out + bias So out = x + bias out = model x bias=bias assertEqual out x + bias rtol= atol= e- test_remove_kwarg_hooks test forward pre forward hooks fired_hooks list int = x torch Tensor = torch ones bias torch Tensor = torch ones model = KwargModel forward_hook_handle = model register_forward_hook partial kwarg_forward_hook fired_hooks model with_kwargs=True forward_pre_hook_handle = model register_forward_pre_hook partial kwarg_forward_pre_hook fired_hooks model with_kwargs=True forward-pre bias = bias forward out = x + bias forward-post out = out + bias So out = x + bias assertEqual fired_hooks out = model x bias=bias assertEqual fired_hooks assertEqual out x + bias rtol= atol= e- forward-pre bias = bias forward out = x + bias So out = x + bias forward_hook_handle remove out = model x bias=bias assertEqual fired_hooks assertEqual out x + bias rtol= atol= e- assertFalse forward_hook_handle id model _forward_hooks_with_kwargs forward out = x + bias So out = x + bias forward_pre_hook_handle remove out = model x bias=bias assertEqual fired_hooks assertEqual out x + bias rtol= atol= e- assertFalse forward_pre_hook_handle id model _forward_pre_hooks_with_kwargs test_always_called_forward_hooks x torch Tensor = torch ones model = FailsInForwardModel stack = ctx = None setup_context nonlocal ctx ctx = DummyContextManager stack ctx_setup_hook m i setup_context ctx __enter__ ctx_setup_failure_hook m i setup_context ctx __enter__ raise RuntimeError failing ctx setup ctx_shutdown_hook m i o ctx __exit__ ctx_shutdown_failure_hook m i o ctx __exit__ raise RuntimeError failing ctx shutdown throw_hook m i o raise RuntimeError failing throw forward_pre_hook_handle = model register_forward_pre_hook ctx_setup_hook forward_hook_handle = model register_forward_hook ctx_shutdown_hook always_call=True assertTrue len model _forward_hooks_always_called == make sure always_called forward hook runs when model forward raises RuntimeError assertRaisesRegex RuntimeError failing forward model x assertEqual stack - make sure always_called forward hook does run twice there no error model x fail=False assertEqual stack - - make sure always_called forward hook runs when forward pre hook raises RuntimeError forward_pre_hook_handle remove model register_forward_pre_hook ctx_setup_failure_hook assertRaisesRegex RuntimeError failing ctx setup model x fail=False assertEqual stack - - - make sure always_called hook runs when another always_called forward hook raises error forward_hook_handle = model register_forward_hook throw_hook prepend=True always_call=True error raised should error forced hook assertRaisesRegex RuntimeError failing ctx setup model x fail=False assertEqual stack - - - - make sure always called forward hooks properly removed forward_hook_handle remove forward_hook_handle remove assertTrue len model _forward_hooks_always_called == make sure always called forward hook run twice fails while running forward_hook_handle = model register_forward_hook ctx_shutdown_failure_hook always_call=True assertRaisesRegex RuntimeError failing ctx setup model x fail=False assertEqual stack - - - - - forward_hook_handle remove global_forward_hook_handle = nn modules module register_module_forward_hook ctx_shutdown_hook always_call=True assertTrue len nn modules module _global_forward_hooks_always_called == make sure global forward hook runs when forward pre hook raises RuntimeError assertRaisesRegex RuntimeError failing ctx setup model x fail=False assertEqual stack - - - - - - make sure forced global forward hook properly removed global_forward_hook_handle remove assertTrue len nn modules module _global_forward_hooks_always_called == assertRaisesRegex RuntimeError failing ctx setup model x assertEqual stack - - - - - - test_bw_hook_warning_for_non_tensor_or_tuple Test verify backward hook raises warning result Tensor tuple Tensors counter = forward backward fw_pre_hook module nn Module _inputs counter forward += fw_hook module nn Module _inputs _outputs counter forward += bw_hook module nn Module _inputs _outputs counter backward += TestModule nn Module forward dict inp = dict x x = torch nn functional softmax inp dim= x x x = torch ones requires_grad=True model = TestModule model register_forward_pre_hook fw_pre_hook model register_forward_hook fw_hook model register_full_backward_pre_hook bw_hook model register_full_backward_hook bw_hook warnings catch_warnings record=True w y = model x x x loss = y sum loss backward assertEqual counter forward assertEqual counter backward assertEqual len w assertTrue should Tensor tuple Tensors str w message _hook_to_pickle args kwargs pass TestStateDictHooks TestCase swap True False test_load_state_dict_pre_hook m = nn Linear m_state_dict = m state_dict m_load = nn Linear hook_called = hook_without_module state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs assertEqual m_state_dict state_dict nonlocal hook_called hook_called += hook_with_module module state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs assertEqual m_state_dict state_dict assertTrue m_load module nonlocal hook_called hook_called += hook_called = Test private API since sets with_module=False which diverges public API m_load _register_load_state_dict_pre_hook hook_without_module m_load load_state_dict m_state_dict assertEqual hook_called hook_called = m_load register_load_state_dict_pre_hook hook_with_module m_load load_state_dict m_state_dict assertEqual hook_called Test private API with_module=True hook_called = m_load _register_load_state_dict_pre_hook hook_with_module True m_load load_state_dict m_state_dict assertEqual hook_called test_no_extra_ref_to_module try gc disable m = nn Linear m register_load_state_dict_pre_hook _hook_to_pickle weak_m = weakref ref m del m assertEqual weak_m None finally gc enable test_pickled_hook m = nn Linear m register_load_state_dict_pre_hook _hook_to_pickle pickle loads pickle dumps m swap True False test_load_state_dict_module_pre_hook hook_called = Test module instance method hook MyModule nn Module __init__ - None super __init__ foo = torch nn Parameter torch rand my_pre_load_hook state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs assert == error_msgs assert == unexpected_keys assert == missing_keys assert strict nonlocal hook_called hook_called += my_pre_load_hook_with_module module state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs assert == error_msgs assert == unexpected_keys assert == missing_keys assert strict assert module nonlocal hook_called hook_called += Test hooks registered submodule also called appropriately i e submodule module argument my_pre_load_hook_with_module MyModuleContainer nn Module __init__ mod super __init__ mod = mod ctor MyModuleContainer lambda x x m = ctor MyModule state_dict = m state_dict isinstance m MyModuleContainer mod = m mod mod = m hook_called = Test private API since sets with_module=False which diverges public API mod _register_load_state_dict_pre_hook mod my_pre_load_hook m load_state_dict state_dict assertEqual hook_called hook_called = mod register_load_state_dict_pre_hook mod my_pre_load_hook_with_module m load_state_dict state_dict assertEqual hook_called swap True False test_load_state_dict_post_hook hook_called = MyModule nn Module __init__ - None super __init__ foo = torch nn Parameter torch rand my_post_load_hook module incompatible_keys assert module nonlocal hook_called incompatible_keys missing_keys append foo incompatible_keys unexpected_keys append bar hook_called += nested = MyModule wrapped = nn ModuleList nested handle = nested register_load_state_dict_post_hook nested my_post_load_hook Hook must called even wrapped ret = wrapped load_state_dict wrapped state_dict strict=False assertEqual hook_called Ensure hook modified missing_keys unexpected_keys missing = ret missing_keys unexpected = ret unexpected_keys assertEqual missing foo assertEqual unexpected bar When called strict=True error raised should mention missing unexpected keys hook added assertRaisesRegex RuntimeError foo \n bar wrapped load_state_dict wrapped state_dict strict=True assertEqual hook_called Removing hook via handle remove should cause fire anymore handle remove Hook did run so should have added any keys ret = wrapped load_state_dict wrapped state_dict strict=False assertEqual ret missing_keys assertEqual ret unexpected_keys hook_called should have been incremented assertEqual hook_called load_hook_clear_incompatible module incompatible_keys incompatible_keys missing_keys clear incompatible_keys unexpected_keys clear nested register_load_state_dict_post_hook load_hook_clear_incompatible state_dict = wrapped state_dict state_dict extra = torch ones load state_dict strict=True should throw ret = wrapped load_state_dict state_dict strict=True explicitly ensure post hook clearned out incompatible_keys assertEqual ret missing_keys assertEqual ret unexpected_keys unittest skipIf IS_WINDOWS Tempfile permission issue windows swap True False test_load_state_dict_post_hook_backward_compatibility my_post_load_hook mod _ nonlocal called called = True m nn Softmin nn Softmax nn LogSoftmax called = False sd = deepcopy m state_dict assertTrue hasattr m _load_state_dict_post_hooks Simulate older model did have attr delattr m _load_state_dict_post_hooks Save load ensure load_state_dict works without proper BC we would run into errors because attribute would expected In particular Softmax runs into issue described here https github com pytorch pytorch issues NamedTemporaryFile f Note torch save torch load recommended save load modules torch save m f name weights_only=False legacy code saves model m = torch load f name weights_only=False m load_state_dict sd assertFalse called Ensure hooks can registered called m register_load_state_dict_post_hook my_post_load_hook m load_state_dict sd assertTrue called _test_register_state_dict_pre_hook model submodule _state_dict_prefix = foo state_dict_pre_hook_count = keep_var_setting = False my_state_dict_pre_hook module prefix keep_vars assertEqual keep_vars keep_var_setting nonlocal state_dict_pre_hook_count state_dict_pre_hook_count += assertTrue prefix startswith _state_dict_prefix model register_state_dict_pre_hook my_state_dict_pre_hook Test ensure submodules run hook well submodule register_state_dict_pre_hook my_state_dict_pre_hook check_results model nonlocal state_dict_pre_hook_count keep_var_setting keep_var_setting True False _ = model state_dict prefix=_state_dict_prefix keep_vars=keep_var_setting assertEqual state_dict_pre_hook_count state_dict_pre_hook_count = Test state dict works expected after model construction check_results model Test state dict works expected after forward model torch ones check_results model test_register_state_dict_pre_hook MyModule torch nn Module __init__ - None super __init__ = nn Sequential nn Linear nn Linear nn Linear forward x x mod = MyModule _test_register_state_dict_pre_hook mod mod test_register_state_dict_pre_hook_lazy_module MyLazyModule torch nn Module __init__ - None super __init__ layer = nn LazyLinear layer = nn LazyLinear forward x layer layer x mod = MyLazyModule _test_register_state_dict_pre_hook mod mod layer unittest skipIf IS_WINDOWS Tempfile permission issue windows test_register_state_dict_pre_hook_backward_compat called = False my_state_dict_pre_hook args kwargs nonlocal called called = True m = nn Linear assertTrue hasattr m _state_dict_pre_hooks delattr m _state_dict_pre_hooks Save load ensure we can still call state_dict without running into issues NamedTemporaryFile f Note torch save torch load recommended save load modules torch save m f name weights_only=False legacy code saves model m = torch load f name weights_only=False Ensure we can run state_dict without issues _ = m state_dict assertFalse called m register_state_dict_pre_hook my_state_dict_pre_hook _ = m state_dict assertTrue called parametrize_test private True False test_register_state_dict_post_hook private m = nn Transformer d_model= nhead= num_encoder_layers= num_decoder_layers= linear_state_dict_post_hook module state_dict prefix local_metadata name _param module named_parameters recurse=False state_dict prefix + name = torch nn Parameter state_dict prefix + name register_linear_hook module isinstance module nn Linear hook_registration_fn = module _register_state_dict_hook private module register_state_dict_post_hook hook_registration_fn linear_state_dict_post_hook _check_sd state_dict k v m state_dict items linear k out_proj k assertTrue isinstance v torch nn Parameter assertFalse isinstance v torch nn Parameter verify type hook registered child submodules has no effect regardless whether using public private API m apply register_linear_hook _check_sd m state_dict verify type hook registered root module has no effect public API has effect private API hook_registration_fn = m _register_state_dict_hook private m register_state_dict_post_hook fn m s p l OrderedDict hook_registration_fn fn private assertFalse hasattr fn _from_public_api assertTrue len m state_dict == assertTrue hasattr fn _from_public_api assertRaisesRegex RuntimeError state_dict post-hook must None m state_dict assertRaisesRegex RuntimeError previously registered via register_state_dict_post_hook m _register_state_dict_hook fn TestModuleGlobalHooks TestCase tearDown nn modules module _global_backward_hooks = OrderedDict nn modules module _global_forward_hooks = OrderedDict nn modules module _global_forward_pre_hooks = OrderedDict skipIfTorchDynamo TorchDynamo does work well hooks test_module_global_hooks module = nn Sigmoid module_ = module module_ = module module_ = module input = torch ones requires_grad=True counter = forwards backwards fw_hook inc h_module input output assertIsInstance input tuple assertTrue isinstance output torch Tensor assertTrue isinstance h_module module assertEqual input torch ones assertEqual output torch empty fill_ + math e counter forwards += inc bw_hook inc h_module grad_input grad_output assertIsInstance grad_input tuple assertIsInstance grad_output tuple assertTrue isinstance h_module module assertEqual grad_output torch ones counter backwards += inc test_fwd = nn modules module register_module_forward_hook lambda args fw_hook args module_ input module_ input module_ input assertEqual counter forwards assertEqual counter backwards test_bwd = nn modules module register_module_backward_hook lambda args bw_hook args output_ = module_ input output_ = module_ input output_ = module_ input assertEqual counter forwards assertEqual counter backwards output_ backward torch ones retain_graph=True output_ backward torch ones retain_graph=False output_ backward torch ones retain_graph=False assertEqual counter forwards assertEqual counter backwards output_ backward torch ones retain_graph=True assertEqual counter forwards assertEqual counter backwards test _fwd = nn modules module register_module_forward_hook lambda args fw_hook args module_ input module_ input module_ input assertEqual counter forwards assertEqual counter backwards test _bwd = nn modules module register_module_backward_hook lambda args bw_hook args module_ input backward torch ones assertEqual counter forwards assertEqual counter backwards test _bwd remove module_ input backward torch ones assertEqual counter forwards assertEqual counter backwards test _fwd remove module_ input backward torch ones assertEqual counter forwards assertEqual counter backwards test_fwd remove test_bwd remove test_module_global_hook_invalid_outputs module = nn Sigmoid input = torch randn requires_grad=True bw_fail grad_input grad_output grad_input - bw_fail grad_input grad_output grad_input + torch randn nn modules module register_module_backward_hook bw_fail assertRaisesRegex RuntimeError got expected module input sum backward nn modules module register_module_backward_hook bw_fail assertRaisesRegex RuntimeError got expected module input sum backward test_module_backward_global_hook_writeable module = nn Sigmoid input = torch randn requires_grad=True sig_x = torch sigmoid input bw_hook module grad_input grad_output grad grad_input assertTrue isinstance grad torch Tensor grad grad_output assertTrue isinstance grad torch Tensor tuple gi gi grad_input nn modules module register_module_backward_hook bw_hook module input backward torch ones expected_grad = sig_x - sig_x assertEqual input grad expected_grad skipIfTorchDynamo TorchDynamo does work well hooks test_module_global_forward_preforward_hook_writeable module = nn Sigmoid input = torch randn requires_grad=True sig_x = torch sigmoid input forward_pre_hook m input torch nn functional relu input forward_hook m input output -output nn modules module register_module_forward_pre_hook forward_pre_hook nn modules module register_module_forward_hook forward_hook output = module input expected_res = -torch sigmoid torch nn functional relu input assertEqual output expected_res output backward torch ones retain_graph=True mask = input expected_grad = -sig_x - sig_x mask assertEqual input grad expected_grad test_module_forward_preforward_hook_removable This test test when multiple pre-forward hook functions can registered successfully used correctly handle can removable during pre-forward hook function call module = nn Sigmoid removable_hook m input nonlocal handle handle remove input removable_hook_ m input nonlocal handle_ handle_ remove input handle = module register_forward_pre_hook removable_hook handle_ = module register_forward_pre_hook removable_hook_ make sure hook register successful assertEqual len handle hooks_dict_ref assertEqual len handle_ hooks_dict_ref input = torch randn output = module input assertEqual torch sigmoid input output make sure hook removal successful assertFalse handle id handle hooks_dict_ref assertFalse handle_ id handle hooks_dict_ref assertEqual len handle hooks_dict_ref assertEqual len handle_ hooks_dict_ref test_module_forward_forward_hook_removable This test test when multiple forward hook functions can registered successfully used correctly handle can removable during forward hook function call module = nn Sigmoid removable_hook m input output nonlocal handle handle remove output removable_hook_ m input output nonlocal handle_ handle_ remove output handle = module register_forward_hook removable_hook handle_ = module register_forward_hook removable_hook_ make sure hook register successful assertEqual len handle hooks_dict_ref assertEqual len handle_ hooks_dict_ref input = torch randn output = module input assertEqual torch sigmoid input output make sure hook removal successful assertFalse handle id handle hooks_dict_ref assertFalse handle_ id handle hooks_dict_ref assertEqual len handle hooks_dict_ref assertEqual len handle_ hooks_dict_ref skipIfTorchDynamo TorchDynamo does work well hooks test_global_and_local_hooks_order module = nn Sigmoid global_forward_pre_called = False local_forward_pre_called = False global_forward_called = False local_forward_called = False global_backward_called = False local_backward_called = False global_forward_pre_hook m input nonlocal global_forward_pre_called assertTrue local_forward_pre_called global_forward_pre_called = True input local_forward_pre_hook m input nonlocal local_forward_pre_called assertTrue global_forward_pre_called local_forward_pre_called = True input global_forward_hook m input output nonlocal global_forward_called assertTrue local_forward_called global_forward_called = True output local_forward_hook m input output nonlocal local_forward_called assertTrue global_forward_called local_forward_called = True output global_backward_hook m input output nonlocal global_backward_called assertTrue local_backward_called global_backward_called = True input local_backward_hook m input output nonlocal local_backward_called assertTrue global_backward_called local_backward_called = True input input = torch randn requires_grad=True nn modules module register_module_forward_pre_hook global_forward_pre_hook module register_forward_pre_hook local_forward_pre_hook nn modules module register_module_forward_hook global_forward_hook module register_forward_hook local_forward_hook nn modules module register_module_backward_hook global_backward_hook module register_backward_hook local_backward_hook output = module input assertTrue local_forward_called local_forward_pre_called global_forward_called global_forward_pre_called output backward torch ones retain_graph=True assertTrue local_backward_called global_backward_called skipIfTorchDynamo TorchDynamo does work well hooks test_module_global_hooks_with_kwargs kwarg_global_forward_hook module nn Module args tuple torch Tensor kwargs dict str Any out torch Tensor - Any out = out + kwargs bias out model = KwargModel nn modules module register_module_forward_hook kwarg_global_forward_hook with_kwargs=True x torch Tensor = torch randn bias torch Tensor = torch randn out = model x bias=bias assertEqual out x + bias rtol= atol= e- TestModuleHookNN NNTestCase _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True _test_hooks backward_register_fn module = nn Sigmoid input = torch ones requires_grad=True counter = forwards backwards fw_hook inc h_module input output assertIsInstance input tuple assertTrue isinstance output torch Tensor assertTrue h_module module assertEqual input torch ones assertEqual output torch empty fill_ + math e counter forwards += inc bw_hook inc h_module grad_input grad_output assertIsInstance grad_input tuple assertIsInstance grad_output tuple assertTrue h_module module assertEqual grad_output torch ones counter backwards += inc backward_pre_hook expects callback only ` module ` ` grad_output ` arguments bw_pre_hook inc h_module grad_output assertIsInstance grad_output tuple assertTrue h_module module assertEqual grad_output torch ones counter backwards += inc test_fwd = module register_forward_hook lambda args fw_hook args module input module input assertEqual counter forwards assertEqual counter backwards bw_hook_fn = bw_pre_hook backward_register_fn == register_full_backward_pre_hook bw_hook test_bwd = getattr module backward_register_fn lambda args bw_hook_fn args output = module input assertEqual counter forwards assertEqual counter backwards output backward torch ones retain_graph=True assertEqual counter forwards assertEqual counter backwards output backward torch ones retain_graph=True assertEqual counter forwards assertEqual counter backwards test _fwd = module register_forward_hook lambda args fw_hook args output = module input assertEqual counter forwards assertEqual counter backwards test _bwd = getattr module backward_register_fn lambda args bw_hook_fn args module input backward torch ones assertEqual counter forwards assertEqual counter backwards test _bwd remove module input backward torch ones assertEqual counter forwards assertEqual counter backwards test _fwd remove module input backward torch ones assertEqual counter forwards assertEqual counter backwards test_fwd remove test_bwd remove test_hooks _test_hooks register_backward_hook _test_hooks register_full_backward_hook _test_hooks register_full_backward_pre_hook test_hook_cpp bn = nn BatchNorm d hook module grad_inputs grad_outputs assertEqual len grad_inputs assertEqual len grad_outputs assertEqual module bn bn register_full_backward_hook hook output = bn torch randn requires_grad=True output sum backward test_backward_hooks_interaction Test make sure grad_outputs updated full_backward_pre_hook received full_backward_hook module = torch nn Sigmoid cnt = backward_cnt bw_pre_hook m grad_output cnt backward_cnt += grad_output bw_hook m grad_in grad_output assertEqual torch full_like grad_output grad_output cnt backward_cnt += grad_output module register_full_backward_pre_hook bw_pre_hook module register_full_backward_hook bw_hook t = torch ones requires_grad=True module t sum backward assertEqual cnt backward_cnt test_hook_invalid_outputs module = nn Sigmoid input = torch randn requires_grad=True bw_fail grad_input grad_output grad_input - bw_fail grad_input grad_output grad_input + torch randn module register_backward_hook bw_fail assertRaisesRegex RuntimeError got expected module input sum backward module register_backward_hook bw_fail assertRaisesRegex RuntimeError got expected module input sum backward bw_pre_fail grad_output bw_pre_fail grad_output grad_output + torch randn module register_full_backward_pre_hook bw_pre_fail assertRaisesRegex RuntimeError got expected module input sum backward module register_full_backward_pre_hook bw_pre_fail assertRaisesRegex RuntimeError got expected module input sum backward test_hook_requires_grad test_self = MyModule nn Module forward arg arg arg test_self assertTrue arg requires_grad test_self assertFalse arg requires_grad test_self assertTrue arg requires_grad arg sum + arg sum + arg sum inp = torch rand requires_grad=True mod = MyModule mod inp inp detach inp Ensure requires grad properly propagated mod register_full_backward_hook lambda mod gI gO None mod inp inp detach inp test_hook_no_requires_grad mod = nn Linear inp = torch rand return_val = None hook_called = hook mod grad_input grad_output hook_called += gI grad_input assertIsNone gI gO grad_output assertEqual gO size return_val == grad_input grad_input return_val == invalid If inputs requiring gradients would valid inp return_val == None None raise RuntimeError Invalid return_val string mod register_full_backward_hook hook This should run trigger hook properly assertWarnsRegex UserWarning Full backward hook firing when gradients computed respect module outputs since no inputs require gradients mod inp sum backward assertEqual hook_called return_val = grad_input mod inp sum backward assertEqual hook_called return_val = invalid assertRaisesRegex RuntimeError where no input requires gradient mod inp sum backward test_hook_last_arg_requires_grad mod = nn L Loss inp = torch rand requires_grad=True mod register_full_backward_hook lambda m gI gO None try mod inp detach inp except Exception ex fail f Unexpected exception ex test_hook_extra_input MyModule nn Module forward non_tensor tensor tensor clone non_tensor inp = torch rand requires_grad=True mod = MyModule hook mod grad_input grad_output assertIsNone grad_input assertIsInstance grad_input torch Tensor assertIsInstance grad_output torch Tensor assertIsNone grad_output mod register_full_backward_hook hook out _ = mod True inp out sum backward test_hook_inplace MyModule nn Module forward inp do_inplace inp = inp do_inplace inp += inp clone hook_called = hook mod grad_input grad_output hook_called += hook_pre mod grad_output hook_called += inp = torch rand requires_grad=True mod = MyModule hook_fn register_fn hook mod register_full_backward_hook hook_pre mod register_full_backward_pre_hook hook_called = register_fn hook_fn No inplace should work mod inp False sum backward assertEqual hook_called Input inplace error should throw error assertRaisesRegex RuntimeError Output BackwardHookFunctionBackward view being modified inplace mod inp clone True Input inplace error should throw error we try re-use view after they have been modified local_inp = inp clone out = mod local_inp False local_inp = assertRaisesRegex RuntimeError Output BackwardHookFunctionBackward view its base another view Any operation involving view will fail here mod inp + Output inplace error should throw error out = mod inp False assertRaisesRegex RuntimeError BackwardHookFunctionBackward view being modified inplace out += test_hook_non_full_warning noop args pass = torch rand requires_grad=True b = torch rand requires_grad=True Check invalid input container MyModule nn Module forward l l clone l clone m = MyModule m register_backward_hook noop assertWarnsRegex FutureWarning does take input single Tensor tuple Tensors m b Check invalid output container MyModule nn Module forward b clone b clone m = MyModule m register_backward_hook noop assertWarnsRegex FutureWarning does single Tensor tuple Tensors m b Check invalid output different Nodes MyModule nn Module forward b clone b clone m = MyModule m register_backward_hook noop assertWarnsRegex FutureWarning outputs generated different autograd Nodes m b Check invalid forward multiple Nodes MyModule nn Module forward clone clone m = MyModule m register_backward_hook noop assertWarnsRegex FutureWarning forward contains multiple autograd Nodes m test_hook_backward_size Make module multiple operations forward And different size input outputs MyModule nn Module forward arg arg tmp = arg sum arg tmp = tmp + arg sum arg sum tmp = tmp sum view tmp = tmp expand contiguous tmp module = MyModule inp = torch randn requires_grad=True inp = torch randn requires_grad=True bw_hook module grad_input grad_output assertEqual len grad_input assertEqual grad_input size torch Size assertEqual grad_input size torch Size assertEqual len grad_output assertEqual grad_output size torch Size module register_full_backward_hook bw_hook module inp inp sum backward test_hook_backward_writeable module = nn Sigmoid input = torch randn requires_grad=True sig_x = torch nn functional sigmoid input bw_hook module grad_input grad_output grad grad_input assertTrue isinstance grad torch Tensor grad grad_output assertTrue isinstance grad torch Tensor tuple gi gi grad_input module register_backward_hook bw_hook module input backward torch ones expected_grad = sig_x - sig_x assertEqual input grad expected_grad test_hook_forward_preforward_writable module = nn Sigmoid input = torch randn requires_grad=True sig_x = torch nn functional sigmoid input forward_pre_hook m input torch nn functional relu input forward_hook m input output -output module register_forward_pre_hook forward_pre_hook module register_forward_hook forward_hook output = module input expected_res = -torch nn functional sigmoid torch nn functional relu input assertEqual output expected_res output backward torch ones retain_graph=True mask = input expected_grad = -sig_x - sig_x mask assertEqual input grad expected_grad test_hook_buffer_registration return_buffer True False buffer_registration_hook module name buffer buffer registered = True return_buffer buffer handle = torch nn modules module register_module_buffer_registration_hook buffer_registration_hook try l n s = _create_basic_net b s buffers assertTrue getattr b registered False finally handle remove test_hook_submodule_registration return_submodule True False module_registration_hook module name submodule module registered = True submodule registered = True return_submodule submodule handle = torch nn modules module register_module_module_registration_hook module_registration_hook try l n s = _create_basic_net m s modules assertTrue getattr m registered False finally handle remove test_hook_parameter_registration return_parameter True False parameter_registration_hook module name parameter parameter registered = True return_parameter parameter handle = torch nn modules module register_module_parameter_registration_hook parameter_registration_hook try l n s = _create_basic_net p s parameters assertTrue getattr p registered False finally handle remove instantiate_parametrized_tests TestModuleHooks instantiate_parametrized_tests TestStateDictHooks __name__ == __main__ run_tests