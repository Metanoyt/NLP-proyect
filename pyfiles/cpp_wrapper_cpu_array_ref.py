mypy allow-untyped-defs collections abc Sequence typing Any Callable Optional Union sympy torch torch _inductor async_compile noqa F required warm up AsyncCompile pools torch _ops config ir utils sympy_product virtualized V cpp_utils DTYPE_TO_CPP cpp_wrapper_cpu CppWrapperCpu wrapper BufferLike EnterSubgraphLine ExitSubgraphLine MemoryPlanningLine MemoryPlanningState PythonWrapperCodegen BufferName = str Default thread stack sizes vary platform - Linux MB - macOS KB - Windows MB Just pick something comfortably smaller than smallest now MAX_STACK_ALLOCATION_SIZE = CppWrapperCpuArrayRef CppWrapperCpu Generates cpp wrapper running CPU calls cpp kernels This forked CppWrapperCpu difference tensors may represented ArrayRef see torch csrc inductor aoti_runtime arrayref_tensor h __init__ super __init__ assert device == cpu ArrayRefTensor only supported CPU allow_stack_allocation = config aot_inductor allow_stack_allocation stack_allocated_buffers dict BufferName BufferLike = staticmethod create is_subgraph bool subgraph_name Optional str parent_wrapper Optional PythonWrapperCodegen partition_signatures Optional ir GraphPartitionSignature = None TODO - support subgraph codegen lifting functions Check comment CppWrapperCpu ` codegen_subgraph ` function CppWrapperCpuArrayRef staticmethod get_input_cpp_type input assert config aot_inductor use_minimal_arrayref_interface isinstance input sympy Expr graph may_get_constant_buffer_dtype dtype = may_get_constant_buffer_dtype input assert dtype None f Failed get dtype sympy Expr input DTYPE_TO_CPP dtype f ArrayRefTensor DTYPE_TO_CPP input get_dtype staticmethod get_device_include_path device str - str assert device == cpu ArrayRef only supported CPU V graph aot_mode #include torch csrc inductor aoti_include array_ref h #include torch csrc inductor cpp_wrapper array_ref h codegen_input_numel_asserts name buf V graph graph_inputs items isinstance buf sympy Expr continue comparing strides size tensor tricky Ignore them now sympy_product buf get_size == continue numel = buf get_numel prefix writeline f assert_numel name numel generate_extern_kernel_alloc args kwargs Disable stack allocation extern kernels allow_stack_allocation = False super generate_extern_kernel_alloc args kwargs generate_extern_kernel_out args kwargs Disable stack allocation extern kernels allow_stack_allocation = False super generate_extern_kernel_out args kwargs generate_fallback_kernel node ir FallbackKernel - None Disable stack allocation extern kernels allow_stack_allocation = False super generate_fallback_kernel node _generate_kernel_call_helper kernel_name str call_args device=None triton=True arg_types=None raw_keys=None raw_args=None triton_meta=None graph_name= original_fxnode_name=None Generates kernel call code triton Defines whether GPU backend uses Triton codegen Otherwise uses CUDA language codegen Only valid when cuda == True assert triton CppWrapperCpuArrayRef generate_kernel_call does support GPU assert arg_types None len call_args == len arg_types Mismatch call_args arg_types generate_kernel_call new_args = idx arg enumerate call_args arg_types idx var_name = f var_ next arg_var_id writeline f auto var_name = get_data_ptr_wrapper arg new_args append f arg_types idx var_name arg scalar new_args append arg debug printer related logic cpp kernel type debug_printer_manager = V graph wrapper_code debug_printer debug_printer_manager set_printer_args call_args kernel_name None None cpp debug_printer_manager writeline wrap_kernel_call kernel_name new_args write_wrapper_decl inputs_len = len V graph graph_inputs keys V graph aot_mode config aot_inductor use_minimal_arrayref_interface V graph is_const_graph input_cpp_types = join f CppWrapperCpuArrayRef get_input_cpp_type x x V graph graph_inputs values output_arrayref_types = join f ArrayRefTensor DTYPE_TO_CPP x get_dtype x V graph graph_outputs prefix splice f using AOTInductorModelInputs = std tuple input_cpp_types using AOTInductorModelOutputs = std tuple output_arrayref_types V graph const_module header splice V graph const_module wrapper_code header assert V graph const_wrapper_code None prefix splice V graph const_wrapper_code assert V graph const_kernel_code None kernel_declarations splice V graph const_kernel_code V graph is_const_graph prefix splice void AOTInductorModel _const_run_impl std vector AtenTensorHandle output_handles DeviceStreamType stream AOTIProxyExecutorHandle proxy_executor config aot_inductor use_runtime_constant_folding If we do split constant graph we ll just create empty implementation when wrapping main module prefix splice void AOTInductorModel _const_run_impl std vector AtenTensorHandle output_handles DeviceStreamType stream AOTIProxyExecutorHandle proxy_executor run_impl_proto = void AOTInductorModel run_impl AtenTensorHandle input_handles array input AtenTensorHandle handles stolen array itself borrowed AtenTensorHandle output_handles array writing output AtenTensorHandle handles will stolen caller array itself borrowed DeviceStreamType stream AOTIProxyExecutorHandle proxy_executor generate_input_output_runtime_checks run_impl_proto += __check_inputs_outputs input_handles output_handles config aot_inductor use_minimal_arrayref_interface prefix splice template AOTInductorModelOutputs AOTInductorModel run_impl_minimal_arrayref_interface AOTInductorModelInputs AOTInductorModelOutputs const AOTInductorModelInputs inputs DeviceStreamType stream AOTIProxyExecutorHandle proxy_executor suffix splice run_impl_proto suffix splice AOTInductorModelInputs inputs convert_handles_to_inputs input_handles inputs auto outputs = run_impl_minimal_arrayref_interface AOTInductorModelInputs AOTInductorModelOutputs inputs stream proxy_executor NOTE outputs full ArrayRef thread_local storage If future we need interface perform well DSO using minimal arrayref interface all we need do provide ThreadLocalCachedTensor each one convert_outputs_to_handles outputs output_handles suffix splice extern C AOTIRuntimeError AOTInductorModelRunMinimalArrayrefInterface AOTInductorModelHandle model_handle const AOTInductorModelInputs inputs AOTInductorModelOutputs outputs auto model = reinterpret_cast torch aot_inductor AOTInductorModel model_handle CONVERT_EXCEPTION_TO_ERROR_CODE outputs = model- run_impl_minimal_arrayref_interface AOTInductorModelInputs AOTInductorModelOutputs inputs torch aot_inductor DeviceStreamType nullptr nullptr prefix splice run_impl_proto cpp entry function JIT cpp wrapper prefix splice void inductor_entry_impl AtenTensorHandle input_handles array input AtenTensorHandle handles stolen array itself borrowed AtenTensorHandle output_handles array writing output AtenTensorHandle handles will stolen caller array itself borrowed prefix indent assign inputs outputs both cases so later codegen can simplified config aot_inductor use_minimal_arrayref_interface V graph is_const_graph V graph aot_mode num_args = len V graph graph_inputs Weights promoted JIT mode num_args = len V graph graph_inputs + len V graph constants release GIL support multiple instances inference different threads same process prefix splice py gil_scoped_release_simple release prefix splice f auto inputs = steal_from_raw_handles_to_raii_handles input_handles num_args inputs_len = idx input_key enumerate V graph graph_inputs keys config aot_inductor use_minimal_arrayref_interface prefix writeline f auto input_key = std get idx inputs continue unwrap input tensor back scalar isinstance V graph graph_inputs input_key sympy Expr graph may_get_constant_buffer_dtype dtype = may_get_constant_buffer_dtype V graph graph_inputs input_key type ignore arg-type assert dtype None Fails get dtype sympy Expr codegen_tensor_item dtype f inputs idx input_key prefix prefix writeline f auto input_key = std move inputs idx assert all isinstance v torch Tensor v list V graph constants values Expect all constants Tensor idx constants_key enumerate V graph constants keys V graph aot_mode Weights stored constants_ owned RAIIAtenTensorHandle there Don t call std move here because will cause constants_ lose ownership prefix writeline f auto constants_key = constants_- idx Append constants inputs graph constants_idx = inputs_len + idx prefix writeline f auto constants_key = std move inputs constants_idx codegen_inputs V graph aot_mode V graph is_const_graph config aot_inductor use_minimal_arrayref_interface TODO input shape checking regular tensor interface well codegen_input_numel_asserts prefix writeline inputs clear prefix writeline maybe_unused auto kernels = static_cast AOTInductorModelKernels this- kernels_ get generate_return output_refs list str cst_names = V graph constants keys arr_iface = V graph is_const_graph config aot_inductor use_minimal_arrayref_interface For brevity use_thread_local_cached_output_tensor idx output cached_output_name = f cached_output_ next cached_output_id cache_type = Array arr_iface Tensor wrapper_call writeline f thread_local ThreadLocalCachedOutput cache_type std decay_t decltype output f cached_output_name output arr_iface wrapper_call writeline f cached_output_name copy_data_from output output_entry = f std get idx output_arrayref_tensors element_type = f std decay_t decltype output_entry data wrapper_call writeline f output_entry = cached_output_name arrayref_tensor element_type wrapper_call writeline f cached_output_name copy_data_from output wrapper_call writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_new_uninitialized_tensor output_handles idx wrapper_call writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_assign_tensors cached_output_name tensor f output_handles idx arr_iface wrapper_call writeline AOTInductorModelOutputs output_arrayref_tensors output idx dict str int = idx output enumerate output_refs output == nullptr continue is_constant_buffer = output cst_names output_buffer = V graph graph_outputs idx isinstance output_buffer ir BaseView output_storage = output_buffer unwrap_view assert isinstance output_storage ir BaseView ir MutableBox isinstance output_storage data ir ConstantBuffer is_constant_buffer = True isinstance output_buffer ir ShapeAsConstantBuffer Need wrap scalar into tensor main function returns vector tensors output_tensor = codegen_scalar_to_tensor output wrapper_call writeline f output_handles idx = output_tensor release continue output_is_tensor_handle_expr = f std is_same_v std decay_t decltype output RAIIAtenTensorHandle &#124; &#124; f std is_same_v std decay_t decltype output AtenTensorHandle &#124; &#124; f std is_same_v std decay_t decltype output ConstantHandle wrapper_call writeline f constexpr output_is_tensor_handle_expr wrapper_call indent arr_iface cached_output_name = f cached_output_ next cached_output_id wrapper_call writeline f thread_local RAIIAtenTensorHandle cached_output_name is_constant_buffer NOTE return_constant In some rare cases where we constant we have copy constant because constants owned Model instance constants remain same cross inference runs assuming they updated runtime Basically we cannot release transfer ownership any original constant user wrapper_call writeline f AtenTensorHandle cached_output_name _tmp wrapper_call writeline f aoti_torch_clone output cached_output_name _tmp wrapper_call writeline f cached_output_name = cached_output_name _tmp wrapper_call writeline f cached_output_name = output release wrapper_call writeline f convert_handle_to_arrayref_tensor cached_output_name f std get idx output_arrayref_tensors is_constant_buffer See NOTE return_constant above wrapper_call writeline f aoti_torch_clone output output_handles idx output output idx src_idx = output idx output wrapper_call writeline f output_handles idx = output_handles src_idx wrapper_call writeline f output_handles idx = output release wrapper_call writeline wrapper_call indent use_thread_local_cached_output_tensor idx output wrapper_call writeline output output idx output idx output = idx arr_iface wrapper_call writeline output_arrayref_tensors memory_plan memory_planning MemoryPlanner lines = MemoryPlanner plan lines TODO integrate memory planning stack allocation allow_stack_allocation = False memory_plan_reuse out_names = V graph get_output_names while lines isinstance lines - MemoryPlanningLine TODO seems legit NullLine has no node lines - node name out_names type ignore attr-defined these lines will pointless lines pop codegen allocations two passes planning_states = MemoryPlanningState past_planning_states = i range len lines line = lines i isinstance line MemoryPlanningLine lines i = line plan planning_states - isinstance line EnterSubgraphLine planning_states append MemoryPlanningState isinstance line ExitSubgraphLine past_planning_states append planning_states pop past_planning_states append planning_states pop assert len planning_states == conservatively use sum all allocated buffer sizes potentially nested scopes total allocated size total_allocated_buffer_size = sum s total_allocated_buffer_size s past_planning_states allow_stack_allocation = allow_stack_allocation False config aot_inductor allow_stack_allocation total_allocated_buffer_size = MAX_STACK_ALLOCATION_SIZE can_stack_allocate_buffer buffer allow_stack_allocation buffer get_device type == cpu can_prove_buffer_has_static_shape buffer ir is_contiguous_strides_for_shape buffer get_stride buffer get_size make_buffer_free buffer isinstance buffer get_output_spec ir MultiOutputLayout V graph aot_mode buffer get_name stack_allocated_buffers config aot_inductor use_minimal_arrayref_interface V graph aot_mode buffer get_name V graph graph_inputs f buffer get_name reset make_buffer_allocation buffer make_allocation buffer get_name buffer get_device buffer get_dtype buffer get_size buffer get_stride buffer can_stack_allocate_buffer buffer None buffer get_is_pinned make_allocation name device dtype shape stride buffer_if_can_stack_allocate=None is_pinned=False orig_stride = stride device_str = codegen_device device dtype_code = codegen_dtype dtype size = codegen_shape_tuple shape stride = codegen_shape_tuple orig_stride size_array_var = codegen_int_array_var size wrapper_call writeline known_statically=self is_statically_known_list_of_ints shape graph=self get_codegened_graph stride_array_var = codegen_int_array_var stride wrapper_call writeline known_statically=self is_statically_known_list_of_ints orig_stride graph=self get_codegened_graph device_type device_id = device_str split device_idx = this- device_idx_ V graph aot_mode device_id buffer_if_can_stack_allocate None stack_allocated_buffers name = buffer_if_can_stack_allocate cpp_type = DTYPE_TO_CPP dtype numel = buffer_if_can_stack_allocate get_numel Note we don t zero storage because empty_strided doesn t zero either wrapper_call writeline f cpp_type name _storage numel args = f name _storage size_array_var stride_array_var device_type device_idx f ArrayRefTensor cpp_type name join args args = str len shape size_array_var stride_array_var dtype_code device_type device_idx f name _handle wrapper_call writeline f AtenTensorHandle name _handle pinned_str = _pinned is_pinned wrapper_call writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_empty_strided pinned_str join args f RAIIAtenTensorHandle name name _handle make_buffer_reuse old BufferLike new BufferLike delete_old bool assert old get_dtype == new get_dtype old_name = old get_name new_name = new get_name del_line = old_name V graph get_output_names delete_old del_line = f make_buffer_free old old get_size == new get_size old get_stride == new get_stride old_name stack_allocated_buffers stack_allocated_buffers new_name = new codegen_exact_buffer_reuse old_name new_name del_line reinterpret_view = codegen_reinterpret_view old new get_size new get_stride wrapper_call writeline reinterpret_view stack_allocated_buffers stack_allocated_buffers new_name = new The only way get into case via exact buffer reuse since all other options result new tensor handle codegen_exact_buffer_reuse old_name new_name del_line f declare new_name = reinterpret_view del_line reuse _assert_safe_to_use_borrow_arrayref_tensor_as_tensor Borrowing arguments shim functions only safe because we know arguments can t stack-allocated Otherwise sure we can t dangling pointer we need either certain shim function cannot alias borrowed argument certain returned Tensor shim function cannot escape assert is_safe_to_use_borrow_arrayref_tensor_as_tensor borrowing arguments shim functions unsafe stack allocation see comment above assertion is_safe_to_use_borrow_arrayref_tensor_as_tensor allow_stack_allocation stack_allocated_buffers generate_c_shim_extern_kernel_call kernel str args list str device str _ - None In abi_compatible mode we call fallback aten ops through C shim layer Setting allow_stack_allocation False because exchange between ArrayRefTensor Tensor still fragile allow_stack_allocation = False wrapped_args = arg args We only really need borrow_arrayref_tensor_as_tensor ArrayRefTensors The code flowing into here uses ` ` nullptr which borrow_arrayref_tensor_as_tensor would blindly coerce int so just avoid wrapping integers Name matching find tensor hacky fixing all ArrayRefTensor issues priority now isinstance arg str arg startswith buf arg wrap_with_raii_handle_if_needed _assert_safe_to_use_borrow_arrayref_tensor_as_tensor arg = f borrow_arrayref_tensor_as_tensor arg wrapped_args append arg super generate_c_shim_extern_kernel_call kernel wrapped_args device debug_args=args generate_scatter_fallback node ir ScatterFallback No stack allocation when there fallback op allow_stack_allocation = False super generate_scatter_fallback node _generate_scatter_fallback output inputs cpp_kernel_name python_kernel_name src_is_tensor reduce kwargs reduce = _get_scatter_reduce_enum reduce call ABI shim function instead ATen one cpp_kernel_name = get_c_shim_func_name cpp_kernel_name device TODO consider remove _out add missing inplace variants fallback_ops py cpp_kernel_name = cpp_kernel_name replace __ _ + _out _assert_safe_to_use_borrow_arrayref_tensor_as_tensor inputs_wrapped = f borrow_arrayref_tensor_as_tensor x isinstance x str str x x inputs line = f cpp_kernel_name borrow_arrayref_tensor_as_tensor output join inputs_wrapped python_kernel_name startswith aten scatter_reduce line += f join kwargs src_is_tensor reduce line += f V graph wrapper_code val_to_arg_str reduce assert reduce None Expect reduce None aten scatter_ scalar src line += writeline line generate_index_put_fallback node ir IndexPutFallback - None No stack allocation when there fallback op allow_stack_allocation = False super generate_index_put_fallback node _generate_index_put_fallback kernel x indices values accumulate _assert_safe_to_use_borrow_arrayref_tensor_as_tensor TODO update aoti_torch_index_put_out ir py use autogen out version See comment codegen_reinterpret_view about why having something like RAIIAtenTensorHandle tmp_tensor_handle_ tmp array can cause corresponding tensor prematurely deallocated thus temporary array trick here indices_str = _generate_temporary_array_pointer AtenTensorHandle f borrow_arrayref_tensor_as_tensor i i indices args = f borrow_arrayref_tensor_as_tensor x indices_str str len indices f borrow_arrayref_tensor_as_tensor values accumulate args insert f borrow_arrayref_tensor_as_tensor x set x output tensor fallback mutates x writeline wrap_kernel_call kernel args generate_fallback_kernel_with_runtime_lookup buf_name str python_kernel_name str get_args Callable Sequence str op_overload Union torch _ops OpOverload torch _ops HigherOrderOperator raw_args Sequence Any outputs Sequence ir Buffer - None No stack allocation when there fallback op allow_stack_allocation = False super generate_fallback_kernel_with_runtime_lookup buf_name python_kernel_name get_args op_overload raw_args outputs codegen_device_copy src dst non_blocking Union bool str aoti_torch_tensor_copy_ takes AtenTensorHandle input while stack-allocation results ArrayRefTensor so disable stack allocation here allow_stack_allocation = False writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_copy_ expensive_copy_to_tensor_if_needed dst src non_blocking codegen_reinterpret_view data size stride offset writeline Callable None dtype=None - str Returns newly-created temporary RAII tensor handle containing reinterpreted tensor data Callers function responsible saving handle persistent access needed dim = str len size create_reinterpret_call - str args = f data get_name dim codegen_int_array_var codegen_shape_tuple size writeline known_statically=self is_statically_known_list_of_ints size graph=self get_codegened_graph codegen_int_array_var codegen_shape_tuple stride writeline known_statically=self is_statically_known_list_of_ints stride graph=self get_codegened_graph offset f wrap_with_raii_handle_if_needed reinterpret_tensor_wrapper join args create_new_tensor_handle - tuple str list str Calling reset ArrayRefTensor does nothing since array const-allocated stack Thus s safe reference original array name = data get_name stack_allocated_buffers name tmp_AtenTensorHandle = f tmp_ name _ next tmp_tensor_id tmp_call_strs = f AtenTensorHandle tmp_AtenTensorHandle f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_new_tensor_handle data get_name tmp_AtenTensorHandle f RAIIAtenTensorHandle tmp_AtenTensorHandle tmp_call_strs size == data layout size stride == data layout stride offset == data layout offset dtype None dtype == data dtype final_tensor_str call_strs = create_new_tensor_handle line call_strs writeline line final_tensor_str super codegen_reinterpret_view data size stride offset writeline dtype val_to_arg_str val type_=None - str val None isinstance type_ torch OptionalType isinstance type_ getElementType torch TensorType Handle optional tensors special case parent base_handle = val_to_arg_str val torch TensorType config aot_inductor use_minimal_arrayref_interface is_safe_to_use_borrow_arrayref_tensor_as_tensor base_handle = f borrow_arrayref_tensor_as_tensor base_handle base_handle = f copy_arrayref_tensor_to_tensor base_handle f temporary_reference base_handle get super val_to_arg_str val type_ codegen_tensor_item dtype torch dtype tensor str scalar str indented_buffer=None dtype_str = str dtype split - writer = indented_buffer dtype == torch float dtype == torch bfloat scalar_tmp = f scalar _tmp writer writeline f DTYPE_TO_CPP dtype scalar_tmp We know item_ doesn t alias input so borrowing should safe tensor = f borrow_arrayref_tensor_as_tensor tensor writer writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_item_ dtype_str tensor scalar_tmp writer writeline f float scalar = float scalar_tmp writer writeline f DTYPE_TO_CPP dtype scalar We know item_ doesn t alias input so borrowing should safe tensor = f borrow_arrayref_tensor_as_tensor tensor writer writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_item_ dtype_str tensor scalar