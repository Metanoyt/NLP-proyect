Owner s module masked operators torch torch testing _internal common_utils TestCase run_tests make_tensor parametrize instantiate_parametrized_tests torch testing _internal common_device_type instantiate_device_type_tests ops torch testing _internal common_methods_invocations SampleInput binary_ufuncs reduction_ops unary_ufuncs torch masked as_masked_tensor masked_tensor _combine_input_and_mask torch masked maskedtensor core _masks_match _tensors_match torch masked maskedtensor unary NATIVE_INPLACE_UNARY_FNS NATIVE_UNARY_FNS UNARY_NAMES torch masked maskedtensor binary NATIVE_BINARY_FNS NATIVE_INPLACE_BINARY_FNS BINARY_NAMES torch masked maskedtensor reductions REDUCE_NAMES _compare_mt_t mt_result t_result rtol= e- atol= e- mask = mt_result get_mask mt_result_data = mt_result get_data mask layout torch sparse_coo torch sparse_csr mask = mask to_dense mt_result_data layout torch sparse_coo torch sparse_csr mt_result_data = mt_result_data to_dense = mt_result_data detach masked_fill_ ~mask b = t_result detach masked_fill_ ~mask _tensors_match b exact=False rtol=rtol atol=atol raise ValueError The data MaskedTensor Tensor b do match _compare_mts mt mt rtol= e- atol= e- mt_data = mt get_data mt_data = mt get_data mt_data layout = mt_data layout raise ValueError mt s data mt s data do have same layout f mt get_data layout = mt_data layout while mt get_data layout = mt_data layout mask = mt get_mask mask = mt get_mask _masks_match mt mt raise ValueError mt mt must have matching masks mask layout = mask layout raise ValueError mt s mask mt s mask do have same layout f mt get_mask layout = mask layout while mt get_mask layout = mask layout mask layout torch sparse_coo torch sparse_csr mask = mask to_dense mt_data layout torch sparse_coo torch sparse_csr mt_data = mt_data to_dense mt_data = mt_data to_dense = mt_data detach masked_fill_ ~mask b = mt_data detach masked_fill_ ~mask _tensors_match b exact=False rtol=rtol atol=atol raise ValueError The data MaskedTensor mt MaskedTensor mt do match _compare_forward_backward data mask fn mt = masked_tensor data mask requires_grad=True masked_res = fn mt masked_res sum backward t = data masked_fill ~mask float -inf detach clone requires_grad_ tensor_res = fn t tensor_res sum backward _compare_mt_t masked_res tensor_res _compare_mt_t mt grad t grad atol= e- _create_random_mask shape device make_tensor shape device=device dtype=torch bool _generate_sample_data device= cpu dtype=torch float requires_grad=True layout=torch strided assert layout torch strided torch sparse_coo torch sparse_csr Layout must strided sparse_coo sparse_csr shapes = inputs = s shapes data = make_tensor s device=device dtype=dtype requires_grad=requires_grad type ignore arg-type mask = _create_random_mask s device layout == torch sparse_coo mask = mask to_sparse_coo coalesce data = data sparse_mask mask requires_grad_ requires_grad layout == torch sparse_csr data ndim = mask ndim = continue mask = mask to_sparse_csr data = data sparse_mask mask inputs append SampleInput data kwargs= mask mask inputs _fix_fn_name fn_name fn_name - == _ fn_name = fn_name - fn_name TestBasics TestCase test_invalid_tensor_inputs device data = torch randn device=device mask = _create_random_mask device=device mt = masked_tensor data mask assertRaisesRegex TypeError data must Tensor masked_tensor mt mask assertRaisesRegex TypeError data must Tensor masked_tensor mask assertRaisesRegex TypeError mask must Tensor masked_tensor data mt assertRaisesRegex TypeError mask must Tensor masked_tensor data test_diff_layouts device data = torch randn device=device to_sparse_coo mask = _create_random_mask device=device assertRaisesRegex TypeError data mask must have same layout masked_tensor data mask test_diff_dim device data = torch randn device=device mask = _create_random_mask device=device assertRaisesRegex ValueError data dim\\ \\ must equal mask dim\\ \\ masked_tensor data mask test_diff_sizes device data = torch randn device=device mask = _create_random_mask device=device assertRaisesRegex ValueError data size\\ \\ must equal mask size\\ \\ masked_tensor data mask test_grad_warning device data = torch randn device=device requires_grad=True mask = _create_random_mask device=device msg = It recommended create MaskedTensor tensor requires_grad assertWarnsRegex UserWarning msg masked_tensor data mask test_add device data = torch arange device=device mask = torch tensor True True False True False device=device m = masked_tensor data mask m = masked_tensor data ~mask assertRaisesRegex ValueError Input masks must match m + m _compare_mts m + m masked_tensor torch tensor device=device mask test_softmax device data = torch randn device=device mask = torch tensor True True True False False True False True True True False False device=device _compare_forward_backward data mask lambda t torch softmax t - test_where device data = torch tensor - - device=device mask = data mx = masked_tensor data mask requires_grad=True my = masked_tensor torch ones_like data ~mask requires_grad=True masked_res = torch where mask torch exp mx my masked_res sum backward x = data detach clone requires_grad_ y = torch ones_like x device=device requires_grad=True tensor_res = torch where mask torch exp x y tensor_res sum backward _compare_mt_t masked_res tensor_res _compare_mt_t mx grad x grad _compare_mt_t my grad y grad test_unfold device data = torch rand device=device mask = torch rand device=device _compare_forward_backward data mask lambda t t unfold test_nn_unfold device data = torch rand device=device mask = torch rand device=device _compare_forward_backward data mask lambda t torch nn functional unfold t kernel_size= test_stack device masked_tensors = masked_tensor torch rand device=device torch rand device=device requires_grad=True _ range data_tensors = mt get_data detach clone requires_grad_ mt masked_tensors masked_res = torch stack masked_tensors tensor_res = torch stack data_tensors masked_res sum backward tensor_res sum backward _compare_mt_t masked_res tensor_res mt t zip masked_tensors data_tensors _compare_mt_t mt grad t grad atol= e- test_to_sparse device sample _generate_sample_data device=device data = sample input mask = sample kwargs mask mt = masked_tensor data detach clone mask requires_grad=True sparse_mt = mt to_sparse data to_sparse to_dense sum backward sparse_mt to_dense sum backward _compare_mt_t sparse_mt data _compare_mt_t mt grad data grad test_to_device device sample _generate_sample_data device=device data = sample input mask = sample kwargs mask mt = masked_tensor data mask requires_grad=True new_device = torch device cuda device = cuda torch cuda is_available torch device cpu mt_device = mt new_device assertEqual mt_device device type new_device type assertEqual mt_device get_mask device type new_device type assertEqual mt_device get_data device type new_device type test_to_dtype device sample _generate_sample_data device=device data = sample input mask = sample kwargs mask mt = masked_tensor data mask requires_grad=True new_dtype = torch float data dtype == torch float torch float mt_dtype = mt new_dtype assertEqual mt_dtype dtype new_dtype assertEqual mt_dtype get_mask dtype torch bool assertEqual mt_dtype get_data dtype new_dtype test_to_dense device samples = _generate_sample_data device=device layout=torch sparse_coo + _generate_sample_data device=device layout=torch sparse_csr sample samples data = sample input mask = sample kwargs mask mt = masked_tensor data mask requires_grad=True dense_data = data to_dense detach clone requires_grad_ True dense_mt = mt to_dense dense_data sum backward dense_mt sum backward _compare_mt_t dense_mt dense_data _compare_mt_t mt grad to_dense dense_data grad test_to_dense_and_sparse_coo device sample _generate_sample_data device=device layout=torch strided data = sample input mask = sample kwargs mask ms = mask to_sparse_coo coalesce mt = masked_tensor data mask requires_grad=True mts = masked_tensor data sparse_mask ms ms requires_grad=True converted = mt to_sparse to_dense converted sum backward converted = mts to_dense converted sum backward _compare_mts converted converted _compare_mts mt grad mts grad to_dense test_to_dense_and_sparse_csr device sample _generate_sample_data device=device layout=torch strided data = sample input mask = sample kwargs mask data ndim = continue ms = mask to_sparse_csr mt = masked_tensor data mask requires_grad=True mts = masked_tensor data sparse_mask ms ms requires_grad=True converted = mt to_sparse_csr to_dense converted sum backward converted = mts to_dense converted sum backward _compare_mts converted converted _compare_mts mt grad mts grad to_dense test_invalid_sparse_layout device data = torch randn device=device to_sparse_csc mask = _create_random_mask device=device to_sparse_csc assertRaisesRegex TypeError data layout torch sparse_csc supported masked_tensor data mask test_invalid_sparse_coo_values device v = torch tensor dtype=torch float i = torch tensor i = torch tensor t = torch sparse_coo_tensor i v device=device mask = torch sparse_coo_tensor i torch tensor True True True device=device msg = data mask both sparse COO tensors do have same indices assertRaisesRegex ValueError msg masked_tensor t mask test_invalid_sparse_csr_values device crow_indices = crow_indices = col_indices = col_indices = values = mask_values = True True True t = torch sparse_csr_tensor torch tensor crow_indices dtype=torch int torch tensor col_indices dtype=torch int torch tensor values size= mask = torch sparse_csr_tensor torch tensor crow_indices dtype=torch int torch tensor col_indices dtype=torch int torch tensor mask_values dtype=torch bool size= t = torch sparse_csr_tensor torch tensor crow_indices dtype=torch int torch tensor col_indices dtype=torch int torch tensor values size= mask = torch sparse_csr_tensor torch tensor crow_indices dtype=torch int torch tensor col_indices dtype=torch int torch tensor mask_values dtype=torch bool size= msg = data mask both sparse CSR tensors do share either crow col indices assertRaisesRegex ValueError msg masked_tensor t mask assertRaisesRegex ValueError msg masked_tensor t mask test_contiguous device data = torch randn device=device contiguous_data = data clone mask = contiguous_data bool not_contiguous_data = torch as_strided data clone mask = not_contiguous_data bool contiguous_mt = masked_tensor contiguous_data mask not_contiguous_mt = masked_tensor not_contiguous_data mask contiguous_mt_sparse = masked_tensor contiguous_data to_sparse_coo mask to_sparse_coo not_contiguous_mt_sparse = masked_tensor not_contiguous_data to_sparse_coo mask to_sparse_coo assertEqual contiguous_data is_contiguous True assertEqual not_contiguous_data is_contiguous False assertEqual contiguous_mt is_contiguous True assertEqual not_contiguous_mt is_contiguous False error_msg = MaskedTensors sparse data do have is_contiguous t contiguous_mt_sparse not_contiguous_mt_sparse assertRaisesRegex ValueError error_msg t is_contiguous assertRaisesRegex ValueError error_msg t contiguous now_contiguous_mt = not_contiguous_mt contiguous _compare_mts not_contiguous_mt now_contiguous_mt assertEqual now_contiguous_mt is_contiguous True assertEqual now_contiguous_mt get_data is_contiguous True assertEqual now_contiguous_mt is_contiguous True TestUnary TestCase _get_test_data fn_name data = torch randn mask = torch rand fn_name = _fix_fn_name fn_name fn_name log log log p log sqrt data = data mul abs fn_name rsqrt data = data abs + Void division zero fn_name acos arccos asin arcsin logit data = data abs mul clamp fn_name atanh arctanh erfinv data = data mul clamp - fn_name acosh arccosh data = data abs + fn_name bitwise_not data = data mul torch int data mask _get_sample_kwargs fn_name fn_name = _fix_fn_name fn_name kwargs = fn_name clamp clip kwargs min = - kwargs max = kwargs _get_sample_args fn_name data mask fn_name = _fix_fn_name fn_name mt = masked_tensor data mask t_args = data mt_args = mt fn_name pow t_args += mt_args += t_args mt_args parametrize fn NATIVE_UNARY_FNS test_unary fn torch random manual_seed fn_name = fn __name__ data mask = _get_test_data fn_name kwargs = _get_sample_kwargs fn_name t_args mt_args = _get_sample_args fn_name data mask mt_result = fn mt_args kwargs t_result = fn t_args kwargs _compare_mt_t mt_result t_result parametrize fn NATIVE_INPLACE_UNARY_FNS test_inplace_unary fn torch random manual_seed fn_name = fn __name__ data mask = _get_test_data fn_name kwargs = _get_sample_kwargs fn_name t_args mt_args = _get_sample_args fn_name data mask mt_result = fn mt_args kwargs t_result = fn t_args kwargs _compare_mt_t mt_result t_result TestBinary TestCase _get_test_data fn_name fn_name = _fix_fn_name fn_name data = torch randn data = torch randn mask = torch rand fn_name bitwise_and bitwise_or bitwise_xor data = data mul torch int data = data mul torch int fn_name bitwise_left_shift bitwise_right_shift data = data abs torch int data = data abs torch int data data mask _get_sample_kwargs fn_name fn_name = _fix_fn_name fn_name kwargs = kwargs _yield_sample_args fn_name data data mask Returns two sets Tensor MaskedTensor args binary function compute Tensor args all same just two provided data tensors while MaskedTensor args tests both MaskedTensor MaskedTensor MaskedTensor Tensor fn_name = _fix_fn_name fn_name mt = masked_tensor data mask mt = masked_tensor data mask t_args = data data mt_args = mt mt yield t_args mt_args t_args = data data mt_args = mt data yield t_args mt_args parametrize fn NATIVE_BINARY_FNS test_binary fn torch random manual_seed fn_name = fn __name__ data data mask = _get_test_data fn_name kwargs = _get_sample_kwargs fn_name t_args mt_args _yield_sample_args fn_name data data mask mt_result = fn mt_args kwargs t_result = fn t_args kwargs _compare_mt_t mt_result t_result parametrize fn NATIVE_INPLACE_BINARY_FNS test_inplace_binary fn torch random manual_seed fn_name = fn __name__ data data mask = _get_test_data fn_name kwargs = _get_sample_kwargs fn_name t_args mt_args _yield_sample_args fn_name data data mask mt_result = fn mt_args kwargs t_result = fn t_args kwargs _compare_mt_t mt_result t_result parametrize fn_name add add_ test_masks_match fn_name torch random manual_seed fn = getattr torch ops aten fn_name data data mask = _get_test_data fn_name mask = mask mask = torch rand mask size mt = masked_tensor data mask mt = masked_tensor data mask try fn mt mt raise AssertionError except ValueError e assert Input masks must match If you need support please open issue Github == str e TestReductions TestCase test_max_not_implemented d = torch tensor m = torch tensor True False False False True False mt = masked_tensor d m assertRaisesRegex TypeError torch _ops aten max default mt max test_sum d = torch tensor m = torch tensor True False False True False True False True mt = masked_tensor d m _compare_mts masked_tensor torch tensor torch tensor True mt sum _compare_mts masked_tensor torch tensor torch tensor True True False True mt sum dim= test_sum_grad d = torch tensor m = torch tensor True False False False True False mt = masked_tensor d m requires_grad=True mt sum backward _compare_mts mt grad masked_tensor torch tensor expand_as m m test_mean d = torch tensor m = torch tensor True False False True False True False True mt = masked_tensor d m _compare_mts masked_tensor torch tensor torch tensor True mt mean _compare_mts masked_tensor torch tensor torch tensor True True False True mt mean dim= The following block tests test_mean_grad_case_ through e used test functionality two different ways constructing MaskedTensors masked_tensor data mask requires_grad=True False -- NO differentiable constructor always leaf as_masked_tensor data mask -- differentiable constructor Like torch tensor data masked_tensor data mask will provide UserWarning data requires_grad=True as_masked_tensor does take requires_grad -- just takes requires_grad data Therefore there cases test we use ` mean ` proxy test different combinations Assuming mt mean backward run after each constructor Case values requires_grad = True mt = masked_tensor values mask requires_grad=True yields - Provide UserWarning because values requires_grad=True - values grad = None - mt grad MaskedTensor correct gradient Case b values requires_grad = False mt = masked_tensor values mask requires_grad=True yields - values grad = None - mt grad MaskedTensor correct gradient Case b values requires_grad = True False mt = masked_tensor values mask requires_grad=False will both yield RuntimeError element tensors does require grad does have grad_fn expected When values requires_grad=True we will also get UserWarning Case values requires_grad = True mt = as_masked_tensor values mask yields - values grad MaskedTensor correct gradient - mt grad None gives UserWarning The grad attribute Tensor leaf Tensor being accessed Its grad Case b values requires_grad = False mt = as_masked_tensor values mask will yield RuntimeError element tensors does require grad does have grad_fn expected test_mean_grad_case_ values requires_grad = True mt = masked_tensor values mask requires_grad=True d = torch tensor requires_grad=True m = torch tensor True False False False True False assertWarnsRegex UserWarning It recommended create MaskedTensor mt = masked_tensor d m requires_grad=True mt mean backward assertIsNone d grad _compare_mts mt grad masked_tensor torch tensor m test_mean_grad_case_ b values requires_grad = False mt = masked_tensor values mask requires_grad=True d = torch tensor m = torch tensor True False False False True False mt = masked_tensor d m requires_grad=True mt mean backward assertIsNone d grad _compare_mts mt grad masked_tensor torch tensor m test_mean_grad_case_ c values requires_grad = True mt = masked_tensor values mask requires_grad=False d = torch tensor requires_grad=True m = torch tensor True False False False True False assertWarnsRegex UserWarning It recommended create MaskedTensor mt = masked_tensor d m requires_grad=False result = mt mean msg = element tensors does require grad does have grad_fn assertRaisesRegex RuntimeError msg result backward test_mean_grad_case_ d values requires_grad = False mt = masked_tensor values mask requires_grad=False d = torch tensor m = torch tensor True False False False True False mt = masked_tensor d m requires_grad=False result = mt mean msg = element tensors does require grad does have grad_fn assertRaisesRegex RuntimeError msg result backward test_mean_grad_case_ e values requires_grad = True mt = as_masked_tensor values mask d = torch tensor requires_grad=True m = torch tensor True False False False True False mt = as_masked_tensor d m mt mean backward _compare_mts d grad masked_tensor torch tensor m msg = The grad attribute Tensor leaf Tensor being accessed Its grad assertWarnsRegex UserWarning msg assertIsNone mt grad test_mean_grad_case_ f values requires_grad = False mt = as_masked_tensor values mask d = torch tensor m = torch tensor True False False False True False mt = as_masked_tensor d m result = mt mean msg = element tensors does require grad does have grad_fn assertRaisesRegex RuntimeError msg result backward test_mean_dim_grad d = torch tensor m = torch tensor True True False False True False mt = masked_tensor d m requires_grad=True mt mean sum backward _compare_mts mt grad masked_tensor torch tensor m test_amax d = torch tensor - - m = torch tensor True False False True False True False True mt = masked_tensor d m _compare_mts masked_tensor torch tensor torch tensor True mt amax _compare_mts masked_tensor torch tensor - torch tensor True True False True mt amax dim= test_amax_grad d = torch tensor m = torch tensor True False False False True False mt = masked_tensor d m requires_grad=True mt amax backward _compare_mts mt grad masked_tensor torch tensor m test_amin d = torch tensor - - m = torch tensor True False False True False True False True mt = masked_tensor d m _compare_mts masked_tensor torch tensor - torch tensor True mt amin _compare_mts masked_tensor torch tensor - - torch tensor True True False True mt amin dim= test_amin_grad d = torch tensor m = torch tensor True False False False True False mt = masked_tensor d m requires_grad=True mt amin backward _compare_mts mt grad masked_tensor torch tensor m test_prod d = torch tensor float nan m = torch tensor True False False True False True False True mt = masked_tensor d m _compare_mts masked_tensor torch tensor torch tensor True mt prod _compare_mts masked_tensor torch tensor torch tensor True True False True mt prod dim= test_prod_grad d = torch tensor float nan m = torch tensor True False False False True False mt = masked_tensor d m requires_grad=True mt prod backward _compare_mts mt grad masked_tensor torch tensor m test_all d = torch tensor True True False False False True True True m = torch tensor True False False True False True False True mt = masked_tensor d m _compare_mts masked_tensor torch tensor False torch tensor True mt all _compare_mts masked_tensor torch tensor True True True False torch tensor True True False True mt all dim= m = torch tensor True False True False False True False False mt = masked_tensor d m _compare_mts masked_tensor torch tensor True True False True torch tensor True True True False mt all dim= test_grad_dtype d = torch tensor True True False False True True m = torch tensor True False False False True False msg = Only Tensors floating point complex dtype can require gradients assertRaisesRegex RuntimeError msg masked_tensor d m requires_grad=True test_any_true_dtype mt = torch masked MaskedTensor torch rand torch rand msg = expected boolean tensor assertRaisesRegex ValueError msg mt _is_any_true test__is_any_true mt = torch masked MaskedTensor torch tensor True True False False False True torch tensor True False False False True False _compare_mts masked_tensor torch tensor True torch tensor True mt _is_any_true test__is_any_true_false mt = torch masked MaskedTensor torch tensor True True False False False True torch tensor False False False False False False _compare_mts masked_tensor torch tensor False torch tensor True mt _is_any_true test_backward See https github com pytorch pytorch issues torch autograd detect_anomaly mt = torch masked MaskedTensor torch rand torch rand requires_grad=True mt sum backward is_unary op op name UNARY_NAMES is_binary op op name BINARY_NAMES is_reduction op op name REDUCE_NAMES op name all mean std var mt_unary_ufuncs = op op unary_ufuncs is_unary op mt_binary_ufuncs = op op binary_ufuncs is_binary op mt_reduction_ufuncs = op op reduction_ops is_reduction op MASKEDTENSOR_FLOAT_TYPES = torch float torch float torch float TestOperators TestCase _convert_mt_args args mask layout masked_tensor arg sparse_mask mask layout = torch strided arg mask torch is_tensor arg arg arg args _test_unary_binary_equality device dtype op layout=torch strided samples = op sample_inputs device dtype requires_grad=True sample samples input = sample input sample_args sample_kwargs = sample args sample kwargs mask = _create_random_mask input shape device mask sample_kwargs sample_kwargs pop mask layout == torch sparse_coo mask = mask to_sparse_coo coalesce input = input sparse_mask mask layout == torch sparse_csr input ndim = mask ndim = continue mask = mask to_sparse_csr input = input sparse_mask mask Binary operations currently only support same size masks is_binary op input shape = sample_args shape continue Binary operations also don t support kwargs right now sample_kwargs = mt = masked_tensor input mask mt_args = _convert_mt_args sample_args mask layout mt_result = op mt mt_args sample_kwargs t_result = op sample input sample_args sample_kwargs _compare_mt_t mt_result t_result If operation binary check lhs = masked rhs = regular tensor also works is_binary op layout == torch strided mt_result = op mt sample_args sample_kwargs _compare_mt_t mt_result t_result _test_reduction_equality device dtype op layout=torch strided samples = op sample_inputs device dtype requires_grad=True sample samples input = sample input Reduction operations don t support more advanced args kwargs right now sample_args sample_kwargs = input dim == input numel == continue mask = _create_random_mask input shape device torch count_nonzero mask == continue tensor_input = _combine_input_and_mask op op input mask layout == torch sparse_coo mask = mask to_sparse_coo coalesce input = input sparse_mask mask layout == torch sparse_csr input ndim = mask ndim = continue mask = mask to_sparse_csr input = input sparse_mask mask mt = masked_tensor input mask mt_args = _convert_mt_args sample_args mask layout mt_result = op mt mt_args sample_kwargs t_result = op tensor_input sample_args sample_kwargs _compare_mt_t mt_result t_result ops mt_unary_ufuncs allowed_dtypes=MASKEDTENSOR_FLOAT_TYPES type ignore arg-type parametrize layout torch strided torch sparse_coo torch sparse_csr test_unary_core device dtype op layout Skip tests don t have len kwargs == skip_variants = decimals_ decimals_ decimals_neg_ op name == round op variant_test_name skip_variants _test_unary_binary_equality device dtype op ops mt_binary_ufuncs allowed_dtypes=MASKEDTENSOR_FLOAT_TYPES type ignore arg-type parametrize layout torch strided torch sparse_coo torch sparse_csr test_binary_core device dtype op layout _test_unary_binary_equality device dtype op layout ops mt_reduction_ufuncs allowed_dtypes=MASKEDTENSOR_FLOAT_TYPES type ignore arg-type parametrize layout torch strided torch sparse_coo torch sparse_csr test_reduction_all device dtype op layout argmin argmax currently supported torch sparse_csr op name argmin argmax layout == torch sparse_csr _test_reduction_equality device dtype op layout only_for = cpu cuda instantiate_device_type_tests TestOperators globals only_for=only_for instantiate_device_type_tests TestBasics globals only_for=only_for instantiate_parametrized_tests TestUnary instantiate_parametrized_tests TestBinary instantiate_parametrized_tests TestReductions __name__ == __main__ run_tests