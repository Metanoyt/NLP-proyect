Owner s module nn unittest dataclasses dataclass functools partial itertools chain product torch torch nn nn torch nn functional F torch nn CrossEntropyLoss torch nn utils _expanded_weights ExpandedWeight torch nn utils _expanded_weights expanded_weights_utils forward_helper set_grad_sample_if_exists standard_kwargs sum_over_all_but_batch_and_last_n unpack_expanded_weight_or_tensor torch nn utils _per_sample_grad call_for_per_sample_grads torch testing _internal common_cuda TEST_CUDA tf _off torch testing _internal common_device_type instantiate_device_type_tests OpDTypes ops torch testing _internal common_methods_invocations op_db SampleInput torch testing _internal common_modules module_db modules torch testing _internal common_nn get_new_module_tests module_tests TestBase torch testing _internal common_utils freeze_rng_state make_tensor parametrize run_tests skipIfTorchDynamo TestCase torch utils _pytree tree_map_only TestContext pass TestExpandedWeightHelperFunction TestCase test_forward_helper device input = torch randn device=device weight = torch randn device=device bias = torch randn device=device weight_batched bias_batched product True False True False maybe_batched_weight = weight maybe_batched_bias = bias weight_batched maybe_batched_weight = ExpandedWeight weight clone requires_grad_ loss_reduction= sum bias_batched maybe_batched_bias = ExpandedWeight bias clone requires_grad_ loss_reduction= sum args = input maybe_batched_weight maybe_batched_bias expanded_args expanded_kwargs = standard_kwargs bias args res = forward_helper nn functional linear expanded_args expanded_kwargs expected = nn functional linear input weight bias assertEqual res expected assertEqual len expanded_args assert expanded_args args avoids property checks assertEquals assert expanded_args args avoids property checks assertEquals assertEqual len expanded_kwargs assert expanded_kwargs bias args avoids property checks assertEquals test_forward_helper_failure_args device weight = torch randn device=device bias = torch randn device=device assertRaisesRegex RuntimeError r do support inputs also ExpandedWeights input = ExpandedWeight torch randn requires_grad=True loss_reduction= sum expanded_args expanded_kwargs = standard_kwargs bias input weight bias forward_helper nn functional linear expanded_args expanded_kwargs assertRaisesRegex RuntimeError r requires Tensor first input expanded_args expanded_kwargs = standard_kwargs bias weight bias forward_helper nn functional linear expanded_args expanded_kwargs assertRaisesRegex RuntimeError r requires batch dimension got input size expanded_args expanded_kwargs = standard_kwargs bias torch tensor weight bias forward_helper nn functional linear expanded_args expanded_kwargs assertRaisesRegex RuntimeError r valid batch size Expanded Weights expanded_args expanded_kwargs = standard_kwargs bias torch randn weight bias forward_helper nn functional linear expanded_args expanded_kwargs input = torch randn weight_batched bias_batched product True False True False weight_batched bias_batched continue maybe_batched_weight = weight maybe_batched_bias = bias weight_batched maybe_batched_weight = ExpandedWeight weight clone requires_grad_ loss_reduction= sum bias_batched maybe_batched_bias = ExpandedWeight bias clone requires_grad_ loss_reduction= sum assertRaisesRegex RuntimeError r Expected ExpandedWeights have batch size matching input expanded_args expanded_kwargs = standard_kwargs bias input maybe_batched_weight maybe_batched_bias forward_helper nn functional linear expanded_args expanded_kwargs test_set_grad_sample_if_exists device test_fn grad_sample orig_weight = torch randn device=device requires_grad=True expanded_weight = ExpandedWeight orig_weight loss_reduction= sum grad_sample = torch randn set_grad_sample_if_exists expanded_weight test_fn assertTrue hasattr orig_weight grad_sample assertEqual orig_weight grad_sample grad_sample basic_tensor = torch randn device=device set_grad_sample_if_exists basic_tensor test_fn assertFalse hasattr basic_tensor grad_sample non_tensor = set_grad_sample_if_exists non_tensor test_fn assertFalse hasattr non_tensor grad_sample test_set_grad_sample_if_exists_failure device test_fn True grad_tensor = torch randn requires_grad=True device=device assertRaisesRegex RuntimeError r does support mixture ExpandedWeight parameters normal Parameters set_grad_sample_if_exists grad_tensor test_fn test_unpack_expanded_weight_or_tensor device input = torch randn requires_grad=True device=device assertEqual input unpack_expanded_weight_or_tensor ExpandedWeight input loss_reduction= sum input requires_grad_ False assertEqual input unpack_expanded_weight_or_tensor input assertTrue unpack_expanded_weight_or_tensor None test_unpack_expanded_weight_or_tensor_with_custom_function device input = torch randn requires_grad=True device=device assertTrue unpack_expanded_weight_or_tensor ExpandedWeight input loss_reduction= sum lambda x x input input requires_grad_ False assertTrue unpack_expanded_weight_or_tensor input lambda x x input assertTrue unpack_expanded_weight_or_tensor lambda x x input None test_unpack_expanded_weight_or_tensor_failure device input = torch randn requires_grad=True device=device assertRaisesRegex RuntimeError r does support mixture ExpandedWeight parameters normal Parameters unpack_expanded_weight_or_tensor input assertRaisesRegex RuntimeError r does support mixture ExpandedWeight parameters normal Parameters unpack_expanded_weight_or_tensor input lambda x x input test_sum_over_all_but_batch_and_last_n device input = torch randn device=device res = sum_over_all_but_batch_and_last_n input expected = input sum assertEqual res expected res = sum_over_all_but_batch_and_last_n input expected = input sum assertEqual res expected res = sum_over_all_but_batch_and_last_n input assertEqual res input TestExpandedWeightFunctional TestCase _compare_ew_and_for_loop_per_sample_grads op sample_input reduction input = sample_input input args = sample_input args kwargs = sample_input kwargs batch_size = input shape len input shape get per sample grads ExpandedWeights objects loss_reduction = sum reduction == torch sum mean ew_input ew_args ew_kwargs = make_expanded_weight sample_input batch_size loss_reduction diff_input_list = ew_input + tuple ew_args + tuple ew_kwargs values diff_input_list = i i diff_input_list is_diff_tensor i diff_input_list = i orig_weight isinstance i ExpandedWeight i i diff_input_list diff_input_list result = run_op op ew_input ew_args ew_kwargs reduction result backward grad doesn t work ExpandedWeight because calls __torch_function__ expanded_weight_grad = tuple i grad_sample hasattr i grad_sample i grad i diff_input_list get per sample grads loop func = partial run_op op per_sample_grad = for_loop_per_sample_grad batch_size reduction input func args kwargs check equality assertEqual len per_sample_grad len expanded_weight_grad loss_reduction == mean don t check equality ` input grad ` s since these vanilla tensors won t scaled expanded_weight_grad = expanded_weight_grad per_sample_grad = per_sample_grad result_grad expected_grad zip expanded_weight_grad per_sample_grad assertEqual result_grad expected_grad ops filter lambda op op supports_expanded_weight op_db dtypes=OpDTypes supported allowed_dtypes= torch double test_expanded_weight_per_sample_grad_sum device dtype op sample_inputs = op sample_inputs device dtype requires_grad=True sample_input supported_inputs op sample_inputs op name == nn functional embedding embedding flips its argument order autograd tests sample_input = SampleInput sample_input args args= sample_input input kwargs=sample_input kwargs _compare_ew_and_for_loop_per_sample_grads op sample_input torch sum ops filter lambda op op supports_expanded_weight op_db dtypes=OpDTypes supported allowed_dtypes= torch double test_expanded_weight_per_sample_grad_mean device dtype op sample_inputs = op sample_inputs device dtype requires_grad=True sample_input supported_inputs op sample_inputs op name == nn functional embedding embedding flips its argument order autograd tests sample_input = SampleInput sample_input args args= sample_input input kwargs=sample_input kwargs _compare_ew_and_for_loop_per_sample_grads op sample_input torch mean ops filter lambda op op supports_expanded_weight op_db dtypes=OpDTypes supported allowed_dtypes= torch double test_expanded_weights_per_sample_grad_input_no_grad device dtype op sample_inputs = op sample_inputs device dtype requires_grad=True sample_input supported_inputs op sample_inputs op name == nn functional embedding embedding flips its argument order autograd tests sample_input = SampleInput sample_input args args= sample_input input kwargs=sample_input kwargs sample_input input requires_grad_ False _compare_ew_and_for_loop_per_sample_grads op sample_input torch mean skipIfTorchDynamo Checking error message doesn t work dynamo ops filter lambda op op supports_expanded_weight op_db dtypes=OpDTypes supported allowed_dtypes= torch double test_unsupported_expand_weights device dtype op sample_inputs = op sample_inputs device dtype requires_grad=True unsupported_inputs = supported_inputs op sample_inputs supported_inputs=False sample_input unsupported_inputs assertRaisesRegex RuntimeError r Expanded Weights op name == nn functional embedding embedding flips its argument order autograd tests sample_input = SampleInput sample_input args args= sample_input input kwargs=sample_input kwargs input = sample_input input batch_size = input shape len input shape get per sample grads ExpandedWeights objects ew_input ew_args ew_kwargs = make_expanded_weight sample_input batch_size result = run_op op ew_input ew_args ew_kwargs diff_input_list = ew_input + tuple ew_args + tuple ew_kwargs values diff_input_list = i i diff_input_list is_diff_tensor i diff_input_list = i orig_weight isinstance i ExpandedWeight i i diff_input_list result sum backward grad doesn t work ExpandedWeight because calls __torch_function__ ops filter lambda op op supports_expanded_weight op_db dtypes=OpDTypes supported test_expanded_weight_forward device dtype op sample_inputs = op sample_inputs device dtype sample_input supported_inputs op sample_inputs op name == nn functional embedding embedding flips its argument order autograd tests sample_input = SampleInput sample_input args clone args= sample_input input clone kwargs=sample_input kwargs cuda device max_norm sample_input kwargs padding_idx sample_input kwargs skipTest embedding non-determinstic case see issue batch_size = sample_input input shape len sample_input input shape loss_reduction sum mean ew_input ew_args ew_kwargs = make_expanded_weight sample_input batch_size loss_reduction expanded_weight_result = run_op op ew_input ew_args ew_kwargs normal_result = run_op op sample_input input sample_input args sample_input kwargs assertEqual expanded_weight_result normal_result test_expanded_weight_error device batch_size = sample_input = make_tensor batch_size dtype=torch float device=device requires_grad=True sample_weight = make_tensor dtype=torch float device=device requires_grad=True assertRaisesRegex RuntimeError r Expanded Weights encountered cannot handle function torch add sample_input ExpandedWeight sample_weight batch_size loss_reduction= sum _test_embedding_model model num_embedding device batch_size = input = torch randint num_embedding batch_size device=device _test_model partial model num_embedding=num_embedding batch_size input device _test_conv_model model input_size num_dim device loss_reduction= sum atol= e- rtol= e- batch_size = input_ending = input_size num_dim input = torch randn batch_size + input_ending device=device _test_model partial model num_dim=num_dim batch_size input device loss_reduction atol rtol _test_model model batch_size input device loss_reduction= sum atol= e- rtol= e- model = model device targets = torch randint batch_size device=device criterion = CrossEntropyLoss reduction=loss_reduction result = call_for_per_sample_grads model loss_reduction=loss_reduction input loss = criterion result targets loss backward result = weight model parameters result append weight grad_sample del weight grad_sample expected = i range batch_size loss = criterion model input i unsqueeze targets i unsqueeze expected append torch autograd grad loss model parameters torch ones_like loss expected = torch stack grad grad zip expected res exp zip result expected assertEqual res exp atol=atol rtol=rtol _compute_tolerances device is_cuda_sm = device startswith cuda torch cuda get_device_capability == e- e- is_cuda_sm e- e- tf _off test_cnn_model_sum device convnet num_classes num_dim nn Sequential nn Conv d kernel_size= stride= padding= nn ReLU nn AvgPool d kernel_size= stride= nn Conv d kernel_size= stride= padding= nn ReLU nn AvgPool d kernel_size= stride= nn Conv d kernel_size= stride= padding= nn ReLU nn AvgPool d kernel_size= stride= nn Conv d kernel_size= stride= padding= nn ReLU nn AdaptiveAvgPool d nn Flatten start_dim= end_dim=- nn Linear num_classes bias=True atol rtol = _compute_tolerances device _test_conv_model convnet device atol=atol rtol=rtol tf _off test_cnn_model_mean device convnet num_classes num_dim nn Sequential nn Conv d kernel_size= stride= padding= nn ReLU nn AvgPool d kernel_size= stride= nn Conv d kernel_size= stride= padding= nn ReLU nn AvgPool d kernel_size= stride= nn Conv d kernel_size= stride= padding= nn ReLU nn AvgPool d kernel_size= stride= nn Conv d kernel_size= stride= padding= nn ReLU nn AdaptiveAvgPool d nn Flatten start_dim= end_dim=- nn Linear num_classes bias=True atol rtol = _compute_tolerances device _test_conv_model convnet device loss_reduction= mean atol=atol rtol=rtol parametrize num_dim tf _off test_instance_norm_model num_dim device instance_norm_model num_classes num_dim conv_layer = nn Conv d num_dim == nn Conv d num_dim == nn Conv d norm_layer = nn InstanceNorm d num_dim == nn InstanceNorm d num_dim == nn InstanceNorm d nn Sequential conv_layer kernel_size= stride= padding= norm_layer affine=True nn Flatten start_dim= end_dim=- nn Linear num_dim num_classes bias=True atol rtol = _compute_tolerances device _test_conv_model instance_norm_model num_dim device atol=atol rtol=rtol parametrize num_dim tf _off test_group_norm_model num_dim device group_norm_model num_classes num_dim conv_layer = nn Conv d num_dim == nn Conv d num_dim == nn Conv d nn Sequential conv_layer kernel_size= stride= padding= nn GroupNorm affine=True nn Flatten start_dim= end_dim=- nn Linear num_dim num_classes bias=True atol rtol = _compute_tolerances device _test_conv_model group_norm_model num_dim device atol=atol rtol=rtol parametrize num_dim tf _off test_layer_norm_model num_dim device layer_norm_model num_classes num_dim conv_layer = nn Conv d num_dim == nn Conv d num_dim == nn Conv d normalized_shape = num_dim nn Sequential conv_layer kernel_size= stride= padding= nn LayerNorm normalized_shape elementwise_affine=True nn Flatten start_dim= end_dim=- nn Linear num_dim num_classes bias=True atol rtol = _compute_tolerances device _test_conv_model layer_norm_model num_dim device atol=atol rtol=rtol test_embedding_model device embedding_model num_classes num_embedding nn Sequential nn Embedding num_embedding nn Flatten start_dim= end_dim=- nn Linear num_classes bias=True _test_embedding_model embedding_model device test_group_norm_error device group norm has call native_group_norm This checks hits same errors normal group norm would N = C = inp = torch randn N C assertRaisesRegex RuntimeError r Expected number channels input divisible F group_norm inp divisible TestExpandedWeightModule TestCase _do_test module input args=None kwargs=None batch_first=True atol=None rtol=None args = args kwargs = kwargs batch_dim = batch_first batch_size = input shape batch_dim diff_input = input dtype == torch float input dtype == torch double diff_input input requires_grad_ freeze_rng_state get per sample grads ExpandedWeights context manager actual_res = call_for_per_sample_grads module batch_size=batch_size loss_reduction= sum batch_first=batch_first input args kwargs sum actual_res backward actual_grads = param module parameters actual_grads append param grad_sample del param grad_sample diff_input actual_grads append input grad clone input grad = torch zeros_like input grad get per sample grads loop expected_res = torch tensor device=input device dtype=actual_res dtype expected_grads = i range batch_size input_slice = input narrow batch_dim i input_slice = input_slice squeeze batch_dim h s batch dim always first dim Must contiguous CUDA sliced_args = tree_map_only torch Tensor lambda t t narrow i contiguous args diff_params = module parameters diff_input diff_params = chain diff_params input_slice res = module input_slice unsqueeze batch_dim contiguous sliced_args kwargs sum out_grads = torch autograd grad res diff_params torch ones_like res allow_unused=True expected_grads append out_grads expected_res += res expected_grads = torch stack grad grad zip expected_grads batch_first expected_grads - = expected_grads - transpose assertEqual actual_res expected_res atol=atol rtol=rtol assertEqual actual expected atol=atol rtol=rtol actual expected zip actual_grads expected_grads _do_test_multi_input module input TestModule nn Module __init__ module super __init__ module = module forward input module input + module input batch_size = input shape diff_input = input dtype == torch float input dtype == torch double diff_input input requires_grad_ freeze_rng_state get per sample grads ExpandedWeights context manager calling backward twice test_module = TestModule module actual_res = call_for_per_sample_grads test_module loss_reduction= sum input sum actual_res backward actual_grads = param module parameters actual_grads append param grad_sample del param grad_sample diff_input actual_grads append input grad clone input grad = torch zeros_like input grad get per sample grads loop running over input twice expected_grads = i range batch_size input_slice = input i diff_params = module parameters diff_input diff_params = chain diff_params input_slice res = module input_slice unsqueeze sum out_grads = torch autograd grad res diff_params torch ones_like res allow_unused=True expected_grads append out_grads expected_grads = tuple torch stack grad grad zip expected_grads expected_grads = tuple expected_grad expected_grad expected_grads expected_grad None assert assertEqual actual expected actual expected zip actual_grads expected_grads _do_test_rnn_packed_sequence module input args=None kwargs=None atol=None rtol=None args = args args None kwargs = kwargs kwargs None batch_size = max tuple input batch_sizes item freeze_rng_state get per sample grads ExpandedWeights context manager actual_res = call_for_per_sample_grads module batch_size=batch_size loss_reduction= sum input args kwargs data sum actual_res backward actual_grads = param module parameters assertEqual param grad_sample shape batch_size actual_grads append param grad_sample del param grad_sample input data grad = torch zeros_like input data compute per sample grads loop expected_res = torch zeros_like actual_res expected_grads = padded_input seq_sizes = torch nn utils rnn pad_packed_sequence input batch_first=True i range len seq_sizes input_slice = padded_input i narrow seq_sizes i diff_params = module parameters batch_dim = module m batch_first res = module input_slice unsqueeze batch_dim args kwargs sum expected_res += res out_grads = torch autograd grad res diff_params torch ones_like res allow_unused=True expected_grads append out_grads expected_grads = torch stack grad grad zip expected_grads assertEqual actual_res expected_res atol=atol rtol=rtol assertEqual actual expected atol=atol rtol=rtol actual expected zip actual_grads expected_grads modules filter lambda m_info m_info module_cls torch nn RNN torch nn LSTM torch nn GRU module_db tf _off test_module device dtype module_info training RNNWrapper torch nn Module __init__ m_cons args kwargs super __init__ m = m_cons args kwargs forward inps ret = m inps assert isinstance ret tuple ret batch_hidden h new_h_shape = len h shape + new_h_shape = h unsqueeze repeat new_h_shape module_cls = module_info module_cls atol rtol = e- e- dtype == torch float None None module_inputs = module_info module_inputs_func module_info device=device dtype=dtype requires_grad=True training=training with_packed_sequence=True module_input module_inputs module_input forward_input None continue args kwargs = module_input constructor_input args module_input constructor_input kwargs m = RNNWrapper module_cls args kwargs batch_first = m m batch_first m device dtype args kwargs = module_input forward_input args module_input forward_input kwargs RNN tests use unbatched inputs -- batch inputs input = args isinstance input torch Tensor input dim == input = input detach new_input_shape = len input shape + batch_first new_input_shape = input = input repeat new_input_shape new_input_shape = input = input unsqueeze repeat new_input_shape h = args len args None h None h = batch_hidden h isinstance h torch Tensor tuple batch_hidden hx hx h args = list args args = h isinstance input torch nn utils rnn PackedSequence _do_test_rnn_packed_sequence m input args kwargs atol=atol rtol=rtol _do_test m input args kwargs batch_first=batch_first atol=atol rtol=rtol test_per_sample_api_failing module = nn Linear input = torch randn assertRaisesRegex RuntimeError r Module passed must nn Module call_for_per_sample_grads fail input assertRaisesRegex RuntimeError r Batch size passed must None integer call_for_per_sample_grads module batch_size= input assertRaisesRegex RuntimeError r Batch size must positive call_for_per_sample_grads module batch_size=- input assertRaisesRegex RuntimeError r incorrect multiple calls loss = call_for_per_sample_grads module input sum loss backward populate grad_sample fields call_for_per_sample_grads module input module = nn Linear reset have grad_sample fields assertRaisesRegex RuntimeError r Expected loss_reduction argument sum mean call_for_per_sample_grads module loss_reduction= input test_per_sample_api_compute_batch_size CustomModule nn Module __init__ - None super __init__ linear = nn Linear forward input input linear input + linear input module = CustomModule input = torch randn input = torch randn assertRaisesRegex RuntimeError found least one input batch size one batch size call_for_per_sample_grads module input input input = torch randn call_for_per_sample_grads module input input module = CustomModule call_for_per_sample_grads module input input =input module = CustomModule call_for_per_sample_grads module input =input input =input test_per_sample_api_compute_batch_size_not_pytreeable dataclass NonPytreeableTuple elem torch Tensor elem torch Tensor CustomModule nn Module __init__ - None super __init__ linear = nn Linear forward input input linear input elem + linear input elem input = NonPytreeableTuple torch randn torch randn model = CustomModule assertRaisesRegex RuntimeError ExpandedWeights cannot compute batch size inputs call_for_per_sample_grads model input would prefer error because input pytree-able s hard detect assertRaisesRegex RuntimeError Expected ExpandedWeights have batch size matching input call_for_per_sample_grads model input torch randn model = CustomModule TODO functional call bug sam will fix call_for_per_sample_grads model input torch randn model = CustomModule call_for_per_sample_grads model batch_size= input torch randn ContextManagerTests TestBase __init__ args kwargs test_cpu = kwargs get test_cpu True test_cuda = kwargs get test_cuda True super __init__ args kwargs property constructor_args _get_arg constructor_args False test_context_manager test_case device kwargs = device device dtype torch double module = constructor constructor_args kwargs Embedding get_name kwargs dtype = torch long input = _get_input kwargs len input shape == input shape == raise unittest SkipTest Can t get per sample gradients when no batch dim batch dim constructor == torch nn Linear len input shape == raise unittest SkipTest Can t get per sample gradients input rank test_case _do_test module input test_context_manager_multiple_inputs test_case device module = constructor constructor_args device input = _get_input len input shape == input shape == raise unittest SkipTest Can t get per sample gradients when no batch dim batch dim constructor == torch nn Linear len input shape == raise unittest SkipTest Can t get per sample gradients input rank test_case _do_test_multi_input module input filter_supported_tests t supported_modules = Linear Conv d Conv d Conv d Embedding LayerNorm GroupNorm InstanceNorm module_name t t module_name supported_modules True TODO Once all these use ModuleInfo replace ModuleInfo tests These currently use legacy nn tests supported_tests = t t module_tests + get_new_module_tests filter_supported_tests t test_param supported_tests constructor test_param name = test_param pop module_name test_param constructor = getattr nn name decorator = test_param pop decorator lambda test test test = ContextManagerTests test_param test_name = test get_name hasattr TestExpandedWeightModule test_name raise RuntimeError Found two tests same name + test_name test_name_multi_input = test get_name + _multiple_inputs hasattr TestExpandedWeightModule test_name_multi_input raise RuntimeError Found two tests same name + test_name test test_cpu setattr TestExpandedWeightModule test_name decorator lambda test=test test test_context_manager cpu setattr TestExpandedWeightModule test_name_multi_input decorator lambda test=test test test_context_manager_multiple_inputs cpu TEST_CUDA test test_cuda since checks derivatives only use double precision setattr TestExpandedWeightModule test_name + _cuda_double decorator lambda test=test test test_context_manager cuda ------------- HELPER FUNCTIONS ----------------- run_op op input args kwargs r OpInfo Embedding switches input weight so autograd tests will only check derivative weight input which can t differentiable since its dtype int Calls op using special ordering Embedding s OpInfo expects case op name == nn functional embedding op args input kwargs op input args kwargs make_expanded_weight sample_input batch_size loss_reduction= sum expanded_weight_or_clone arg is_diff_tensor arg ExpandedWeight torch clone arg batch_size loss_reduction clone_if_tensor arg ew_input = clone_if_tensor sample_input input ew_args = tuple expanded_weight_or_clone arg arg sample_input args ew_kwargs = name expanded_weight_or_clone arg name arg sample_input kwargs items ew_input ew_args ew_kwargs supported_inputs op sample_inputs supported_inputs=True r ExpandedWeights currently does support some use cases when there s no batch dimension operations would cause inter-batch operations Removes all cases cannot deal filter_fn input convolutions = nn functional conv d nn functional conv d nn functional conv d batched_input_size = dict zip convolutions op name == nn functional linear is_supported_input = input input dim input rank means no batch dim op name == nn functional layer_norm normalized_shape = input args is_supported_input = input input shape = normalized_shape would cause inter-batch operations op name convolutions currently can t deal padding computation Python level is_supported_input = input input dim == batched_input_size op name op name == nn functional embedding idx = input args is_supported_input = len idx shape there s no batch size is_supported_input = True is_supported_input = is_supported_input input input shape valid batch size is_supported_input supported_inputs is_supported_input input input sample_inputs filter_fn input for_loop_per_sample_grad batch_size reduction input func args kwargs get per sample grads getting derivative each input loop per_sample_grad = i range batch_size per_sample_input = input i result = reduction func per_sample_input unsqueeze args kwargs diff_input_list = per_sample_input + tuple args + tuple kwargs values diff_input_list = i i diff_input_list isinstance i torch Tensor i requires_grad per_sample_grad append torch autograd grad result diff_input_list torch ones_like result allow_unused=True len per_sample_grad == batch_size per_sample_grad = tuple torch stack grad grad zip per_sample_grad per_sample_grad is_diff_tensor t isinstance t ExpandedWeight isinstance t torch Tensor t requires_grad clone_if_tensor t isinstance t torch Tensor res = torch clone t detach res requires_grad_ t requires_grad res t instantiate_device_type_tests TestExpandedWeightHelperFunction globals instantiate_device_type_tests TestExpandedWeightFunctional globals instantiate_device_type_tests TestExpandedWeightModule globals __name__ == __main__ run_tests