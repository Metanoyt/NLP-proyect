Owner s module optimizer itertools pickle torch torch optim swa_utils AveragedModel get_ema_multi_avg_fn get_swa_multi_avg_fn update_bn torch testing _internal common_utils instantiate_parametrized_tests load_tests parametrize TestCase load_tests common_utils used automatically filter tests sharding sandcastle This line silences flake warnings load_tests = load_tests noqa PLW TestSWAUtils TestCase SWATestDNN torch nn Module __init__ input_features super __init__ n_features = fc = torch nn Linear input_features n_features bn = torch nn BatchNorm d n_features compute_preactivation x fc x forward x x = fc x x = bn x x SWATestCNN torch nn Module __init__ input_channels super __init__ n_features = conv = torch nn Conv d input_channels n_features kernel_size= padding= bn = torch nn BatchNorm d n_features momentum= compute_preactivation x conv x forward x x = conv x x = bn x x _test_averaged_model net_device swa_device ema dnn = torch nn Sequential torch nn Conv d kernel_size= torch nn ReLU torch nn MaxPool d kernel_size= torch nn BatchNorm d momentum= torch nn Conv d kernel_size= torch nn ReLU torch nn Linear torch nn ReLU torch nn Linear net_device averaged_params averaged_dnn = _run_averaged_steps dnn swa_device ema p_avg p_swa zip averaged_params averaged_dnn parameters assertEqual p_avg p_swa Check AveragedModel correct device assertTrue p_swa device == swa_device assertTrue p_avg device == net_device assertTrue averaged_dnn n_averaged device == swa_device _run_averaged_steps dnn swa_device ema ema_decay = ema averaged_dnn = AveragedModel dnn device=swa_device multi_avg_fn=get_ema_multi_avg_fn ema_decay averaged_dnn = AveragedModel dnn device=swa_device multi_avg_fn=get_swa_multi_avg_fn averaged_params = torch zeros_like param param dnn parameters n_updates = i range n_updates p p_avg zip dnn parameters averaged_params p detach add_ torch randn_like p ema p_avg += p detach ema_decay n_updates - i - - ema_decay i p_avg += p detach n_updates averaged_dnn update_parameters dnn averaged_params averaged_dnn parametrize ema True False test_averaged_model_all_devices ema cpu = torch device cpu _test_averaged_model cpu cpu ema torch cuda is_available cuda = torch device _test_averaged_model cuda cpu ema _test_averaged_model cpu cuda ema _test_averaged_model cuda cuda ema parametrize ema True False test_averaged_model_mixed_device ema torch cuda is_available dnn = torch nn Sequential torch nn Conv d kernel_size= torch nn Linear dnn cuda dnn cpu averaged_params averaged_dnn = _run_averaged_steps dnn None ema p_avg p_swa zip averaged_params averaged_dnn parameters assertEqual p_avg p_swa Check AveragedModel correct device assertTrue p_avg device == p_swa device test_averaged_model_state_dict dnn = torch nn Sequential torch nn Conv d kernel_size= torch nn Linear averaged_dnn = AveragedModel dnn averaged_dnn = AveragedModel dnn n_updates = _ range n_updates p dnn parameters p detach add_ torch randn_like p averaged_dnn update_parameters dnn averaged_dnn load_state_dict averaged_dnn state_dict p_swa p_swa zip averaged_dnn parameters averaged_dnn parameters assertEqual p_swa p_swa assertTrue averaged_dnn n_averaged == averaged_dnn n_averaged test_averaged_model_default_avg_fn_picklable dnn = torch nn Sequential torch nn Conv d kernel_size= torch nn BatchNorm d torch nn Linear averaged_dnn = AveragedModel dnn pickle dumps averaged_dnn parametrize use_multi_avg_fn True False parametrize use_buffers True False test_averaged_model_exponential use_multi_avg_fn use_buffers Test AveragedModel EMA avg_fn use_buffers True dnn = torch nn Sequential torch nn Conv d kernel_size= torch nn BatchNorm d momentum= torch nn Linear decay = use_multi_avg_fn averaged_dnn = AveragedModel dnn multi_avg_fn=get_ema_multi_avg_fn decay use_buffers=use_buffers avg_fn p_avg p n_avg decay p_avg + - decay p averaged_dnn = AveragedModel dnn avg_fn=avg_fn use_buffers=use_buffers use_buffers dnn_params = list itertools chain dnn parameters dnn buffers dnn_params = list dnn parameters averaged_params = torch zeros_like param param dnn_params param size = torch Size n_updates = i range n_updates updated_averaged_params = p p_avg zip dnn_params averaged_params p size == torch Size continue p detach add_ torch randn_like p i == updated_averaged_params append p clone updated_averaged_params append p_avg decay + p - decay clone averaged_dnn update_parameters dnn averaged_params = updated_averaged_params use_buffers p_avg p_swa zip averaged_params itertools chain averaged_dnn module parameters averaged_dnn module buffers assertEqual p_avg p_swa p_avg p_swa zip averaged_params averaged_dnn parameters assertEqual p_avg p_swa b_avg b_swa zip dnn buffers averaged_dnn module buffers assertEqual b_avg b_swa _test_update_bn dnn dl_x dl_xy cuda preactivation_sum = torch zeros dnn n_features preactivation_squared_sum = torch zeros dnn n_features cuda preactivation_sum = preactivation_sum cuda preactivation_squared_sum = preactivation_squared_sum cuda total_num = x dl_x x = x cuda x = x cuda dnn forward x preactivations = dnn compute_preactivation x len preactivations shape == preactivations = preactivations transpose preactivations = preactivations contiguous view - dnn n_features total_num += preactivations shape preactivation_sum += torch sum preactivations dim= preactivation_squared_sum += torch sum preactivations dim= preactivation_mean = preactivation_sum total_num preactivation_var = preactivation_squared_sum total_num preactivation_var = preactivation_var - preactivation_mean update_bn dl_xy dnn device=x device assertEqual preactivation_mean dnn bn running_mean assertEqual preactivation_var dnn bn running_var atol= e- rtol= _reset_bn module issubclass module __class__ torch nn modules batchnorm _BatchNorm module running_mean = torch zeros_like module running_mean module running_var = torch ones_like module running_var reset batch norm run update_bn again dnn apply _reset_bn update_bn dl_xy dnn device=x device assertEqual preactivation_mean dnn bn running_mean assertEqual preactivation_var dnn bn running_var atol= e- rtol= using dl_x loader instead dl_xy dnn apply _reset_bn update_bn dl_x dnn device=x device assertEqual preactivation_mean dnn bn running_mean assertEqual preactivation_var dnn bn running_var atol= e- rtol= test_update_bn_dnn Test update_bn fully-connected network BatchNorm d objects input_features = x = torch rand objects input_features y = torch rand objects ds_x = torch utils data TensorDataset x ds_xy = torch utils data TensorDataset x y dl_x = torch utils data DataLoader ds_x batch_size= shuffle=True dl_xy = torch utils data DataLoader ds_xy batch_size= shuffle=True dnn = SWATestDNN input_features=input_features dnn train _test_update_bn dnn dl_x dl_xy False torch cuda is_available dnn = SWATestDNN input_features=input_features dnn train _test_update_bn dnn cuda dl_x dl_xy True assertTrue dnn training test_update_bn_cnn Test update_bn convolutional network BatchNorm d objects = input_channels = height width = x = torch rand objects input_channels height width y = torch rand objects ds_x = torch utils data TensorDataset x ds_xy = torch utils data TensorDataset x y dl_x = torch utils data DataLoader ds_x batch_size= shuffle=True dl_xy = torch utils data DataLoader ds_xy batch_size= shuffle=True cnn = SWATestCNN input_channels=input_channels cnn train _test_update_bn cnn dl_x dl_xy False torch cuda is_available cnn = SWATestCNN input_channels=input_channels cnn train _test_update_bn cnn cuda dl_x dl_xy True assertTrue cnn training test_bn_update_eval_momentum check update_bn preserves eval mode objects = input_channels = height width = x = torch rand objects input_channels height width ds_x = torch utils data TensorDataset x dl_x = torch utils data DataLoader ds_x batch_size= shuffle=True cnn = SWATestCNN input_channels=input_channels cnn eval update_bn dl_x cnn assertFalse cnn training check momentum preserved assertEqual cnn bn momentum instantiate_parametrized_tests TestSWAUtils __name__ == __main__ print These tests should run through test test_optim py instead