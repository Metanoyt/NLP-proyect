Owner s module __torch_dispatch__ ruff noqa F pickle sys tempfile unittest copy deepcopy torch torch _dynamo torch SymInt torch _C DispatchKey DispatchKeySet torch _subclasses fake_tensor FakeTensorMode torch cuda jiterator _create_jit_fn torch fx experimental proxy_tensor make_fx torch fx experimental symbolic_shapes ShapeEnv torch library _scoped_library fallthrough_kernel impl Library torch multiprocessing reductions StorageWeakRef torch testing _internal common_device_type instantiate_device_type_tests ops torch testing _internal common_methods_invocations op_db torch testing _internal common_utils first_sample IS_WINDOWS run_tests TEST_WITH_ROCM TestCase torch testing _internal custom_op_db custom_op_db torch testing _internal logging_tensor capture_logs capture_logs_with_logging_tensor_mode log_input LoggingTensor LoggingTensorMode LoggingTensorReentrant torch testing _internal two_tensor TwoTensor torch utils _pytree pytree torch utils _mode_utils all_same_mode no_dispatch torch utils _python_dispatch _get_current_dispatch_mode _get_current_dispatch_mode_stack is_in_torch_dispatch_mode TorchDispatchMode torch utils _pytree tree_map tree_map_only used DataLoader collate_fn below named here avoid trying pickle lambda _identity x x TestDispatcherPythonBindings TestCase test_call_boxed - None sin = torch _C _dispatch_find_schema_or_throw aten sin x = torch randn y = torch _C _dispatch_call_boxed sin x assertEqual y x sin TestPythonRegistration TestCase test_ns = _test_python_registration tearDown hasattr torch ops test_ns del torch ops _test_python_registration test_fallback - None test_key = TESTING_ONLY_GenericMode test_keyset = torch _C DispatchKeySet test_key include_to_set = torch _C _dispatch_tls_local_include_set &#124; test_keyset exclude_to_set = torch _C _dispatch_tls_local_exclude_set _scoped_library _ IMPL my_lib expected_op = None expected_args = None expected_kwargs = None Use out shape make sure result our fallback what returned user out_shape = None my_fallback op args kwargs Disable our handler during checks generating output torch _C _ForceDispatchKeyGuard include_to_set exclude_to_set &#124; test_keyset assertIs op expected_op assertEqual args expected_args assertEqual kwargs expected_kwargs Return something specific torch empty out_shape my_lib fallback my_fallback test_key b = torch rand torch rand torch _C _ForceDispatchKeyGuard include_to_set exclude_to_set Check factory function expected_op = torch ops aten empty memory_format expected_args = Extra kwargs bypass issues default args factory functions expected_kwargs = dtype torch float pin_memory False device torch device cpu out_shape = out = torch empty expected_args expected_kwargs assertEqual out size out_shape Check regular function expected_op = torch ops aten add Tensor expected_args = b expected_kwargs = out_shape = out = + b assertEqual out size out_shape test_fallback_keyset - None test_key_first = TESTING_ONLY_GenericMode test_key_second = TESTING_ONLY_GenericWrapper test_keyset = torch _C DispatchKeySet test_key_first &#124; torch _C DispatchKeySet test_key_second include_to_set = torch _C _dispatch_tls_local_include_set &#124; test_keyset exclude_to_set = torch _C _dispatch_tls_local_exclude_set _scoped_library _ IMPL my_lib first_called = False second_called = False first_fallback keyset op args kwargs nonlocal first_called second_called Recursive call first_called = True torch _C _ForceDispatchKeyGuard include_to_set exclude_to_set &#124; test_keyset op args kwargs Redispatch down keyset = keyset remove test_key_first op redispatch keyset args kwargs second_fallback op args kwargs nonlocal second_called Set avoid infinite recursion second_called = True New dispatcher call should hit first callback again assertFalse first_called b = args Make subtraction here instead add c = - b assertTrue first_called c my_lib fallback first_fallback test_key_first with_keyset=True my_lib fallback second_fallback test_key_second b = torch rand torch rand torch _C _ForceDispatchKeyGuard include_to_set exclude_to_set c = + b assertEqual c - b assertTrue first_called assertTrue second_called test_fallback_fallthrough - None test_key_first = TESTING_ONLY_GenericMode test_key_second = TESTING_ONLY_GenericWrapper test_keyset = torch _C DispatchKeySet test_key_first &#124; torch _C DispatchKeySet test_key_second include_to_set = torch _C _dispatch_tls_local_include_set &#124; test_keyset exclude_to_set = torch _C _dispatch_tls_local_exclude_set _scoped_library _ IMPL my_lib is_called = False my_fallback op args kwargs nonlocal is_called is_called = True torch _C _ForceDispatchKeyGuard include_to_set exclude_to_set &#124; test_keyset op args kwargs my_lib fallback torch library fallthrough_kernel test_key_first my_lib fallback my_fallback test_key_second b = torch rand torch rand torch _C _ForceDispatchKeyGuard include_to_set exclude_to_set c = + b assertEqual c + b assertTrue is_called unittest skip Causing flakiness see https github com pytorch pytorch issues test_fallthrough_for_dense_key_with_meta_in_tls - None This tests meta included TlS dispatch key set then meta kernel should called regardless dense backend has fallthrough kernel = torch randn _scoped_library custom DEF my_lib my_lib define sum Tensor - Tensor meta_is_called = False sum_meta args kwargs nonlocal meta_is_called meta_is_called = True args my_lib impl sum fallthrough_kernel CPU my_lib impl sum sum_meta Meta torch _C _IncludeDispatchKeyGuard torch DispatchKey Meta torch ops custom sum default assertTrue meta_is_called test_dispatchkeyset_pickle - None keyset = torch _C DispatchKeySet torch _C DispatchKey AutogradCPU serialized = pickle dumps keyset new_keyset = pickle loads serialized assertEqual new_keyset keyset test_dispatchkeyset_eq - None = torch _C DispatchKeySet torch _C DispatchKey AutogradCPU b = torch _C DispatchKeySet torch _C DispatchKey AutogradCPU c = torch _C DispatchKeySet torch _C DispatchKey CPU assertTrue == b assertFalse = b assertTrue = c test_override_aten_ops_with_multiple_libraries - None x = torch tensor _scoped_library aten IMPL my_lib _scoped_library aten IMPL my_lib Example my_neg args kwargs args _neg_view Now we secretly making operator view op so autograd needs know how handle my_lib impl neg my_neg AutogradCPU assertTrue torch neg x is_neg RuntimeError impl aten neg Explicitly provided namespace aten operator name does match assertRaisesRegex RuntimeError operator name does match namespace _scoped_library foo DEF my_lib my_lib define neg Tensor - Tensor my_lib impl torch ops aten neg default my_neg AutogradCPU Example my_mul args kwargs torch zeros_like args torch ops aten mul Tensor my_lib impl aten mul Tensor my_mul ZeroTensor y = torch _efficientzerotensor assertFalse torch mul x y _is_zerotensor Assert user can t override behavior ns op dispatch_key combination someone overridden behavior same before them assertRaisesRegex RuntimeError already kernel registered python my_lib impl torch ops aten mul Tensor my_mul ZeroTensor Validate lib affected removing lib assertFalse torch mul x y _is_zerotensor Validate old behavior restored neg mul assertFalse torch neg x is_neg assertTrue torch mul x y _is_zerotensor test_error_if_fn_not_callable assertRaisesRegex TypeError Input function required callable _scoped_library aten IMPL my_lib my_lib impl torch ops aten neg default AutogradCPU test_finalizer impls_refcnt = sys getrefcount torch library _impls lib = Library test_ns FRAGMENT noqa TOR lib define foo Tensor x - Tensor ` lib ` sys getrefcount assertEqual sys getrefcount lib We gained additional reference gets cleared when finalizer runs assertEqual sys getrefcount torch library _impls impls_refcnt + ` lib ` finalizer sys getrefcount assertEqual sys getrefcount lib _op_impls foo x pass lib impl f test_ns foo foo CPU key = f test_ns foo CPU assertTrue key torch library _impls saved_op_impls = lib _op_impls del will definitely work following passes assertEqual sys getrefcount lib del lib saved_op_impls sys getrefcount This function should last user lib _op_impls - lib should have reference anymore del ed - lib s finalizer should have reference anymore assertEqual sys getrefcount saved_op_impls assertTrue key torch library _impls lib s finalizer should have reference anymore assertEqual sys getrefcount torch library _impls impls_refcnt test_override_cpu_sum - None Example run = False my_sum args kwargs run = True args clone _scoped_library aten IMPL my_lib my_lib impl aten sum my_sum CPU x = torch tensor assertEqual torch sum x x assertTrue run Validate old behavior restored sum assertEqual torch sum x torch tensor test_override_cuda_with_jiterator - None override_where_cuda - None Example Invert behavior where s condition input not_where_code_string = template typename T T inverted_where bool cond T T b cond b jitted_where = _create_jit_fn not_where_code_string CALLED = False inverted_where args kwargs CALLED = True jitted_where args kwargs overriding where s cuda kernel Jiterator generated kernel _scoped_library aten IMPL my_lib my_lib impl aten where inverted_where CUDA device = cuda cond = torch tensor True True False device=device dtype=torch bool x = torch tensor device=device y = torch tensor - - - device=device assertEqual torch where cond x y torch tensor - - assertTrue CALLED behavior restored after deregistration assertEqual torch where cond x y torch tensor - override_gelu_cuda - None Example Use relu approximate gelu faster compute fastest_gelu_code_string = template typename T T fast_gelu T jitted_gelu = _create_jit_fn fastest_gelu_code_string CALLED = False fast_gelu args kwargs CALLED = True jitted_gelu args kwargs overriding gelu s cuda kernel Jiterator generated relu kernel _scoped_library aten IMPL my_lib my_lib impl aten gelu fast_gelu CUDA x = torch rand device= cuda dtype=torch float assertEqual torch nn functional gelu x torch nn functional relu x assertTrue CALLED behavior restored after deregistration assertNotEqual torch nn functional gelu x torch nn functional relu x override_exp_cuda - None Example Preventing exp exploding float clipped_exp_code_string = template typename T T clipped_exp T T T exp jitted_exp = _create_jit_fn clipped_exp_code_string CALLED = False clipped_exp args kwargs CALLED = True jitted_exp args kwargs overriding exp s cuda kernel clipped_exp kernel _scoped_library aten IMPL my_lib my_lib impl aten exp clipped_exp CUDA x = torch tensor device= cuda dtype=torch float assertEqual torch exp x torch tensor dtype=torch float assertTrue CALLED behavior restored after deregistration assertEqual torch exp x torch tensor torch inf dtype=torch float override_add_cuda - None Example simulate hardware bug where adder always off buggy_add_code_string = template typename T T buggy_add T T b + b + T jitted_add = _create_jit_fn buggy_add_code_string CALLED = False buggy_add args kwargs CALLED = True jitted_add args kwargs _scoped_library aten IMPL my_lib my_lib impl aten add Tensor buggy_add CUDA x_cpu = torch rand device= cpu y_cpu = torch rand device= cpu x_cuda = x_cpu cuda y_cuda = y_cpu cuda assertEqual x_cuda + y_cuda x_cpu + y_cpu + assertTrue CALLED behavior restored after deregistration assertEqual x_cuda + y_cuda x_cpu + y_cpu torch cuda is_available TEST_WITH_ROCM override_where_cuda override_gelu_cuda override_exp_cuda override_add_cuda test_extend_library_with_dispatch_key_arg my_sum args kwargs args clone _scoped_library aten IMPL dispatch_key= CPU my_lib RuntimeError Explicitly provided dispatch key Conjugate inconsistent dispatch key enclosing TORCH_LIBRARY_IMPL block assertRaisesRegex RuntimeError inconsistent dispatch key my_lib impl sum my_sum Conjugate my_lib impl aten sum my_sum x = torch tensor assertEqual torch sum x x test_create_new_library - None _scoped_library test_ns DEF my_lib my_lib define sum Tensor - Tensor Example torch library impl my_lib sum CPU my_sum args kwargs args clone x = torch tensor op = getattr torch ops test_ns sum assertEqual op x x _scoped_library test_ns IMPL my_lib Example torch library impl my_lib op default ZeroTensor my_sum_zt args kwargs args _is_zerotensor torch _efficientzerotensor args shape args clone y = torch _efficientzerotensor assertTrue op y _is_zerotensor assertEqual op x x test_create_new_library_fragment_no_existing _scoped_library test_ns FRAGMENT my_lib my_lib define sum Tensor - Tensor torch library impl my_lib sum CPU my_sum args kwargs args x = torch tensor assertEqual getattr torch ops test_ns sum x x test_create_new_library_fragment_with_existing _scoped_library test_ns DEF my_lib Create fragment _scoped_library test_ns FRAGMENT my_lib my_lib define sum Tensor - Tensor torch library impl my_lib sum CPU my_sum args kwargs args x = torch tensor assertEqual getattr torch ops test_ns sum x x Create another fragment _scoped_library test_ns FRAGMENT my_lib my_lib define sum Tensor - Tensor torch library impl my_lib sum CPU my_sum args kwargs args x = torch tensor assertEqual getattr torch ops test_ns sum x x unittest skipIf IS_WINDOWS Skipped under Windows test_alias_analysis test_helper alias_analysis= my_lib = Library test_ns DEF noqa TOR called = torch library define my_lib _op - None alias_analysis=alias_analysis _op args kwargs called += torch jit script _test torch ops _test_python_registration _op assert _test_python_registration _op str _test graph assertRaises AssertionError test_helper alias_analysis= FROM_SCHEMA test_helper CONSERVATIVE test_error_for_unsupported_ns_or_kind - None assertRaisesRegex ValueError Unsupported kind my_lib = Library myns BLA noqa TOR kind DEF FRAGMENT assertRaisesRegex ValueError reserved namespace my_lib = Library prim kind noqa TOR test_dispatcher_error_filenames - None Test dispatcher errors report correct Python filenames line numbers when defining duplicate libraries which triggers filename tracking linecache re Create first library NOTE Using Library directly instead _scoped_library because test specifically verifies filename tracking error messages _scoped_library would report library py locations instead actual test file locations lib = Library test_ns DEF FIRST_LIB_MARKER noqa TOR try lib define duplicate_op Tensor x - Tensor Try create another library same namespace - should trigger error assertRaises RuntimeError cm lib = Library test_ns DEF SECOND_LIB_MARKER noqa TOR finally lib _destroy error_msg = str cm exception The error should NOT contain dev null old placeholder assertNotIn dev null error_msg The error should contain test file name both registrations assertIn test_python_dispatch py error_msg Extract line numbers error message verify they point right lines line_matches = re findall r test_python_dispatch\ py \d+ error_msg assertEqual len line_matches Should have exactly line number references Get actual source lines verify they contain our markers first_line_num second_line_num = sorted int x x line_matches first_line = linecache getline __file__ first_line_num strip second_line = linecache getline __file__ second_line_num strip Verify lines contain our expected markers assertIn FIRST_LIB_MARKER first_line assertIn SECOND_LIB_MARKER second_line test_returning_symint - None shape_env = ShapeEnv fake_tensor_mode = FakeTensorMode shape_env=shape_env ft = fake_tensor_mode from_tensor torch rand s s = ft shape _scoped_library test_ns DEF tlib tlib define sqsum SymInt SymInt b - SymInt impl tlib sqsum CompositeExplicitAutograd sqsum SymInt b SymInt + b b out = getattr torch ops test_ns sqsum default s s out_val = shape_env evaluate_expr out node expr assertEqual out_val test_register_fallthrough _scoped_library aten IMPL my_lib my_lib impl mm fallthrough_kernel AutocastCPU = torch randn device= cpu dtype=torch float b = torch randn device= cpu dtype=torch float torch autocast device_type= cpu dtype=torch bfloat dtype mm should float since we registered fallthrough assertEqual torch mm b dtype torch float ops don t have fallthrough registered should affected assertEqual torch matmul b dtype torch bfloat torch autocast device_type= cpu dtype=torch bfloat default behavior should have been restored assertEqual torch mm b dtype torch bfloat TestPythonDispatch TestCase test_basic - None capture_logs logs x = LoggingTensor torch tensor requires_grad=True log_input x x y = x x saved_x = y grad_fn _saved_self grad_y = LoggingTensor torch tensor log_input grad_y grad_y g = torch autograd grad y x grad_y assertEqual g elem torch tensor torch no_grad assertEqual saved_x x assertEqual saved_x _version x _version x add_ assertEqual saved_x x TODO figure out why broken assertEqual saved_x _version x _version assertExpectedInline \n join logs \ $ f = input x $ f = torch _ops aten mul Tensor $ $ $ f = input grad_y $ f = torch _ops aten mul Tensor $ $ $ f = torch _ops aten mul Tensor $ $ $ f = torch _ops aten add Tensor $ $ test_out - None capture_logs logs x = LoggingTensor torch ones y = LoggingTensor torch zeros log_input x x log_input y y torch abs x out=y assertEqual y elem torch ones TODO arguably shouldn t pass we should complain out isn t kwarg assertExpectedInline \n join logs \ $ f = input x $ f = input y $ f = torch _ops aten abs out $ out=$ test_kwarg_only - None capture_logs logs x = LoggingTensor torch ones y = LoggingTensor torch ones z = LoggingTensor torch ones log_input x x log_input y y log_input z z torch addmv x y z torch addmv x y z beta= torch addmv x y z beta= torch addmv x y z alpha= torch addmv x y z beta= alpha= The expectation beta alpha don t show up when they re defaulted This even user explicitly specified assertExpectedInline \n join logs \ $ f = input x $ f = input y $ f = input z $ f = torch _ops aten addmv default $ $ $ $ f = torch _ops aten addmv default $ $ $ $ f = torch _ops aten addmv default $ $ $ beta= $ f = torch _ops aten addmv default $ $ $ alpha= $ f = torch _ops aten addmv default $ $ $ beta= alpha= test_kwarg_only_and_positional_default - None capture_logs logs x = LoggingTensor torch ones log_input x x torch ops aten _foobar x torch ops aten _foobar x False torch ops aten _foobar x arg =False torch ops aten _foobar x False arg =False What we testing here we omit arg defaulted even kwarg set assertExpectedInline \n join logs \ $ f = input x $ f = torch _ops aten _foobar default $ $ f = torch _ops aten _foobar default $ False $ f = torch _ops aten _foobar default $ arg =False $ f = torch _ops aten _foobar default $ False arg =False test_produce_real_type - None capture_logs logs x = LoggingTensor torch ones log_input x x x dtype=torch double non-optional dtype torch cumprod x dtype=torch double optional dtype x contiguous memory_format=torch contiguous_format optional memory format There doesn t appear any layout signatures which triggerable using tensor subclasses need use mode assertExpectedInline \n join logs \ $ f = input x $ f = torch _ops aten _to_copy default $ dtype=torch float $ f = torch _ops aten cumprod default $ dtype=torch float $ f = torch _ops aten select int $ $ f = torch _ops aten clone default $ memory_format=torch contiguous_format test_optional_tensor_list - None weird xs print woof torch empty _scoped_library my_lib DEF my_lib my_lib define weird Tensor - Tensor my_lib impl weird weird CPU capture_logs logs x = LoggingTensor torch ones log_input x x torch ops my_lib weird default None x assertExpectedInline \n join logs \ $ f = input x $ f = torch _ops my_lib weird default None $ test_list_ret - None test all sequence types permissible returns list_type list tuple A torch Tensor staticmethod __new__ cls elem torch Tensor _make_subclass cls elem elem requires_grad classmethod __torch_dispatch__ cls func types args= kwargs=None func overloadpacket == torch ops aten split no_dispatch list_type torch split args raise AssertionError f unrecognized func func assertEqual torch split A torch tensor torch split torch tensor test_invalid_ret - None test invalid gets reasonable error message A torch Tensor staticmethod __new__ cls elem torch Tensor _make_subclass cls elem elem requires_grad classmethod __torch_dispatch__ cls func types args= kwargs=None arf Wobbles depending NDEBUG mode pybind assertRaisesRegex RuntimeError Unable cast lambda A torch zeros neg assertRaisesRegex RuntimeError Unable cast lambda A torch zeros detach test_detach_appears_once_when_called_once - None capture_logs logs x = LoggingTensor torch tensor requires_grad=True log_input x x x detach FIXME We actually want emit single detach However currently emits two reasons unclear us Leaving test here make sure we don t regress even further would bad calling detach once emits + detaches assertExpectedInline \n join logs \ $ f = input x $ f = torch _ops aten detach default $ test_storage - None For now just make sure doesn t crash Ideally we should some virtual storage safe work x = LoggingTensor torch ones storage = x untyped_storage assertRaises RuntimeError lambda storage data_ptr test_make_wrapper_subclass_noalloc - None This ludicrously big TB should pass because wrapper subclasses don t allocate torch Tensor _make_wrapper_subclass LoggingTensor test_version - None x = LoggingTensor torch ones prev_vc = x _version x detach add_ cur_vc = x _version assertNotEqual prev_vc cur_vc x data add_ assertEqual cur_vc x _version test_subclass_priority - None ErrorA RuntimeError pass ErrorB RuntimeError pass The big tests code coverage test_precedence_semantics test_overrides py just make sure wired up all correctly __torch_dispatch__ A torch Tensor staticmethod __new__ cls elem torch Tensor _make_subclass cls elem elem requires_grad classmethod __torch_dispatch__ cls func types args= kwargs=None raise ErrorA B A staticmethod __new__ cls elem torch Tensor _make_subclass cls elem elem requires_grad classmethod __torch_dispatch__ cls func types args= kwargs=None raise ErrorB assertRaises ErrorA lambda torch add A torch empty A torch empty assertRaises ErrorB lambda torch add A torch empty B torch empty assertRaises ErrorB lambda torch add B torch empty A torch empty assertRaises ErrorB lambda torch add B torch empty B torch empty test_format - None x = LoggingTensor torch ones s = str x s = repr x s = f x assertExpectedInline s LoggingTensor tensor assertEqual s s assertEqual s s test_custom_autograd - None escape = None Square torch autograd Function staticmethod forward ctx x y = x ctx save_for_backward x y staticmethod backward ctx grad_output assert isinstance grad_output LoggingTensor x = ctx saved_tensors assert isinstance x LoggingTensor escape = x grad_output x capture_logs logs x = LoggingTensor torch ones requires_grad=True log_input x x x grad = LoggingTensor torch zeros log_input x grad x grad y = Square apply x grad_output = LoggingTensor torch ones log_input grad_output grad_output y backward grad_output torch no_grad assertEqual escape x assertEqual escape _version x _version TODO figure out why x requires_grad = False doesn t trigger error LoggingTensor x add_ assertEqual escape x TODO figure out why broken assertEqual escape _version x _version assertExpectedInline \n join logs \ $ f = input x $ f = input x grad $ f = torch _ops aten pow Tensor_Scalar $ $ f = input grad_output $ f = torch _ops aten mul Tensor $ $ f = torch _ops aten mul Tensor $ $ $ f = torch _ops aten add_ Tensor $ $ test_subclass_creation Make sure these statements runs without error In particular checking when internal detach returns subclasses these cleanly overwritten Foo torch Tensor pass err_msg = subclass Foo already associated python object type LoggingTensor assertRaisesRegex RuntimeError err_msg = torch Tensor _make_subclass Foo LoggingTensor torch rand assertRaisesRegex RuntimeError err_msg b = LoggingTensor torch rand as_subclass Foo assertRaisesRegex RuntimeError err_msg Foo LoggingTensor torch rand assertRaisesRegex TypeError Foo must define __torch_dispatch__ torch Tensor _make_wrapper_subclass Foo test_new_ones - None MyTensor torch Tensor classmethod __torch_dispatch__ cls func types args= kwargs=None MyTensor assertEqual type MyTensor new_ones MyTensor test_like - None MyTensor torch Tensor classmethod __torch_dispatch__ cls func types args= kwargs=None MyTensor f empty ones rand randn zeros f_name = f + _like assertEqual type getattr torch f_name MyTensor MyTensor assertEqual type torch full_like MyTensor MyTensor assertEqual type torch randint_like MyTensor high= MyTensor test_make_fx_with_subclass - None f x y Returns TwoTensor Tensor x y y + y x_a = torch zeros x_b = torch zeros y = torch ones make_fx responsible unwrapping tensor subclass inputs so we do manually here Why In general make_fx f args promises graph returned has same calling convention f args Unwrapping tensor subclass inputs can potentially change number input args graph breaking assumption f_to_trace x_a x_b y x = TwoTensor x_a x_b out out = f x y out _unwrapped_attrs _ = out __tensor_flatten__ getattr out attr attr out _unwrapped_attrs out fx_g = make_fx f_to_trace tracing_mode= fake x_a x_b y assertExpectedInline fx_g code \ forward x_a_ x_b_ y_ mul = torch ops aten mul Tensor x_a_ y_ x_a_ = None mul_ = torch ops aten mul Tensor x_b_ y_ x_b_ = None add = torch ops aten add Tensor y_ y_ y_ = None mul mul_ add See https github com pytorch pytorch issues test_return_and_correct_aliasing_gives_correct_stride t = TwoTensor torch randn torch randn x = torch randn slicing should result same stride TwoTensor dense tensor would give assertEqual t stride x stride test_make_wrapper_subclass_propagates_metadata - None WrapperTensor torch Tensor elem torch Tensor __slots__ = elem staticmethod __new__ cls elem args kwargs r = torch Tensor _make_wrapper_subclass type ignore attr-defined cls elem size dtype=elem dtype layout=elem layout device=elem device requires_grad=elem requires_grad strides=elem stride storage_offset=elem storage_offset r elem = elem r classmethod __torch_dispatch__ cls func types args= kwargs=None raise RuntimeError NYI non-contiguous strides non-zero storage offset x = torch randn t diagonal offset= y = WrapperTensor x assertEqual y size x size assertEqual y stride x stride assertEqual y storage_offset x storage_offset test_wrapper_subclass_serializes - None tempfile TemporaryFile f purposefully use int test non-default dtype x = LoggingTensor torch randperm torch save x f f seek torch serialization safe_globals LoggingTensor x_loaded = torch load f assertTrue type x_loaded type x assertEqual x x_loaded assertEqual x elem x_loaded elem assertFalse x x_loaded test_deepcopy_wrapper_subclass - None purposefully use int test non-default dtype x = LoggingTensor torch randperm x_copy = deepcopy x assertTrue type x_copy type x assertEqual x x_copy assertEqual x elem x_copy elem assertFalse x x_copy test_deepcopy_wrapper_subclass_with_clone_returning_different_type - None MyWrapperTensor torch Tensor elem torch Tensor __slots__ = elem staticmethod __new__ cls elem args kwargs r = torch Tensor _make_wrapper_subclass type ignore attr-defined cls elem size dtype=elem dtype layout=elem layout device=elem device requires_grad=elem requires_grad strides=elem stride storage_offset=elem storage_offset r elem = elem r classmethod __torch_dispatch__ cls func types args= kwargs=None func overloadpacket __name__ == clone Return plain tensor clone args elem clone raise RuntimeError NYI NB The default Tensor __torch_function__ implementation called deepcopy disables __torch_function__ time we get clone so there no need explicitly disable __torch_function__ subclass x = MyWrapperTensor torch randn assertRaisesRegex RuntimeError which cloning returns another instance same subclass x_copy = deepcopy x test_deepcopy_non_wrapper_subclass - None Ensure correct error thrown common error cases SubTensorError torch Tensor Default implementation new_empty returns plain tensor pass SubTensorError torch Tensor new_empty incorrectly returns different type i e plain tensor new_empty shape torch Tensor shape error_cls SubTensorError SubTensorError x = error_cls assertRaisesRegex RuntimeError which function returns another instance same subclass x_copy = deepcopy x Ensure correctly implemented new_empty causes deepcopy work SubTensorSuccess torch Tensor new_empty shape type shape x = SubTensorSuccess x_copy = deepcopy x assertIs type x_copy type x test_wrapper_subclass_extra_dispatch_keys - None ExtraKeysTensor torch Tensor staticmethod __new__ cls elem args kwargs NB only non-kwarg overload _make_wrapper_subclass supports extra dispatch keys We probably want unify two APIs future r = torch Tensor _make_wrapper_subclass type ignore attr-defined cls elem size elem stride elem storage_offset torch contiguous_format elem dtype elem layout elem device False False None False False DispatchKeySet DispatchKey NestedTensor r classmethod __torch_dispatch__ cls func types args= kwargs=None pass x = ExtraKeysTensor torch randn assertTrue torch _C _dispatch_keys x has DispatchKey NestedTensor assertFalse torch _C _dispatch_keys x has DispatchKey AutogradNestedTensor test_wrapper_subclass_multiprocessing_preserves_dtype b have dtype int which purposefully different default assumed _make_wrapper_subclass = torch randperm b = torch randperm data = TwoTensor b expected_dtype = data dtype loader = torch utils data DataLoader data data batch_size= num_workers= collate_fn=_identity batch loader assertEqual batch dtype expected_dtype test_index_put_where_only_index_is_subclass - None called_funcs = MyTensor torch Tensor elem torch Tensor __slots__ = elem staticmethod __new__ cls elem args kwargs r = torch Tensor _make_wrapper_subclass cls elem size dtype=elem dtype layout=elem layout device=elem device requires_grad=elem requires_grad r elem = elem r classmethod __torch_dispatch__ cls func types args= kwargs=None called_funcs append func MyTensor torch tensor x = torch randn idxs = MyTensor torch tensor v = torch randn res = x index_put_ idxs v assertEqual called_funcs torch ops aten index_put_ default test_torch_dispatch_mode_basic - None capture_logs is_mode=True logs LoggingTensorMode torch empty assertExpectedInline \n join logs \ $ f = torch _ops aten empty memory_format device=device type= cpu pin_memory=False test_torch_dispatch_mode_unrelated_tensors - None x = torch randn y = torch randn capture_logs is_mode=True logs LoggingTensorMode x + y assertExpectedInline \n join logs $ f = torch _ops aten add Tensor $ $ test_nested_push_logging_tensor_mode x = torch randn y = torch randn capture_logs is_mode=True logs LoggingTensorMode LoggingTensorMode torch empty x + y assertExpectedInline \n join logs \ $ f = torch _ops aten empty memory_format device=device type= cpu pin_memory=False $ f = torch _ops aten empty memory_format device=device type= cpu pin_memory=False $ f = torch _ops aten add Tensor $ $ $ f = torch _ops aten add Tensor $ $ test_capture_logs_with_torch_dispatch_mode x = torch randn y = torch randn capture_logs_with_logging_tensor_mode logs torch empty x + y assertExpectedInline \n join logs \ $ f = torch _ops aten empty memory_format device=device type= cpu pin_memory=False $ f = torch _ops aten add Tensor $ $ x = torch randn y = torch randn capture_logs_with_logging_tensor_mode logs capture_logs_with_logging_tensor_mode logs torch empty x + y assertExpectedInline \n join logs \ $ f = torch _ops aten empty memory_format device=device type= cpu pin_memory=False $ f = torch _ops aten empty memory_format device=device type= cpu pin_memory=False $ f = torch _ops aten add Tensor $ $ $ f = torch _ops aten add Tensor $ $ assertEqual logs logs test_torch_dispatch_mode_subclass_priority - None ErrorA RuntimeError pass ErrorB RuntimeError pass A torch Tensor staticmethod __new__ cls elem torch Tensor _make_subclass cls elem elem requires_grad classmethod __torch_dispatch__ cls func types args= kwargs=None AMode raise ErrorA B A staticmethod __new__ cls elem torch Tensor _make_subclass cls elem elem requires_grad classmethod __torch_dispatch__ cls func types args= kwargs=None BMode func args kwargs AMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None raise ErrorA BMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None raise ErrorB = A torch empty b = B torch empty assertRaises ErrorA + assertRaises ErrorB + b B has precedence over A due subclass relationship yet modes take precedence over arguments assertRaises ErrorA AMode b + b assertRaises ErrorB BMode + assertRaises ErrorB BMode + b test_mode_with_make_subclass SubTensor torch Tensor staticmethod __new__ cls elem torch Tensor _make_subclass cls elem elem requires_grad BasicMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None func args kwargs x = torch randn BasicMode y = SubTensor x assertIsInstance y SubTensor test_torch_dispatch_mode_respects_no_dispatch - None capture_logs is_mode=True logs LoggingTensorMode torch ones no_dispatch torch ones capture_logs is_mode=True logs LoggingTensorMode torch ones assertEqual logs logs test_shallow_copy_and_detach - None seen = set test_case = TestMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None tree_map_only torch Tensor lambda t test_case assertIn t seen args kwargs kwargs None kwargs = r = func args kwargs tree_map_only torch Tensor lambda t seen add t r r TestMode x = torch randn requires_grad=True loss = x x sum loss backward test_exception_handling A torch Tensor staticmethod __new__ cls elem torch Tensor _make_subclass cls elem elem requires_grad AMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None func __name__ == randn default raise RuntimeError A torch zeros AMode try torch randn except RuntimeError pass assertTrue isinstance torch zeros A test_with_mode_created_separately ErrorA RuntimeError pass A TorchDispatchMode __torch_dispatch__ func types args= kwargs=None raise ErrorA x = A assertRaises ErrorA x torch empty test_with_nested_modes ErrorA RuntimeError __init__ msg super __init__ msg A TorchDispatchMode __init__ msg msg = msg __torch_dispatch__ func types args= kwargs=None raise ErrorA msg assertRaisesRegex ErrorA layer A layer A layer torch empty test_make_subclass_with_modes ModeTensor torch Tensor __new__ cls elem mode r = torch Tensor _make_subclass cls elem elem requires_grad r elem = elem r mode = mode r classmethod __torch_dispatch__ cls func types args= kwargs=None raise NotImplementedError Shouldn t here Mode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None unwrap e isinstance e ModeTensor e elem e wrap t isinstance t torch Tensor ModeTensor t t wrap func tuple unwrap args kwargs BasicMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None func args kwargs x = torch tensor Mode y = x + x z = y + y assertIsInstance y ModeTensor assertIsInstance z ModeTensor Mode BasicMode we can t nest two modes call make_subclass because only accepts vanilla tensors y = x + x z = y + y assertIsInstance y ModeTensor assertIsInstance z ModeTensor assert assertRaisesRegex RuntimeError subclass Mode associated python object type Mode test_notimplemented_mode sub_count = PoliteMode TorchDispatchMode __init__ - None pre_count = post_count = __torch_dispatch__ func types args= kwargs=None pre_count += any t torch Tensor t types NotImplemented post_count += func args kwargs SubTensor torch Tensor __new__ cls elem r = torch Tensor _make_wrapper_subclass cls elem shape r elem = elem r classmethod __torch_dispatch__ cls func types args= kwargs=None nonlocal sub_count sub_count += unwrap t isinstance t SubTensor t elem t func tree_map unwrap args tree_map unwrap kwargs = SubTensor torch randn PoliteMode mode abs assertEqual mode pre_count assertEqual mode post_count assertEqual sub_count make sure doesn t error PoliteMode PoliteMode abs test_nesting_same_mode If pushed mode same instance current mode we allow pushing already active mode capture_logs is_mode=True logs LoggingTensorMode reenabled reenabled torch empty assertExpectedInline \n join logs \ $ f = torch _ops aten empty memory_format device=device type= cpu pin_memory=False $ f = torch _ops aten empty memory_format device=device type= cpu pin_memory=False test_error_using_class_method_on_mode A TorchDispatchMode classmethod __torch_dispatch__ cls func types args= kwargs=None func args kwargs x = torch tensor assertRaisesRegex RuntimeError classmethod supported please make plain method A x + x test_get_cur_mode A TorchDispatchMode __torch_dispatch__ func types args= kwargs=None pass assertEqual _get_current_dispatch_mode None A mode assertEqual _get_current_dispatch_mode mode mode A mode assertEqual _get_current_dispatch_mode mode test_get_mode_stack A TorchDispatchMode __torch_dispatch__ func types args= kwargs=None pass assertEqual _get_current_dispatch_mode_stack A mode assertEqual _get_current_dispatch_mode_stack mode mode A mode assertEqual _get_current_dispatch_mode_stack mode mode test_all_same_mode x = LoggingTensorMode y = LoggingTensorMode assertTrue all_same_mode x x x assertFalse all_same_mode x None assertFalse all_same_mode x y test_mode_detection InfraMode TorchDispatchMode classmethod is_infra_mode cls True NonInfraMode TorchDispatchMode pass InfraMode assertTrue is_in_torch_dispatch_mode assertFalse is_in_torch_dispatch_mode include_infra_modes=False NonInfraMode assertTrue is_in_torch_dispatch_mode assertTrue is_in_torch_dispatch_mode include_infra_modes=False InfraMode assertTrue is_in_torch_dispatch_mode assertTrue is_in_torch_dispatch_mode include_infra_modes=False assertTrue is_in_torch_dispatch_mode assertTrue is_in_torch_dispatch_mode include_infra_modes=False assertTrue is_in_torch_dispatch_mode assertFalse is_in_torch_dispatch_mode include_infra_modes=False assertFalse is_in_torch_dispatch_mode assertFalse is_in_torch_dispatch_mode include_infra_modes=False test_tolist_numpy_with_torch_dispatch_mode - None x = LoggingTensor torch tensor assertRaisesRegex RuntimeError supported tensor subclasses x tolist assertRaisesRegex RuntimeError supported tensor subclasses x numpy assertRaises AssertionError assertEqual x None See https github com pytorch pytorch issues test_view_returns_alias_under_torch_dispatch MyMode TorchDispatchMode __init__ testcase testcase = testcase __torch_dispatch__ func types args= kwargs=None out = func args kwargs func == torch ops aten view dtype view should fresh TensorImpl testcase assertTrue out args out MyMode x = torch ones dtype=torch float out = x view torch float test_record_stream - None TestMode TorchDispatchMode __init__ testcase testcase = testcase __torch_dispatch__ func types args= kwargs=None testcase assertEqual func name aten record_stream testcase assertIsInstance args torch Tensor testcase assertIsInstance args torch Stream testcase assertEqual args stream_id testcase assertEqual args device_index testcase assertEqual args device_type t = torch tensor s = torch Stream stream_id= device_index= device_type= TestMode t record_stream s test_return_stream - None _scoped_library test_return_stream DEF l_def l_def define return_stream Tensor - Stream _scoped_library test_return_stream IMPL CPU l_impl l_impl impl return_stream lambda _ torch Stream stream_id= device_index= device_type= TestMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None torch Stream stream_id= device_index= device_type= t = torch tensor s = torch ops test_return_stream return_stream t assertIsInstance s torch Stream assertEqual s stream_id assertEqual s device_index assertEqual s device_type TestMode s = torch ops test_return_stream return_stream t assertIsInstance s torch Stream assertEqual s stream_id assertEqual s device_index assertEqual s device_type test_none_wrapping A Tensor subclass returns None when doing add See LoggingTensor above more details subclass SubclassWithNone torch Tensor staticmethod __new__ cls elem args kwargs r = torch Tensor _make_wrapper_subclass cls elem size dtype=elem dtype layout=elem layout device=elem device requires_grad=elem requires_grad r elem = elem r classmethod __torch_dispatch__ cls func types args= kwargs=None unwrap e e elem isinstance e SubclassWithNone e wrap e SubclassWithNone e isinstance e torch Tensor e rs = tree_map wrap func tree_map unwrap args tree_map unwrap kwargs func overloadpacket __name__ == add None rs x = SubclassWithNone torch rand Make sure both run without error assertIsInstance x SubclassWithNone assertIsNone x + x requires_grad_ out = x acos sum The backward acos does add then rsqrt so here we make sure undefined Tensor generated user code nicely handled If acos formula changes future can replaced any other function does add then something backward composite way assertRaisesRegex RuntimeError got None out backward test_storage_can_be_converted_to_python_object s = torch Storage z = LoggingTensor torch empty z set_ s test_autograd_in_attr We want wrapped Tensor require gradients true_t = torch rand requires_grad=True t = LoggingTensorReentrant true_t out = t + assertFalse out requires_grad assertIsNone out grad_fn assertTrue out elem requires_grad assertIsNotNone out elem grad_fn assertRaisesRegex RuntimeError does require grad out sum backward out elem sum backward assertIsNone t grad assertIsNotNone t elem grad test_dispatch_super_call called = SubTensor torch Tensor staticmethod __new__ cls elem torch Tensor _make_subclass cls elem classmethod __torch_dispatch__ cls func types args= kwargs=None called append func super __torch_dispatch__ func types args kwargs x = torch randn y = torch randn assertEqual SubTensor x + SubTensor y x + y assertEqual called torch ops aten add Tensor test_dispatch_super_call_list_arg called = SubTensorWithListArg torch Tensor staticmethod __new__ cls elem torch Tensor _make_subclass cls elem classmethod __torch_dispatch__ cls func types args= kwargs=None called append func super __torch_dispatch__ func types list args kwargs x = torch randn assertEqual SubTensorWithListArg x neg x neg assertEqual called torch ops aten neg default test_dispatch_super_dont_autograd called = SubTensor torch Tensor staticmethod __new__ cls elem torch Tensor _make_subclass cls elem elem requires_grad classmethod __torch_dispatch__ cls func types args= kwargs=None called append func This argument still requires grad because passed through directly assertTrue args requires_grad r = super __torch_dispatch__ func types args kwargs But output better require grad because means you did autograd again torch dispatch oops assertFalse r requires_grad r x = SubTensor torch randn requires_grad=True x neg assertEqual called torch ops aten neg default test_set_data called = SubTensor torch Tensor classmethod __torch_dispatch__ cls func types args= kwargs=None nonlocal called called += super __torch_dispatch__ func types args kwargs x = SubTensor torch empty x data assertEqual called x data = torch empty assertEqual called x data assertEqual called assertIs type x SubTensor x set_ torch empty assertEqual called x data assertEqual called assertIs type x SubTensor test_construct_int_tensor SubTensor torch Tensor pass should fail SubTensor torch zeros dtype=torch int test_multiple_ops_subclass This Direct Subclass don t do MySubclass torch Tensor staticmethod __new__ cls elem r = torch Tensor _make_subclass cls elem r classmethod __torch_dispatch__ cls func types args= kwargs=None no_dispatch func args kwargs x = MySubclass torch rand dtype=torch complex y = x conj Details bug tests Here y dispatch keys PythonTLSSnapshot AutogradCPU Conjugate Python CPU There few calls dispatcher going happen here - call_exp User calling exp y - PythonTLSSnapshot records TLS entry redispatch - AutogradCPU no input requires grad so does nothing redispatch - Conjugate no special implementation exp use fallback first clone Tensor materialize conj then redispatch - call_clone conjugate fallback calling clone y - PythonTLSSnapshot records TLS entry redispatch - AutogradCPU skipped autograd added itself exclude set above - Conjugate special implementation clone just skip key - Python Reset TLS based snapshot above call user implementation actually calls into dispatcher again since we disable both our keys before detailed here - exit Python restore TLS exit - exit Conjugate nothing inplace so just exit - exit PythonTLSSnapshot done call reset saved TLS empty - Python Reset TLS again based snapshot - used fail - More steps y exp staticmethod subclass_helper cls data use_wrapper_subclass kwargs use_wrapper_subclass kwargs device = data device kwargs dtype = data dtype kwargs layout = data layout kwargs requires_grad = True torch Tensor _make_wrapper_subclass cls data size kwargs type ignore attr-defined torch Tensor _make_subclass cls data True kwargs test_is_contiguous_slow_path data = torch randn contiguous_data = data clone not_contiguous_data = torch as_strided data clone use_wrapper_subclass True False ExampleTensor torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= strides classmethod __torch_dispatch__ cls func types args kwargs NotImplemented ExampleTensor torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= strides classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops aten is_contiguous contiguous_data is_contiguous func overloadpacket == torch ops aten sym_is_contiguous torch ops aten sym_is_contiguous contiguous_data NotImplemented ExampleTensor torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= strides classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops aten is_contiguous not_contiguous_data is_contiguous func overloadpacket == torch ops aten sym_is_contiguous torch ops aten sym_is_contiguous not_contiguous_data NotImplemented err_msg = Multiple dispatch failed torch ops aten is_contiguous e = ExampleTensor torch randn use_wrapper_subclass assertRaisesRegex TypeError err_msg e is_contiguous assertRaisesRegex TypeError err_msg e contiguous e = ExampleTensor torch randn use_wrapper_subclass assertEqual e is_contiguous True e contiguous will just original TensorImpl since is_contiguous = True err_msg = Multiple dispatch failed e = ExampleTensor torch randn use_wrapper_subclass assertEqual e is_contiguous False assertRaisesRegex TypeError err_msg e contiguous test_fancy_strides calls = ExampleTensor torch Tensor staticmethod __new__ cls data TestPythonDispatch subclass_helper cls data False dispatch_sizes_strides_policy= strides classmethod __torch_dispatch__ cls func types args kwargs func torch ops aten sym_is_contiguous default torch ops aten is_contiguous default torch ops aten is_contiguous memory_format torch ops aten is_strides_like_format default torch ops aten is_non_overlapping_and_dense default torch ops aten stride default calls append func list args None no_dispatch func args kwargs e = ExampleTensor torch randn assertFalse e is_contiguous memory_format=torch channels_last assertEqual calls torch ops aten is_contiguous memory_format torch channels_last calls clear assertFalse torch ops aten is_strides_like_format default e torch channels_last assertEqual calls torch ops aten is_strides_like_format default torch channels_last calls clear assertTrue torch ops aten is_non_overlapping_and_dense default e assertEqual calls torch ops aten is_non_overlapping_and_dense default test_device_slowpath use_wrapper_subclass True ExampleTensor torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_device=True classmethod __torch_dispatch__ cls func types args kwargs NotImplemented ExampleTensor torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_device=True classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops prim device torch device meta NotImplemented ExampleTensor torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_device=True classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops prim device torch device meta NotImplemented err_msg = Multiple dispatch failed torch ops prim device assertRaisesRegex TypeError err_msg e = ExampleTensor torch randn use_wrapper_subclass e device ten = torch rand e = ExampleTensor torch randn device= cpu use_wrapper_subclass assertEqual e device type meta assertEqual ten type_as e device type meta e = ExampleTensor torch randn device= cpu use_wrapper_subclass assertEqual e device type meta assertEqual ten type_as e device type meta test_dim_slowpath data = torch randn use_wrapper_subclass True False DimNotImplementedTensor torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= sizes classmethod __torch_dispatch__ cls func types args kwargs NotImplemented DimImplementedTensor torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= sizes classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops aten dim data dim NotImplemented err_msg = Multiple dispatch failed torch ops aten dim e = DimNotImplementedTensor torch randn use_wrapper_subclass assertRaisesRegex TypeError err_msg e dim t = DimImplementedTensor torch randn use_wrapper_subclass assertEqual t dim test_maybe_tuple_bug T torch Tensor classmethod __torch_function__ cls args kwargs pass = torch rand T T test_standard_is_not_subclass https github com pytorch pytorch issues assertFalse torch _C _dispatch_isTensorSubclassLike torch empty test_sym_sizes_strides_slow_path TestTensor torch Tensor staticmethod __new__ cls args kwargs r = torch Tensor _make_wrapper_subclass type ignore attr-defined cls dispatch_sizes_strides_policy= sizes r classmethod __torch_dispatch__ cls func types args= kwargs=None func torch ops aten sym_size default torch ops aten sym_stride default torch _dynamo source ConstantSource torch fx experimental symbolic_shapes DimDynamic ShapeEnv shape_env = ShapeEnv si = shape_env create_symintnode shape_env create_symbol source=ConstantSource abc dynamic_dim=DimDynamic DUCK constraint_dim=None hint= si t = TestTensor si = t size assertIsInstance si torch SymInt si = t stride assertIsInstance si torch SymInt test_strides_slow_path use_wrapper_subclass True False StridesNotImplemented torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= strides classmethod __torch_dispatch__ cls func types args kwargs NotImplemented StridesCustomReturn torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= strides classmethod __torch_dispatch__ cls func types args kwargs func == torch ops aten sym_stride default NotImplemented StridesDefaultReturn torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= strides classmethod __torch_dispatch__ cls func types args kwargs func == torch ops aten sym_stride default None NotImplemented err_msg = Multiple dispatch failed torch ops aten sym_stride e = StridesNotImplemented torch randn use_wrapper_subclass assertRaisesRegex TypeError err_msg e stride e = StridesCustomReturn torch randn use_wrapper_subclass assertEqual e stride e = StridesDefaultReturn torch randn use_wrapper_subclass assertEqual e stride test_sizes_slow_path use_wrapper_subclass True False data = torch randn SizesNotImplemented torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= sizes classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops aten dim data dim NotImplemented SizesCustomReturn torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= sizes classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops aten dim data dim func overloadpacket == torch ops aten sym_size NotImplemented SizesDefaultReturn torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= sizes classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops aten dim data dim func overloadpacket == torch ops aten sym_size None NotImplemented err_msg = Multiple dispatch failed torch ops aten sym_size e = SizesNotImplemented torch randn use_wrapper_subclass assertRaisesRegex TypeError err_msg e size e = SizesCustomReturn torch randn use_wrapper_subclass assertEqual e size e = SizesDefaultReturn torch randn use_wrapper_subclass assertEqual e size test_custom_size_policy_dynamic_shapes data = torch randn CustomSizeDynamicShapesTensor torch Tensor staticmethod __new__ cls inner torch Tensor _make_wrapper_subclass TODO right now _make_wrapper_subclass s dynamic shape interaction great Calling overload has kwargs causes us go down first overload path which will always specialize sizes We should probably eventually fix so first overload can just handle dynamic shapes cls inner size inner stride None None inner dtype inner layout inner device False inner requires_grad sizes __init__ inner inner = inner classmethod __torch_dispatch__ cls func types args kwargs func == torch ops aten sym_size default args inner shape func == torch ops aten sym_stride default args inner shape NotImplemented x = torch ones trace_fn x x_wrapper = CustomSizeDynamicShapesTensor x x_wrapper size x_wrapper stride fx_g = make_fx trace_fn tracing_mode= symbolic x assertExpectedInline fx_g code strip \ forward x_ sym_size_int = torch ops aten sym_size int x_ sym_size_int_ = torch ops aten sym_size int x_ x_ = None sym_size_int sym_size_int_ sym_size_int sym_size_int_ test_data_ptr_respects_numel_slow_path data = torch randn NumelDefaultReturn torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_sizes_strides_policy= sizes classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops aten dim data dim func overloadpacket == torch ops aten numel numel_called = True None NotImplemented use_wrapper_subclass False True numel_called = False e = NumelDefaultReturn torch randn use_wrapper_subclass e data_ptr assertTrue numel_called test_layout_slow_path use_wrapper_subclass True False data = torch randn LayoutNotImplemented torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_layout=True classmethod __torch_dispatch__ cls func types args kwargs NotImplemented LayoutCustomReturn torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_layout=True classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops prim layout torch sparse_csr NotImplemented LayoutDefaultReturn torch Tensor staticmethod __new__ cls data wrapper TestPythonDispatch subclass_helper cls data wrapper dispatch_layout=True classmethod __torch_dispatch__ cls func types args kwargs func overloadpacket == torch ops prim layout data layout NotImplemented err_msg = Multiple dispatch failed torch ops prim layout e = LayoutNotImplemented torch randn use_wrapper_subclass assertRaisesRegex TypeError err_msg e layout e = LayoutCustomReturn torch randn use_wrapper_subclass assertEqual e layout torch sparse_csr e = LayoutDefaultReturn torch randn use_wrapper_subclass assertEqual e layout torch strided test_wrapper_subclass_reentrant_dispatch_with_mode Tests interaction between wrapper subclass using reentrant dispatch TorchDispatchMode See https github com pytorch pytorch issues simple passthrough TorchDispatchMode CustomDispatchMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None func args kwargs derive TwoTensor minimize boilerplate MySubclass TwoTensor __torch_dispatch__ func types args kwargs=None torch overrides enable_reentrant_dispatch func args t = MySubclass torch rand torch rand CustomDispatchMode res = t clone assertEqual res t assertIs type res torch Tensor test_custom_dispatch_mode_supports_higher_order_operators Mode TorchDispatchMode supports_higher_order_operators = True __torch_dispatch__ func types args= kwargs=None func torch ops higher_order cond torch ones NotImplemented pred = torch tensor True x = torch randn Mode out = torch cond pred lambda x x sin lambda x x cos x assertEqual out torch ones test_custom_dispatch_mode_not_supports_higher_order_operators Mode TorchDispatchMode supports_higher_order_operators = False __torch_dispatch__ func types args= kwargs=None func torch ops higher_order cond torch ones NotImplemented pred = torch tensor True x = torch randn assertRaisesRegex NotImplementedError There no rule registered HigherOrderOperator cond mode Mode torch cond pred lambda x x sin lambda x x cos x test_dispatch_uint DummyMode TorchDispatchMode __torch_dispatch__ func types args kwargs last_args = args func args kwargs Value could interpreted signed int uarg = + DummyMode m = torch full uarg dtype=torch uint assertEqual m last_args uarg assertTrue == uarg all item TestPythonDispatcher TestCase test_basic x = torch randn requires_grad=True r = torch _C _EnablePythonDispatcher torch add x x test_lstsq = torch randn b = torch rand expected_shape = torch linalg lstsq b solution shape r = torch _C _EnablePythonDispatcher python_disp_shape = torch linalg lstsq b solution shape assertEqual expected_shape python_disp_shape TestWrapperSubclassAliasing TestCase _test_wrapper_subclass_aliasing op args kwargs to_subclass t torch Tensor TwoTensor t t clone result_ref = op args kwargs args_subclass = pytree tree_map_only torch Tensor to_subclass args kwargs_subclass = pytree tree_map_only torch Tensor to_subclass kwargs result_test = op args_subclass kwargs_subclass args_ref_flat = pytree arg_tree_leaves args kwargs args_ref_flat_tensors = x x args_ref_flat isinstance x torch Tensor args_test_flat = pytree tree_leaves args_subclass kwargs_subclass args_test_flat_tensors = x x args_test_flat isinstance x torch Tensor result_ref_flat = pytree tree_leaves result_ref result_ref_flat_tensors = x x result_ref_flat isinstance x torch Tensor result_test_flat = pytree tree_leaves result_test result_test_flat_tensors = x x result_test_flat isinstance x torch Tensor o_ref o_test zip result_ref_flat_tensors result_test_flat_tensors a_ref a_test zip args_ref_flat_tensors args_test_flat_tensors out_is_inpt = o_ref a_ref out_is_inpt assertTrue o_test a_test out_aliases_inpt = StorageWeakRef o_ref untyped_storage == StorageWeakRef a_ref untyped_storage out_aliases_inpt assertTrue StorageWeakRef o_test untyped_storage == StorageWeakRef a_test untyped_storage assertFalse StorageWeakRef o_test untyped_storage == StorageWeakRef a_test untyped_storage This tests correctness ` torch utils _python_dispatch return_and_correct_aliasing ` util wrapper subclasses promise correct aliasing behavior It s probably overkill test every OpInfo so I picked sampling ops representative schemas ops op op op_db op name mul out-of-place cat out-of-place TensorList input index out-of-place Optional TensorList input mul_ inplace view view t_ inplace-view split view multi-return native_batch_norm mutable op returns outputs mutates some inputs allowed_dtypes= torch float test_wrapper_subclass_aliasing device dtype op samples = op sample_inputs device dtype sample = first_sample samples args = sample input sample args kwargs = sample kwargs _test_wrapper_subclass_aliasing op args kwargs ops custom_op_db allowed_dtypes= torch float test_wrapper_subclass_aliasing_custom device dtype op samples = op sample_inputs device dtype sample = first_sample samples args = sample input sample args kwargs = sample kwargs _test_wrapper_subclass_aliasing op args kwargs test_wrapper_subclass_aliasing_conv d device args = torch randn torch randn kwargs = conv d has default arg int strides= which torchscript expands into int strides= Make sure _return_and_correct_aliasing can handle case I m using inference_mode make sure conv d doesn t decompose goes torch_dispatch torch inference_mode _test_wrapper_subclass_aliasing torch ops aten conv d default args kwargs test_wrapper_subclass_aliasing_out_op device Make sure _return_and_correct_aliasing can handle kwargs w mutable tensors args = torch ones torch ones kwargs = out torch empty _test_wrapper_subclass_aliasing torch ops aten add out args kwargs test_wrapper_subclass_aliasing_fft_fft device args = torch randn kwargs = fft_fft has default arg int dim= - - Make sure _return_and_correct_aliasing can handle case I m using inference_mode make sure fft_fft doesn t decompose goes torch_dispatch torch inference_mode _test_wrapper_subclass_aliasing torch ops aten fft_fft args kwargs instantiate_device_type_tests TestWrapperSubclassAliasing globals __name__ == __main__ run_tests