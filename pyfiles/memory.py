__future__ annotations collections dataclasses heapq logging typing Callable Optional TYPE_CHECKING TypedDict Union torch _environment is_fbcode torch _utils_internal signpost_event torch utils _ordered_set OrderedSet config ir MultiOutputLayout NoneLayout utils get_dtype_size is_nonfreeable_buffers virtualized V TYPE_CHECKING dependencies Dep scheduler BaseSchedulerNode SchedulerBuffer torch_log = logging getLogger __name__ dataclasses dataclass PeakMemoryResult order list BaseSchedulerNode peak_memory int method str dataclasses dataclass MemoryPlanningInfoForBuffer size_alloc int = size_free int = succ_nodes OrderedSet BaseSchedulerNode = dataclasses field default_factory=OrderedSet dataclasses dataclass MemoryPlanningInfoForNode index int = size int = pred_buffers OrderedSet Union SchedulerBuffer FreeableInputBuffer = dataclasses field default_factory=OrderedSet pred_nodes OrderedSet BaseSchedulerNode = dataclasses field default_factory=OrderedSet succ_nodes OrderedSet BaseSchedulerNode = dataclasses field default_factory=OrderedSet dataclasses dataclass FreeableInputBuffer name str mpi_buffer MemoryPlanningInfoForBuffer = dataclasses field default_factory=MemoryPlanningInfoForBuffer get_name - str name __hash__ - int hash name get_freeable_input_buf nodes list BaseSchedulerNode graph_inputs OrderedSet str - dict str FreeableInputBuffer Create keep track all input buffers can freed during program Returns A dictionary containing all freeable input buffers keyed their names _dep_size_hint dep Dep - int V graph get_dep_size_hint dep get freeable input buffers successor nodes their sizes note different deps can have same name so we use name keys dep_name_to_succ_nodes dict str OrderedSet BaseSchedulerNode = collections defaultdict OrderedSet dep_name_to_size dict str int = dict node nodes dep node read_writes reads dep name graph_inputs is_nonfreeable_buffers dep dep_name_to_succ_nodes dep name add node dep_name_to_size dep name = _dep_size_hint dep create FreeableInputBuffer objects add them returned dictionary name_to_freeable_input_buf dict str FreeableInputBuffer = dict dep_name succ_nodes dep_name_to_succ_nodes items name_to_freeable_input_buf dep_name = FreeableInputBuffer dep_name MemoryPlanningInfoForBuffer size_free=dep_name_to_size dep_name succ_nodes=succ_nodes name_to_freeable_input_buf compute_size_for_scheduler_buffer name_to_buf dict str SchedulerBuffer - dict str tuple int int Compute size each scheduler buffer including memory allocated when created memory deallocated when freed We specially handle case MultiOutputLayout Consider following case buf = some_ops_with_multi_outputs buf = buf assume bytes buf = buf assume bytes In such cases buf creation bytes allocated when deleted bytes freed buf creation bytes allocated when deleted bytes freed buf creation bytes allocated when deleted bytes freed When operation mutates buffer in-place scheduler creates new buffer name track before after states even though they share same memory The mutated buffer represents rename zero allocation deallocation cost During dependency tracking we transfer dependencies mutated name back original buffer ensuring original memory only freed when all aliases done This handles cases where buffer has multiple non-overlapping aliases - rather than trying assign free costs individual aliases we forward all alias dependencies original buffer Consider buf = op buf = mutation_op_ buf del buf op buf del buf The only memory events creation prior op deletion following buf Returns A dictionary mapping scheduler buffer tuple size_alloc size_free ir MultiOutput scheduler OutputNode sched_buf_to_size dict str tuple int int = dict _compute_and_update_buf_size sched_buf SchedulerBuffer user_of_MultiOutputLayout bool = False - int sched_buf get_name V graph scheduler mutation_real_name sched_buf_to_size sched_buf get_name = isinstance sched_buf node layout NoneLayout sched_buf_to_size sched_buf get_name = isinstance sched_buf node layout MultiOutputLayout size_alloc = user sched_buf users isinstance user node OutputNode continue buf user node get_outputs isinstance buf node MultiOutput size_alloc += _compute_and_update_buf_size buf True sched_buf_to_size sched_buf get_name = user_of_MultiOutputLayout size_alloc size_alloc buf_size = V graph sizevars size_hint sched_buf node get_numel fallback= get_dtype_size sched_buf node get_dtype sched_buf_to_size sched_buf get_name = user_of_MultiOutputLayout buf_size buf_size buf_size sched_buf name_to_buf values skip sched_buf already processed user another SchedulerBuffer whose layout type MultiOutputLayout sched_buf get_name sched_buf_to_size _compute_and_update_buf_size sched_buf sched_buf_to_size assign_memory_planning_info_for_scheduler_buffers nodes list BaseSchedulerNode name_to_buf dict str SchedulerBuffer - None For each SchedulerBuffer assign its size info successor nodes A buffer s successor nodes determines when buffer can freed get buffer sizes sched_buf_to_size = compute_size_for_scheduler_buffer name_to_buf get buffer s successor nodes note different deps can have same name so we use name keys dep_name_to_succ_nodes dict str OrderedSet BaseSchedulerNode = collections defaultdict OrderedSet node nodes dep node unmet_dependencies dep_name_to_succ_nodes dep name add node iterate reverse so dependencies picked up transitively mutating_buf_name real_buf_name reversed V graph scheduler mutation_real_name items dep_name_to_succ_nodes real_buf_name &#124; = dep_name_to_succ_nodes mutating_buf_name populate MemoryPlanningInfoForBuffer attribute each scheduler buffer note there scheduler buffers dep_name_to_succ_nodes e g graph outputs buf_name name_to_buf keys name_to_buf buf_name mpi_buffer = MemoryPlanningInfoForBuffer size_alloc=sched_buf_to_size buf_name size_free=sched_buf_to_size buf_name succ_nodes=dep_name_to_succ_nodes buf_name assign_memory_planning_info_for_scheduler_nodes nodes list BaseSchedulerNode name_to_fused_node dict str BaseSchedulerNode name_to_buf dict str SchedulerBuffer name_to_freeable_input_buf dict str FreeableInputBuffer - None Assign each scheduler node its predecessor successor nodes node_to_pred_nodes dict BaseSchedulerNode OrderedSet BaseSchedulerNode = collections defaultdict OrderedSet node_to_succ_nodes dict BaseSchedulerNode OrderedSet BaseSchedulerNode = node_to_pred_buffers dict BaseSchedulerNode OrderedSet SchedulerBuffer &#124; FreeableInputBuffer = collections defaultdict OrderedSet collect all predecessors using existing successor mappings node nodes succ_nodes = OrderedSet succ_node buffer node get_outputs succ_node buffer mpi_buffer succ_nodes node_to_succ_nodes node = succ_nodes For each successor add current node its predecessor succ_node succ_nodes node_to_pred_nodes succ_node add node For each output buffer add predecessor its successor nodes TODO - pred buffers needed buffer node get_outputs succ_node buffer mpi_buffer succ_nodes node_to_pred_buffers succ_node add buffer freeable_buffer name_to_freeable_input_buf values succ_node freeable_buffer mpi_buffer succ_nodes node_to_pred_buffers succ_node add freeable_buffer Second pass assign memory planning info using completed predecessor mappings index node enumerate nodes size_alloc = sum buffer mpi_buffer size_alloc buffer node get_outputs succ_nodes = node_to_succ_nodes node pred_nodes = node_to_pred_nodes node make sure we do make node successor predecessor itself succ_nodes discard node pred_nodes discard node node mpi_node = MemoryPlanningInfoForNode index=index size=size_alloc pred_buffers=node_to_pred_buffers node pred_nodes=node_to_pred_nodes node succ_nodes=succ_nodes map each scheduler buffer its size start step end step dataclasses dataclass BufferInfo buffer Union SchedulerBuffer FreeableInputBuffer size_alloc int size_free int start_step int end_step int compute_memory_timeline nodes list BaseSchedulerNode name_to_freeable_input_buf dict str FreeableInputBuffer graph_outputs OrderedSet str - tuple list BufferInfo dict BaseSchedulerNode int dict Union FreeableInputBuffer SchedulerBuffer BaseSchedulerNode Compute buffer allocation deallocation sizes map their lifetime node schedule get execution step each node will used determine end_step buffers node_to_step dict BaseSchedulerNode int = node step step node enumerate nodes get buffers size liveliness information buf_info_list list BufferInfo = buf_to_snode_last_use dict Union FreeableInputBuffer SchedulerBuffer BaseSchedulerNode = _get_end_step_and_snode buf Union FreeableInputBuffer SchedulerBuffer - tuple int Optional BaseSchedulerNode max_step int = - max_step_snode Optional BaseSchedulerNode = None succ_nodes = buf mpi_buffer succ_nodes succ_nodes succ_node succ_nodes step = node_to_step succ_node step max_step max_step = step max_step_snode = succ_node assert max_step_snode None max_step max_step_snode freeable input buffers buf_name input_buf name_to_freeable_input_buf items end_step = - buf_name graph_outputs end_step end_step_snode = _get_end_step_and_snode input_buf assert end_step_snode None buf_to_snode_last_use input_buf = end_step_snode buf_info_list append BufferInfo input_buf input_buf mpi_buffer size_free input_buf mpi_buffer size_free end_step scheduler buffers step node enumerate nodes sched_buf node get_outputs note possible non-graph-output sched_buf have no succ_nodes only used its defining op e g due fusion when all consumers buffer fused its defining op In such cases end_step step buf_name = sched_buf get_name end_step = - buf_name graph_outputs end_step end_step_snode = _get_end_step_and_snode sched_buf end_step == - end_step = step buf_to_snode_last_use sched_buf = node assert end_step_snode None buf_to_snode_last_use sched_buf = end_step_snode buf_info_list append BufferInfo sched_buf sched_buf mpi_buffer size_alloc sched_buf mpi_buffer size_free step end_step buf_info_list node_to_step buf_to_snode_last_use estimate_peak_memory nodes list BaseSchedulerNode name_to_freeable_input_buf dict str FreeableInputBuffer graph_outputs OrderedSet str - tuple int list int Given list nodes their execution order estimate peak memory keeping track liveliness SchedulerBuffers FreeableInputBuffers Returns int peak memory List int memory usage each node each step buf_info_list _ _ = compute_memory_timeline nodes name_to_freeable_input_buf graph_outputs incremental memory changes each step memory = _ range len nodes + each buffer update memory when created when freed buf_info buf_info_list memory buf_info start_step += buf_info size_alloc memory buf_info end_step + -= buf_info size_free get peak memory compute cumulative memories max_memory = cur_memory = memories_at_nodes = t range len nodes + cur_memory += memory t memories_at_nodes append cur_memory max_memory = max max_memory cur_memory max_memory memories_at_nodes dataclasses dataclass SNodeMemory size_alloc int size_free int estimate_peak_memory_allocfree nodes list BaseSchedulerNode name_to_freeable_input_buf dict str FreeableInputBuffer graph_outputs OrderedSet str - tuple int list tuple int int dict BaseSchedulerNode SNodeMemory dict Union FreeableInputBuffer SchedulerBuffer BaseSchedulerNode Alternative version estimate_peak_memory respects fact every SchedulerNode has multiple phases alloc outputs run_kernel dealloc last_use buffers estimate_peak_memory collapses memory into one value size_alloc - size_free While peak memory happens after alloc Duplicating code migrate all callsites once In future usages estimate_peak_memory will migrate version buf_info_list _ buf_to_snode_last_use = compute_memory_timeline nodes name_to_freeable_input_buf graph_outputs incremental memory changes each step step_idx_allocfree = SNodeMemory _ range len nodes each buffer update memory when created when freed buf_info buf_info_list step_idx_allocfree buf_info start_step size_alloc += buf_info size_alloc buf_info end_step = - step_idx_allocfree buf_info end_step size_free += buf_info size_free snodes_allocfree = i node enumerate nodes snodes_allocfree node = step_idx_allocfree i max_memory = cur_memory = snodes_curr_memory = t range len nodes alloc = step_idx_allocfree t size_alloc free = step_idx_allocfree t size_free cur_memory += alloc post_alloc = cur_memory max_memory = max max_memory cur_memory cur_memory -= free post_free = cur_memory snodes_curr_memory append post_alloc post_free max_memory snodes_curr_memory snodes_allocfree buf_to_snode_last_use topological_sort_lpmf nodes list BaseSchedulerNode name_to_freeable_input_buf dict str FreeableInputBuffer name_to_buf dict str SchedulerBuffer graph_outputs OrderedSet str - list BaseSchedulerNode A bfs-based greedy topological order LPMF stands Least Peak Memory First The idea paper Buffer memory optimization video codec application modeled Simulink https www cs york ac uk rts docs DAC- - PAPERS DAC PDFFILES P PDF The algorithm maintains max memory so far At every iteration each scheduleable node computes - how much memory needs allocated output buffers node - how much memory can freed result executing node This gives us two values each node mem memory during execution node mem memory after executing node after some input buffers freed The greedy approach select follows i there nodes whose mem values below max memory so far then pick node lowest mem value ii otherwise pick one lowest mem value NodeInfo TypedDict indegree int memory_to_free int BufferInfo TypedDict outdegree int node_info dict BaseSchedulerNode NodeInfo = dict buf_info dict Union SchedulerBuffer FreeableInputBuffer BufferInfo = dict compute nodes number unmet dependencies schedulability initialize list nodes ready scheduled nodes_to_schedule OrderedSet BaseSchedulerNode = OrderedSet node nodes node_info node = indegree len node mpi_node pred_nodes memory_to_free node_info node indegree == nodes_to_schedule add node compute buffers number unmet successors used decide when free buf list name_to_buf values + list name_to_freeable_input_buf values buf_info buf = outdegree len buf mpi_buffer succ_nodes + buf get_name graph_outputs initialize memory estimations live_memory = sum input_buf mpi_buffer size_free input_buf name_to_freeable_input_buf values total output memory which lower bound peak memory we do include memory non freeable input buffers output_memory = buf_name graph_outputs buf_name name_to_buf output_memory += name_to_buf buf_name mpi_buffer size_free buf_name name_to_freeable_input_buf output_memory += name_to_freeable_input_buf buf_name mpi_buffer size_free max_memory = max live_memory output_memory memory_gap = max_memory - live_memory compute amount memory allocated when node scheduled amount memory can freed when node scheduled node nodes buffer read node last used node buf node mpi_node pred_buffers buf_info buf outdegree == node_info node memory_to_free += buf mpi_buffer size_free buffer written node used internally used later buf node get_outputs buf_info buf outdegree == node_info node memory_to_free += buf mpi_buffer size_free schedule nodes one time schedule list BaseSchedulerNode = size_threshold = config size_threshold_for_succ_based_strategy num_iters int = while num_iters len nodes nodes_to_schedule select node schedule size_threshold min node mpi_node size node nodes_to_schedule size_threshold selected_node = min nodes_to_schedule key=lambda node min succ_node mpi_node index succ_node node mpi_node succ_nodes default=len nodes selected_node = min nodes_to_schedule key=lambda node node mpi_node size node mpi_node size memory_gap node mpi_node size - node_info node memory_to_free node mpi_node index nodes_to_schedule remove selected_node schedule append selected_node num_iters += update memory usage live_memory += selected_node mpi_node size max_memory = max max_memory live_memory live_memory -= node_info selected_node memory_to_free memory_gap = max_memory - live_memory update successor nodes nodes_to_schedule succ_node selected_node mpi_node succ_nodes assert node_info succ_node indegree node_info succ_node indegree -= node_info succ_node indegree == nodes_to_schedule add succ_node update predecessor nodes buf selected_node mpi_node pred_buffers assert buf_info buf outdegree buf_info buf outdegree -= buf_info buf outdegree == succ_node buf mpi_buffer succ_nodes node_info succ_node memory_to_free += buf mpi_buffer size_free num_iters len nodes raise RuntimeError Failed schedule while loop ran too long lpmf schedule topological_sort_bfs nodes list BaseSchedulerNode - list BaseSchedulerNode A BFS topological sort selects nodes whose dependencies executed earliest This follows FIFO idea Specifically every iteration each node schedulable we gather order which its predecessor nodes executed sorted list execution orders predecessor nodes defines priority We select node whose predecessors nodes executed earliest The FIFO idea aims reduce liveness duration buffers created NodeInfo TypedDict indegree int order int node_info dict BaseSchedulerNode NodeInfo = dict dataclasses dataclass NodeWithPriority priority list int node BaseSchedulerNode __lt__ other NodeWithPriority - bool priority == other priority node mpi_node index other node mpi_node index priority other priority _node_priority node BaseSchedulerNode - list int priority order which predecessor nodes executed assert node_info node indegree == exec_orders = sorted OrderedSet node_info pred_node order pred_node node mpi_node pred_nodes exec_orders compute nodes number unmet dependencies schedulability initialize list nodes ready scheduled nodes_to_schedule list NodeWithPriority = node nodes node_info node = indegree len node mpi_node pred_nodes order - node_info node indegree == heapq heappush nodes_to_schedule NodeWithPriority _node_priority node node schedule nodes one time schedule list BaseSchedulerNode = num_iters int = while num_iters len nodes nodes_to_schedule select node schedule selected_node = heapq heappop nodes_to_schedule node node_info selected_node order = len schedule schedule append selected_node num_iters += update successor nodes nodes_to_schedule succ_node selected_node mpi_node succ_nodes assert node_info succ_node indegree node_info succ_node indegree -= node_info succ_node indegree == heapq heappush nodes_to_schedule NodeWithPriority _node_priority succ_node succ_node num_iters len nodes raise RuntimeError Failed schedule while loop ran too long bfs schedule topological_sort_dfs nodes list BaseSchedulerNode - list BaseSchedulerNode This DFS topological sort The setup similar ` topological_sort_schedule ` scheduler py The difference order nodes visited outer loop In ` topological_sort_schedule ` nodes visited their original order In function nodes visited based their priority -- each node we compute total memory all buffers reads writes we visit nodes ascending order priority seen OrderedSet BaseSchedulerNode = OrderedSet name_to_node dict str BaseSchedulerNode = dict result list BaseSchedulerNode = size_with_reads dict BaseSchedulerNode int = dict visit n BaseSchedulerNode - None n seen seen add n dep_nodes = name_to_node dep name dep n unmet_dependencies dep name name_to_node node sorted dep_nodes key=lambda n size_with_reads n n mpi_node index visit node result append n node nodes name node get_buffer_names name_to_node name = node node nodes size_with_reads node = node mpi_node size + sum pred_buf mpi_buffer size_free pred_buf node mpi_node pred_buffers node sorted nodes key=lambda n size_with_reads n n mpi_node index visit node result validate_graph_acyclic nodes list BaseSchedulerNode - None Validate graph acyclic checking predecessor relationships Raises RuntimeError If cycle detected graph DFS coloring scheme cycle detection WHITE Node has been visited yet GRAY Node currently being processed recursion stack BLACK Node has been completely processed finished exploring all its predecessors A back edge cycle detected when we encounter GRAY node during DFS traversal WHITE GRAY BLACK = color = dict fromkeys nodes WHITE path list BaseSchedulerNode = Track current DFS path dfs_visit node BaseSchedulerNode - None color node == BLACK color node == GRAY path append node path_info = - join node get_name node path raise RuntimeError f Cycle detected memory planning graph f Path containing cycle i - j j dependency i path_info f This indicates invalid dependency relationships scheduler graph color node = GRAY path append node pred_node node mpi_node pred_nodes assert pred_node = node dfs_visit pred_node path pop color node = BLACK Start DFS all unvisited nodes node nodes color node == WHITE dfs_visit node validate_unique_buffer_names nodes list BaseSchedulerNode name_to_buf dict str SchedulerBuffer name_to_freeable_input_buf dict str FreeableInputBuffer - None Validate each node s output buffer name_to_buf mapping correct For each output buffer buf we should have name_to_buf buf get_name == buf Also validate no buffer names overlap freeable input buffer names Raises RuntimeError If buffer name mapping incorrect names overlap node nodes buf node get_outputs buf_name = buf get_name Check buffer name exists mapping buf_name name_to_buf raise RuntimeError f buf_name node get_name found name_to_buf mapping f This indicates missing buffer mapping Check mapping points correct buffer object name_to_buf buf_name = buf raise RuntimeError f Buffer name mapping incorrect buf_name f Expected name_to_buf buf_name buf debug_str f got name_to_buf buf_name debug_str f This indicates some buffers share same name Check buffer name conflicts freeable input buffer names buf_name name_to_freeable_input_buf raise RuntimeError f Buffer name conflict detected buf_name node node get_name f also used freeable input buffer name prepare_planning_info nodes list BaseSchedulerNode name_to_buf dict str SchedulerBuffer name_to_fused_node dict str BaseSchedulerNode graph_inputs OrderedSet str graph_outputs OrderedSet str - tuple int dict str FreeableInputBuffer Prepare planning info As nodes scheduled one time these help keep track when buffer can freed when node can scheduled Returns int peak memory estimation dict str FreeableInputBuffer name freeable input buffer name_to_freeable_input_buf = get_freeable_input_buf nodes graph_inputs assign_memory_planning_info_for_scheduler_buffers nodes name_to_buf assign_memory_planning_info_for_scheduler_nodes nodes name_to_fused_node name_to_buf name_to_freeable_input_buf default estimated_peak_memory _ = estimate_peak_memory nodes name_to_freeable_input_buf graph_outputs estimated_peak_memory name_to_freeable_input_buf reorder_for_peak_memory nodes list BaseSchedulerNode name_to_buf dict str SchedulerBuffer name_to_fused_node dict str BaseSchedulerNode graph_inputs OrderedSet str graph_outputs OrderedSet str methods list Callable list BaseSchedulerNode = noqa B topological_sort_lpmf topological_sort_bfs topological_sort_dfs - list BaseSchedulerNode Try few heuristics based topological sort algorithms pick one whose resulting topological order has lowest peak memory estimation torch_log info Reordering peak memory -- d nodes len nodes estimated_peak_memory name_to_freeable_input_buf = prepare_planning_info nodes name_to_buf name_to_fused_node graph_inputs graph_outputs export graph simulator needed config reorder_for_peak_memory_debug export_graph_for_simulator nodes name_to_freeable_input_buf name_to_fused_node graph_inputs graph_outputs Validate planning info before proceeding reordering try validate_graph_acyclic nodes validate_unique_buffer_names nodes name_to_buf name_to_freeable_input_buf except RuntimeError torch_log exception Memory planning validation failed is_fbcode TODO remove after ensuring OSS side safe raise keep track peak memory estimates different methods peak_memory_diff_methods list PeakMemoryResult = peak_memory_diff_methods append PeakMemoryResult nodes estimated_peak_memory baseline torch_log info Baseline peak memory d estimated_peak_memory other methods method methods try method topological_sort_lpmf order = method nodes name_to_freeable_input_buf name_to_buf graph_outputs order = method nodes assert len order == len nodes peak_memory _ = estimate_peak_memory order name_to_freeable_input_buf graph_outputs peak_memory_diff_methods append PeakMemoryResult order peak_memory method __name__ torch_log info s peak memory d method __name__ peak_memory except Exception torch_log exception Failed reorder s method __name__ is_fbcode TODO remove after ensuring OSS side safe raise signpost_event category= inductor name= memory parameters= orm elem method elem peak_memory elem peak_memory_diff_methods get optimal one best_result = min peak_memory_diff_methods key=lambda x x peak_memory best_result order export_graph_for_simulator nodes list BaseSchedulerNode name_to_freeable_input_buf dict str FreeableInputBuffer name_to_fused_node dict str BaseSchedulerNode graph_inputs OrderedSet str graph_outputs OrderedSet str - None This debugging purposes It will dump json file records graph information The graph can then used simulator https fburl com code l d qi ORMBuffer TypedDict name str size_alloc int size_free int size int backward compatibility is_input bool is_output bool deps list str unmet_deps list str ORMNode TypedDict name str buffer_names list str ORMGraph TypedDict nodes list ORMNode buffers list ORMBuffer orm_buffers list ORMBuffer = orm_nodes list ORMNode = get orm buffers freeable input buffers buf_name input_buf name_to_freeable_input_buf items orm_buf_input_buffer ORMBuffer = name buf_name size_alloc input_buf mpi_buffer size_free size_free input_buf mpi_buffer size_free size input_buf mpi_buffer size_free is_input True is_output buf_name graph_outputs deps unmet_deps orm_buffers append orm_buf_input_buffer get orm buffers scheduler buffers name_to_buf dict str SchedulerBuffer = buf get_name buf node nodes buf node get_outputs need reassign due probably node pruning buf_name sched_buf name_to_buf items sched_buf defining_op None continue deps = pred_buf get_name pred_buf name_to_fused_node sched_buf defining_op get_name mpi_node pred_buffers orm_buf_scheduler_buffer ORMBuffer = name buf_name size_alloc sched_buf mpi_buffer size_alloc size_free sched_buf mpi_buffer size_free size sched_buf mpi_buffer size_free is_input False is_output buf_name graph_outputs deps deps unmet_deps buf_name buf_name deps buf_name graph_inputs orm_buffers append orm_buf_scheduler_buffer get orm nodes node nodes orm_node ORMNode = name node get_name buffer_names list node get_buffer_names orm_nodes append orm_node create graph object g ORMGraph = nodes orm_nodes buffers orm_buffers dump graph json os torch functorch compile get_graph_being_compiled name = os path splitext get_graph_being_compiled + _fused g_str = json dumps g indent= torch _logging trace_structured artifact metadata_fn=lambda name name encoding string payload_fn=lambda g_str