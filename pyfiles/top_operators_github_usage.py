mypy ignore-errors From https docs google com spreadsheets d R nCOLskxPYjjiNkdqy OdQ eQp_htebXGODsjSeA edit#gid= Try keep list sync operator top_torch = t tensor mode cat max zeros load no_grad save from_numpy manual_seed ones randn stack sum arange rand mean exp zeros_like min sigmoid log matmul clamp sqrt abs tanh empty argmax bmm pow norm mm is_tensor ones_like nonzero full unsqueeze where randperm eye mul topk as_tensor sort squeeze randint linspace add transpose split gather set_grad_enabled sin cos div index_select multinomial flatten isnan randn_like eq einsum round floor allclose reshape diag chunk std set_default_tensor_type triu meshgrid set_num_threads unique full_like tril dot sign equal normal cumsum dist isfinite gt set_printoptions range empty_like flip masked_select bernoulli atan var prod erf inverse addmm logsumexp fft lt log enable_grad rand_like argsort seed mv ger ge atan ceil ne bincount acos rsqrt svd numel log p unbind le isinf cross set_default_dtype argmin sparse_coo_tensor log kthvalue set_rng_state get_rng_state get_default_dtype det qr histc symeig trace median addcmul remainder baddbmm lgamma repeat_interleave fmod reciprocal tan initial_seed take stft get_num_threads real cholesky quantize_per_tensor diag_embed lerp asin eig trunc diagonal cosh rfft cumprod addr roll narrow digamma square sinh logspace broadcast_tensors irfft frac hann_window solve logdet expm cdist addmv randint_like tensordot ifft true_divide erfinv addcdiv addbmm renorm pinverse isclose erfc is_storage triangular_solve rot logical_not geqrf slogdet lu hamming_window orgqr ormqr is_floating_point diagflat cholesky_solve tril_indices chain_matmul triu_indices angle poisson matrix_power unique_consecutive quantize_per_channel std_mean bartlett_window var_mean lstsq logical_and mvlgamma blackman_window bitwise_not cholesky_inverse as_strided floor_divide cartesian_prod lu_solve set_flush_denormal empty_strided logical_xor polygamma logical_or set_num_interop_threads combinations trapz matrix_rank lu_unpack result_type conj cummax lobpcg bitwise_xor promote_types get_num_interop_threads cummin bitwise_and dequantize bitwise_or imag can_cast istft compiled_with_cxx _abi is_complex block_diag pca_lowrank absolute svd_lowrank neg top_nn_functional = nn functional softmax nn functional relu nn functional interpolate nn functional pad nn functional log_softmax nn functional normalize nn functional cross_entropy nn functional grid_sample nn functional one_hot nn functional mse_loss nn functional conv d nn functional dropout nn functional softplus nn functional sigmoid nn functional linear nn functional gelu nn functional avg_pool d nn functional max_pool d nn functional nll_loss nn functional embedding nn functional tanh nn functional leaky_relu nn functional adaptive_avg_pool d nn functional cosine_similarity nn functional unfold nn functional conv d nn functional binary_cross_entropy_with_logits nn functional l _loss nn functional binary_cross_entropy nn functional elu nn functional batch_norm nn functional upsample nn functional fold nn functional affine_grid nn functional max_pool d nn functional torch nn functional threshold nn functional smooth_l _loss nn functional pairwise_distance nn functional logsigmoid nn functional adaptive_max_pool d nn functional relu nn functional pixel_shuffle nn functional avg_pool d nn functional bilinear nn functional conv_transpose d nn functional gumbel_softmax nn functional max_unpool d nn functional kl_div nn functional hardtanh nn functional ctc_loss nn functional layer_norm nn functional conv d nn functional max_unpool d nn functional hardshrink nn functional hardswish nn functional selu nn functional glu nn functional assert_int_or_pair nn functional hardsigmoid nn functional upsample_bilinear nn functional max_pool d nn functional adaptive_avg_pool d nn functional instance_norm nn functional embedding_bag nn functional upsample_nearest nn functional avg_pool d nn functional prelu nn functional celu nn functional dropout d nn functional hinge_embedding_loss nn functional softsign nn functional max_unpool d nn functional silu nn functional softshrink nn functional leaky_relu_ nn functional softmin nn functional channel_shuffle nn functional multilabel_margin_loss nn functional dropout d nn functional multi_margin_loss nn functional lp_pool d nn functional conv_transpose d nn functional triplet_margin_loss nn functional tanhshrink nn functional adaptive_max_pool d nn functional cosine_embedding_loss nn functional multi_head_attention_forward nn functional max_pool d_with_indices nn functional poisson_nll_loss nn functional margin_ranking_loss nn functional soft_margin_loss nn functional adaptive_max_pool d nn functional group_norm nn functional local_response_norm nn functional multilabel_soft_margin_loss nn functional relu_ nn functional alpha_dropout nn functional feature_alpha_dropout nn functional lp_pool d nn functional adaptive_max_pool d_with_indices nn functional adaptive_max_pool d_with_indices nn functional adaptive_max_pool d_with_indices nn functional fractional_max_pool d nn functional fractional_max_pool d_with_indices nn functional fractional_max_pool d nn functional fractional_max_pool d_with_indices nn functional max_pool d_with_indices nn functional max_pool d_with_indices nn functional handle_torch_function nn functional has_torch_function nn functional adaptive_avg_pool d nn functional pdist nn functional rrelu_ nn functional elu_ nn functional boolean_dispatch nn functional hardtanh_ nn functional triplet_margin_with_distance_loss nn functional selu_ nn functional pixel_unshuffle nn functional conv_transpose d nn functional gaussian_nll_loss nn functional has_torch_function_unary nn functional has_torch_function_variadic nn functional celu_ nn functional huber_loss nn functional mish nn functional threshold_ nn functional grad nn functional conv_tbc nn functional math top_nn_module = nn Module None nn Linear nn functional linear nn Sequential None nn Conv d nn functional conv d nn ReLU nn functional relu nn BatchNorm d nn functional batch_norm nn Dropout nn functional dropout nn ModuleList None nn Parameter None nn CrossEntropyLoss nn functional cross_entropy nn MaxPool d nn functional max_pool d nn Embedding nn functional embedding nn DataParallel None nn MSELoss nn functional mse_loss nn Sigmoid nn functional sigmoid nn LeakyReLU nn functional leaky_relu nn BatchNorm d nn functional batch_norm nn Softmax nn functional softmax nn Tanh nn functional tanh nn AdaptiveAvgPool d nn functional adaptive_avg_pool d nn AvgPool d nn functional avg_pool d nn ConvTranspose d nn functional conv_transpose d nn LSTM None nn Conv d nn functional conv d nn LayerNorm nn functional layer_norm nn BCELoss nn functional binary_cross_entropy nn Upsample nn functional interpolate nn BCEWithLogitsLoss nn functional binary_cross_entropy_with_logits nn GRU None nn Dropout d nn functional dropout d nn LogSoftmax nn functional log_softmax nn L Loss nn functional l _loss nn GroupNorm nn functional group_norm nn NLLLoss nn functional nll_loss nn Conv d nn functional conv d nn Identity None nn InstanceNorm d nn functional instance_norm nn BatchNorm d nn functional batch_norm nn PReLU nn functional prelu nn ReLU nn functional relu nn ELU nn functional elu nn LSTMCell None nn Flatten torch flatten nn ModuleDict None nn ReflectionPad d nn functional pad nn MaxPool d nn functional max_pool d nn MaxPool d nn functional max_pool d nn RNN None nn ZeroPad d nn functional pad nn ParameterList None nn SyncBatchNorm None nn PixelShuffle nn functional pixel_shuffle nn SmoothL Loss nn functional smooth_l _loss nn Hardswish nn functional hardswish nn AdaptiveMaxPool d nn functional adaptive_max_pool d nn SELU nn functional selu nn ConvTranspose d nn functional conv_transpose d nn GRUCell None nn ReplicationPad d nn functional pad nn KLDivLoss nn functional kl_div nn ConvTranspose d nn functional conv_transpose d nn Softplus nn functional softplus nn SiLU nn functional silu nn AvgPool d nn functional avg_pool d nn CosineSimilarity nn functional cosine_similarity nn GELU nn functional gelu nn UpsamplingBilinear d nn functional interpolate nn InstanceNorm d nn functional instance_norm nn Transformer None nn MultiheadAttention nn functional multi_head_attention_forward nn AvgPool d nn functional avg_pool d nn Dropout d nn functional dropout d nn AdaptiveAvgPool d nn functional adaptive_avg_pool d nn InstanceNorm d nn functional instance_norm nn Hardtanh nn functional hardtanh nn MarginRankingLoss nn functional margin_ranking_loss nn GLU nn functional glu nn AdaptiveAvgPool d nn functional adaptive_avg_pool d nn EmbeddingBag nn functional embedding_bag nn TransformerEncoderLayer None nn TransformerEncoder None nn MaxUnpool d nn functional max_unpool d nn UpsamplingNearest d nn functional interpolate nn ConstantPad d nn functional pad nn ConstantPad d nn functional pad nn CTCLoss nn functional ctc_loss nn AdaptiveMaxPool d nn functional adaptive_max_pool d nn AdaptiveLogSoftmaxWithLoss None nn Bilinear nn functional bilinear nn RNNCell None nn MultiLabelSoftMarginLoss nn functional multilabel_soft_margin_loss nn Unfold nn functional unfold nn RReLU nn functional rrelu nn CosineEmbeddingLoss nn functional cosine_embedding_loss nn LocalResponseNorm nn functional local_response_norm nn Softmax d nn functional softmax nn PairwiseDistance nn functional pairwise_distance nn LogSigmoid nn functional logsigmoid nn TripletMarginLoss nn functional triplet_margin_loss nn RNNBase None nn Threshold nn functional threshold nn AdaptiveMaxPool d nn functional adaptive_max_pool d nn CELU nn functional celu nn NLLLoss d nn functional nll_loss nn Softsign nn functional softsign nn ReplicationPad d nn functional pad nn SoftMarginLoss nn functional soft_margin_loss nn ParameterDict None nn ReflectionPad d nn functional pad nn Softshrink nn functional softshrink nn AlphaDropout nn functional alpha_dropout nn Tanhshrink nn functional tanhshrink nn PoissonNLLLoss nn functional poisson_nll_loss nn MaxUnpool d nn functional max_unpool d nn Fold nn functional fold nn MultiMarginLoss nn functional multi_margin_loss nn TransformerDecoderLayer None nn TransformerDecoder None nn Hardshrink nn functional hardshrink nn ConstantPad d nn functional pad nn MultiLabelMarginLoss nn functional multilabel_margin_loss nn LPPool d nn functional lp_pool d nn Softmin nn functional softmin nn MaxUnpool d nn functional max_unpool d nn FractionalMaxPool d nn functional fractional_max_pool d nn Hardsigmoid nn functional hardsigmoid nn ReplicationPad d nn functional pad nn HingeEmbeddingLoss nn functional hinge_embedding_loss nn LPPool d nn functional lp_pool d nn FractionalMaxPool d nn functional fractional_max_pool d nn Container None nn Unflatten nn functional unflatten nn FeatureAlphaDropout nn functional feature_alpha_dropout nn TripletMarginWithDistanceLoss nn functional triplet_margin_with_distance_loss nn ChannelShuffle nn functional channel_shuffle nn RNNCellBase None nn LazyLinear nn functional linear nn UninitializedParameter None nn CrossMapLRN d None nn GaussianNLLLoss nn functional gaussian_nll_loss nn PixelUnshuffle nn functional pixel_unshuffle nn Mish nn functional mish nn ReflectionPad d nn functional pad nn HuberLoss nn functional huber_loss nn LazyConv d None nn LazyConv d None nn LazyConv d None nn LazyConvTranspose d None nn LazyConvTranspose d None nn LazyConvTranspose d None nn LazyBatchNorm d None nn LazyBatchNorm d None nn LazyBatchNorm d None nn UninitializedBuffer None No rankings because these little hard get rankings method_only_ops = bfloat bool byte char contiguous cpu cuda detach double expand expand_as float get_device half hardshrink index_add index_copy index_fill index_put int is_contiguous is_pinned is_set_to is_shared is_signed item long masked_scatter masked_fill narrow_copy numpy pin_memory repeat reshape_as select short storage_offset sum_to_size to_mkldnn tolist type type_as unfold view view_as get_nn_functional_top_list top_nn_functional_ = dict top_nn_functional _ count functional_name top_nn_module functional_name None continue functional_name == torch flatten continue functional_name top_nn_functional_ top_nn_functional_ functional_name = count top_nn_functional_ functional_name += count top_nn_functional_ = list top_nn_functional_ items top_nn_functional_ sort key=operator itemgetter reverse=True top_nn_functional_ usage_count = dict get_nn_functional_top_list usage_count update top_torch