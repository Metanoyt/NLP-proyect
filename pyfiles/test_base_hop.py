Owner s module dynamo unittest mock mock torch torch _dynamo test_case torch _functorch config torch utils checkpoint torch _dynamo testing AotEagerAndRecordGraphs EagerAndRecordGraphs normalize_gm torch _higher_order_ops schema find_hop_schema torch testing _internal common_utils instantiate_parametrized_tests normalize_graph gm normalize_gm gm print_readable print_output=False InvokeQuantTest torch _higher_order_ops BaseHOP __init__ super __init__ invoke_quant_test __call__ subgraph operands scheme super __call__ subgraph operands scheme=scheme invoke_quant_test = InvokeQuantTest BaseHOPTest torch _dynamo test_case TestCase TODO flip False later we re landing refactor PR don t want merge conflict torch _dynamo config patch assume_static_by_default=True test_dynamo inner x y x y sin cos x = torch randn requires_grad=True y = torch randn requires_grad=True backend = EagerAndRecordGraphs torch compile backend=backend f x y invoke_quant_test inner x y scheme= nf out = f x y assertEqual out inner x y assert len backend graphs == assertExpectedInline normalize_graph backend graphs \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ subgraph_ = subgraph_ invoke_quant_test = torch ops higher_order invoke_quant_test subgraph_ l_x_ l_y_ scheme = nf subgraph_ = l_x_ = l_y_ = None getitem f = invoke_quant_test invoke_quant_test = None getitem subgraph_ torch nn Module forward l_x_ f l_y_ f matmul f = l_x_ l_y_ l_x_ = l_y_ = None sin f = matmul sin matmul = None cos f = sin cos sin = None cos NOQA B test_schema_gen_single_return inner x y x y sin cos x = torch randn requires_grad=False y = torch randn requires_grad=False backend = EagerAndRecordGraphs torch compile backend=backend f x y invoke_quant_test inner x y scheme= nf out = f x clone y assertEqual out inner x clone y schemas = find_hop_schema backend graphs invoke_quant_test assertEqual len schemas assertExpectedInline str schemas invoke_quant_test Any subgraph Tensor arg Tensor arg str scheme= nf - Tensor noqa B test_schema_gen_pytree_in_out inner x_y x y = x_y x y sin cos x + y x - y out x y make x require grad because we want inplace mutate x = torch randn requires_grad=False y = torch randn requires_grad=True backend = EagerAndRecordGraphs torch compile backend=backend f x y invoke_quant_test inner x y scheme= nf out = f x clone y assertEqual out inner x clone y schemas = find_hop_schema backend graphs invoke_quant_test assertEqual len schemas assertExpectedInline str schemas invoke_quant_test Any subgraph Tensor arg Tensor arg str scheme= nf - Tensor Tensor Tensor Tensor noqa B test_schema_gen_single_return_with_mutation inner x y x add_ y mul_ - x y sin cos x = torch randn requires_grad=False y = torch randn requires_grad=False backend = EagerAndRecordGraphs f x y invoke_quant_test inner x y scheme= nf mock patch torch _dynamo variables higher_order_ops BaseHOPVariable supports_input_mutation True torch compile f backend=backend fullgraph=True x clone y assertEqual len backend graphs assertExpectedInline normalize_graph backend graphs \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ subgraph_ = subgraph_ invoke_quant_test = torch ops higher_order invoke_quant_test subgraph_ l_x_ l_y_ scheme = nf subgraph_ = l_x_ = l_y_ = None getitem f = invoke_quant_test invoke_quant_test = None getitem subgraph_ torch nn Module forward l_x_ f l_y_ f add_ f = l_x_ add_ add_ = None mul_ f = l_y_ mul_ - mul_ = None matmul f = l_x_ l_y_ l_x_ = l_y_ = None sin f = matmul sin matmul = None cos f = sin cos sin = None cos noqa B assertExpectedInline str find_hop_schema backend graphs invoke_quant_test invoke_quant_test Any subgraph Tensor arg Tensor arg str scheme= nf - Tensor test_schema_gen_pytree_in_out_with_mutation inner x_y x y = x_y x add_ x y sin cos x + y x - y out x y make x require grad because we want inplace mutate x = torch randn requires_grad=False y = torch randn requires_grad=True bk = EagerAndRecordGraphs f x y invoke_quant_test inner x y scheme= nf mock patch torch _dynamo variables higher_order_ops BaseHOPVariable supports_input_mutation True torch no_grad torch compile f backend=bk fullgraph=True x clone y assertEqual len bk graphs assertExpectedInline normalize_graph bk graphs \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ subgraph_ = subgraph_ invoke_quant_test = torch ops higher_order invoke_quant_test subgraph_ l_x_ l_y_ scheme = nf subgraph_ = l_x_ = l_y_ = None getitem f = invoke_quant_test getitem_ f = invoke_quant_test getitem_ f = invoke_quant_test getitem_ f = invoke_quant_test invoke_quant_test = None getitem getitem_ getitem_ getitem_ subgraph_ torch nn Module forward l_x_ f l_y_ f add_ f = l_x_ add_ add_ = None matmul f = l_x_ l_y_ sin f = matmul sin matmul = None child f = sin cos sin = None child_ f = l_x_ + l_y_ child_ f = l_x_ - l_y_ child_ f = l_x_ l_y_ l_x_ = l_y_ = None child child_ child_ child_ noqa B assertExpectedInline str find_hop_schema bk graphs invoke_quant_test invoke_quant_test Any subgraph Tensor arg Tensor arg str scheme= nf - Tensor Tensor Tensor Tensor noqa B test_none_input inner x y x None y sin y cos backend = EagerAndRecordGraphs torch compile backend=backend fullgraph=True f x y invoke_quant_test inner x y scheme= nf x = None y = torch randn out = f x y assertEqual out inner x y assertExpectedInline normalize_graph backend graphs \ GraphModule torch nn Module forward L_y_ f l_y_ = L_y_ subgraph_ = subgraph_ invoke_quant_test = torch ops higher_order invoke_quant_test subgraph_ l_y_ scheme = nf subgraph_ = l_y_ = None getitem f = invoke_quant_test invoke_quant_test = None getitem subgraph_ torch nn Module forward l_y_ f cos f = l_y_ cos l_y_ = None cos test_int_input inner x y x + y backend = EagerAndRecordGraphs torch compile backend=backend fullgraph=True f x y invoke_quant_test inner x y scheme= nf x = y = torch randn out = f x y assertEqual out inner x y assertExpectedInline normalize_graph backend graphs \ GraphModule torch nn Module forward L_y_ f l_y_ = L_y_ subgraph_ = subgraph_ invoke_quant_test = torch ops higher_order invoke_quant_test subgraph_ l_y_ scheme = nf subgraph_ = l_y_ = None getitem f = invoke_quant_test invoke_quant_test = None getitem subgraph_ torch nn Module forward l_y_ f add f = + l_y_ l_y_ = None add test_auto_functionalize inner x y x add_ x + y backend = AotEagerAndRecordGraphs f x y invoke_quant_test inner x y scheme= nf x = torch randn requires_grad=False x_clone = x clone y = torch randn requires_grad=True mock patch torch _dynamo variables higher_order_ops BaseHOPVariable supports_input_mutation True torch no_grad compiled_out = torch compile f backend=backend fullgraph=True x y assertEqual x x_clone + assertEqual compiled_out x_clone + y + assertEqual len backend fw_graphs assertExpectedInline normalize_graph backend fw_graphs \ lambda torch nn Module forward arg _ f arg _ f auto_functionalized_subgraph_ = auto_functionalized_subgraph_ _tree_spec_constant = _tree_spec_constant auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops higher_order invoke_quant_test subgraph = auto_functionalized_subgraph_ arg = arg _ scheme = nf _arg _base_index = _all_bases = arg _ _op_schema = _tree_spec_constant auto_functionalized_subgraph_ = arg _ = _tree_spec_constant = None getitem f = auto_functionalized_v getitem_ f = auto_functionalized_v auto_functionalized_v = None copy_ f = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy_ = None getitem auto_functionalized_subgraph_ torch nn Module forward arg _ f arg _ f add f = torch ops aten add Tensor arg _ add_ f = torch ops aten add Tensor add arg _ arg _ = None copy_ f = torch ops aten copy_ default arg _ add arg _ = add = copy_ = None add_ noqa B torch _dynamo config patch assume_static_by_default=True test_aot_eager inner x y x y sin_ cos x = torch randn requires_grad=True y = torch randn requires_grad=True backend = AotEagerAndRecordGraphs torch compile backend=backend f x y invoke_quant_test inner x y scheme= nf out = f x y result = torch autograd grad out x y out = inner x y expected = torch autograd grad out x y assertEqual result expected assert len backend fw_graphs == assertExpectedInline normalize_graph backend fw_graphs \ GraphModule torch nn Module forward primals_ f primals_ f subgraph = subgraph invoke_quant_test = torch ops higher_order invoke_quant_test subgraph primals_ primals_ scheme = nf subgraph = None getitem f = invoke_quant_test invoke_quant_test = None getitem primals_ primals_ subgraph torch nn Module forward arg _ f arg _ f mm f = torch ops aten mm default arg _ arg _ arg _ = arg _ = None sin f = torch ops aten sin default mm mm = None cos f = torch ops aten cos default sin sin = None cos NOQA B assert len backend bw_graphs == assertExpectedInline normalize_graph backend bw_graphs \ GraphModule torch nn Module forward primals_ f primals_ f tangents_ f subgraph = subgraph invoke_quant_test_ = torch ops higher_order invoke_quant_test subgraph primals_ primals_ tangents_ scheme = nf subgraph = primals_ = primals_ = tangents_ = None getitem_ f = invoke_quant_test_ getitem_ f = invoke_quant_test_ invoke_quant_test_ = None getitem_ getitem_ subgraph torch nn Module forward arg _ f arg _ f arg _ f mm f = torch ops aten mm default arg _ arg _ clone f = torch ops aten clone default mm sin f = torch ops aten sin default mm mm = None sin_ f = torch ops aten sin default sin sin = None neg f = torch ops aten neg default sin_ sin_ = None mul f = torch ops aten mul Tensor arg _ neg arg _ = neg = None cos_ f = torch ops aten cos default clone clone = None mul_ f = torch ops aten mul Tensor mul cos_ mul = cos_ = None t f = torch ops aten t default arg _ arg _ = None mm_ f = torch ops aten mm default t mul_ t = None t_ f = torch ops aten t default arg _ arg _ = None mm_ f = torch ops aten mm default mul_ t_ mul_ = t_ = None mm_ mm_ NOQA B test_aliasing_mutation_error inner x y x inner x y x sin_ x + y x = torch randn y = torch randn torch compile backend= eager fullgraph=True f inner x y invoke_quant_test inner x y scheme= nf assertRaisesRegex RuntimeError Encountered aliasing during higher order op tracing f inner x y assertRaisesRegex RuntimeError Encountered input mutation during higher order op tracing f inner x y test_eager_call inner x y x + y x = torch randn y = torch randn assertRaisesRegex RuntimeError torch fx GraphModule invoke_quant_test inner x y scheme= nf functorch make_fx result = make_fx inner x y smoke test invoke_quant_test result x y scheme= nf instantiate_parametrized_tests BaseHOPTest __name__ == __main__ torch _dynamo test_case run_tests run_tests