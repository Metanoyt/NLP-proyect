mypy allow-untyped-defs inspect itertools collections abc Callable Sequence dataclasses dataclass field enum auto Enum typing Any cast Optional torch torch nn nn torch _prims_common make_contiguous_strides_for torch distributed _functional_collectives AsyncCollectiveTensor torch distributed device_mesh DeviceMesh torch distributed tensor DTensor Replicate Shard torch distributed tensor _dtensor_spec DTensorSpec TensorMeta torch distributed tensor placement_types _StridedShard Placement _fsdp_api CPUOffloadPolicy MixedPrecisionPolicy OffloadPolicy _fsdp_common _chunk_with_empty _from_local_no_grad _get_dim_chunked_size _raise_assert_with_print _to_dtype_if_needed compiled_autograd_enabled FSDPMeshInfo HSDPMeshInfo Note FSDP tensors FSDP considers following tensors - Original parameter parameter passed ` FSDPParam ` i e one module when applying FSDP - Sharded parameter sharding original parameter dim- user-specified dim DTensor over main mesh - All-gather inputs ` ` torch Tensor ` ` ` ` Tensor ` ` s passed all-gather derived sharded parameter - All-gather output ` ` torch Tensor ` ` ` ` Tensor ` ` s resulting all-gathering all-gather inputs - Unsharded parameter parameter used forward backward computation derived all-gather output autograd leaf We define these tensors describe general framework can accommodate extensions where - all-gather-inputs = pre-all-gather-transform sharded-parameter - unsharded-parameter = post-all-gather-transform all-gather-outputs For default ` ` torch Tensor ` ` case there only one all-gather input shares same underlying tensor data sharded parameter meaning they can thought same tensors The same applies all-gather output unsharded parameter For non- ` ` torch Tensor ` ` extensions these equivalences may no longer hold due pre post-all-gather transforms some may have multiple all-gather inputs outputs e g quantized data scales Note FSDP autograd FSDP dynamically frees allocates unsharded parameter Since autograd can pack reference view save backward we use storage resizing implement freeing allocation since preserves aliasing This implies we construct unsharded parameter object once write in-place thereafter For default ` ` torch Tensor ` original parameter case all-gather output unsharded parameter share same data so we use storage resizing all-gather output lib = torch library Library fsdp FRAGMENT noqa TOR lib define copy_ Tensor tensor Tensor data - torch library impl lib copy_ Meta torch library impl lib copy_ CUDA torch library impl lib copy_ XPU torch library impl lib copy_ HPU torch library impl lib copy_ CPU torch library impl lib copy_ MTIA copy_ tensor data tensor copy_ data Note Avoiding functionalization fsdp copy_ inductor resize_storage_bytes_ Currently we don t functionalize ` fsdp copy_ ` op ` inductor resize_storage_bytes_ ` op i e they show up mutation op middle AOT joint graph Reason Traceable FSDP compiled autograd BWD graph have following traits Two inputs graph aliased each other one hook closed-over tensors one FWD saved tensors One them mutated copy_ resize_ handle all-gathered param They both subclasses The combination these traits supported AOTAutograd s difficult reason about subclass aliasing So doesn t work all Traceable FSDP The compromise we use avoid functionalization FSDP copy_ resize_ ops This avoids problem above because AOTAutograd point-of-view there no mutations functionalization needs handle Although we need careful DCE those mutable ops We can avoid functionalization because The nn Parameter never used before its copy_ called eager code i e no alias created so s safe call copy_ middle graph update its content start using nn Parameter downstream We always re-allocate buffer nn Parameter store AllGather output used downstream user ops So calling resize-to- middle graph free nn Parameter memory after use should always okay since we always allocate anew next time we need we strictly don t need keep old tensor storage around anymore Q Wouldn t extra resize_ copy_ ops hurt both memory usage performance A Yes would As optimization we have Inductor post-grad FX pass remove those resize_ copy_ ops unsharded params have pattern resize_ full - copy_ - resize_ TODO Now we maintaining invariant no aliased + mutated graph inputs both forward backward now more feasible functionalize all mutable FSDP ops Some pros cons Cons functionalizing those ops By functionalizing them we today we making more likely they will run correct time generated code If we start functionalize them we will need make sure Inductor reinplaces them way where properly moves mutations back exactly where they should have run we risk suffering worse peak memory than eager We probably already need do something similar Inductor s reinplacing copy_ https github com pytorch pytorch issues #issuecomment- Pros functionalizing Better safety we don t need worry about graph passes inductor partitioning handling input mutations mid-graph quite much fair we ve already done some amount auditing we might have do some more Better perf each mutation midway through graph prevents Inductor pattern matching across But maybe there few enough mutations induced FSDP matter torch library impl lib copy_ Functionalize copy__functionalize tensor data torch _sync tensor torch _sync data tensor_inner = torch _from_functional_tensor tensor data_inner = torch _from_functional_tensor data torch _C _ExcludeDispatchKeyGuard torch _C DispatchKeySet torch _C DispatchKey Functionalize torch ops fsdp copy_ default tensor_inner data_inner torch fx node has_side_effect torch ops fsdp copy_ default ShardedState Enum - ` ` SHARDED ` ` The sharded parameter registered module It only contributor parameter memory - ` ` SHARDED_POST_FORWARD ` ` The unsharded parameter resharded smaller world size Since data should used computation we do register module Users should reshard module before any in-place modifications Both sharded parameter contribute parameter memory - ` ` UNSHARDED ` ` The unsharded parameter registered module Both sharded parameter contribute parameter memory SHARDED = auto SHARDED_POST_FORWARD = auto UNSHARDED = auto dataclass ParamModuleInfo For parameter stores module parameter name able do parameter swap via ` ` setattr module param_name ` ` get parameter via ` ` getattr module param_name ` ` We additionally save shared modules shared parameter names update them accordingly Parameter names unprefixed e g weight lin weight module nn Module param_name str shared_modules list nn Module = field default_factory=list shared_param_names list str = field default_factory=list dataclass ExtensionsData User-defined metadata passed pre post-all-gather all_gather_metadata Optional Any = None Save all-gather input sizes unflatten all-gather outputs ND all_gather_input_sizes Sequence torch Size = ND clear all_gather_metadata = None all_gather_input_sizes = FSDPParam This manages parameter FSDP FSDP variants applied implementing dim- per-parameter sharding orig_dtype torch dtype param_dtype Optional torch dtype reduce_dtype Optional torch dtype _orig_size torch Size ND sharded_size torch Size ND contiguous_sharded_stride tuple int padded_sharded_param_size torch Size ND sharded_post_forward_size torch Size ND contiguous_sharded_post_forward_stride tuple int _sharded_param_data torch Tensor D sharded_param nn Parameter ND _sharded_post_forward_param_data Optional torch Tensor D _sharded_post_forward_param Optional nn Parameter ND _unsharded_param nn Parameter ND unsharded_accumulated_grad Optional torch Tensor ND _sharding_spec DTensorSpec DTensor attributes only defined DTensor ` param ` _tp_spec DTensorSpec all_gather_outputs list torch Tensor D All-gather extension attributes _extensions_data ExtensionsData _unsharded_inner_tensors list torch Tensor __init__ param nn Parameter module_info ParamModuleInfo mesh_info FSDPMeshInfo post_forward_mesh_info Optional FSDPMeshInfo device torch device shard_placement_fn Optional Callable nn Parameter Optional Shard mp_policy MixedPrecisionPolicy offload_policy OffloadPolicy _module_info ParamModuleInfo = module_info mesh_info = mesh_info post_forward_mesh_info = post_forward_mesh_info pyrefly ignore read-only device = device mp_policy = mp_policy offload_to_cpu bool = isinstance offload_policy CPUOffloadPolicy pin_memory = offload_to_cpu cast CPUOffloadPolicy offload_policy pin_memory grad_offload_event Optional torch Event = None _init_sharded_param param device shard_placement_fn post_forward_mesh_info _init_sharded_post_forward_param_metadata param _init_extensions all_gather_outputs list torch Tensor = unsharded_accumulated_grad = None _param_fqn Optional str = None prefixed root module TODO Remove padding logic once DTensor pads local tensor https github com pytorch pytorch issues _post_load_hook_handle = module_info module register_load_state_dict_post_hook lambda args kwargs reset_sharded_param torch no_grad _init_sharded_param param nn Parameter device torch device shard_placement_fn Optional Callable param device = device param device type = meta raise AssertionError f Expects parameter already moved device device got param device param is_contiguous raise NotImplementedError f FSDP does support non-contiguous parameters yet param shape= param stride = fsdp_placement = shard_placement_fn param shard_placement_fn None fsdp_placement None fsdp_placement = Shard fsdp_placement dim fsdp_placement = Shard fsdp_placement dim + param ndim isinstance fsdp_placement Shard raise AssertionError f Expected Shard got type fsdp_placement fsdp_placement fsdp_placement = fsdp_placement shard_dim = fsdp_placement dim TODO Replace sharded DTensor parameter construction logic ` distribute_tensor ` after https github com pytorch pytorch issues TODO Simplify following sharded parameter padding logic after https github com pytorch pytorch issues is_dtensor = isinstance param DTensor is_dtensor _tp_spec = cast DTensor param _spec dp_mesh tp_mesh = mesh_info mesh _tp_spec mesh dp_mesh None tp_mesh None raise AssertionError FSDP requires DP model parallel TP EP mesh None got \n f DP s mesh dp_mesh \nTP EP s mesh tp_mesh _spmd_mesh = DeviceMesh _concatenate dp_mesh tp_mesh len _tp_spec placements raise NotImplementedError f FSDP only supports D TP EP D EP+TP _tp_spec placements split_factor = _tp_spec num_shards_map shard_dim = _spmd_mesh ndim = raise AssertionError _spmd_mesh ndim can only FSDP+TP EP FSDP+EP+TP HSDP+TP EP f HSDP+EP+TP got _spmd_mesh ndim _spmd_placements tuple Placement dp_shard_tp_placement = _StridedShard shard_dim split_factor=split_factor split_factor fsdp_placement _tp_spec placements dp_mesh ndim == FSDP _spmd_placements = dp_shard_tp_placement HSDP mesh_info replicate_mesh_dim = raise AssertionError f Expected replicate_mesh_dim got mesh_info replicate_mesh_dim _spmd_placements = Replicate + dp_shard_tp_placement _sharding_spec = DTensorSpec _spmd_mesh _spmd_placements tensor_meta=self _tp_spec tensor_meta param_data = cast DTensor param _local_tensor _spmd_mesh = mesh_info mesh isinstance mesh_info HSDPMeshInfo _spmd_placements = Replicate fsdp_placement _spmd_placements = fsdp_placement _sharding_spec = DTensorSpec _spmd_mesh _spmd_placements tensor_meta=TensorMeta param size param stride param dtype param_data = param param_data is_contiguous raise AssertionError f Expected contiguous tensor got param_data shape= param_data stride = shard_dim = fsdp_placement dim shard_dim = param_data ndim raise AssertionError f Shard dim shard_dim invalid param_data ndim D tensor param shape _orig_size = param_data size _contiguous_orig_stride = make_contiguous_strides_for _orig_size shard_rank = mesh_info shard_mesh_rank shard_world_size = mesh_info shard_mesh_size shard_dim param_data size shard_dim shard_world_size = If sharding nonzero dim require even sharding now because uneven sharding requires extra copies before after FSDP collectives introduces extra complexity handle padding unpadding raise NotImplementedError f FSDP does support uneven sharding dim shard_dim f param_data size world size shard_world_size chunks = _chunk_with_empty param_data shard_world_size dim=shard_dim sharded_param = chunks shard_rank sharded_size = _get_dim_chunked_size sharded_param param_data size dim=shard_dim contiguous_sharded_stride = make_contiguous_strides_for sharded_size padded_sharded_size = chunks size th always padded padded_sharded_param_size = padded_sharded_size Pre-pad sharded parameter avoid padding before all-gather padded_sharded_param = param_data new_zeros padded_sharded_size sharded_param numel padded_sharded_param narrow dim=shard_dim start= length=sharded_param size shard_dim copy_ sharded_param offload_to_cpu padded_sharded_param is_meta padded_sharded_param = padded_sharded_param cpu pin_memory padded_sharded_param = padded_sharded_param pin_memory _sharded_param_data = padded_sharded_param view - length = sharded_param size shard_dim sharded_param numel sharded_param = padded_sharded_param narrow dim=shard_dim start= length=length sharded_param is_contiguous raise AssertionError f Expected contiguous tensor fsdp_placement= sharded_param = nn Parameter to_sharded_dtensor sharded_param sharded_param requires_grad_ param requires_grad Let ` param_data ` freed normally when its ref count reaches when ` fully_shard ` call returns allow provided parameters alias _setattr_on_modules sharded_param sharded_state = ShardedState SHARDED _init_sharded_post_forward_param_metadata param torch Tensor - None mesh_info = post_forward_mesh_info mesh_info None raise AssertionError Expected post_forward_mesh_info None param_data = param _local_tensor isinstance param DTensor param chunks = _chunk_with_empty param_data mesh_info shard_mesh_size dim= sharded_post_forward_size = _get_dim_chunked_size chunks mesh_info shard_mesh_rank param_data size dim=self fsdp_placement dim contiguous_sharded_post_forward_stride = make_contiguous_strides_for sharded_post_forward_size init_dtype_attrs mp_policy MixedPrecisionPolicy param_dtype reduce_dtype = mp_policy param_dtype mp_policy reduce_dtype orig_dtype = sharded_param dtype Clamp ` reduce_dtype ` ` None ` no casting required since gradients computed ` param_dtype ` ` reduce_dtype ` matches then we do need extra casting reduce_dtype == param_dtype reduce_dtype = None Clamp ` param_dtype ` ` None ` no casting required param_dtype == orig_dtype param_dtype = None param_dtype = param_dtype reduce_dtype = reduce_dtype None indicates mixed precision enabled _init_extensions - None inner_tensor = _sharded_local_tensor has_fsdp_pre_all_gather = hasattr inner_tensor fsdp_pre_all_gather has_fsdp_post_all_gather = hasattr inner_tensor fsdp_post_all_gather has_fsdp_pre_all_gather = has_fsdp_post_all_gather raise AssertionError Both fsdp_pre_all_gather fsdp_post_all_gather should defined f using all-gather extensions inner_tensor has_fsdp_pre_all_gather _extensions_data = ExtensionsData _unsharded_inner_tensors list torch Tensor = init_all_gather_outputs all_gather_input_numels list int all_gather_input_dtypes list torch dtype world_size int device torch device force_recreate bool = False force_recreate len all_gather_outputs already initialized all_gather_outputs = torch empty torch Size numel world_size dtype=dtype device=device numel dtype zip all_gather_input_numels all_gather_input_dtypes init_unsharded_param Note Invariants torch compile Traceable FSDP Under compile we always re-populate content ` _unsharded_param ` per AllGather using slow path Under compile we always recreate ` all_gather_outputs ` per AllGather This ensure buffer creation internal graph avoid ` all_gather_outputs ` being captured graph input Under compile end ` free_unsharded_param ` we always clean up ` all_gather_outputs ` ` _unsharded_inner_tensors ` avoid them being captured graph output With these invariants only these tensors will inputs graph - Sharded parameters - Placeholders ` _unsharded_param ` nn Parameter compiled_autograd_enabled hasattr _unsharded_param after st all-gather inner_tensor = _sharded_local_tensor hasattr inner_tensor fsdp_post_all_gather already initialized tensor _unsharded_inner_tensors alloc_storage tensor all_gather_outputs = _unflatten_all_gather_outputs inner_tensor fsdp_post_all_gather all_gather_outputs _extensions_data all_gather_metadata param_dtype orig_dtype out=self _unsharded_param _extensions_data clear inner_tensor = _sharded_local_tensor compiled_autograd_enabled hasattr inner_tensor fsdp_post_all_gather all_gather_outputs = _unflatten_all_gather_outputs unsharded_tensor _unsharded_inner_tensors = inner_tensor fsdp_post_all_gather all_gather_outputs _extensions_data all_gather_metadata param_dtype orig_dtype _extensions_data clear For default path no post-all-gather all-gather output gives unsharded parameter data directly len all_gather_outputs = raise AssertionError f Expected all_gather_output got len all_gather_outputs unsharded_tensor = all_gather_outputs unsharded_param = torch as_strided unsharded_tensor _orig_size _contiguous_orig_stride storage_offset= is_dtensor unsharded_param = _from_local_no_grad unsharded_param _tp_spec hasattr _unsharded_param compiled_autograd_enabled raise AssertionError Expected compiled_autograd enabled torch no_grad torch autograd _unsafe_preserve_version_counter _unsharded_param NOTE Under compile unsharded param goes through resize_ full - copy_ - resize_ pattern we will remove those resize_ copy_ ops compiler graph pass ` remove_fsdp _unsharded_param_graph_input_usage ` recover performance _unsharded_param untyped_storage resize_ _unsharded_param numel _unsharded_param itemsize torch ops fsdp copy_ _unsharded_param unsharded_param _unsharded_param = nn Parameter unsharded_param requires_grad=self sharded_param requires_grad _unflatten_all_gather_outputs - tuple torch Tensor tuple t view - s t s zip all_gather_outputs _extensions_data all_gather_input_sizes to_sharded - None _setattr_on_modules sharded_param free_unsharded_param sharded_state = ShardedState SHARDED to_sharded_post_forward - None is_dtensor raise NotImplementedError Resharding smaller mesh TP supported yet _assert_in_states ShardedState UNSHARDED post_forward_mesh_info None raise AssertionError Expected post_forward_mesh_info None len all_gather_outputs = raise AssertionError f Expected all_gather_output got len all_gather_outputs shard_world_size = post_forward_mesh_info shard_mesh_size numel = all_gather_outputs numel shard_world_size = _raise_assert_with_print f All-gather output size numel must divisible shard f world size shard_world_size shard_rank = post_forward_mesh_info shard_mesh_rank pyrefly ignore unbound-name sharded_numel = numel shard_world_size _sharded_post_forward_param_data = all_gather_outputs narrow sharded_numel shard_rank sharded_numel clone clone able free all-gather output sharded_post_forward_tensor = torch as_strided _sharded_post_forward_param_data size=self sharded_post_forward_size stride=self contiguous_sharded_post_forward_stride storage_offset= _sharded_post_forward_param = nn Parameter to_sharded_post_forward_dtensor sharded_post_forward_tensor _setattr_on_modules _sharded_post_forward_param free_unsharded_param sharded_state = ShardedState SHARDED_POST_FORWARD to_unsharded - None Assume data has been allocated all-gathered set_requires_grad_if_needed sharded_param _unsharded_param _setattr_on_modules _unsharded_param sharded_state == ShardedState SHARDED_POST_FORWARD The data allocated default stream via post-forward reshard must kept alive next all-gather copy-in Since we call method after copy-out data s lifetime ensured without further synchronization _sharded_post_forward_param = None _sharded_post_forward_param_data = None free sharded_state = ShardedState UNSHARDED _setattr_on_modules param nn Parameter - None unsafe_setattr_param _module_info module _module_info param_name param shared_module shared_param_name zip _module_info shared_modules _module_info shared_param_names unsafe_setattr_param shared_module shared_param_name param to_sharded_dtensor tensor torch Tensor - DTensor Converts local tensor representing either sharded parameter sharded gradient DTensor tensor shape = sharded_size _raise_assert_with_print f Expects size sharded_size got tensor shape _from_local_no_grad tensor _sharding_spec to_sharded_post_forward_dtensor tensor torch Tensor - DTensor tensor shape = sharded_post_forward_size _raise_assert_with_print f Expects size sharded_post_forward_size got tensor shape isinstance post_forward_mesh_info HSDPMeshInfo raise AssertionError f Expected HSDPMeshInfo got type post_forward_mesh_info TODO Prefer DTensor read-only generalize placement once we support TP post_forward_sharding_spec = DTensorSpec post_forward_mesh_info mesh Replicate Shard tensor_meta=self _sharding_spec tensor_meta _from_local_no_grad tensor post_forward_sharding_spec to_accumulated_grad_if_needed - None Access ` _unsharded_param ` bypass sharded state check since we prefer reshard before upcasting gradient save memory reduce_dtype None _unsharded_param grad None _unsharded_param grad dtype == reduce_dtype unsharded_grad = _unsharded_param grad _unsharded_param grad = None unsharded_accumulated_grad = unsharded_grad reduce_dtype accumulate_unsharded_grad_if_needed - None unsharded_accumulated_grad None unsharded_param grad None unsharded_accumulated_grad += unsharded_param grad unsharded_param grad = None alloc_all_gather_outputs - None tensor all_gather_outputs alloc_storage tensor free_unsharded_param - None compiled_autograd_enabled Assumptions under compile - ` _unsharded_param ` NOT alias ` all_gather_outputs ` Instead we resize ` _unsharded_param ` storage size full then explicitly copy data ` all_gather_outputs ` ` _unsharded_param ` ` init_unsharded_param ` For full-graph FSDP case we will then remove resize_ copy_ ops compiler graph pass recover performance - ` all_gather_outputs ` ` _unsharded_inner_tensors ` NOT graph inputs They created within graph guaranteed freed end graph They don t leak outside graph _unsharded_param untyped_storage resize_ all_gather_outputs = _unsharded_inner_tensors = tensor itertools chain all_gather_outputs _unsharded_inner_tensors free_storage tensor property all_gather_inputs - list torch Tensor D _assert_in_states ShardedState SHARDED ShardedState SHARDED_POST_FORWARD sharded_state == ShardedState SHARDED compiled_autograd_enabled hasattr _sharded_local_tensor fsdp_pre_all_gather sharded_local_tensor = _sharded_local_tensor offload_to_cpu sharded_local_tensor = sharded_local_tensor device non_blocking=True pre_all_gather_signature = inspect signature pyrefly ignore missing-attribute sharded_local_tensor fsdp_pre_all_gather num_fn_params = len pre_all_gather_signature parameters Old signature only passes mesh keep BC now num_fn_params raise AssertionError f Invalid fsdp_pre_all_gather pre_all_gather_signature \n Expects fsdp_pre_all_gather mesh DeviceMesh outer_size torch Size outer_stride tuple int module nn Module mp_policy MixedPrecisionPolicy num_fn_params == all_gather_inputs _extensions_data all_gather_metadata pyrefly ignore missing-attribute = sharded_local_tensor fsdp_pre_all_gather shard_mesh_from_root all_gather_inputs _extensions_data all_gather_metadata pyrefly ignore missing-attribute = sharded_local_tensor fsdp_pre_all_gather shard_mesh_from_root _orig_size _contiguous_orig_stride _module_info module mp_policy sharded_local_tensor size = padded_sharded_param_size any all_gather_input size = padded_sharded_param_size all_gather_input all_gather_inputs NOTE Since error can only raised ranks have padding can manifest NCCL watchdog timeout other ranks will error raise AssertionError When parameter unevenly sharded FSDP f orig size= _orig_size FSDP world size= mesh_info mesh size fsdp_pre_all_gather must all-gather inputs padded sharded size f padded_sharded_param_size got t size t all_gather_inputs _extensions_data all_gather_input_sizes = t size t all_gather_inputs t view - t all_gather_inputs sharded_param_data = _sharded_param_data offload_to_cpu sharded_param_data = sharded_param_data device non_blocking=True _to_dtype_if_needed sharded_param_data param_dtype sharded_state == ShardedState SHARDED_POST_FORWARD compiled_autograd_enabled hasattr _sharded_local_tensor fsdp_pre_all_gather raise NotImplementedError all_gather_input = _to_dtype_if_needed cast torch Tensor _sharded_post_forward_param_data param_dtype all_gather_input torch empty mypy property unsharded_param - nn Parameter ND _unsharded_param property unsharded_grad_data - torch Tensor grad = unsharded_param grad grad None raise AssertionError Expects unsharded_param grad None _get_grad_inner_tensor grad property unsharded_accumulated_grad_data - torch Tensor grad = unsharded_accumulated_grad grad None raise AssertionError Expects unsharded_accumulated_grad None _get_grad_inner_tensor grad _get_grad_inner_tensor grad torch Tensor - torch Tensor is_dtensor isinstance grad AsyncCollectiveTensor grad = grad wait isinstance grad DTensor raise AssertionError f Expected DTensor got type grad placements = _tp_spec placements placements = grad placements len _tp_spec placements = len grad placements raise AssertionError f Expected same placement length _tp_spec= grad placements= grad = grad redistribute placements=placements grad = grad _local_tensor grad property _sharded_local_tensor - torch Tensor cast DTensor sharded_param _local_tensor property shard_mesh mesh = mesh_info mesh mesh ndim == mesh mesh ndim == mesh mesh_dim_names None raise AssertionError Expected mesh_dim_names None mesh mesh mesh_dim_names - raise ValueError f Invalid mesh mesh property shard_mesh_from_root mesh = mesh_info mesh mesh ndim == mesh mesh mesh_dim_names None raise AssertionError Expected mesh_dim_names None shard_dim_name = mesh mesh_dim_names - mesh shard_dim_name _assert_in_states states ShardedState - None sharded_state states _raise_assert_with_print f Expects one states sharded_state reset_sharded_param For ops like ` nn Module _apply ` ` load_state_dict assign=True ` change sharded parameter tensor we may need re-pad sharded local tensor re-save reference module_info = _module_info new_param = getattr module_info module module_info param_name new_param sharded_param torch __future__ get_swap_module_params_on_conversion raise AssertionError f Expects swap_tensors preserve object got new_param f instead sharded_param sharded_param = new_param pyrefly ignore missing-attribute local_tensor = new_param _local_tensor local_tensor is_meta updated_local_tensor = False local_tensor can padded twice st time fully_shard model nd time model input lazy_init nd time should no-op parameters remain unchanged nd time shouldn t no-op people call model load_state_dict before lazy_init makes possible trainer call ` sd = model state_dict ` before training loop use ` sd ` without calling state_dict per iteration same_local_tensor = False TODO need support tensor subclass type _sharded_param_data torch Tensor same_local_tensor = when sharding param shape over ranks local_tensor rank can size data_ptr can _sharded_param_data untyped_storage data_ptr _sharded_param_data untyped_storage data_ptr == local_tensor untyped_storage data_ptr padded_sharded_size = padded_sharded_param_size shard_dim = fsdp_placement dim length = local_tensor size shard_dim local_tensor numel local_tensor size = padded_sharded_size same_local_tensor shard_dim = raise AssertionError f Shard shard_dim requires even sharding local_tensor size = padded_local_tensor = local_tensor new_zeros padded_sharded_size padded_local_tensor narrow dim=shard_dim start= length=length copy_ local_tensor local_tensor = padded_local_tensor updated_local_tensor = True pin_memory local_tensor is_pinned local_tensor = local_tensor cpu pin_memory updated_local_tensor = True same_local_tensor _sharded_param_data = local_tensor view - isinstance sharded_param DTensor raise AssertionError f Expected DTensor got type sharded_param updated_local_tensor Only change local tensor object needed sharded_param _local_tensor = local_tensor narrow dim=shard_dim start= length=length sharded_param _local_tensor is_contiguous raise AssertionError Expected sharded_param _local_tensor contiguous _sharding_spec = sharded_param _spec __repr__ f FSDPParam fqn= _param_fqn orig_size= _orig_size alloc_storage tensor torch Tensor - None size = tensor numel tensor itemsize storage = tensor untyped_storage size = size storage resize_ size free_storage tensor torch Tensor - None storage = tensor untyped_storage size = storage resize_ NOTE These bypass ` nn Module __setattr__ ` checks which incur non-trivial CPU overhead module did override For FSDP we know we do need those checks when transitioning between sharded unsharded parameters unsafe_setattr_param module nn Module param_name str param nn Parameter - None getattr module __setattr__ __func__ None nn Module __setattr__ module _parameters param_name = param slow path setattr module param_name param set_requires_grad_if_needed src_tensor torch Tensor dst_tensor torch Tensor - None Only call ` requires_grad_ ` needed avoid Python C++ context switch overhead src_tensor requires_grad = dst_tensor requires_grad dst_tensor requires_grad_ src_tensor requires_grad