usr bin env python Copyright c Facebook Inc its affiliates All Rights Reserved Runs CIFAR training differential privacy argparse logging shutil sys datetime datetime timedelta numpy np opacus PrivacyEngine torchvision models transforms torchvision datasets CIFAR tqdm tqdm torch torch nn nn torch optim optim torch utils data logging basicConfig format= asctime s levelname s message s datefmt= m d Y H M S stream=sys stdout logger = logging getLogger ddp logger setLevel level=logging INFO save_checkpoint state is_best filename= checkpoint tar torch save state filename is_best shutil copyfile filename model_best pth tar accuracy preds labels preds == labels mean train args model train_loader optimizer privacy_engine epoch device start_time = datetime now model train criterion = nn CrossEntropyLoss losses = top _acc = i images target enumerate tqdm train_loader images = images device target = target device compute output output = model images loss = criterion output target preds = np argmax output detach cpu numpy axis= labels = target detach cpu numpy measure accuracy record loss acc = accuracy preds labels losses append loss item top _acc append acc compute gradient do SGD step loss backward make sure we take step after processing last mini-batch epoch ensure we start next epoch clean state optimizer step optimizer zero_grad i args print_freq == args disable_dp epsilon best_alpha = privacy_engine accountant get_privacy_spent delta=args delta alphas= + x x range + list range print f \tTrain Epoch epoch \t f Loss np mean losses f f Acc np mean top _acc f f ε = epsilon f δ = args delta α = best_alpha print f \tTrain Epoch epoch \t f Loss np mean losses f f Acc np mean top _acc f train_duration = datetime now - start_time train_duration test args model test_loader device model eval criterion = nn CrossEntropyLoss losses = top _acc = torch no_grad images target tqdm test_loader images = images device target = target device output = model images loss = criterion output target preds = np argmax output detach cpu numpy axis= labels = target detach cpu numpy acc = accuracy preds labels losses append loss item top _acc append acc top _avg = np mean top _acc print f \tTest set Loss np mean losses f Acc top _avg f np mean top _acc flake noqa C main args = parse_args args debug = logger setLevel level=logging DEBUG device = args device args secure_rng try torchcsprng prng except ImportError e msg = To use secure RNG you must install torchcsprng package Check out instructions here https github com pytorch csprng#installation raise ImportError msg e generator = prng create_random_device_generator dev urandom generator = None augmentations = transforms RandomCrop padding= transforms RandomHorizontalFlip normalize = transforms ToTensor transforms Normalize train_transform = transforms Compose augmentations + normalize args disable_dp normalize test_transform = transforms Compose normalize train_dataset = CIFAR root=args data_root train=True download=True transform=train_transform train_loader = torch utils data DataLoader train_dataset batch_size=int args sample_rate len train_dataset generator=generator num_workers=args workers pin_memory=True test_dataset = CIFAR root=args data_root train=False download=True transform=test_transform test_loader = torch utils data DataLoader test_dataset batch_size=args batch_size_test shuffle=False num_workers=args workers best_acc = model = models __dict__ args architecture pretrained=False norm_layer= lambda c nn GroupNorm args gn_groups c model = model device args optim == SGD optimizer = optim SGD model parameters lr=args lr momentum=args momentum weight_decay=args weight_decay args optim == RMSprop optimizer = optim RMSprop model parameters lr=args lr args optim == Adam optimizer = optim Adam model parameters lr=args lr raise NotImplementedError Optimizer recognized Please check spelling privacy_engine = None args disable_dp args clip_per_layer Each layer has same clipping threshold The total grad norm still bounded ` args max_per_sample_grad_norm ` n_layers = len n p n p model named_parameters p requires_grad max_grad_norm = args max_per_sample_grad_norm np sqrt n_layers n_layers max_grad_norm = args max_per_sample_grad_norm privacy_engine = PrivacyEngine secure_mode=args secure_rng clipping = per_layer args clip_per_layer flat model optimizer train_loader = privacy_engine make_private module=model optimizer=optimizer data_loader=train_loader noise_multiplier=args sigma max_grad_norm=max_grad_norm clipping=clipping Store some logs accuracy_per_epoch = time_per_epoch = epoch range args start_epoch args epochs + args lr_schedule == cos lr = args lr + np cos np pi epoch args epochs + param_group optimizer param_groups param_group lr = lr train_duration = train args model train_loader optimizer privacy_engine epoch device top _acc = test args model test_loader device remember best acc save checkpoint is_best = top _acc best_acc best_acc = max top _acc best_acc time_per_epoch append train_duration accuracy_per_epoch append float top _acc save_checkpoint epoch epoch + arch Convnet state_dict model state_dict best_acc best_acc optimizer optimizer state_dict is_best filename=args checkpoint_file + tar time_per_epoch_seconds = t total_seconds t time_per_epoch avg_time_per_epoch = sum time_per_epoch_seconds len time_per_epoch_seconds metrics = accuracy best_acc accuracy_per_epoch accuracy_per_epoch avg_time_per_epoch_str str timedelta seconds=int avg_time_per_epoch time_per_epoch time_per_epoch_seconds logger info \nNote \n- total_time includes data loading time training time testing time \n- time_per_epoch measures training time only \n logger info metrics parse_args parser = argparse ArgumentParser description= PyTorch CIFAR DP Training parser add_argument -j -- workers default= type=int metavar= N help= number data loading workers default parser add_argument -- epochs default= type=int metavar= N help= number total epochs run parser add_argument -- start-epoch default= type=int metavar= N help= manual epoch number useful restarts parser add_argument -b -- batch-size-test default= type=int metavar= N help= mini-batch size test dataset default parser add_argument -- sample-rate default= type=float metavar= SR help= sample rate used batch construction default parser add_argument -- lr -- learning-rate default= type=float metavar= LR help= initial learning rate dest= lr parser add_argument -- momentum default= type=float metavar= M help= SGD momentum parser add_argument -- wd -- weight-decay default= type=float metavar= W help= SGD weight decay dest= weight_decay parser add_argument -p -- print-freq default= type=int metavar= N help= print frequency default parser add_argument -- resume default= type=str metavar= PATH help= path latest checkpoint default none parser add_argument -e -- evaluate dest= evaluate action= store_true help= evaluate model validation set parser add_argument -- seed default=None type=int help= seed initializing training parser add_argument -- sigma type=float default= metavar= S help= Noise multiplier default parser add_argument -c -- max-per-sample-grad_norm type=float default= metavar= C help= Clip per-sample gradients norm default parser add_argument -- disable-dp action= store_true default=False help= Disable privacy training just train vanilla SGD parser add_argument -- secure-rng action= store_true default=False help= Enable Secure RNG have trustworthy privacy guarantees Comes performance cost Opacus will emit warning secure rng off indicating production use s recommender turn parser add_argument -- delta type=float default= e- metavar= D help= Target delta default e- parser add_argument -- checkpoint-file type=str default= checkpoint help= path save check points parser add_argument -- data-root type=str default= cifar help= Where CIFAR will stored parser add_argument -- log-dir type=str default= tmp stat tensorboard help= Where Tensorboard log will stored parser add_argument -- optim type=str default= SGD help= Optimizer use Adam RMSprop SGD parser add_argument -- lr-schedule type=str choices= constant cos default= cos parser add_argument -- device type=str default= cuda help= Device which run code parser add_argument -- architecture type=str default= resnet help= model torchvision run parser add_argument -- gn-groups type=int default= help= Number groups GroupNorm parser add_argument -- clip-per-layer -- clip_per_layer action= store_true default=False help= Use static per-layer clipping same clipping threshold each layer Necessary DDP If ` False ` default uses flat clipping parser add_argument -- debug type=int default= help= debug level default parser parse_args __name__ == __main__ main