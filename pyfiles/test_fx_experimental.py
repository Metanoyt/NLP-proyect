Owner s module fx ruff noqa F functools math numbers operator pickle sys sympy tempfile typing unittest types BuiltinFunctionType typing NamedTuple Optional Union collections abc Callable torch torch fx experimental meta_tracer torch fx experimental optimization optimization torch fx _symbolic_trace symbolic_trace torch fx experimental merge_matmul torch fx experimental accelerator_partitioner Partitioner torch fx experimental proxy_tensor make_fx torch fx experimental normalize NormalizeArgs NormalizeOperators torch fx experimental partitioner_utils Device get_latency_of_partitioned_graph get_partition_to_latency_mapping NodeLatency PartitionerConfig PartitionMode torch fx experimental rewriter RewritingTracer torch fx experimental schema_type_annotation AnnotateTypesWithSchema torch fx graph_module GraphModule torch fx node Node torch fx operator_schemas _torchscript_type_to_python_type create_type_hint normalize_function normalize_module type_matches torch fx passes graph_manipulation torch fx passes param_fetch lift_lowering_attrs_to_nodes torch fx passes shape_prop ShapeProp torch fx passes split_module split_module torch fx passes annotate_getitem_nodes annotate_getitem_nodes torch testing _internal common_device_type instantiate_device_type_tests onlyCPU ops torch testing _internal common_methods_invocations op_db torch testing _internal common_nn module_tests get_new_module_tests torch testing _internal common_utils TEST_Z run_tests TestCase TEST_WITH_CROSSREF torch testing _internal jit_utils JitTestCase torch utils _pytree pytree try torchvision models torchvision models resnet HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False skipIfNoTorchVision = unittest skipIf HAS_TORCHVISION no torchvision skipIfNoMkldnn = unittest skipIf torch backends mkldnn enabled torch backends mkldnn is_available no MKLDNN symbolic_trace_with_rewrite root Union torch nn Module Callable - GraphModule GraphModule root isinstance root torch nn Module torch nn Module RewritingTracer trace root TestFXExperimental JitTestCase test_find_single_partition TestModule torch nn Module forward b + b m = TestModule traced = symbolic_trace m = torch rand b = torch rand graph_manipulation get_size_of_all_nodes traced b partitioner = Partitioner devices = Device dev_ Device dev_ Device dev_ partitioner_config = PartitionerConfig devices ret = partitioner partition_graph traced m partitioner_config module_with_submodules = ret module_with_submodules dag = ret dag assertEqual traced b module_with_submodules b assert dag nodes logical_device_ids == test_lack_of_devices TestModule torch nn Module forward b + b m = TestModule traced = symbolic_trace m = torch rand b = torch rand graph_manipulation get_size_of_all_nodes traced b partitioner = Partitioner devices = Device dev_ Device dev_ partitioner_config = PartitionerConfig devices PartitionMode size_based catch_runtime_error = False try ret = partitioner partition_graph traced m partitioner_config except RuntimeError catch_runtime_error = True assert catch_runtime_error test_large_node_error TestModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward linear = linear add = linear + add m = TestModule traced = symbolic_trace m = torch rand graph_manipulation get_size_of_all_nodes traced partitioner = Partitioner devices = Device dev_ Device dev_ Device dev_ Device dev_ Device dev_ partitioner_config = PartitionerConfig devices PartitionMode size_based catch_runtime_error = False try ret = partitioner partition_graph traced m partitioner_config except RuntimeError catch_runtime_error = True assert catch_runtime_error test_partition_node_manipulation TestModule torch nn Module forward b add_ = + b add_ = add_ + torch rand add_ = add_ + torch rand add_ m = TestModule traced = symbolic_trace m b = torch rand torch rand graph_manipulation get_size_of_all_nodes traced b partitioner = Partitioner devices = Device dev_ partitioner_config = PartitionerConfig devices ret = partitioner partition_graph traced m partitioner_config partition = partitioner partitions assert partition used_mem_bytes == Select add_ node remove selected_node = None node partition nodes node name == add_ selected_node = node partition remove_node selected_node assert partition used_mem_bytes == test_size_based_partition TestModule torch nn Module __init__ - None super __init__ linear = torch nn Linear c = torch rand forward b add_ = + b linear = linear add_ add_ = linear + c add_ m = TestModule traced = symbolic_trace m = torch rand b = torch rand graph_manipulation get_size_of_all_nodes traced b partitioner = Partitioner devices = Device dev_ Device dev_ Device dev_ partitioner_config = PartitionerConfig devices PartitionMode size_based ret = partitioner partition_graph traced m partitioner_config module_with_submodules = ret module_with_submodules dag = ret dag assertEqual traced b module_with_submodules b i node enumerate dag nodes assert node logical_device_ids == i test_partition_device_mapping TestModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward b = torch rand add_ = + b linear_ = linear add_ add_ = torch rand + add_ = add_ + linear_ add_ m = TestModule traced = symbolic_trace m = torch rand graph_manipulation get_size_of_all_nodes traced partitioner = Partitioner devices = Device dev_ Device dev_ partitioner_config = PartitionerConfig devices PartitionMode size_based ret = partitioner partition_graph traced m partitioner_config module_with_submodules = ret module_with_submodules dag = ret dag assertEqual traced module_with_submodules i node enumerate dag nodes i == assert node logical_device_ids == assert node logical_device_ids == test_sparse_nn_partition MyRecommendationModule torch nn Module create_mlp num_of_layers int input_size int output_size int layers = torch nn ModuleList _ range num_of_layers ll = torch nn Linear input_size output_size layers append ll layers append torch nn ReLU layers __init__ - None super __init__ layers = create_mlp bottom_layers = torch nn Sequential layers layers = create_mlp top_layers = torch nn Sequential layers embedding_layers = torch nn ModuleList el = torch nn EmbeddingBag mode= sum sparse=True embedding_layers append el _ range el = torch nn EmbeddingBag mode= sum sparse=True embedding_layers append el el = torch nn EmbeddingBag mode= sum sparse=True embedding_layers append el forward b offset x = bottom_layers y = c = _ range len embedding_layers temp = torch randint c append temp + b i range len embedding_layers i == y append embedding_layers i c i offset y append embedding_layers i torch randint offset z = torch cat x + y dim= p = top_layers z p m = MyRecommendationModule = torch rand b = torch randint offset = torch randint traced = symbolic_trace m graph_manipulation get_size_of_all_nodes traced b offset devices = Device dev_ Device dev_ Device dev_ partitioner_config = PartitionerConfig devices PartitionMode sparse_nn partitioner = Partitioner ret = partitioner partition_graph traced m partitioner_config module_with_submodules = ret module_with_submodules dag = ret dag assertEqual traced b offset module_with_submodules b offset assert len module_with_submodules graph nodes == test_partition_latency TestModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward add_ = + torch rand add_ = add_ + torch rand linear_ = linear add_ add_ = add_ + linear_ add_ = add_ + add_ add_ get_node_to_latency_mapping fx_module GraphModule Given fx module generate node latency each node based size each node node_to_latency_mapping dict Node NodeLatency = node fx_module graph nodes node op output placeholder get_attr node size_bytes total_size == node size_bytes output_size node_to_latency_mapping node = NodeLatency node size_bytes total_size node size_bytes total_size node_to_latency_mapping node = NodeLatency node size_bytes total_size node size_bytes output_size node_to_latency_mapping m = TestModule traced = symbolic_trace m = torch rand graph_manipulation get_size_of_all_nodes traced node_to_latency_mapping = get_node_to_latency_mapping traced devices = Device dev_ Device dev_ partitioner = Partitioner partitioner_config = PartitionerConfig devices ret = partitioner partition_graph traced m partitioner_config module_with_submodules = ret module_with_submodules assertEqual traced module_with_submodules partitions = partitioner partitions partition_to_latency_mapping = get_partition_to_latency_mapping partitions node_to_latency_mapping p partition_to_latency_mapping p partition_id == assert partition_to_latency_mapping p == assert partition_to_latency_mapping p == transfer_rate_bytes_per_sec = critical_path_latency_sec = get_latency_of_partitioned_graph partitions partition_to_latency_mapping transfer_rate_bytes_per_sec assert critical_path_latency_sec == test_cost_aware_partition MyModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward add_ = + torch rand add_ = add_ + torch rand linear_ = linear add_ add_ = add_ + torch rand add_ = add_ + linear_ add_ = add_ + add_ add_ get_node_to_latency_mapping fx_module GraphModule node_to_latency_mapping dict Node NodeLatency = node fx_module graph nodes node op output placeholder get_attr node size_bytes total_size == node size_bytes output_size node_to_latency_mapping node = NodeLatency node size_bytes total_size node_to_latency_mapping node = NodeLatency node size_bytes total_size node size_bytes output_size node_to_latency_mapping m = MyModule traced = symbolic_trace m = torch rand graph_manipulation get_size_of_all_nodes traced devices = Device dev_ Device dev_ Device dev_ Device dev_ node_to_latency_mapping = get_node_to_latency_mapping traced partitioner_config = PartitionerConfig devices mode=PartitionMode cost_aware transfer_rate_bytes_per_sec= node_to_latency_mapping=node_to_latency_mapping partitioner = Partitioner ret = partitioner partition_graph traced m partitioner_config module_with_submodules = ret module_with_submodules dag = ret dag assertEqual traced module_with_submodules partitions = partitioner partitions partition_to_latency_mapping = get_partition_to_latency_mapping partitions node_to_latency_mapping critical_path_latency_sec = get_latency_of_partitioned_graph partitions partition_to_latency_mapping partitioner_config transfer_rate_bytes_per_sec assert critical_path_latency_sec == test_aot_based_partition TestModule torch nn Module __init__ - None super __init__ b = torch rand c = torch rand forward add_ = + b add_ = c + add_ add_ m = TestModule traced = symbolic_trace m = torch rand node_to_partition_id = partition_to_logical_devices = count = graph_manipulation get_size_of_all_nodes traced node traced graph nodes node op placeholder get_attr output node_to_partition_id node = count partition_to_logical_devices count = count += devices = Device dev_ partitioner_config = PartitionerConfig devices=devices mode=PartitionMode aot_based node_to_partition_mapping=node_to_partition_id partition_to_logical_device_mapping=partition_to_logical_devices partitioner = Partitioner ret = partitioner partition_graph traced m partitioner_config module_with_submodules = ret module_with_submodules dag = ret dag assertEqual module_with_submodules traced node dag nodes assert node size_bytes == assert node logical_device_ids == test_replace_target_nodes_with testModule torch nn Module forward b + b m = testModule traced = symbolic_trace m input = torch randn input = torch randn assert input + input == traced input input graph_manipulation replace_target_nodes_with fx_module=traced old_op= call_function old_target=operator add new_op= call_function new_target=operator mul assert input input == traced input input test_saturate_host TestModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward add_ = + torch rand add_ = add_ + torch rand linear_ = linear add_ add_ = add_ + linear_ add_ = add_ + add_ add_ m = TestModule traced = symbolic_trace m = torch rand graph_manipulation get_size_of_all_nodes traced devices = Device dev_ Device dev_ Device dev_ Device dev_ Device dev_ Device dev_ partitioner = Partitioner Without host saturation model will split into two partitions dev_ holds partition bytes dev_ holds partition bytes partitioner_config = PartitionerConfig devices saturate_host=True ret = partitioner partition_graph traced m partitioner_config module_with_submodules = ret module_with_submodules assertEqual traced module_with_submodules partitions = partitioner partitions assertEqual len partitions With host saturation partition will replicated dev_ partition will replicated dev_ assertEqual partitions logical_device_ids assertEqual partitions logical_device_ids skipIfNoTorchVision test_conv_bn_fusion rn = resnet eval traced = symbolic_trace rn fused = optimization fuse traced assertTrue all isinstance m torch nn BatchNorm d m fused modules N C H W = inp = torch randn N C H W assertEqual fused inp rn inp test_conv_bn_fusion_not_running_state M torch nn Module __init__ - None super __init__ conv = torch nn Conv d stride= bn = torch nn BatchNorm d eps= e- momentum= affine=True track_running_stats=False forward x x = conv x x = bn x x model = M eval traced = symbolic_trace model fused = optimization fuse traced inp = torch randn bn need folded conv assertTrue any isinstance m torch nn BatchNorm d m fused modules assertEqual fused inp model inp test_conv_bn_fusion_mixed_dtype M torch nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= padding= bias=False dtype=torch bfloat bn = torch nn BatchNorm d eps= momentum= affine=True track_running_stats=True forward x x = conv x x = bn x x model = M eval traced = symbolic_trace model fused = optimization fuse traced inp = torch randn dtype=torch bfloat assertTrue all isinstance m torch nn BatchNorm d m fused modules assertEqual fused inp model inp test_call_to_assert_no_msg M torch nn Module forward b assert == b + b m = M traced = symbolic_trace_with_rewrite m Make sure graph well-formed traced graph lint Check IR make sure there s call_function node target == Assert assertTrue any node op == call_function node target == torch _assert node traced graph nodes Ensure assert throws when s supposed doesn t throw when s supposed traced assertRaisesRegex AssertionError traced Confirm output correct assertEqual traced m test_meta_tracer MetaTracerTestModule torch nn Module __init__ - None super __init__ emb = torch nn Embedding num_embeddings= embedding_dim= layernorm = torch nn LayerNorm forward x emb = emb x emb = emb + torch arange emb shape - dtype=torch float device=emb device lol = layernorm emb torch relu lol lol shape torch sigmoid lol mttm = MetaTracerTestModule BS x = torch zeros BS dtype=torch long random_ meta_args = x x device= meta gm = torch fx experimental meta_tracer symbolic_trace mttm meta_args=meta_args torch testing assert_close gm x mttm x Test serialization deserialization tempfile TemporaryDirectory tmp_dir open f tmp_dir meta_module pkl wb f pickle dump gm f open f tmp_dir meta_module pkl rb f loaded = pickle load f torch testing assert_close loaded x mttm x test_call_to_assert_with_msg M torch nn Module forward b assert == b test message + b m = M traced = symbolic_trace_with_rewrite m Make sure graph well-formed traced graph lint Check IR make sure there s call_function node target == Assert assertTrue any node op == call_function node target == torch _assert node traced graph nodes Ensure assert throws when s supposed doesn t throw when s supposed traced assertRaisesRegex AssertionError test message traced Confirm output correct assertEqual traced m test_call_to_assert_with_empty_msg M torch nn Module forward b assert == b + b m = M traced = symbolic_trace_with_rewrite m Make sure graph well-formed traced graph lint Check IR make sure there s call_function node target == Assert assertTrue any node op == call_function node target == torch _assert node traced graph nodes Ensure assert throws when s supposed doesn t throw when s supposed traced assertRaisesRegex AssertionError traced Confirm output correct assertEqual traced m test_call_to_assert_with_multiline_message M torch nn Module forward b error_msg = An error message terrible spacing assert == b error_msg + b m = M traced = symbolic_trace_with_rewrite m Make sure graph well-formed traced graph lint Check IR make sure there s call_function node target == Assert assertTrue any node op == call_function node target == torch _assert node traced graph nodes Ensure assert throws when s supposed doesn t throw when s supposed error_msg = An error message terrible spacing traced assertRaisesRegex AssertionError error_msg traced Confirm output correct assertEqual traced m test_subgraph_creation MyModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand linear = torch nn Linear forward x y z = linear x + param clamp min= max= w = linear y clamp min= max= z + w symbolically trace model my_module = MyModule my_module_traced = symbolic_trace my_module random mod partitioning partition_counter = NPARTITIONS = Add some random meta info make sure kept around node my_module_traced graph nodes node op = output node meta test_meta_info = True mod_partition node Node nonlocal partition_counter partition = partition_counter NPARTITIONS partition_counter = partition_counter + NPARTITIONS partition split module module submodules module_with_submodules = split_module my_module_traced my_module mod_partition Check test_meta_info still all nodes submodules = dict module_with_submodules named_modules node module_with_submodules graph nodes node op == call_module submod = submodules node target assertTrue isinstance submod torch fx GraphModule submod_node submod graph nodes submod_node op = output stored_op = submod_node meta get test_meta_info assertTrue stored_op None stored_op x = torch rand y = torch rand orig_out = my_module_traced x y submodules_out = module_with_submodules x y assertEqual orig_out submodules_out test_split_module_input_names Mod torch nn Module forward x b b c c x = x + + x = x + b + b x = x + c + c x mod = Mod traced = torch fx symbolic_trace mod seen = split n nonlocal seen result = seen seen += result split = split_module traced mod split keep_original_input_name=False All submodules should take inputs same order args = torch tensor torch tensor torch tensor output = split submod_ args output = split submod_ args output = split submod_ args assertEqual output output assertEqual output output Each submodule should have normalized input names check_ph gm nodes = list gm graph nodes assertEqual nodes target arg_ assertEqual nodes target arg_ assertEqual nodes target arg_ check_ph split submod_ check_ph split submod_ check_ph split submod_ test_split_module_dead_code ModWithDeadCode torch nn Module forward x output = x we want dead_line = x + dead output mod = ModWithDeadCode traced = torch fx symbolic_trace mod split into before target after saw_mul = False split_callback n nonlocal saw_mul n target == operator mul saw_mul = True saw_mul saw_mul split = split_module traced mod split_callback x = torch randn torch testing assert_close split x traced x test_split_module_return_node foo x x add_ gm = make_fx foo tracing_mode= fake torch randn cb _ sp_gm = split_module gm None cb submod_gm = sp_gm submod_ node submod_gm graph nodes node op == output break raise RuntimeError Expected subgraph have output node test_split_module_kwargs_expansion ModuleWithKwargsExpansion torch nn Module forward x kwargs x + kwargs foo mod = ModuleWithKwargsExpansion traced = torch fx symbolic_trace mod seen_getitem = False split_callback n nonlocal seen_getitem split_idx = int seen_getitem n target == operator getitem seen_getitem = True split_idx split = split_module traced mod split_callback x = torch randn foo = torch randn torch testing assert_close split x foo=foo traced x foo=foo skipIfNoTorchVision test_subgraph_trivial_resnet Smoke test trivially splitting resnet into partition works There issue before causing submodule names aliased m = resnet traced = symbolic_trace m = torch rand module_with_submodules = split_module traced m lambda node module_with_submodules test_split_module_default_arg ModelToTrace torch nn Module __init__ - None super __init__ lin = torch nn Linear forward x targets=None x = lin x targets None x = x + targets x mtt = ModelToTrace traced = torch fx symbolic_trace mtt concrete_args= targets None split = split_module traced mtt lambda node x = torch randn torch testing assert_close split x traced x test_split_module_keep_original_order_and_noop_graph Verify split_module returns similar no-op graph ` keep_original_order= True &#124; False ` fn x x g = make_fx fn tracing_mode= fake torch randn g graph print_tabular opcode name target args kwargs ----------- ------ -------- --------- -------- placeholder x_ x_ output output output x_ _test_split_graph split_gm Verify split_gm has same structure original assertEqual len split_gm graph nodes nodes = list split_gm graph nodes assertEqual nodes op placeholder assertEqual nodes op output ` keep_original_order=False ` _test_split_graph split_module g None split_callback=lambda _ keep_original_order=False ` keep_original_order=True ` _test_split_graph split_module g None split_callback=lambda _ keep_original_order=True unittest skipIf TEST_WITH_CROSSREF See https github com pytorch pytorch issues test_split_module_symint_dependency_handling Based code - transformers models granitemoe modeling_granitemoe py GraniteMoeTopKGating torch nn Module __init__ input_size int num_experts int top_k int super __init__ num_experts = num_experts input_size = input_size top_k = top_k layer = torch nn Linear input_size num_experts bias=False forward hidden_states compute top_k routing decision logits = layer hidden_states float batch_size x seq_len num_experts top_k_logits top_k_indices = logits topk top_k dim= num_tokens top_k top_k_gates = torch softmax top_k_logits dim= type_as hidden_states num_tokens top_k compute number input given each expert zeros = torch zeros top_k_gates size num_experts dtype=top_k_gates dtype device=top_k_gates device num_tokens num_experts gates = zeros scatter top_k_indices num_tokens num_experts expert_size = gates long sum num_experts expert_size = expert_size tolist sort group input tokens according expert assignment top_k_experts = top_k_indices flatten num_tokens top_k _ index_sorted_experts = top_k_experts sort num_tokens top_k batch_index = index_sorted_experts div top_k rounding_mode= trunc num_tokens top_k gather gate values grouped input tokens top_k_gates = top_k_gates flatten num_tokens top_k batch_gates = top_k_gates index_sorted_experts num_tokens top_k index_sorted_experts batch_index batch_gates expert_size logits GraniteMoeMoE torch nn Module __init__ super __init__ input_size = num_local_experts = num_experts_per_tok = router = GraniteMoeTopKGating input_size=self input_size num_experts=self num_local_experts top_k=num_experts_per_tok forward layer_input _ batch_index _ expert_size _ = router layer_input expert_inputs = layer_input batch_index expert_inputs split expert_size dim= moe = GraniteMoeMoE inp = torch randn expected = moe inp PARTITION_ID = PARTITION_OPS_CTR = NODE_PARTITION_MAP = ` callback ` called multiple times same ` node ` ` split_module ` Cache result such partition id consistent across calls callback node - int nonlocal PARTITION_ID PARTITION_OPS_CTR NODE_PARTITION_MAP node NODE_PARTITION_MAP NODE_PARTITION_MAP node PARTITION_OPS_CTR == PARTITION_ID += PARTITION_OPS_CTR += NODE_PARTITION_MAP node = PARTITION_ID PARTITION_ID backend gm inps split_gm = split_module gm root_m=None split_callback=callback keep_original_order=True keep_original_node_name=True split_gm actual = torch compile moe backend=backend inp torch testing assert_close actual expected test_normalize_binary_operators ops_to_test = torch add torch mul torch sub torch div torch floor_divide torch remainder torch eq torch ne torch lt torch le torch gt torch ge Test Tensor Tensor callsite op ops_to_test WrapperMod torch nn Module forward x y op x y traced = symbolic_trace WrapperMod normalized = NormalizeOperators traced transform x y = torch randn torch randn torch testing assert_close traced x y normalized x y assertFalse any n target ops_to_test n normalized graph nodes Test Tensor scalar callsite op ops_to_test WrapperMod torch nn Module forward x op x traced = symbolic_trace WrapperMod normalized = NormalizeOperators traced transform x = torch randn torch testing assert_close traced x normalized x assertFalse any n target ops_to_test n normalized graph nodes skipIfNoTorchVision test_normalize_args m = resnet FunctionalTracer torch fx Tracer is_leaf_module m torch nn Module module_qualified_name str - bool ` leaves ` contains set standard ` nn Modules ` currently symbolically traceable Ideally set would empty leaves = torch nn BatchNorm d type m leaves traced = torch fx GraphModule m FunctionalTracer trace m input = torch randn ref_outs = traced input ShapeProp traced propagate input traced = NormalizeArgs traced transform modules = dict traced named_modules node traced graph nodes node op == call_function node target = operator add assertEqual len node args node op == call_module submod_class = modules node target __class__ nn_class = getattr torch nn submod_class __name__ submod_class == nn_class assertEqual len node args traced input assertEqual traced input ref_outs test_normalize_modules_exhaustive Exhaustively test ` Node normalized_arguments ` all standard torch nn Module classes test_params module_tests + get_new_module_tests constructor test_params constructor = getattr torch nn test_params module_name constructor = test_params constructor constructor_args test_params args = args = test_params constructor_args mod = constructor args Skip modules standard ` torch nn ` instances including functionals functionals tested test_normalize_args mod __class__ __name__ dir torch nn continue input_fn test_params inputs = torch randn test_params input_size inputs = test_params input_fn isinstance inputs tuple list inputs = inputs params = join f v i i range len inputs Generate wrap standard ` nn Module ` instance test_classname = f Test mod __class__ __name__ test_mod_code = f test_classname torch nn Module __init__ mod super __init__ mod = mod forward params mod params gbls = torch torch exec test_mod_code gbls test_instance = gbls test_classname mod traced = symbolic_trace test_instance Use ` Node normalized_arguments ` get new set arguments feed Module Then rewrite node only take those arguments kwargs modules = dict traced named_modules node traced graph nodes node op == call_module submod_class = modules node target __class__ nn_class = getattr torch nn submod_class __name__ submod_class == nn_class normalized_args = node normalized_arguments traced normalized_args = normalize_module traced node target node args node kwargs assert normalized_args == normalized_args assert normalized_args node args = normalized_args args node kwargs = normalized_args kwargs traced recompile These Modules have RNG their forward so testing correctness comparing outputs correct Skip check these stochastic_modules = FractionalMaxPool d FractionalMaxPool d RReLU mod __class__ __name__ stochastic_modules assertEqual traced inputs mod inputs traced = NormalizeArgs symbolic_trace test_instance transform modules = dict traced named_modules node traced graph nodes node op == call_module submod_class = modules node target __class__ nn_class = getattr torch nn submod_class __name__ submod_class == nn_class assertEqual len node args test_normalize_args_preserve_meta MyModule torch nn Module forward torch add m = MyModule traced = symbolic_trace m node traced graph nodes node op == call_function node target == torch add node meta my_key = break fail Didn t find call_function torch add input = torch randn ShapeProp traced propagate input traced = NormalizeArgs traced transform node traced graph nodes node op == call_function node target == torch add assertTrue my_key node meta assertEqual node meta my_key break fail Didn t find call_function torch add test_normalize_args_perserve_type MyModule torch nn Module forward list torch Tensor torch add m = MyModule traced = symbolic_trace m traced = NormalizeArgs traced transform node traced graph nodes node op == placeholder assertEqual node type list torch Tensor skipIfNoTorchVision test_annotate_returns_with_schema m = resnet traced_modules = symbolic_trace m traced_modules_annotated = AnnotateTypesWithSchema traced_modules transform node traced_modules_annotated graph nodes node type None check = node op node target assertIn check placeholder x call_module maxpool call_function operator add call_function torch flatten output output Smoke test torchscript compilation since now we re emitting type annotations torch jit script traced_modules_annotated FunctionalTracer torch fx Tracer is_leaf_module m torch nn Module module_qualified_name str - bool ` leaves ` contains set standard ` nn Modules ` currently symbolically traceable Ideally set would empty leaves = torch nn BatchNorm d type m leaves traced_functionals = torch fx GraphModule m FunctionalTracer trace m traced_functionals_annotated = AnnotateTypesWithSchema traced_functionals transform node traced_functionals_annotated graph nodes node type None check = node op node target excluded_nodes = placeholder x Return type differs based boolean dispatch call_function torch nn functional max_pool d output output AnnotateTypesWithSchema doesn t work bound C++ functions isinstance node target BuiltinFunctionType assertIn check excluded_nodes Smoke test torchscript compilation since now we re emitting type annotations torch jit script traced_functionals_annotated test_annotate_getitem_node CustomType pass CustomNamedTuple NamedTuple x int y float MyModule torch nn Module forward inp tuple CustomType torch Tensor inp list CustomType inp CustomNamedTuple inp_ = inp inp_ = inp inp _ = inp inp _x = inp x inp _y = inp y inp_ + inp_ + inp _ + inp _x + inp _y MyModule torch nn Module forward inp tuple CustomType torch Tensor inp list CustomType inp CustomNamedTuple inp_ = inp inp_ = inp inp _ = inp inp _x = inp x inp _y = inp y inp_ + inp_ + inp _ + inp _x + inp _y my_module = MyModule my_module_traced = torch fx symbolic_trace my_module default fx transform loses type annotation getitem nodes node my_module_traced graph nodes node target == operator getitem assert node type None annotate_getitem_nodes my_module_traced graph node my_module_traced graph nodes node target == operator getitem assertIsNotNone node type f Node node should annotated my_module = MyModule my_module_traced = torch fx symbolic_trace my_module default fx transform loses type annotation getitem nodes node my_module_traced graph nodes node target == operator getitem assert node type None annotate_getitem_nodes my_module_traced graph node my_module_traced graph nodes node target == operator getitem assertIsNotNone node type f Node node should annotated test_subgraph_uniquename MyModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward b c d add_ = + b add_ = add_ + c linear_ = linear add_ add_ = add_ + d add_ = add_ + linear_ add_ = add_ + add_ add_ b c d = torch ones torch ones torch ones torch ones mm = MyModule traced = symbolic_trace mm split_cb node torch fx Node node name == node name == b node name == add module_with_submodule = split_module traced mm split_cb assertEqual module_with_submodule b c d traced b c d test_split_qualname_mapping d_hid = ExampleCode torch nn Module __init__ - None super __init__ mm_param = torch nn Parameter torch randn d_hid d_hid mm_param = torch nn Parameter torch randn d_hid d_hid lin = torch nn Linear d_hid d_hid forward x x = torch mm x mm_param x = torch relu x x = torch mm x mm_param x = lin x x = torch relu x x = torch mm x mm_param x = lin x x my_module = ExampleCode my_module_traced = symbolic_trace my_module part_idx = split_callback n torch fx Node nonlocal part_idx n op n target == call_module lin part_idx += part_idx split module module submodules qualname_map dict str str = module_with_submodules = split_module my_module_traced my_module split_callback qualname_map expected_qualname_map = submod_ lin lin submod_ lin lin assertEqual qualname_map expected_qualname_map test_traceable_function_with_nonstandard_name foo x torch relu x traced = symbolic_trace_with_rewrite foo test_to_folder Test torch nn Module __init__ - None super __init__ W = torch nn Parameter torch randn seq = torch nn Sequential torch nn BatchNorm d linear = torch nn Linear attr = torch randn attr = torch nn Buffer torch randn attr = torch nn Buffer torch ones dtype=torch int forward x linear seq W + attr + attr + attr + x mod = symbolic_trace Test module_name = Foo tempfile pathlib Path tempfile TemporaryDirectory tmp_dir tmp_dir = Path tmp_dir mod to_folder tmp_dir module_name Recipe taken here https docs python org library importlib html#importing-a-source-file-directly importlib util spec = importlib util spec_from_file_location module_name tmp_dir __init__ py module = importlib util module_from_spec spec sys modules module_name = module spec loader exec_module module t = torch randn assertEqual module Foo t mod t test_fetch attrs_for_lowering dict str list str = torch nn modules conv Conv d weight bias kernel_size stride padding dilation groups padding_mode torch nn modules batchnorm BatchNorm d weight bias running_mean running_var eps TestModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d forward = conv += bn mod = TestModule traced = symbolic_trace mod lift_lowering_attrs_to_nodes traced node traced graph nodes node op == call_module assert hasattr node attrs_for_lowering para_list = attrs_for_lowering node attrs_for_lowering name node attrs_for_lowering has addition field name assert len para_list + == len node attrs_for_lowering p_name para_list assert p_name node attrs_for_lowering test_merge_matmuls A collection test cases torch fx experimental merge_matmul graph transformation merges matrix multiplication operations Utility function counting matmuls test assertions _count_matmuls mod gm = torch fx symbolic_trace mod num_matmuls = node gm graph nodes node target == torch matmul num_matmuls += num_matmuls Simple test case which there two matmuls same size merge SimpleMergeMatmulModule torch nn Module __init__ rhs super __init__ rhs = rhs forward x y = torch matmul x rhs b = torch matmul y rhs + b Initialize inputs = torch randn b = torch randn Initialize RHS matmuls rhs = torch randn Construct SimpleMergeMatmulModule call merge_matmul module = SimpleMergeMatmulModule rhs opt_module = merge_matmul merge_matmul module Numerical correctness check before = module b after = opt_module b before allclose after Basic graph structure check original module should have matmuls optimized module should have assertEqual _count_matmuls module assertEqual _count_matmuls opt_module Test case which there multiple matmuls different sizes merge FiveMergeMatmulModule torch nn Module __init__ rhs super __init__ rhs = rhs forward b c d e s = torch tensor matmuls = For some reason using list comprehension for-loop doesn t work matmuls append torch matmul rhs matmuls append torch matmul b rhs matmuls append torch matmul c rhs matmuls append torch matmul d rhs matmuls append torch matmul e rhs m matmuls s += torch sum m s Initialize inputs inputs = torch randn i + i range Initialize RHS rhs = torch randn Construct FiveMergeMatmulModule call merge_matmul module = FiveMergeMatmulModule rhs opt_module = merge_matmul merge_matmul module Numerical correctness check before = module inputs after = opt_module inputs before allclose after Basic graph structure check original module should have len inputs matmuls optimized module should have assertEqual _count_matmuls module len inputs assertEqual _count_matmuls opt_module Simple test case which two matmuls cannot merged due data dependency between LHS operands UnmergeableMatmulModule torch nn Module __init__ rhs super __init__ rhs = rhs forward x = torch matmul x rhs a_abs = torch abs b = torch matmul a_abs transpose rhs b Initialize inputs = torch randn Initialize RHS matmuls rhs = torch randn Construct UnmergeableMatmulModule call merge_matmul module = UnmergeableMatmulModule rhs opt_module = merge_matmul merge_matmul module Numerical correctness check before = module after = opt_module before allclose after Basic graph structure check number matrix multiplcations should have changed assertEqual _count_matmuls module assertEqual _count_matmuls opt_module test_type_matches should_be_equal = int int numbers Number int numbers Number float int type torch float Union int float int Union int float float list int int list int create_type_hint int int list int create_type_hint int int list torch Tensor create_type_hint torch Tensor torch Tensor list torch Tensor create_type_hint torch nn Parameter torch nn Parameter torch Tensor torch nn Parameter list torch Tensor create_type_hint torch nn Parameter torch Tensor list torch Tensor create_type_hint torch Tensor torch nn Parameter list torch Tensor create_type_hint torch Tensor torch Tensor list torch Tensor create_type_hint torch nn Parameter torch nn Parameter torch Tensor torch nn Parameter list torch Tensor create_type_hint torch nn Parameter torch Tensor list torch Tensor create_type_hint torch Tensor torch nn Parameter Optional list torch Tensor list torch Tensor Optional list int list int + pre-PEP signatures typing List int int noqa UP typing List int create_type_hint int int noqa UP typing List int create_type_hint int int noqa UP typing List torch Tensor create_type_hint torch Tensor torch Tensor noqa UP typing List torch Tensor noqa UP create_type_hint torch nn Parameter torch nn Parameter typing List torch Tensor create_type_hint torch nn Parameter torch Tensor noqa UP typing List torch Tensor create_type_hint torch Tensor torch nn Parameter noqa UP typing List torch Tensor create_type_hint torch Tensor torch Tensor noqa UP typing List torch Tensor noqa UP create_type_hint torch nn Parameter torch nn Parameter typing List torch Tensor create_type_hint torch nn Parameter torch Tensor noqa UP typing List torch Tensor create_type_hint torch Tensor torch nn Parameter noqa UP Optional typing List torch Tensor typing List torch Tensor noqa UP Optional typing List int typing List int noqa UP sig_type arg_type should_be_equal assertTrue type_matches sig_type arg_type should_fail = int float Union int float str list torch Tensor typing List int noqa UP + pre-PEP signatures list torch Tensor list int sig_type arg_type should_fail assertFalse type_matches sig_type arg_type skipIfNoMkldnn test_optimize_for_inference_cpu torch nn nn Foo nn Module __init__ - None super __init__ layers = layers = _ range layers append nn Conv d layers append nn BatchNorm d layers append nn ReLU layers append nn Conv d layers append nn BatchNorm d layers append nn ReLU model = nn Sequential layers model = nn Sequential layers forward x model x + model x N C H W = inp = torch randn N C H W torch no_grad model = Foo eval optimized_model = optimization optimize_for_inference model torch testing assert_close model inp optimized_model inp optimized_model = optimization optimize_for_inference model pass_config= remove_dropout False torch testing assert_close model inp optimized_model inp skipIfNoTorchVision skipIfNoMkldnn test_optimize_for_inference_cpu_torchvision models = torchvision models resnet torchvision models resnet torchvision models densenet torchvision models shufflenet_v _x _ torchvision models vgg torchvision models mobilenet_v torchvision models mnasnet _ torchvision models resnext _ x d torch no_grad model_type models model = model_type C H W = inp = torch randn C H W model inp model eval inp = torch randn C H W heuristic = optimization gen_mkl_autotuner inp iters= warmup= optimized_model = optimization optimize_for_inference model orig_out = model inp new_out = optimized_model inp torch testing assert_close orig_out new_out TestNormalizeOperators JitTestCase onlyCPU ops op_db allowed_dtypes= torch float test_normalize_operator_exhaustive device dtype op These ops currently don t trace FX various reasons i e they take list tensors fx_fail = cat stack hstack vstack dstack linalg multi_dot _upsample_bilinear d_aa _chunk_cat sample_inputs_itr = op sample_inputs device dtype requires_grad=False isinstance op op torch _ops OpOverload skipTest normalize operator doesn t work torch ops sample_input sample_inputs_itr unsupported_arg_type = False arg_values = sample_input input + list sample_input args kwarg_values = sample_input kwargs arg_types = kwarg_types = jit_infer_type v inferred_arg_type = torch _C _jit_try_infer_type v assert inferred_arg_type success t = _torchscript_type_to_python_type inferred_arg_type type t v arg_values isinstance v torch Tensor arg_types append type v isinstance v complex Complex type supported FX unsupported_arg_type = True arg_types append jit_infer_type v k v kwarg_values items isinstance v torch Tensor kwarg_types k = type v isinstance v complex Complex type supported FX unsupported_arg_type = True kwarg_types k = jit_infer_type v unsupported_arg_type continue Test normalize_function itself ref_out = op op arg_values kwarg_values norm_args_and_kwargs = normalize_function op op arg_values kwarg_values arg_types kwarg_types norm_args_and_kwargs None raise RuntimeError FX failed normalize op - add op op_skip list A common reason your OpInfo implemented lambda - otherwise file issue test_out = op op norm_args_and_kwargs args norm_args_and_kwargs kwargs assertEqual test_out ref_out Test normalized_arguments part FX op name fx_fail continue param_names = param_values = fx_args = idx = process_arg arg name isinstance arg torch Tensor param_names append name param_values append arg name f repr arg process_arg_with_idx arg nonlocal idx res = process_arg arg f arg_ idx idx = idx + res str_arg arg isinstance arg tuple args = f str_arg v v arg f join args isinstance arg list args = f str_arg v v arg f join args arg v arg_values arg = pytree tree_map process_arg_with_idx v fx_args append str_arg arg k v kwarg_values items arg = pytree tree_map functools partial process_arg name=k v fx_args append f k = str_arg arg code = f TestModule torch nn Module forward join param_names torch op name join fx_args g = torch torch inf math inf exec code g TestModule = g TestModule m = TestModule traced = torch fx symbolic_trace m ref_out = traced param_values node traced graph nodes node op == call_function normalized_args = node normalized_arguments traced arg_types kwarg_types assert normalized_args node args = normalized_args args node kwargs = normalized_args kwargs traced recompile test_out = traced param_values assertEqual test_out ref_out test_normalize_quantized_eb target = torch ops quantized embedding_bag_byte_rowwise_offsets args = torch empty dtype=torch uint torch empty dtype=torch int torch empty dtype=torch int norm_args_and_kwargs = normalize_function target args normalize_to_only_use_kwargs=True assertTrue norm_args_and_kwargs None assertEqual set norm_args_and_kwargs kwargs keys weight indices offsets scale_grad_by_freq mode pruned_weights per_sample_weights compressed_indices_mapping include_last_offset assertEqual norm_args_and_kwargs args test_normalize_args_op_overload target torch ops aten resize_as_ default torch ops aten resize_as_ inp = torch rand inp = torch rand args kwargs = normalize_function target inp the_template inp normalize_to_only_use_kwargs=True assertIs kwargs input inp assertIs kwargs the_template inp TEST_Z z torch _dynamo config torch fx experimental validator SympyToZ TranslationValidator ValidationException z str torch utils _sympy functions FloorDiv Mod BitwiseFn_bitwise_and TestTranslationValidation TestCase _prepare_for_translation_validation validator = TranslationValidator SymPy symbols s s s = sympy symbols s s s integer=True Z symbols validator add_var s int s s s s z z z = validator z var s s s s s s s s z z z validator test_sympy_to_z s s s z z z validator = _prepare_for_translation_validation test_cases = Integer constants sympy S Zero z IntVal sympy S One z IntVal sympy S NegativeOne z IntVal - sympy Integer z IntVal s z Arithmetic operations op s s op z z op operator add operator mul operator pow Logical operations sympy_op s s z _op z z sympy_op z _op sympy Eq operator eq sympy Ne operator ne sympy Lt operator lt sympy Le operator le sympy Gt operator gt sympy Ge operator ge Bitwise operations BitwiseFn_bitwise_and s s z BV Int z Int BV z z Int BV z Other operations s - s z + z IntVal - z s s z ToReal z z - FloorDiv s s z ToInt z ToReal z z ToReal z Mod s s z - z ToInt z ToReal z z ToReal z z Mod s s s z - z ToReal z ToInt z ToReal z z ToReal z z - z ToReal z z - Mod s s z - z ToReal z ToInt z ToReal z z z toZ = SympyToZ validator sympy_expr z _expr test_cases result = toZ run sympy_expr assertTrue z _expr eq result msg=f expected z _expr Got result test_sat s s s z z z validator = _prepare_for_translation_validation validator add_source_expr z validator add_source_expr z z Solutions target subset solutions source validator add_target_expr s validator add_target_expr s s validator validate test_sat_bitwise s s s z z z validator = _prepare_for_translation_validation validator add_source_expr z BV Int z Int BV z z Int BV z == validator add_source_expr z == b validator validate test_unsat s s s z z z validator = _prepare_for_translation_validation validator add_source_expr z validator add_source_expr z z Solutions target NOT subset solutions source validator add_target_expr s This expression less restrictive than its counterpart validator add_target_expr s s + assertRaisesRegex ValidationException translation validation failed validator validate test_z str = z Int b = z Int b special = z Real size test_cases = z IntVal Variable Name special characters special size Renamed function fpplications = b = b b pow b Chain associative operations op op b f opstr b op opstr operator add + operator mul Revert Not conversions = b = b b b b b Ignore ToInt ToReal functions z ToInt special + + size z ToReal + b + b Convert floor division idiv z ToInt z ToReal z ToReal b idiv b expr expected test_cases assertEqual z str expr expected instantiate_device_type_tests TestNormalizeOperators globals __name__ == __main__ run_tests