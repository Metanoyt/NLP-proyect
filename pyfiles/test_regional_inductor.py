Owner s module dynamo functools typing TYPE_CHECKING torch torch _inductor test_case torch fx traceback fx_traceback torch utils checkpoint torch _dynamo backends common aot_autograd torch _functorch _aot_autograd autograd_cache BundledCompiledForward torch _guards detect_fake_mode torch _inductor output_code RegionalOutputCode torch _inductor test_case run_tests torch _inductor utils run_fw_bw_and_get_code torch fx _graph_pickler GraphPickler torch fx passes regional_inductor regional_inductor torch nn attention flex_attention create_block_mask flex_attention torch testing _internal common_utils instantiate_parametrized_tests parametrize skipIfTorchDynamo torch testing _internal triton_utils requires_cuda_and_triton TYPE_CHECKING torch _inductor compile_fx _CompileFxKwargs Open questions follow-ups CSE behavior meta custom nodes Common subexpression elimination may differentiate between distinct meta custom nodes could remove expressions which might confuse users SAC recompute vs forward size If recomputed forward smaller than original forward do we end up compiling only smaller region fx_traceback annotate nesting How does nesting behave Are there any ordering requirements Planned uses annotations compile flex b streams c nn Module info organize MoE runtime d pipeline-parallel stages e rename graph nodes easier debugging f disallow nested regional compile aot_eager_regional_inductor serialize=False serialize regional_inductor_pickle gm example_args result = regional_inductor gm example_args serialized = GraphPickler dumps result fake_mode = detect_fake_mode example_args assert fake_mode None Serialize deserialize result confirm pickling works Use fresh tracing context new process context = torch _guards TracingContext fake_mode torch _guards tracing context result = GraphPickler loads serialized fake_mode assert isinstance result torch fx GraphModule result recompile result aot_autograd fw_compiler=regional_inductor_pickle bw_compiler=regional_inductor_pickle aot_autograd fw_compiler=regional_inductor bw_compiler=regional_inductor skipIfTorchDynamo Not suitable dynamo wrapped test instantiate_parametrized_tests RegionalInductorTests torch _inductor test_case TestCase parametrize serialize False True test_simple serialize fn x y sin = torch sin x fx_traceback annotate compile_with_inductor mul = sin y add = mul + torch sin add opt_fn = torch compile fn backend=aot_eager_regional_inductor serialize=serialize fullgraph=True x = torch randn requires_grad=True y = torch randn requires_grad=True Check inductor compilation called twice _ codes = run_fw_bw_and_get_code lambda opt_fn x y assertEqual len codes parametrize serialize False True test_repeated_blocks serialize fn x y sin = torch sin x fx_traceback annotate compile_with_inductor mul = sin y add = mul + torch sin add Mod torch nn Module __init__ super __init__ forward x y = fn x y fn y mod = Mod opt_mod = torch compile mod backend=aot_eager_regional_inductor serialize=serialize fullgraph=True x = torch randn requires_grad=True y = torch randn requires_grad=True Check inductor compilation called times there will partitions fwd bwd totalling _ codes = run_fw_bw_and_get_code lambda opt_mod x y assertEqual len codes parametrize serialize False True test_invoke_subgraph serialize Checks get_attr nodes custom metadata propagated torch compiler nested_compile_region gn x torch sin x fn x x = x + fx_traceback annotate compile_with_inductor z = gn x torch sigmoid z opt_fn = torch compile fn backend=aot_eager_regional_inductor serialize=serialize fullgraph=True x = torch randn requires_grad=True _ codes = run_fw_bw_and_get_code lambda opt_fn x assertEqual len codes parametrize serialize False True test_invoke_subgraph_inner serialize Checks inductor regions searched recursively torch compiler nested_compile_region gn x fx_traceback annotate compile_with_inductor torch sin x fn x x = x + x = gn x x = x + x = gn x torch sigmoid x opt_fn = torch compile fn backend=aot_eager_regional_inductor serialize=serialize fullgraph=True x = torch randn requires_grad=True _ codes = run_fw_bw_and_get_code lambda opt_fn x invoke_subgraph called twice - inside code compiled once - so total fwd + bwd assertEqual len codes requires_cuda_and_triton parametrize serialize False True test_flex_attention serialize _squared score b h m n score score mask_mod b h q k q = = b = block_mask = create_block_mask mask_mod None None b b fn x x = torch sin x fx_traceback annotate compile_with_inductor x = flex_attention x x x block_mask=block_mask score_mod=_squared torch cos x x = torch randn b b dtype=torch bfloat device= cuda requires_grad=True opt_fn = torch compile fn backend=aot_eager_regional_inductor serialize fullgraph=True _ codes = run_fw_bw_and_get_code lambda opt_fn x flex forward flex_backward backward assertEqual len codes parametrize serialize False True test_max_autotune_no_cudagraphs serialize Test max-autotune-no-cudagraphs options properly applied via annotations torch _inductor config inductor_config fn x y sin = torch sin x Use annotation API specify inductor configs fx_traceback annotate compile_with_inductor inductor_configs max_autotune True triton cudagraphs False mul = sin y add = mul + torch sin add Hook verify options original_compile = torch _inductor standalone_compile captured_options = verify_options args kwargs options = kwargs get options captured_options append options Verify config set expected explicit options assert inductor_config max_autotune max_autotune should True assert inductor_config triton cudagraphs triton cudagraphs should False original_compile args kwargs torch _inductor standalone_compile = verify_options try Use backend without options - they come annotations backend = aot_eager_regional_inductor serialize=serialize opt_fn = torch compile fn backend=backend fullgraph=True x = torch randn requires_grad=True y = torch randn requires_grad=True Run check options passed _ codes = run_fw_bw_and_get_code lambda opt_fn x y assertEqual len codes Verify compilation happened assertTrue len captured_options Compilation should have occurred finally torch _inductor standalone_compile = original_compile test_annotation_inductor_configs Test inductor_configs can passed through annotation API torch _inductor config inductor_config fn_with_annotation_configs x y New annotation format inductor_configs fx_traceback annotate compile_with_inductor inductor_configs max_autotune True triton cudagraphs False torch matmul x y + Capture config during compilation config_snapshots = original_compile = torch _inductor standalone_compile capture_config args kwargs config_snapshots append max_autotune inductor_config max_autotune triton cudagraphs inductor_config triton cudagraphs original_compile args kwargs torch _inductor standalone_compile = capture_config try backend = aot_eager_regional_inductor opt_fn = torch compile fn_with_annotation_configs backend=backend fullgraph=True x = torch randn requires_grad=True y = torch randn requires_grad=True Run forward backward result = opt_fn x y result sum backward assertTrue len config_snapshots No compilation occurred snapshot config_snapshots assertEqual snapshot max_autotune True assertEqual snapshot triton cudagraphs False finally torch _inductor standalone_compile = original_compile test_invalid_inductor_config Test invalid inductor config keys caught clear error fn x y fx_traceback annotate compile_with_inductor inductor_configs invalid_config_key True x y + backend = aot_eager_regional_inductor opt_fn = torch compile fn backend=backend fullgraph=True x = torch randn requires_grad=True y = torch randn requires_grad=True assertRaisesRegex torch _dynamo exc BackendCompilerFailed Invalid inductor config key invalid_config_key opt_fn x y requires_cuda_and_triton parametrize serialize False True test_selective_ac_flex serialize FlexAttentionModule torch nn Module __init__ hidden_size num_heads super __init__ hidden_size = hidden_size num_heads = num_heads head_dim = hidden_size num_heads In-projections query key value q_proj = torch nn Linear hidden_size hidden_size k_proj = torch nn Linear hidden_size hidden_size v_proj = torch nn Linear hidden_size hidden_size Out-projection out_proj = torch nn Linear hidden_size hidden_size forward x batch_size seq_len _ = x size Project queries keys values q = q_proj x view batch_size seq_len num_heads head_dim transpose k = k_proj x view batch_size seq_len num_heads head_dim transpose v = v_proj x view batch_size seq_len num_heads head_dim transpose Apply flex attention torch fx traceback annotate compile_with_inductor attn_output = flex_attention q k v Reshape output attn_output = attn_output transpose contiguous view batch_size seq_len hidden_size Out projection output = out_proj attn_output output torch utils checkpoint checkpoint create_selective_checkpoint_contexts ops_to_save = torch ops aten mm default context_fn = functools partial create_selective_checkpoint_contexts ops_to_save Define model uses FlexAttention selective activation checkpointing SacModule torch nn Module __init__ hidden_size num_heads context_fn super __init__ flex_attn = FlexAttentionModule hidden_size num_heads context_fn = context_fn forward x flex_attn_fn x flex_attn x output = checkpoint flex_attn_fn x use_reentrant=False context_fn=self context_fn output flex_module = SacModule hidden_size= num_heads= context_fn=context_fn cuda dtype=torch bfloat x = torch ones device= cuda dtype=torch bfloat compiled_module = torch compile flex_module backend=aot_eager_regional_inductor fullgraph=True _ codes = run_fw_bw_and_get_code lambda compiled_module x flex forward flex_backward backward assertEqual len codes skipIfTorchDynamo Not suitable dynamo wrapped test TestRegionalOutputCode torch _inductor test_case TestCase Tests RegionalOutputCode BundledAOTAutogradResult test_regional_output_code_serialization Test RegionalOutputCode can serialized deserialized fn x y sin = torch sin x fx_traceback annotate compile_with_inductor mul = sin y add = mul + torch sin add x = torch randn requires_grad=True y = torch randn requires_grad=True Compile regional inductor torch fx traceback preserve_node_meta enable=False torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor make_fx fake_mode = FakeTensorMode fake_mode fake_x = fake_mode from_tensor x fake_y = fake_mode from_tensor y gm = make_fx fn fake_x fake_y Run regional_inductor graph result_gm = regional_inductor gm fake_x fake_y Create RegionalOutputCode output_code = RegionalOutputCode result_gm Test we can call assertIsNotNone output_code _graph_module Serialize output_code prepare_for_serialization assertIsNone output_code _graph_module assertIsNotNone output_code _serialized_graph_module Deserialize via post_compile torch _inductor output_code CompiledFxGraphConstants fx_config _CompileFxKwargs = is_backward False output_code post_compile fake_x fake_y CompiledFxGraphConstants fx_config assertIsNotNone output_code _graph_module assertIsInstance output_code _graph_module torch fx GraphModule Test deserialized graph works fake_mode result = output_code fake_x fake_y assertIsNotNone result test_regional_output_code_with_backward Test RegionalOutputCode both forward backward compilation fn x y sin = torch sin x fx_traceback annotate compile_with_inductor mul = sin y add = mul + torch sin add x = torch randn requires_grad=True y = torch randn requires_grad=True Compile regional inductor backend torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor make_fx fake_mode = FakeTensorMode fake_mode fake_x = fake_mode from_tensor x fake_y = fake_mode from_tensor y Create forward graph torch fx traceback preserve_node_meta enable=False gm = make_fx fn fake_x fake_y forward_gm = regional_inductor gm fake_x fake_y Create forward output code fw_code = RegionalOutputCode forward_gm Verify can called fake_mode result = fw_code fake_x fake_y assertIsNotNone result Test serialization round-trip fw_code prepare_for_serialization Deserialize via post_compile torch _inductor output_code CompiledFxGraphConstants fx_config _CompileFxKwargs = is_backward False fw_code post_compile fake_x fake_y CompiledFxGraphConstants fx_config fake_mode result = fw_code fake_x fake_y assertIsNotNone result test_regional_compiled_forward_backward Test BundledCompiledForward BundledCompiledBackward RegionalOutputCode fn x fx_traceback annotate compile_with_inductor torch sin x x = torch randn requires_grad=True torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor make_fx fake_mode = FakeTensorMode fake_mode fake_x = fake_mode from_tensor x torch fx traceback preserve_node_meta enable=False gm = make_fx fn fake_x compiled_gm = regional_inductor gm fake_x Create forward using generic BundledCompiledForward fw_code = RegionalOutputCode compiled_gm fw_compiled = BundledCompiledForward RegionalOutputCode result=fw_code Test pre_save fw_compiled pre_save After pre_save fw_compiled result copy serialized graph assertIsNotNone fw_compiled result _serialized_graph_module assertIsNone fw_compiled result _graph_module Should cleared after serialization Test load doesn t deserialize yet loaded_code = fw_compiled load fake_x assertIsNone loaded_code _graph_module Not yet deserialized assertIsNotNone loaded_code _serialized_graph_module fx_config _CompileFxKwargs = is_backward False post_compiled = fw_compiled post_compile loaded_code fx_config assertIsNotNone post_compiled assertIsNotNone post_compiled _graph_module Now deserialized __name__ == __main__ run_tests