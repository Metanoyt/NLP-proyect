mypy allow-untyped-defs contextlib abc ABC abstractmethod functools cached_property typing Any torch torch utils _pytree pytree torch _C _functorch CFunctionalizeInterpreterPtr CGradInterpreterPtr CInterpreter CJvpInterpreterPtr CVmapInterpreterPtr pop_dynamic_layer_stack push_dynamic_layer_stack RandomnessType TransformType torch autograd forward_ad _set_fwd_grad_enabled This file contains functorch integration PyDispatcher PyDispatcher does understand functorch s DynamicLayerStack dispatching logic because entirely implemented C++ fallbacks two dispatch keys FuncTorchDynamicLayer Front Back Mode PyDispatcher unable directly reuse C++ boxed fallbacks Instead trying hammer PyDispatcher into understanding those fallbacks we re-implement logic peeking top stack interpreter selecting interpreter dispatch etc Python This leads simpler design The main difference between C++ functorch PyDispatcher s functorch logic - C++ functorch needs manually tweak dispatch keys ping-pong between DynamicLayerFrontMode DynamicLayerBackMode - PyDispatcher s functorch logic pops Interpreter top stack asks execute rule associated Interpreter In C++ we do ping-pong because e g vmap rules associated batched DispatchKey PyDispatcher we able avoid asking user register batching rule directly transform interpreter then invokes FuncTorchInterpreter Python version Interpreter recall DynamicLayerStack stack interpreters It wrapper around actual C++ Interpreter object Keep methods sync aten src ATen functorch Interpreter h FuncTorchInterpreter ABC __init__ cptr Any _cptr = cptr Process operation eg vmap invoking batching rule Conceptually analogous Interpreter process C++ abstractmethod process op args kwargs pass lower operation Interpreter next Interpreter stack Concretely involves temporarily popping current Interpreter Conceptually analogous Interpreter sendToNextInterpreter C++ lower temporarily_pop_interpreter_stack level _cptr level key _cptr key get_state raise NotImplementedError check_state state state == get_state __getstate__ state = __dict__ copy state pop _cptr None state contextlib contextmanager temporarily_pop_interpreter_stack try saved = pop_dynamic_layer_stack yield finally push_dynamic_layer_stack saved contextlib contextmanager temporarily_clear_interpreter_stack stack = try while torch _C _functorch peek_interpreter_stack None stack append pop_dynamic_layer_stack yield list stack finally while stack push_dynamic_layer_stack stack pop contextlib contextmanager temporarily_restore_interpreter_stack stack pushed = try s reversed stack push_dynamic_layer_stack s pushed append s yield finally _ reversed pushed TODO would nice assert layers same Python object identity preserved pop_dynamic_layer_stack VmapInterpreter FuncTorchInterpreter __init__ cdata CInterpreter assert cdata key == TransformType Vmap NOTE Interpreter cdata vs cptr cdata generic CInterpreter We wrap CVmapInterpreterPtr so we can access methods specific vmap interpreter _cdata = cdata cached_property pyrefly ignore bad-override _cptr CVmapInterpreterPtr _cdata process op args kwargs kernel = op functorch_table TransformType Vmap kernel args kwargs batch_size _cptr batchSize randomness typ = _cptr randomness typ == RandomnessType Error error typ == RandomnessType Same same typ == RandomnessType Different different raise RuntimeError f Unknown RandomnessType typ get_state key name level randomness contextlib contextmanager nested contexts contextlib ExitStack stack ctx contexts stack enter_context ctx yield contexts GradInterpreter FuncTorchInterpreter __init__ cdata CInterpreter assert cdata key == TransformType Grad See NOTE Interpreter cdata vs cptr _cdata = cdata cached_property pyrefly ignore bad-override _cptr CGradInterpreterPtr _cdata lift args kwargs args kwargs = pytree tree_map_only torch Tensor _cptr lift args kwargs args kwargs process op args kwargs kernel = op functorch_table TransformType Grad args kwargs = lift args kwargs kernel args kwargs GradInterpreter has custom lower because no_grad interaction See NOTE grad vjp interaction no_grad This logic mirrored C++ GradInterpreterPtr sendToNextInterpreter lower prev_grad_mode = prev_grad_mode prev_grad_mode nested torch no_grad super lower super lower prev_grad_mode _cptr prevGradMode get_state key name level prev_grad_mode JvpInterpreter FuncTorchInterpreter __init__ cdata CInterpreter assert cdata key == TransformType Jvp See NOTE Interpreter cdata vs cptr _cdata = cdata cached_property pyrefly ignore bad-override _cptr CJvpInterpreterPtr _cdata lift args kwargs args kwargs = pytree tree_map_only torch Tensor _cptr lift args kwargs args kwargs process op args kwargs kernel = op functorch_table TransformType Jvp args kwargs = lift args kwargs kernel args kwargs Jvp has custom lower because no_fwd_grad interaction See NOTE grad vjp interaction no_grad related info This logic mirrored C++ JvpInterpreterPtr sendToNextInterpreter lower prev_fwd_grad_mode = prev_fwd_grad_mode prev_fwd_grad_mode nested _set_fwd_grad_enabled False super lower super lower prev_fwd_grad_mode _cptr prevFwdGradMode get_state key name level prev_fwd_grad_mode FunctionalizeInterpreter FuncTorchInterpreter __init__ cdata CInterpreter assert cdata key == TransformType Functionalize _cdata = cdata cached_property pyrefly ignore bad-override _cptr CFunctionalizeInterpreterPtr _cdata process op args kwargs kernel = op functorch_table TransformType Functionalize kernel args kwargs functionalize_add_back_views _cptr functionalizeAddBackViews get_state key name level coerce_cinterpreter cinterpreter CInterpreter - FuncTorchInterpreter key = cinterpreter key key == TransformType Grad GradInterpreter cinterpreter key == TransformType Vmap VmapInterpreter cinterpreter key == TransformType Jvp JvpInterpreter cinterpreter key == TransformType Functionalize FunctionalizeInterpreter cinterpreter raise RuntimeError f NYI PyDispatcher has implemented support key retrieve_current_functorch_interpreter - FuncTorchInterpreter interpreter = torch _C _functorch peek_interpreter_stack assert interpreter None coerce_cinterpreter interpreter retrieve_all_functorch_interpreters - list FuncTorchInterpreter cis = torch _C _functorch get_interpreter_stack cis None coerce_cinterpreter ci ci cis compare_functorch_state states list tuple Any - bool There four possible cases covered here Current stack empty AND stack when generated empty - Invalidate Current stack empty AND stack when generated empty - Invalidate Current stack generated stack empty - Valid FX graph Current stack generated stack empty - Valid both states match peek = torch _C _functorch peek_interpreter_stack peek None len states = peek None len states == False cis = retrieve_all_functorch_interpreters len cis == len states all ci check_state state ci state zip cis states dispatch_functorch op args kwargs interpreter = retrieve_current_functorch_interpreter In traditional PyTorch operators DispatchKey FuncTorchTensorWrapper s unwrap_dead_tensors fallback handles unwrapping dead tensor wrappers PyDispatcher sidesteps PyTorch dispatcher when dealing functorch transforms so we manually unwrap dead tensors here This logic won t need exist when we have mode-only functorch args kwargs = pytree tree_map_only torch Tensor torch _C _functorch unwrap_if_dead args kwargs interpreter process op args kwargs