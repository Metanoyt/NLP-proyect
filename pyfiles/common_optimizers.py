mypy ignore-errors functools itertools sys unittest copy deepcopy enum Enum typing Any Union torch torch Tensor torch nn Parameter torch optim Adadelta Adafactor Adagrad Adam Adamax AdamW ASGD LBFGS Muon NAdam Optimizer RAdam RMSprop Rprop SGD SparseAdam torch optim lr_scheduler ConstantLR ExponentialLR LinearLR PolynomialLR ReduceLROnPlateau StepLR torch testing _internal common_device_type tol toleranceOverride torch testing _internal common_methods_invocations DecorateInfo torch testing _internal common_utils _TestParametrizer skipIfMPS skipIfTorchDynamo TEST_WITH_TORCHDYNAMO torch utils _foreach_utils _get_foreach_kernels_supported_devices CUDA_CONFIG_GPUS = cuda xpu OptimizerInput Contains args kwargs passed optimizer constructor __slots__ = params kwargs desc __init__ params Union list Parameter list Tensor dict Any Any list dict str Any kwargs dict str Any desc str = params can list Tensors OR param_groups OR None params = params kwargs = kwargs desc = desc __repr__ f params= params kwargs= kwargs desc= desc OptimizerErrorEnum Enum Enumerates when error raised when testing optimizers CONSTRUCTION_ERROR = STEP_ERROR = ErrorOptimizerInput An OptimizerInput will cause optimizer throw error when constructed Includes type string resulting error __slots__ = optimizer_error_input error_on error_type error_regex __init__ optimizer_error_input error_on=OptimizerErrorEnum CONSTRUCTION_ERROR error_type=RuntimeError error_regex= optimizer_error_input = optimizer_error_input error_on = error_on error_type = error_type error_regex = error_regex OptimizerInfo Optimizer information used testing __init__ optim_cls Optimizer Class object Optimizer under test Function generate optimizer inputs EXCLUDING params We delegate params responsibility test using OptimizerInfo OptimizerInput params likely None Can optionally take device filter out certain unsupported configs optim_inputs_func Tuple lambdas generate LRScheduler instances run optimizer LRScheduler tests like test_forloop_goes_right_direction with_lrsched We DO NOT expect thoroughly test LRSchedulers through optimizers so every LRScheduler configuration will included See test_lrscheduler py instead A few optimizers like SGD Adam will test more LRSchedulers scheduler_inputs= lambda opt StepLR opt gamma= step_size= lambda opt ReduceLROnPlateau opt A subset global-cliquey flags fused foreach differentiable optimizer supports See NOTE optimizer kwarg categories what global-cliquey means supported_impls tuple str = foreach differentiable A subset all flags signifying which ones only supported after original optimizer had already been released aka impls where we need check BC not_og_supported_flags tuple str = foreach differentiable maximize capturable optim supports passing sparse gradients well dense grads supports_sparse bool = False optimizer constructor supports passing capturable kwarg has_capturable_arg bool = False optim only supports one config sparse grads w dense params see SparseAdam only_supports_sparse_grads bool = False Tuple optimizer kwargs schedulers_constructors specifically sparse tests especially tuned hyperparameters These only apply optimizer supports sparse parameters grads metadata_for_sparse= optim supports complex parameters supports_complex bool = True whether optimizer step function requires closure passed step_requires_closure bool = False whether optimizer supports per-param options parameter groups supports_param_groups bool = True whether optimizer supports parameters multiple devices supports_multiple_devices bool = True skips= Indicates which tests skip decorators=None Additional decorators apply generated tests optim_error_inputs_func=None Function generate optim inputs error supports_fused_on tuple str = optim_cls = optim_cls optim_inputs_func = optim_inputs_func scheduler_inputs = scheduler_inputs supported_impls = supported_impls not_og_supported_flags = not_og_supported_flags supports_sparse = supports_sparse has_capturable_arg = has_capturable_arg metadata_for_sparse = metadata_for_sparse only_supports_sparse_grads = only_supports_sparse_grads supports_complex = supports_complex step_requires_closure = step_requires_closure supports_param_groups = supports_param_groups supports_multiple_devices = supports_multiple_devices decorators = decorators decorators skips skips optim_error_inputs_func = optim_error_inputs_func supports_fused_on = supports_fused_on get_decorators test_class test_name device dtype param_kwargs result = decorator decorators isinstance decorator DecorateInfo decorator is_active test_class test_name device dtype param_kwargs result extend decorator decorators result append decorator result property name optim_cls __name__ optims _TestParametrizer Decorator specifying list optimizers over which run test __init__ optim_info_iterable dtypes=None optim_info_list = list optim_info_iterable optimizers aren t limited one dtype parameters can have different dtypes We default torch float dtypes should specified through passed parameters dtypes = dtypes dtypes None torch float _parametrize_test test generic_cls device_cls device_cls None raise RuntimeError The optims decorator only intended used device-specific context use instantiate_device_type_tests instead instantiate_parametrized_tests optim_info dtype itertools product optim_info_list dtypes Construct test name device dtype parts handled outside See Note device dtype suffix placement test_name = optim_info name Construct parameter kwargs pass test param_kwargs = optim_info optim_info dtype dtype try functools wraps test test_wrapper args kwargs test args kwargs decorator_fn = functools partial optim_info get_decorators generic_cls __name__ test __name__ device_cls device_type dtype yield test_wrapper test_name param_kwargs decorator_fn except Exception ex Provides error message debugging before rethrowing exception print f Failed instantiate test_name module optim_info name raise ex Helper function generating error inputs all optimizers used below get_error_inputs_for_all_optims device dtype _get_device_type device == cpu Creating D parameters compatibility Muon sample_param = Parameter torch randn device=device dtype=dtype sample_param = Parameter torch randn device=device dtype=dtype ErrorOptimizerInput OptimizerInput params=sample_param kwargs= desc= invalid param type error_type=TypeError error_regex= params argument given optimizer should iterable Tensors dicts ErrorOptimizerInput OptimizerInput params= sample_param sample_param kwargs= desc= param group cannot have duplicate parameters error_type=UserWarning error_regex= parameter group duplicate parameters ErrorOptimizerInput OptimizerInput params= params sample_param params sample_param kwargs= desc= duplicate parameters should occur across param groups either error_type=ValueError error_regex= some parameters appear more than one parameter group ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr=torch tensor desc= Tensor lr must -element error_type=ValueError error_regex= Tensor lr must -element ErrorOptimizerInput OptimizerInput params= weight sample_param sample_param kwargs= desc= all optimizer params should without names error_type=ValueError error_regex= all optimizer params should without names Some param names missing ErrorOptimizerInput OptimizerInput params= params sample_param lr e- params weight sample_param kwargs= desc= all optimizer param groups should without names error_type=ValueError error_regex= all optimizer param groups should without names cannot add param group names optimizer ------------------------------------------------------------------------------------------ NOTE optimizer kwarg categories We categorize optimizer kwargs types optimizer-specific flags like amsgrad rho beta flags specific algorithms thus only show up certain optimizers There many these so I do bother gathering them all listing them here The converse these would global flags every optimizer ideally _should_ support We break global flags into further categories list them all below global-friendly = lr weight_decay maximize capturable global-friendly flags global flags who play nicely all other global flags i e mutually exclusive function This means any pair following flags can toggled once e g maximize weight_decay Furthermore any following flags theoretically can enabled ANY other global flag including cliquey ones e g capturable foreach global-cliquey = foreach fused differentiable global-cliquey flags global flags do NOT coexist other cliquey flags usually because they contradict each other function For example one should flip both foreach AND fused True because they two differing performance optimizations which you can only opt into one The following optim_inputs_func_ sampling functions only constructor combinations optimizer-specific global-friendly flags This because we confident they would mesh well additional kwargs On flip side same coin we reserve setting global-cliquey flags individual tests fully expect tests edit OptimizerInput kwargs optim_inputs_func_adadelta device dtype=None cuda_supported_configs = OptimizerInput params=None kwargs= capturable True desc= capturable OptimizerInput params=None kwargs= weight_decay capturable True desc= capturable weight decay OptimizerInput params=None kwargs= lr torch tensor capturable True desc= Tensor lr capturable OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr desc= non-default lr OptimizerInput params=None kwargs= weight_decay desc= nonzero weight_decay OptimizerInput params=None kwargs= maximize True desc= maximize OptimizerInput params=None kwargs= weight_decay maximize True desc= maximize weight_decay OptimizerInput params=None kwargs= rho weight_decay desc= rho + cuda_supported_configs _get_device_type device CUDA_CONFIG_GPUS optim_error_inputs_func_adadelta device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- rho= desc= rho should between error_type=ValueError error_regex= Invalid rho value error_inputs optim_inputs_func_adafactor device dtype=None OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= weight_decay lr desc= nonzero weight_decay OptimizerInput params=None kwargs= weight_decay maximize True desc= maximize OptimizerInput params=None kwargs= beta _decay - desc= non-default beta _decay OptimizerInput params=None kwargs= d desc= non-default clipping threshold d optim_error_inputs_func_adafactor device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu complex_param = torch rand device=device dtype=torch complex complex_param grad = torch rand_like complex_param error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict eps= - e- e- desc= epsilon should = error_type=ValueError error_regex= epsilon should = ErrorOptimizerInput OptimizerInput params=None kwargs=dict d= desc= invalid d error_type=ValueError error_regex= Clipping threshold d should = ErrorOptimizerInput OptimizerInput params=None kwargs=dict beta _decay= desc= invalid beta _decay error_type=ValueError error_regex= beta _decay should = ErrorOptimizerInput OptimizerInput params= complex_param kwargs=dict desc= does support complex parameters error_type=RuntimeError error_regex= Adafactor does support complex parameters error_on=OptimizerErrorEnum STEP_ERROR error_inputs optim_inputs_func_adagrad device dtype=None OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= weight_decay desc= nonzero weight_decay OptimizerInput params=None kwargs= weight_decay maximize True desc= maximize OptimizerInput params=None kwargs= lr desc= non-default lr OptimizerInput params=None kwargs= initial_accumulator_value weight_decay desc= initial_accumulator_value OptimizerInput params=None kwargs= lr lr_decay weight_decay desc= lr_decay TODO Move out testing param_group OptimizerInput params=None kwargs= lr torch tensor desc= Tensor lr optim_error_inputs_func_adagrad device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- lr_decay=- desc= lr_decay must bigger than error_type=ValueError error_regex= Invalid lr_decay value - error_inputs TODO consider tensor LR See multi_tensor_optimizer_configs test_optim py -- tensor LR should work all implementation code paths optim_inputs_func_adam device dtype=None cuda_supported_configs = OptimizerInput params=None kwargs= capturable True desc= capturable OptimizerInput params=None kwargs= weight_decay amsgrad True capturable True desc= capturable amsgrad OptimizerInput params=None kwargs= lr torch tensor amsgrad True capturable True desc= Tensor lr capturable amsgrad OptimizerInput params=None kwargs= lr torch tensor betas torch tensor torch tensor amsgrad True capturable True desc= Tensor lr Tensor betas capturable amsgrad OptimizerInput params=None kwargs= lr torch tensor betas torch tensor torch tensor amsgrad False capturable True desc= Tensor lr Tensor betas capturable mps_supported_configs = OptimizerInput params=None kwargs= lr torch tensor desc= Tensor lr total = OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr desc= non-default lr OptimizerInput params=None kwargs= weight_decay desc= nonzero weight_decay OptimizerInput params=None kwargs= weight_decay maximize True desc= maximize OptimizerInput params=None kwargs= weight_decay amsgrad True desc= amsgrad + cuda_supported_configs _get_device_type device CUDA_CONFIG_GPUS + mps_supported_configs _get_device_type device == mps dtype == torch float input total Too small eps will make denom zero low precision dtype denom = exp_avg_sq sqrt bias_correction _sqrt add_ eps For example tensor dtype=torch float + e- tensor dtype=torch float input kwargs eps = total optim_error_inputs_func_adam device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- betas= desc= beta should between error_type=ValueError error_regex= Invalid beta parameter index ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- weight_decay=- desc= weight_decay should error_type=ValueError error_regex= Invalid weight_decay value - ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr=torch tensor foreach=True desc= lr Tensor doesn t work foreach capturable error_type=ValueError error_regex= lr Tensor supported capturable=False foreach=True ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- betas= torch tensor desc= betas must either both floats both Tensors error_type=ValueError error_regex= betas must either both floats both Tensors ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- betas= torch tensor desc= betas must either both floats both Tensors error_type=ValueError error_regex= betas must either both floats both Tensors ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- betas= torch tensor torch tensor foreach=True desc=r betas\ \ Tensor supported capturable=False foreach=True error_type=ValueError error_regex=r betas\ \ Tensor supported capturable=False foreach=True _get_device_type device CUDA_CONFIG_GPUS sample_tensor = torch empty device=device dtype=dtype error_inputs += ErrorOptimizerInput OptimizerInput params= sample_tensor kwargs= foreach True fused True desc= ` fused ` ` foreach ` cannot ` True ` together error_type=RuntimeError error_regex= ` fused ` ` foreach ` cannot ` True ` together ErrorOptimizerInput OptimizerInput params= sample_tensor kwargs= fused True differentiable True desc= ` fused ` does support ` differentiable ` error_type=RuntimeError error_regex= ` fused ` does support ` differentiable ` error_inputs optim_inputs_func_adamax device dtype=None cuda_supported_configs = OptimizerInput params=None kwargs= capturable True desc= capturable OptimizerInput params=None kwargs= weight_decay maximize True capturable True desc= capturable maximize weight_decay OptimizerInput params=None kwargs= weight_decay maximize True capturable True desc= capturable maximize OptimizerInput params=None kwargs= weight_decay maximize False capturable True desc= capturable weight_decay OptimizerInput params=None kwargs= lr torch tensor weight_decay maximize False capturable True desc= capturable weight_decay tensor LR OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr desc= non-default lr OptimizerInput params=None kwargs= weight_decay desc= nonzero weight_decay OptimizerInput params=None kwargs= maximize True desc= maximize OptimizerInput params=None kwargs= weight_decay maximize True desc= maximize weight_decay + cuda_supported_configs _get_device_type device CUDA_CONFIG_GPUS optim_error_inputs_func_adamax device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- betas= desc= beta should between error_type=ValueError error_regex= Invalid beta parameter index error_inputs optim_inputs_func_adamw device dtype=None optim_inputs_func_adam device dtype optim_error_inputs_func_adamw device dtype optim_error_inputs_func_adam device dtype optim_inputs_func_asgd device dtype=None cuda_supported_configs = OptimizerInput params=None kwargs= capturable True desc= capturable OptimizerInput params=None kwargs= maximize True capturable True desc= maximize capturable OptimizerInput params=None kwargs= weight_decay capturable True desc= weight_decay capturable OptimizerInput params=None kwargs= weight_decay maximize True capturable True desc= maximize weight_decay capturable OptimizerInput params=None kwargs= lr torch tensor weight_decay maximize True capturable True desc= maximize weight_decay capturable tensor LR OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lambd desc= non-default lambd OptimizerInput params=None kwargs= lr desc= non-default lr OptimizerInput params=None kwargs= t desc= t OptimizerInput params=None kwargs= maximize True desc= maximize OptimizerInput params=None kwargs= weight_decay desc= nonzero weight_decay OptimizerInput params=None kwargs= weight_decay maximize True desc= maximize nonzero weight_decay + cuda_supported_configs _get_device_type device CUDA_CONFIG_GPUS optim_error_inputs_func_asgd device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- weight_decay=- desc= weight_decay should error_type=ValueError error_regex= Invalid weight_decay value - error_inputs optim_inputs_func_lbfgs device dtype=None OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr desc= non-default lr OptimizerInput params=None kwargs= lr torch tensor desc= Tensor lr OptimizerInput params=None kwargs= tolerance_grad e- desc= tolerance_grad OptimizerInput params=None kwargs= line_search_fn strong_wolfe desc= strong_wolfe optim_error_inputs_func_lbfgs device dtype error_inputs = get_error_inputs_for_all_optims device dtype error_inputs optim_inputs_func_muon device dtype=None OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr desc= non-default lr OptimizerInput params=None kwargs= lr torch tensor desc= Tensor lr OptimizerInput params=None kwargs= weight_decay desc= non-default weight_decay OptimizerInput params=None kwargs= momentum desc= non-default momentum OptimizerInput params=None kwargs= ns_steps desc= passing alternative ns_steps OptimizerInput params=None kwargs= ns_coefficients - desc= passing alternative ns_coefficients optim_error_inputs_func_muon device dtype error_inputs = get_error_inputs_for_all_optims device dtype complex_param = torch rand device=device dtype=torch complex complex_param grad = torch rand_like complex_param non_ d_param = torch rand device=device dtype=dtype non_ d_param grad = torch rand_like non_ d_param param = torch rand device=device dtype=dtype param grad = torch rand_like param error_inputs += ErrorOptimizerInput OptimizerInput params= non_ d_param kwargs=dict desc= only support D parameters error_type=ValueError error_regex= Muon only supports D parameters error_on=OptimizerErrorEnum CONSTRUCTION_ERROR ErrorOptimizerInput OptimizerInput params= param kwargs= adjust_lr_fn arbitrary desc= only support ` original ` ` match_rms_adamw ` error_type=ValueError error_regex= Adjust learning rate function arbitrary supported error_on=OptimizerErrorEnum CONSTRUCTION_ERROR ErrorOptimizerInput OptimizerInput params= complex_param kwargs=dict desc= does support complex parameters error_type=RuntimeError error_regex= Muon does support complex parameters error_on=OptimizerErrorEnum STEP_ERROR error_inputs optim_inputs_func_nadam device dtype=None cuda_supported_configs = OptimizerInput params=None kwargs= capturable True desc= capturable OptimizerInput params=None kwargs= weight_decay momentum_decay e- capturable True desc= weight_decay capturable OptimizerInput params=None kwargs= weight_decay momentum_decay e- decoupled_weight_decay True capturable True desc= decoupled_weight_decay capturable OptimizerInput params=None kwargs= lr torch tensor weight_decay momentum_decay e- decoupled_weight_decay True capturable True desc= decoupled_weight_decay capturable OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr e- desc= non-default lr OptimizerInput params=None kwargs= momentum_decay e- desc= non-zero momentum_decay OptimizerInput params=None kwargs= weight_decay desc= weight_decay OptimizerInput params=None kwargs= weight_decay momentum_decay e- desc= weight_decay momentum_decay OptimizerInput params=None kwargs= weight_decay momentum_decay e- decoupled_weight_decay True desc= decoupled_weight_decay OptimizerInput params=None kwargs= weight_decay maximize True desc= maximize + cuda_supported_configs _get_device_type device CUDA_CONFIG_GPUS optim_error_inputs_func_nadam device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- betas= desc= beta should between error_type=ValueError error_regex= Invalid beta parameter index ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- momentum_decay=- desc= momentum_decay should error_type=ValueError error_regex= Invalid momentum_decay value - error_inputs Weird story bro NAdam RAdam do have maximize optim_inputs_func_radam device=None dtype=None cuda_supported_configs = OptimizerInput params=None kwargs= capturable True desc= capturable OptimizerInput params=None kwargs= capturable True weight_decay desc= capturable weight_decay OptimizerInput params=None kwargs= capturable True weight_decay decoupled_weight_decay True desc= capturable weight_decay decoupled_weight_decay OptimizerInput params=None kwargs= lr torch tensor capturable True weight_decay decoupled_weight_decay True desc= capturable weight_decay decoupled_weight_decay tensor LR OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr e- desc= non-default lr OptimizerInput params=None kwargs= eps e- desc= non-default eps OptimizerInput params=None kwargs= weight_decay desc= nonzero weight_decay OptimizerInput params=None kwargs= weight_decay decoupled_weight_decay True desc= decoupled_weight_decay OptimizerInput params=None kwargs= weight_decay maximize True desc= maximize + cuda_supported_configs _get_device_type device CUDA_CONFIG_GPUS optim_error_inputs_func_radam device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- betas= desc= beta should between error_type=ValueError error_regex= Invalid beta parameter index ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- weight_decay=- desc= weight_decay should error_type=ValueError error_regex= Invalid weight_decay value - error_inputs optim_inputs_func_rmsprop device dtype=None cuda_supported_configs = OptimizerInput params=None kwargs= capturable True desc= capturable OptimizerInput params=None kwargs= weight_decay maximize True capturable True desc= capturable maximize OptimizerInput params=None kwargs= lr torch tensor capturable True desc= Tensor lr capturable OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr e- desc= non-default lr OptimizerInput params=None kwargs= weight_decay desc= nonzero weight_decay OptimizerInput params=None kwargs= maximize True desc= maximize OptimizerInput params=None kwargs= weight_decay centered True desc= centered OptimizerInput params=None kwargs= maximize True weight_decay desc= maximize weight_decay OptimizerInput params=None kwargs= weight_decay centered True momentum desc= momentum OptimizerInput params=None kwargs= weight_decay centered True momentum maximize True desc= maximize centered weight_decay w momentum + cuda_supported_configs _get_device_type device CUDA_CONFIG_GPUS optim_error_inputs_func_rmsprop device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- momentum=- desc= momentum should between error_type=ValueError error_regex= Invalid momentum value - error_inputs optim_inputs_func_rprop device dtype=None cuda_supported_configs = OptimizerInput params=None kwargs= capturable True desc= capturable OptimizerInput params=None kwargs= lr torch tensor capturable True desc= Tensor lr capturable OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr e- desc= non-default lr OptimizerInput params=None kwargs= etas desc= non-default etas OptimizerInput params=None kwargs= step_sizes e- desc= non-default step_sizes OptimizerInput params=None kwargs= maximize True desc= maximize + cuda_supported_configs _get_device_type device CUDA_CONFIG_GPUS optim_error_inputs_func_rprop device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- etas= desc= eta eta error_type=ValueError error_regex= Invalid eta values error_inputs optim_inputs_func_sgd device dtype=None OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr e- desc= non-default lr OptimizerInput params=None kwargs= lr torch tensor desc= tensor lr OptimizerInput params=None kwargs= weight_decay desc= non-zero weight_decay OptimizerInput params=None kwargs= momentum desc= momentum OptimizerInput params=None kwargs= weight_decay maximize True desc= maximize OptimizerInput params=None kwargs= momentum dampening desc= dampening OptimizerInput params=None kwargs= momentum weight_decay desc= weight_decay w momentum OptimizerInput params=None kwargs= momentum nesterov True weight_decay desc= nesterov optim_error_inputs_func_sgd device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- momentum=- desc= momentum should between error_type=ValueError error_regex= Invalid momentum value - error_inputs optim_inputs_func_sparseadam device dtype=None OptimizerInput params=None kwargs= desc= default OptimizerInput params=None kwargs= lr desc= non-default lr TODO Move out testing param_group OptimizerInput params=None kwargs= lr torch tensor desc= Tensor lr OptimizerInput params=None kwargs= maximize True desc= maximize optim_error_inputs_func_sparseadam device dtype error_inputs = get_error_inputs_for_all_optims device dtype _get_device_type device == cpu error_inputs += ErrorOptimizerInput OptimizerInput params=None kwargs=dict lr= e- betas= desc= beta should between error_type=ValueError error_regex= Invalid beta parameter index ErrorOptimizerInput OptimizerInput params= torch zeros layout=torch sparse_coo device=device dtype=dtype kwargs= desc= dense params required error_type=ValueError error_regex= SparseAdam requires dense parameter tensors ErrorOptimizerInput OptimizerInput params= params torch zeros layout=torch sparse_coo device=device dtype=dtype kwargs= desc= dense params required param_groups error_type=ValueError error_regex= SparseAdam requires dense parameter tensors ErrorOptimizerInput OptimizerInput params= torch rand device=device dtype=torch complex kwargs= desc= complex supported error_type=ValueError error_regex= SparseAdam does support complex parameters error_inputs _get_device_type device Union str torch device - str Returns device type string e g cpu cuda isinstance device torch device device = str device type assert isinstance device str device split _get_optim_inputs_including_global_cliquey_kwargs device dtype optim_info skip= - list OptimizerInput Return list all configs given optimizer list OptimizerInputs including configs have supported global cliquey kwargs foreach fused differentiable based optim_info supported_impls The configs optim_inputs returned optim_info optim_inputs_func intentionally do NOT include global cliquey kwargs give flexibility tests For example testing correctness between toggling foreach off now trivial That said we sometimes want test all possible configs optimizer including all supported flags so helper returns all optim inputs assert all x foreach fused differentiable x skip skip must subset foreach fused differentiable optim_inputs = optim_info optim_inputs_func device supported_impls = tuple x x optim_info supported_impls x skip _get_device_type device optim_info supports_fused_on x = fused _get_device_type device _get_foreach_kernels_supported_devices x = foreach all_optim_inputs = optim_input optim_inputs Add base config where all flags False base_kwargs = deepcopy optim_input kwargs len supported_impls = flag supported_impls base_kwargs flag = False all_optim_inputs append OptimizerInput params=None kwargs=base_kwargs desc=optim_input desc all_optim_inputs append optim_input Add config when each global cliquey kwargs True Note optimizer kwarg categories these kwargs mutually exclusive so we do need product them together flag supported_impls new_kwargs = deepcopy base_kwargs new_kwargs flag = True all_optim_inputs append OptimizerInput params=None kwargs=new_kwargs desc=f optim_input desc flag all_optim_inputs Database OptimizerInfo entries alphabetical order optim_db list OptimizerInfo = OptimizerInfo Adadelta optim_inputs_func=optim_inputs_func_adadelta optim_error_inputs_func=optim_error_inputs_func_adadelta supported_impls= foreach differentiable has_capturable_arg=True skips= DecorateInfo skipIfTorchDynamo See TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d Note tolerances test_correctness_Adadelta_cuda_float Mismatched elements Greatest absolute difference e- index up e- allowed Greatest relative difference index up e- allowed This due floating point ordering error + usage sqrt DecorateInfo toleranceOverride torch float tol rtol= e- atol= e- CompiledOptimizerParityTests test_correctness DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach OptimizerInfo Adafactor optim_inputs_func=optim_inputs_func_adafactor optim_error_inputs_func=optim_error_inputs_func_adafactor supported_impls= foreach not_og_supported_flags= foreach supports_complex=False skips= DecorateInfo unittest skip See regarding dtype being None CompiledOptimizerParityTests test_correctness device_type= cuda active_if=lambda kwargs kwargs get use_closure False DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_can_load_older_state_dict device_type= cuda DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_deepcopy_copies_all_public_attrs device_type= cuda DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_foreach_large_tensor DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_foreach_matches_forloop DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_load_nontensor_step device_type= cuda DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_mixed_device_dtype DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_param_groups_lr device_type= cuda DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_param_groups_weight_decay device_type= cuda DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_peak_memory_foreach DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_save_load_equality_with_weights_only device_type= cuda DecorateInfo skipIfTorchDynamo See regarding copy supported TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_state_dict_deterministic device_type= cuda DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_step_is_noop_for_zero_grads device_type= cuda DecorateInfo unittest skip See regarding dtype being None CompiledOptimizerParityTests test_correctness device_type= xpu DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_can_load_older_state_dict device_type= xpu DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_deepcopy_copies_all_public_attrs device_type= xpu DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_load_nontensor_step device_type= xpu DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_param_groups_lr device_type= xpu DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_param_groups_weight_decay device_type= xpu DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_save_load_equality_with_weights_only device_type= xpu DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_state_dict_deterministic device_type= xpu DecorateInfo skipIfTorchDynamo See regarding dtype being None TestOptimRenewed test_step_is_noop_for_zero_grads device_type= xpu OptimizerInfo Adagrad optim_inputs_func=optim_inputs_func_adagrad optim_error_inputs_func=optim_error_inputs_func_adagrad supported_impls= foreach differentiable fused not_og_supported_flags= foreach differentiable fused maximize capturable supports_fused_on= cpu supports_sparse=True metadata_for_sparse= lr weight_decay lr_decay lambda opt StepLR opt gamma= - e- step_size= lambda opt ReduceLROnPlateau opt threshold= e- decorators= DecorateInfo Note tolerances difference comes fact non fused kernel have more dtype cast operations We have another test test_fused_cpu_matches_cuda make sure there no discrepancies between cuda fused kernel cpu fused kernel toleranceOverride torch bfloat tol atol= e- rtol= e- torch float tol atol= e- rtol= e- TestOptimRenewed test_fused_matches_forloop skips= DecorateInfo skipIfTorchDynamo See TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach OptimizerInfo Adam optim_inputs_func=optim_inputs_func_adam scheduler_inputs= lambda opt ExponentialLR opt gamma= lambda opt LinearLR opt start_factor= total_iters= lambda opt ConstantLR opt factor= total_iters= lambda opt ExponentialLR opt gamma= lambda opt ExponentialLR opt gamma= lambda opt ReduceLROnPlateau opt lambda opt ConstantLR opt factor= total_iters= lambda opt PolynomialLR opt power= total_iters= lambda opt StepLR opt gamma= step_size= lambda opt ReduceLROnPlateau opt optim_error_inputs_func=optim_error_inputs_func_adam supported_impls= foreach differentiable fused has_capturable_arg=True not_og_supported_flags= foreach differentiable fused maximize capturable supports_fused_on= cpu cuda xpu mps decorators= Expected floating point error between fused compiled forloop DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestOptimRenewed test_fused_matches_forloop active_if=lambda kwargs TEST_WITH_TORCHDYNAMO kwargs dtype == torch float DecorateInfo Note tolerances difference comes fact non fused kernel have more dtype cast operations We have another test test_fused_cpu_matches_cuda make sure there no discrepancies between cuda fused kernel cpu fused kernel toleranceOverride torch bfloat tol atol= e- rtol= e- torch float tol atol= e- rtol= e- TestOptimRenewed test_fused_matches_forloop DecorateInfo Note tolerances Tracking through toleranceOverride torch float tol atol= e- rtol= e- TestCudaOptims test_grad_scaling_autocast_fused_optimizers skips= DecorateInfo skipIfTorchDynamo Errors w Global state changed see https github com pytorch pytorch issues TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach OptimizerInfo Adamax optim_inputs_func=optim_inputs_func_adamax optim_error_inputs_func=optim_error_inputs_func_adamax supported_impls= foreach differentiable has_capturable_arg=True skips= DecorateInfo skipIfTorchDynamo See TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d DecorateInfo unittest skip Uses too much memory even H surprisingly TestOptimRenewed test_foreach_large_tensor DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach OptimizerInfo AdamW optim_inputs_func=optim_inputs_func_adamw optim_error_inputs_func=optim_error_inputs_func_adamw supported_impls= foreach differentiable fused not_og_supported_flags= foreach differentiable fused maximize capturable supports_fused_on= cpu cuda mps has_capturable_arg=True decorators= Expected error between compiled forloop fused optimizers DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestOptimRenewed test_fused_matches_forloop active_if=lambda kwargs TEST_WITH_TORCHDYNAMO kwargs dtype == torch float DecorateInfo toleranceOverride Note tolerances difference comes fact non fused kernel have more dtype cast operations We have another test test_fused_cpu_matches_cuda make sure there no discrepancies between cuda fused kernel cpu fused kernel torch bfloat tol atol= e- rtol= e- torch float tol atol= e- rtol= e- TestOptimRenewed test_fused_matches_forloop Note tolerances Tracking through DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestCudaOptims test_grad_scaling_autocast_fused_optimizers skips= DecorateInfo skipIfTorchDynamo Errors w Global state changed see https github com pytorch pytorch issues TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach OptimizerInfo ASGD optim_inputs_func=optim_inputs_func_asgd optim_error_inputs_func=optim_error_inputs_func_asgd supported_impls= foreach differentiable has_capturable_arg=True skips= DecorateInfo skipIfTorchDynamo Errors w Global state changed see https github com pytorch pytorch issues TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d DecorateInfo toleranceOverride torch float tol atol= e- rtol= e- TestOptimRenewed test_step_is_noop_for_zero_grads DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach DecorateInfo unittest skip ASGD internally changes weights even zero grad TestOptimRenewed test_step_is_noop_for_zero_grads OptimizerInfo LBFGS optim_inputs_func=optim_inputs_func_lbfgs optim_error_inputs_func=optim_error_inputs_func_lbfgs supported_impls= step_requires_closure=True supports_param_groups=False supports_multiple_devices=False skips= Fails MacOS CI https github com pytorch pytorch issues DecorateInfo skipIfMPS TestOptimRenewed test_can_load_older_state_dict device_type= mps DecorateInfo toleranceOverride torch complex tol rtol= e- atol= e- TestOptimRenewed test_complex_ d DecorateInfo unittest skip Does support param groups TestOptimRenewed test_param_groups_lr DecorateInfo unittest skip Does support param groups TestOptimRenewed test_param_groups_weight_decay DecorateInfo unittest skip LBFGS doesn t support multidevice TestOptimRenewed test_forloop_goes_right_direction_multigpu DecorateInfo unittest skip Does support param groups TestOptimRenewed test_param_group_with_lrscheduler_goes_right_direction https github com pytorch pytorch issues DecorateInfo unittest expectedFailure CompiledOptimizerParityTests test_correctness active_if=lambda kwargs sys platform == darwin kwargs use_closure OptimizerInfo Muon optim_inputs_func=optim_inputs_func_muon optim_error_inputs_func=optim_error_inputs_func_muon supported_impls= not_og_supported_flags= supports_complex=False skips= Note numerical differences ` compile ` applies different matmul tuning which leads deviations compared eager mode In Newton-Schulz iteration orthogonalization computations done bfloat further amplifying these numerical differences DecorateInfo unittest skip Expect high difference between compiled eager due bfloat iterative process CompiledOptimizerParityTests test_correctness OptimizerInfo NAdam optim_inputs_func=optim_inputs_func_nadam optim_error_inputs_func=optim_error_inputs_func_nadam supported_impls= foreach differentiable has_capturable_arg=True skips= DecorateInfo skipIfTorchDynamo Errors w Global state changed see https github com pytorch pytorch issues TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d DecorateInfo skipIfTorchDynamo Errors https github com pytorch pytorch issues TestOptimRenewed test_load_nontensor_step DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach OptimizerInfo RAdam optim_inputs_func=optim_inputs_func_radam optim_error_inputs_func=optim_error_inputs_func_radam supported_impls= foreach differentiable has_capturable_arg=True skips= DecorateInfo skipIfTorchDynamo Errors w Global state changed see https github com pytorch pytorch issues TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d DecorateInfo toleranceOverride previously atol= e- rtol= e- torch float tol atol= e- rtol= e- TestOptimRenewed test_foreach_matches_forloop DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach OptimizerInfo RMSprop optim_inputs_func=optim_inputs_func_rmsprop optim_error_inputs_func=optim_error_inputs_func_rmsprop supported_impls= foreach differentiable has_capturable_arg=True skips= DecorateInfo skipIfTorchDynamo See TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d DecorateInfo toleranceOverride previously atol= - rtol= https github com pytorch pytorch issues torch float tol atol= e- rtol= TestOptimRenewed test_mixed_device_dtype active_if=TEST_WITH_TORCHDYNAMO DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach OptimizerInfo Rprop optim_inputs_func=optim_inputs_func_rprop optim_error_inputs_func=optim_error_inputs_func_rprop supported_impls= foreach differentiable has_capturable_arg=True skips= DecorateInfo skipIfTorchDynamo See TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach OptimizerInfo SGD optim_inputs_func=optim_inputs_func_sgd scheduler_inputs= lambda opt StepLR opt gamma= step_size= lambda opt LinearLR opt start_factor= end_factor= total_iters= lambda opt StepLR opt gamma= step_size= lambda opt LinearLR opt start_factor= end_factor= total_iters= lambda opt StepLR opt gamma= step_size= lambda opt ExponentialLR opt gamma= lambda opt ReduceLROnPlateau opt lambda opt ConstantLR opt factor= total_iters= lambda opt PolynomialLR opt power= total_iters= lambda opt StepLR opt gamma= step_size= lambda opt ReduceLROnPlateau opt optim_error_inputs_func=optim_error_inputs_func_sgd supported_impls= foreach differentiable fused not_og_supported_flags= foreach differentiable fused maximize capturable supports_sparse=True metadata_for_sparse= lr e- maximize False momentum nesterov False weight_decay lambda opt StepLR opt gamma= step_size= supports_fused_on= cpu cuda xpu mps skips= DecorateInfo skipIfTorchDynamo Errors w Global state changed see https github com pytorch pytorch issues TestOptimRenewed test_set_default_dtype_works_with_foreach DecorateInfo skipIfTorchDynamo Accessing grad real errors see https github com pytorch pytorch issues TestOptimRenewed test_complex_ d DecorateInfo skipIfTorchDynamo This test uses mocks which dynamo does support TestOptimRenewed test_defaults_changed_to_foreach OptimizerInfo SparseAdam optim_inputs_func=optim_inputs_func_sparseadam optim_error_inputs_func=optim_error_inputs_func_sparseadam supported_impls= only_supports_sparse_grads=True metadata_for_sparse= lr e- supports_complex=False Missing complex support see skips= DecorateInfo skipIfMPS SparseAdam does support MPS TestOptimRenewed device_type= mps DecorateInfo skipIfTorchDynamo cannot call to_sparse p grad see TestOptimRenewed test_param_groups_lr DecorateInfo skipIfTorchDynamo cannot call to_sparse p grad see TestOptimRenewed test_tensor_lr DecorateInfo unittest skip SparseAdam does support dense gradients see TestOptimRenewed test_can_load_older_state_dict DecorateInfo skipIfTorchDynamo cannot call to_sparse p grad see TestOptimRenewed test_load_nontensor_step DecorateInfo skipIfTorchDynamo cannot call to_sparse p grad see TestOptimRenewed test_forloop_goes_right_direction DecorateInfo skipIfTorchDynamo cannot call to_sparse p grad see TestOptimRenewed test_forloop_goes_right_direction_multigpu DecorateInfo skipIfTorchDynamo cannot call to_sparse p grad see TestOptimRenewed test_param_group_with_lrscheduler_goes_right_direction DecorateInfo skipIfTorchDynamo cannot call to_sparse p grad see TestOptimRenewed test_state_dict_with_cuda_params DecorateInfo skipIfTorchDynamo cannot call to_sparse p grad see TestOptimRenewed test_deepcopy_copies_all_public_attrs TensorTracker A utility track tensor clones list expectation popping them later order make fair comparisons between two multi-step computation The intended use case usually when comparing two supposed equal computations such optimizer step each individually consists multiple steps where numerical deviation could multiply The goal able compare align numbers every milestone so minimize numerical discrepancies so when test fails likely real problem __init__ assert_eq_kwargs=None assert_eq_kwargs None assert_eq_kwargs = assert_eq_kwargs = assert_eq_kwargs tensors = add tensor Add detach clone d version tensor tensors append tensor detach clone pops beginning like queue stack pop_check_set tensor_to_set testcase Pop first element tensor tracker assert equality between popped tensor input tensor then set input tensor have same values popped tensor copy_ testcase assertGreater len tensors no tensors pop ref = tensors pop testcase assertTrue isinstance ref Tensor f type ref = testcase assertEqual tensor_to_set ref assert_eq_kwargs torch no_grad tensor_to_set copy_ ref all_popped len tensors ==