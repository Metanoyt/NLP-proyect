Owner s oncall quantization torch torch testing _internal common_quantization ModelMultipleOps ModelMultipleOpsNoAvgPool QuantizationTestCase torch testing _internal common_quantized override_quantized_engine supported_qengines TestModelNumericsEager QuantizationTestCase test_float_quant_compare_per_tensor qengine supported_qengines override_quantized_engine qengine torch manual_seed my_model = ModelMultipleOps torch float my_model eval calib_data = torch rand dtype=torch float eval_data = torch rand dtype=torch float out_ref = my_model eval_data qModel = torch ao quantization QuantWrapper my_model qModel eval qModel qconfig = torch ao quantization default_qconfig torch ao quantization fuse_modules qModel module conv bn relu inplace=True torch ao quantization prepare qModel inplace=True qModel calib_data torch ao quantization convert qModel inplace=True out_q = qModel eval_data SQNRdB = torch log torch norm out_ref torch norm out_ref - out_q Quantized model output should close floating point model output numerically Setting target SQNR dB so relative error e- below desired output assertGreater SQNRdB msg= Quantized model numerics diverge float expect SQNR dB test_float_quant_compare_per_channel Test per-channel Quant torch manual_seed my_model = ModelMultipleOps torch float my_model eval calib_data = torch rand dtype=torch float eval_data = torch rand dtype=torch float out_ref = my_model eval_data q_model = torch ao quantization QuantWrapper my_model q_model eval q_model qconfig = torch ao quantization default_per_channel_qconfig torch ao quantization fuse_modules q_model module conv bn relu inplace=True torch ao quantization prepare q_model q_model calib_data torch ao quantization convert q_model out_q = q_model eval_data SQNRdB = torch log torch norm out_ref torch norm out_ref - out_q Quantized model output should close floating point model output numerically Setting target SQNR dB assertGreater SQNRdB msg= Quantized model numerics diverge float expect SQNR dB test_fake_quant_true_quant_compare qengine supported_qengines override_quantized_engine qengine torch manual_seed my_model = ModelMultipleOpsNoAvgPool torch float calib_data = torch rand dtype=torch float eval_data = torch rand dtype=torch float my_model eval out_ref = my_model eval_data fq_model = torch ao quantization QuantWrapper my_model fq_model train fq_model qconfig = torch ao quantization default_qat_qconfig torch ao quantization fuse_modules_qat fq_model module conv bn relu inplace=True torch ao quantization prepare_qat fq_model fq_model eval fq_model apply torch ao quantization disable_fake_quant fq_model apply torch ao nn intrinsic qat freeze_bn_stats fq_model calib_data fq_model apply torch ao quantization enable_fake_quant fq_model apply torch ao quantization disable_observer out_fq = fq_model eval_data SQNRdB = torch log torch norm out_ref torch norm out_ref - out_fq Quantized model output should close floating point model output numerically Setting target SQNR dB assertGreater SQNRdB msg= Quantized model numerics diverge float expect SQNR dB torch ao quantization convert fq_model out_q = fq_model eval_data SQNRdB = torch log torch norm out_fq torch norm out_fq - out_q + e- assertGreater SQNRdB msg= Fake quant true quant numerics diverge expect SQNR dB Test compare weight only quantized model numerics activation only quantized model numerics float test_weight_only_activation_only_fakequant qengine supported_qengines override_quantized_engine qengine torch manual_seed calib_data = torch rand dtype=torch float eval_data = torch rand dtype=torch float qconfigset = torch ao quantization default_weight_only_qconfig torch ao quantization default_activation_only_qconfig SQNRTarget = idx qconfig enumerate qconfigset my_model = ModelMultipleOpsNoAvgPool torch float my_model eval out_ref = my_model eval_data fq_model = torch ao quantization QuantWrapper my_model fq_model train fq_model qconfig = qconfig torch ao quantization fuse_modules_qat fq_model module conv bn relu inplace=True torch ao quantization prepare_qat fq_model fq_model eval fq_model apply torch ao quantization disable_fake_quant fq_model apply torch ao nn intrinsic qat freeze_bn_stats fq_model calib_data fq_model apply torch ao quantization enable_fake_quant fq_model apply torch ao quantization disable_observer out_fq = fq_model eval_data SQNRdB = torch log torch norm out_ref torch norm out_ref - out_fq assertGreater SQNRdB SQNRTarget idx msg= Quantized model numerics diverge float __name__ == __main__ raise RuntimeError This test file meant run directly use \n\n \tpython test test_quantization py TESTNAME\n\n instead