mypy ignore-errors This file contains collection context manager classes used Dynamo tracking managing various PyTorch runtime states during graph compilation These context managers handle different aspects PyTorch s execution environment including - Autograd states grad mode inference mode - CUDA streams events - Profiling contexts - Deterministic algorithms - Forward backward AD modes - SDPA Scaled Dot Product Attention kernels - FSDP Fully Sharded Data Parallel states - AMP Automatic Mixed Precision autocast states The context managers ensure proper state transitions during graph compilation tracking enter exit points managing cleanup operations They help maintain consistency between eager execution compiled graph behavior capturing restoring state changes inspect sys warnings contextlib ExitStack typing TYPE_CHECKING Union torch _C torch _guards Guard graph_break_hints variables bytecode_transformation create_call_function create_instruction create_setup_with exc unimplemented_v guards GuardBuilder install_guard source AttrSource GlobalStateSource utils _get_error_on_graph_break _set_error_on_graph_break base VariableTracker functions NestedUserFunctionVariable SkipFunctionVariable UserFunctionVariable UserMethodVariable WrappedNestedUserFunctionVariable WrappedSkipFunctionVariable WrappedUserFunctionVariable WrappedUserMethodVariable user_defined UserDefinedObjectVariable TYPE_CHECKING torch _dynamo codegen PyCodegen torch _dynamo symbolic_convert InstructionTranslator ContextWrappingVariable VariableTracker _nonvar_fields = cm_obj target_values initial_values state VariableTracker _nonvar_fields __init__ target_values initial_values=None kwargs - None super __init__ kwargs target_values = target_values initial_values = initial_values enter tx _call_func tx target_values set_cleanup_hook tx variables ConstantVariable create None set_cleanup_hook tx InstructionTranslator fn=None fn None fn _call_func tx initial_values cleanup_fn = fn tx output add_cleanup_hook cleanup exit tx InstructionTranslator args cleanup_assert variables ConstantVariable create None reconstruct_type codegen PyCodegen codegen AttrSource codegen tx import_source module_name fn_name reconstruct codegen PyCodegen codegen add_push_null lambda reconstruct_type codegen target_values = target_values target_values target_values = codegen extend_output codegen create_load_const val val target_values codegen extend_output create_call_function len target_values False module_name raise NotImplementedError module_name called base fn_name raise NotImplementedError fn_name called base call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker assert len args == assert isinstance args NestedUserFunctionVariable SkipFunctionVariable UserMethodVariable UserFunctionVariable isinstance args NestedUserFunctionVariable WrappedNestedUserFunctionVariable args isinstance args SkipFunctionVariable WrappedSkipFunctionVariable args isinstance args UserMethodVariable WrappedUserMethodVariable args isinstance args UserFunctionVariable WrappedUserFunctionVariable args supports_graph_breaks True exit_on_graph_break True cleanup cleanup_fn None cleanup_fn cleanup_fn = None cleanup_assert assert cleanup_fn multiple exits cleanup GenericContextWrappingVariable UserDefinedObjectVariable Some methods ContextWrappingVariable assumes arguments python constants Which might always case here __init__ cm_obj kwargs - None assert cm_obj None super __init__ value=cm_obj value_type=cm_obj __class__ kwargs cm_obj = cm_obj module_name cm_obj __module__ fn_name type cm_obj __name__ enter tx source = None source None AttrSource source __enter__ variables UserMethodVariable cm_obj __enter__ __func__ source=source call_function tx exit tx InstructionTranslator args source = None source None AttrSource source __exit__ x = variables UserMethodVariable cm_obj __exit__ __func__ source=source call_function tx args tx active_generic_context_managers pop x supports_graph_breaks False exit_on_graph_break True RepararametrizeModuleContextVariable GenericContextWrappingVariable __init__ ctx_manager_vt mod cm_vt = ctx_manager_vt mod = mod We don t call super __init__ because we re delegating most methods cm_vt enter tx InstructionTranslator Custom enter implementation side effects old_parameters_var = mod var_getattr tx _parameters realize old_buffer_var = mod var_getattr tx _buffers realize tx output side_effects ignore_mutations_on old_parameters_var tx output side_effects ignore_mutations_on old_buffer_var cm_vt enter tx exit tx InstructionTranslator args Custom exit implementation side effects x = cm_vt exit tx args tx output side_effects stop_ignoring_mutations_on old_buffer_var tx output side_effects stop_ignoring_mutations_on old_parameters_var x Forward all other method calls cm_vt __getattr__ name This will called any attribute explicitly defined getattr cm_vt name GradInplaceRequiresGradCtxManagerVariable ContextWrappingVariable represents torch grad requires grad staticmethod create tx InstructionTranslator target_values kwargs GradInplaceRequiresGradCtxManagerVariable target_values=target_values initial_values=None kwargs enter tx enabled = target_values prev_state = torch _C _functorch get_inplace_requires_grad_allowed torch _C _functorch set_inplace_requires_grad_allowed enabled set_cleanup_hook tx lambda torch _C _functorch set_inplace_requires_grad_allowed prev_state proxy = tx output create_node call_function torch _C _functorch set_inplace_requires_grad_allowed enabled variables ConstantVariable create None exit tx InstructionTranslator args cleanup tx output create_node call_function torch _C _functorch set_inplace_requires_grad_allowed prev_state variables ConstantVariable create None TemporarilyPopInterpreterStackCtxManagerVariable ContextWrappingVariable represents torch _functorch pyfunction temporarily_pop_interpreter_stack staticmethod create tx InstructionTranslator target_values kwargs TemporarilyPopInterpreterStackCtxManagerVariable target_values=target_values initial_values=None kwargs enter tx saved = torch _C _functorch pop_dynamic_layer_stack set_cleanup_hook tx lambda torch _C _functorch push_dynamic_layer_stack saved proxy = tx output create_node call_function torch _C _functorch pop_dynamic_layer_stack variables ConstantVariable create None exit tx InstructionTranslator args cleanup tx output create_node call_function torch _C _functorch push_dynamic_layer_stack proxy variables ConstantVariable create None JvpIncrementNestingCtxManagerVariable ContextWrappingVariable represents torch func jvp increment decrement nesting A guard needed grad level baked into torch FX graph This fine jvp only called within function being compiled But FX graph may invalid case jvp call eager calls compiled function jvp levels may different _guards_singleton = Guard GlobalStateSource GuardBuilder FUNCTORCH_STACK_MATCH staticmethod create tx InstructionTranslator kwargs var = JvpIncrementNestingCtxManagerVariable target_values=None initial_values=None kwargs var enter tx install_guard _guards_singleton jvp_level = torch _functorch eager_transforms enter_jvp_nesting set_cleanup_hook tx lambda torch _functorch eager_transforms exit_jvp_nesting proxy = tx output create_node call_function torch _C _functorch _jvp_increment_nesting variables ConstantVariable create jvp_level exit tx InstructionTranslator args cleanup tx output create_node call_function torch _C _functorch _jvp_decrement_nesting variables ConstantVariable create None SetFwdGradEnabledContextManager ContextWrappingVariable represents torch autograd forward_ad _set_fwd_grad_enabled enable disable fwd grad staticmethod create tx InstructionTranslator target_values kwargs SetFwdGradEnabledContextManager target_values=target_values initial_values=None kwargs enter tx mode = target_values prev_state = torch _C _is_fwd_grad_enabled torch _C _set_fwd_grad_enabled mode set_cleanup_hook tx lambda torch _C _set_fwd_grad_enabled prev_state proxy = tx output create_node call_function torch _C _set_fwd_grad_enabled mode variables ConstantVariable create None exit tx InstructionTranslator args cleanup tx output create_node call_function torch _C _set_fwd_grad_enabled prev_state variables ConstantVariable create None DualLevelContextManager ContextWrappingVariable Represents torch autograd forward_ad dual_level ctx manager _guards_singleton = Guard GlobalStateSource GuardBuilder DUAL_LEVEL staticmethod create tx InstructionTranslator kwargs DualLevelContextManager target_values=None initial_values=None kwargs enter tx install_guard _guards_singleton new_level = torch autograd forward_ad enter_dual_level set_cleanup_hook tx lambda torch autograd forward_ad exit_dual_level level=self new_level proxy = tx output create_node call_function torch _C _enter_dual_level variables ConstantVariable create new_level exit tx InstructionTranslator args cleanup tx output create_node call_function torch _C _exit_dual_level new_level variables ConstantVariable create None GradIncrementNestingCtxManagerVariable ContextWrappingVariable represents torch func grad increment decrement nesting A guard needed grad level baked into torch FX graph This fine grad only called within function being compiled But FX graph may invalid case grad call eager calls compiled function grad levels may different _guards_singleton = Guard GlobalStateSource GuardBuilder FUNCTORCH_STACK_MATCH staticmethod create tx InstructionTranslator kwargs var = GradIncrementNestingCtxManagerVariable target_values=None initial_values=None kwargs var enter tx install_guard _guards_singleton grad_level = torch _C _functorch _grad_increment_nesting set_cleanup_hook tx lambda torch _C _functorch _grad_decrement_nesting proxy = tx output create_node call_function torch _C _functorch _grad_increment_nesting variables ConstantVariable create grad_level exit tx InstructionTranslator args cleanup tx output create_node call_function torch _C _functorch _grad_decrement_nesting variables ConstantVariable create None CatchWarningsCtxManagerVariable ContextWrappingVariable Delay call warnings catch_warnings staticmethod create tx InstructionTranslator catch_warnings_args CatchWarningsCtxManagerVariable catch_warnings_args=catch_warnings_args target_values=None initial_values=None __init__ catch_warnings_args kwargs - None assert isinstance catch_warnings_args dict catch_warnings_args super __init__ kwargs catch_warnings_args = catch_warnings_args enter tx kwargs = k v as_python_constant k v catch_warnings_args items ctx_val = warnings catch_warnings kwargs set_cleanup_hook tx lambda ctx_val __exit__ None None None variables ConstantVariable create ctx_val __enter__ reconstruct cg cg add_push_null lambda cg load_import_from warnings catch_warnings cg foreach catch_warnings_args values keys = tuple catch_warnings_args keys cg extend_output cg create_call_function_kw len keys keys False VmapIncrementNestingCtxManagerVariable ContextWrappingVariable represents torch VMap increment decrement nesting A guard needed vmap level baked into torch FX graph generated This fine vmap only called within function being compiled But FX graph may invalid case vmap call eager calls compiled function vmap levels may different _guards_singleton = Guard GlobalStateSource GuardBuilder FUNCTORCH_STACK_MATCH staticmethod create tx InstructionTranslator target_values kwargs var = VmapIncrementNestingCtxManagerVariable target_values=target_values initial_values=None kwargs var enter tx install_guard _guards_singleton batch_size randomness = target_values isinstance batch_size variables SymNodeVariable batch_size_value = batch_size sym_num batch_size_value = batch_size as_python_constant randomness = randomness as_python_constant vmap_level = torch _C _functorch _vmap_increment_nesting batch_size_value randomness set_cleanup_hook tx lambda torch _C _functorch _vmap_decrement_nesting proxy = tx output create_proxy call_function torch _functorch predispatch _vmap_increment_nesting batch_size as_proxy randomness variables ConstantVariable create vmap_level exit tx InstructionTranslator args cleanup tx output create_node call_function torch _functorch predispatch _vmap_decrement_nesting variables ConstantVariable create None GradModeVariable ContextWrappingVariable represents torch no_grad enable_grad set_grad_mode _guards_singleton = Guard GlobalStateSource GuardBuilder GRAD_MODE staticmethod create tx InstructionTranslator target_value initialized=False kwargs var = GradModeVariable target_values= target_value initial_values= torch is_grad_enabled kwargs initialized var _call_func tx var target_values var __init__ target_values initial_values=None initialized=True kwargs - None super __init__ target_values=target_values initial_values=initial_values kwargs install_guard _guards_singleton enter tx _call_func tx target_values variables ConstantVariable create None exit tx InstructionTranslator args _call_func tx initial_values variables ConstantVariable create None call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker _call_func tx initial_values undo eager initialization super call_function tx args kwargs _call_func tx InstructionTranslator values assert len values == value = values Coalesce grad mode mutations torch is_grad_enabled = value tx output create_node call_function torch _C _set_grad_enabled value torch _C _set_grad_enabled value module_name torch fn_name set_grad_enabled InferenceModeVariable ContextWrappingVariable staticmethod create tx InstructionTranslator target_value kwargs var = InferenceModeVariable target_value initial_values=torch is_inference_mode_enabled kwargs var __init__ target_values initial_values=None kwargs - None initial_values None This must called here since function defaults evaluated time initial_values = torch is_inference_mode_enabled super __init__ target_values=target_values initial_values=initial_values kwargs target_values = target_values exit tx InstructionTranslator args cleanup_assert tx output create_node call_function torch autograd grad_mode _exit_inference_mode proxy enter tx disabled_inference_mode_forcibly = False torch _dynamo config fake_tensor_disable_inference_mode target_values Do set inference mode because we keep off during compilation Set grad_enabled False reflect relevant part inference_mode torch compile disabled_inference_mode_forcibly = True prior = torch is_grad_enabled torch _C _set_grad_enabled False ctx = torch autograd grad_mode _enter_inference_mode target_values cleanup_hook disabled_inference_mode_forcibly torch _C _set_grad_enabled prior torch autograd grad_mode _exit_inference_mode ctx set_cleanup_hook tx cleanup_hook proxy = tx output create_node call_function torch autograd grad_mode _enter_inference_mode target_values module_name torch fn_name inference_mode CUDADeviceVariable ContextWrappingVariable represents torch cuda device staticmethod create tx InstructionTranslator device kwargs var = CUDADeviceVariable target_values= torch cuda _get_device_index device optional=True initial_values=None kwargs var __init__ target_values initial_values=None kwargs - None super __init__ target_values=target_values initial_values=initial_values kwargs target_values = target_values exit tx InstructionTranslator args cleanup_assert tx output create_node call_function torch cuda _maybe_exchange_device proxy variables ConstantVariable create False enter tx prev_idx = torch cuda _exchange_device target_values set_cleanup_hook tx lambda torch cuda _maybe_exchange_device prev_idx proxy = tx output create_node call_function torch cuda _exchange_device target_values module_name torch cuda fn_name device TorchFunctionDisableVariable ContextWrappingVariable represents whether torch function overrides enabled _guards_singleton = Guard GlobalStateSource GuardBuilder TORCH_FUNCTION_STATE staticmethod create tx InstructionTranslator kwargs var = TorchFunctionDisableVariable target_values= initial_values= kwargs var __init__ target_values initial_values=None only_subclass=True kwargs - None assert len target_values == assert len initial_values == symbolic_convert InstructionTranslator tx = InstructionTranslator current_tx only_subclass = only_subclass initial_torch_function_subclass_enabled = tx symbolic_torch_function_state torch_function_subclass_enabled initial_torch_function_mode_enabled = tx symbolic_torch_function_state torch_function_mode_enabled super __init__ target_values=target_values initial_values=initial_values kwargs install_guard _guards_singleton set_cleanup_hook tx InstructionTranslator fn=None fn None fn tx symbolic_torch_function_state torch_function_subclass_enabled = initial_torch_function_subclass_enabled only_subclass tx symbolic_torch_function_state torch_function_mode_enabled = initial_torch_function_subclass_enabled cleanup_fn = fn tx output add_cleanup_hook cleanup _call_func tx InstructionTranslator values assert len values == tx symbolic_torch_function_state torch_function_subclass_enabled = False only_subclass tx symbolic_torch_function_state torch_function_mode_enabled = False module_name torch _C fn_name only_subclass DisableTorchFunctionSubclass DisableTorchFunction DeterministicAlgorithmsVariable ContextWrappingVariable represents torch are_deterministic_algorithms_enabled use_deterministic_algorithms _guards_singleton = Guard GlobalStateSource GuardBuilder DETERMINISTIC_ALGORITHMS staticmethod create tx InstructionTranslator target_value kwargs var = DeterministicAlgorithmsVariable target_values= target_value initial_values= torch are_deterministic_algorithms_enabled kwargs var _call_func tx target_value var set_cleanup_hook tx var __init__ target_values initial_values=None kwargs - None super __init__ target_values=target_values initial_values=initial_values kwargs install_guard _guards_singleton enter tx variables ConstantVariable create None _call_func tx InstructionTranslator values assert len values == value = values tx output create_node call_function torch _C _set_deterministic_algorithms value torch _C _set_deterministic_algorithms value module_name torch fn_name use_deterministic_algorithms DisabledSavedTensorsHooksVariable ContextWrappingVariable represents torch autograd graph disable_saved_tensors_hook staticmethod create tx InstructionTranslator target_value kwargs var = DisabledSavedTensorsHooksVariable target_values= target_value initial_values= torch _C _autograd _saved_tensors_hooks_get_disabled_error_message kwargs var _call_func tx target_value var set_cleanup_hook tx var __init__ target_values initial_values=None kwargs - None super __init__ target_values=target_values initial_values=initial_values kwargs enter tx variables ConstantVariable create None _call_func tx InstructionTranslator values assert len values == value = values value None Disable ` saved_tensors_hooks ` message ` value ` OR we exiting context restoring previous message tx output create_node call_function torch _C _autograd _saved_tensors_hooks_disable value torch _C _autograd _saved_tensors_hooks_disable value We exiting context prev_message None we re-enable ` saved_tensors_hooks ` tx output create_node call_function torch _C _autograd _saved_tensors_hooks_enable torch _C _autograd _saved_tensors_hooks_enable module_name torch autograd graph fn_name disable_saved_tensors_hooks AutocastModeVariable ContextWrappingVariable staticmethod create func args kwargs assert func torch amp autocast_mode autocast torch cuda amp autocast torch cpu amp autocast device_type str dtype Optional _dtype = None enabled bool = True cache_enabled Optional bool = None cache_enabled bound_args = inspect signature func bind args kwargs bound_args apply_defaults target_values = kwargs clear key device_type dtype enabled cache_enabled key == device_type func torch cuda amp autocast torch cpu amp autocast arg = cuda func torch cuda amp autocast cpu arg = bound_args arguments key isinstance arg VariableTracker target_values append arg as_python_constant target_values append arg var = AutocastModeVariable target_values initial_values=None kwargs var __init__ target_values initial_values=None kwargs - None super __init__ target_values=target_values initial_values=initial_values kwargs target_values = target_values exit tx InstructionTranslator args cleanup_assert tx output create_node call_function torch amp _exit_autocast proxy variables ConstantVariable create None enter tx ctx = torch amp _enter_autocast target_values set_cleanup_hook tx lambda torch amp _exit_autocast ctx proxy = tx output create_node call_function torch amp _enter_autocast target_values module_name torch amp autocast_mode fn_name autocast NullContextVariable ContextWrappingVariable This represents Python contextlib nullcontext __init__ target_values=None kwargs - None super __init__ target_values=target_values kwargs enter tx none = variables ConstantVariable create None target_values target_values none exit tx InstructionTranslator args variables ConstantVariable create None module_name contextlib fn_name nullcontext ProfilerContextVariable ContextWrappingVariable This represents set torch profiler context objects where Dynamo ignores all side-effects __init__ __enter__ __exit__ methods treating object mostly ` contextlib nullcontext ` except edge cases like ` __enter__ ` method which returns object itself rather than ` None ` per implementation torch objects __init__ kwargs - None super __init__ target_values=None kwargs enter tx exit tx InstructionTranslator args variables ConstantVariable create None module_name contextlib fn_name nullcontext reconstruct cg unimplemented_v gb_type= torch profiler object escaped compiled region context=str explanation= Dynamo doesn t support compiling region returns torch profiler context manager hints= graph_break_hints SUPPORTABLE PreserveVersionContextVariable ContextWrappingVariable Wraps torch autograd _unsafe_preserve_version_counter staticmethod _create_lambda_from_tensors tx tensors isinstance tensors variables TensorVariable versions = variables TupleVariable x var_getattr tx _version x tensors tensors = variables TupleVariable tensors versions = variables TupleVariable x var_getattr tx _version x tensors items PreserveVersionContextVariable tensors versions staticmethod constructor tx variables LambdaVariable lambda tensors PreserveVersionContextVariable _create_lambda_from_tensors tx tensors __init__ tensors prev_versions kwargs - None kwargs setdefault target_values None super __init__ kwargs tensors = tensors prev_versions = prev_versions The context manager accepts Union Tensor Tuple Tensor isinstance tensors variables TensorVariable tensors = variables TupleVariable tensors isinstance prev_versions variables ConstantVariable variables SymNodeVariable prev_versions = variables TupleVariable prev_versions enter tx pass exit tx InstructionTranslator args tensor_version_op _unsafe_set_version_counter variables TorchInGraphFunctionVariable _unsafe_set_version_counter call_function tx tensors prev_versions reconstruct codegen PyCodegen unimplemented_v gb_type= torch autograd _unsafe_preserve_version_counter escaped compiled region context=str explanation= Dynamo doesn t support compiling region returns torch autograd _unsafe_preserve_version_counter context manager hints= graph_break_hints SUPPORTABLE FSDPParamGroupUseTrainingStateVariable ContextWrappingVariable _guards_singleton = Guard GlobalStateSource GuardBuilder FSDP_TRAINING_STATE staticmethod create tx InstructionTranslator param_group_var target_value kwargs var = FSDPParamGroupUseTrainingStateVariable param_group_var=param_group_var target_values= target_value initial_values= param_group_var value _training_state kwargs var __init__ param_group_var target_values initial_values=None kwargs - None super __init__ target_values=target_values initial_values=initial_values kwargs param_group_var = param_group_var install_guard _guards_singleton enter tx _call_func tx target_values variables ConstantVariable create None exit tx InstructionTranslator args _call_func tx initial_values variables ConstantVariable create None call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker _call_func tx initial_values undo eager initialization super call_function tx args kwargs _call_func tx InstructionTranslator values assert len values == value = values param_group_var value _training_state = value param_group_var call_method tx __setattr__ variables ConstantVariable create _training_state variables EnumVariable value param_group_var value _training_state = value module_name torch distributed fsdp _fully_shard _fsdp_param_group FSDPParamGroup fn_name use_training_state SDPAKernelVariable ContextWrappingVariable represents torch nn attention sdpa_kernel staticmethod create tx InstructionTranslator backends set_priority=False kwargs isinstance backends torch nn attention SDPBackend backends = backends var = SDPAKernelVariable target_values=backends initial_values=None set_priority=set_priority kwargs var __init__ target_values list torch nn attention SDPBackend initial_values=None set_priority bool = False kwargs - None super __init__ target_values=target_values initial_values=initial_values kwargs set_priority = set_priority staticmethod _backends_to_nodes tx backends convert string order bake backend into FX graph nodes = tx output create_node call_function torch nn attention _backend_from_string backend name backend backends nodes enter tx prev_backends = torch nn attention _cur_sdpa_kernel_backends with_priority=self set_priority set_cleanup_hook tx lambda torch nn attention _sdpa_kernel prev_backends set_priority=self set_priority torch nn attention _sdpa_kernel target_values set_priority=self set_priority arg = _backends_to_nodes tx target_values tx output create_node call_function torch nn attention _sdpa_kernel arg bool set_priority variables ConstantVariable create None exit tx InstructionTranslator args cleanup_assert arg = _backends_to_nodes tx prev_backends tx output create_node call_function torch nn attention _sdpa_kernel arg bool set_priority variables ConstantVariable create None module_name torch nn attention use private version sdpa_kernel accepts variadic arguments since dynamo reconstructs contents target_values one-by-one fn_name _sdpa_kernel_variadic FxTracebackAnnotateVariable ContextWrappingVariable fx traceback annotate context manager allows users annotate fx graph nodes custom metadata In context Dynamo we don t have trace body context manager Instead we want directly run body context manager so Dynamo created Fx graphs have right custom metadata This variable tracker just runs __enter__ __exit__ method instead tracing __init__ target_values initial_values=None kwargs - None super __init__ target_values=target_values initial_values=initial_values kwargs enter tx args Run annotation ctx manager eager Also ensure preserve_node_meta context manager setup This important pass metadata create_proxy nodes stack = ExitStack stack enter_context torch fx traceback annotate target_values stack enter_context torch fx traceback preserve_node_meta set_cleanup_hook tx lambda stack close variables ConstantVariable create None module_name torch fx traceback fn_name annotate reconstruct_type codegen PyCodegen unimplemented_v gb_type= torch fx traceback annotate escaped compiled region context=str explanation= Dynamo doesn t support graph break torch fx traceback annotate hints= graph_break_hints SUPPORTABLE DynamoConfigPatchVariable ContextWrappingVariable represents torch _dynamo patch_dynamo_config NOTE no need guard dynamo config because dynamo config should affect soundness though may affect tracing behavior __init__ target_values kwargs - None target_values = tuple target_values items super __init__ target_values= target_values initial_values=None kwargs initial_values = key _ target_values initial_values key = torch _dynamo config __getattr__ key initial_values = tuple initial_values items _call_func tx InstructionTranslator values assert len values == value = values manually patch dynamo config key val value torch _dynamo config __setattr__ key val No need keep track global side effects because dynamo will properly restore context manager unsupported instructions continuation functions Dynamo config also should affect semantics compiled graph module_name torch _dynamo fn_name patch_dynamo_config ErrorOnGraphBreakVariable ContextWrappingVariable represents torch _dynamo error_on_graph_break __init__ error_on_graph_break kwargs - None super __init__ target_values= error_on_graph_break initial_values= _get_error_on_graph_break kwargs _call_func tx InstructionTranslator values assert len values == _set_error_on_graph_break values module_name torch _dynamo fn_name error_on_graph_break WithEnterFunctionVariable VariableTracker __init__ ctx Union ContextWrappingVariable GenericContextWrappingVariable kwargs - None super __init__ kwargs ctx = ctx call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker assert args assert kwargs NOTE we assume instruction immediately after current CALL instruction first instruction block tx enter_ctx ctx tx current_instruction reconstruct codegen PyCodegen try type_str = f ctx module_name ctx fn_name except NotImplementedError type_str = str type ctx unimplemented_v gb_type= Attempted reconstruct context manager s __enter__ method context=str ctx explanation=f Attempted reconstruct context manager type_str while tracing ` ` hints= It likely there graph break while tracing ` ctx ` outside actual ` ctx __enter__ ` method ` torch compile ` does expect happen graph_break_hints DIFFICULT graph_break_hints DYNAMO_BUG WithExitFunctionVariable VariableTracker _nonvar_fields = target VariableTracker _nonvar_fields __init__ ctx Union ContextWrappingVariable GenericContextWrappingVariable target kwargs - None super __init__ kwargs assert isinstance ctx ContextWrappingVariable GenericContextWrappingVariable ctx = ctx target = target call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker assert kwargs ctx exit tx args reconstruct codegen PyCodegen Note here we reconstruct context manager rather than exit function The handler generated BlockStackEntry will re-enter context resume function ctx reconstruct_type codegen codegen tx output partial_convert sys version_info = codegen append_output create_instruction PUSH_NULL sys version_info codegen append_output create_instruction SWAP arg= codegen extend_output codegen create_load_const val val ctx target_values codegen extend_output create_call_function len ctx target_values False codegen append_output create_setup_with target codegen append_output create_instruction POP_TOP