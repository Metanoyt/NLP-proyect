mypy allow-untyped-defs warnings abc ABC abstractmethod collections abc Iterable typing Optional Union torch torch distributed dist torch distributed algorithms model_averaging utils utils torch utils _typing_utils not_none _not_none __all__ = ModelAverager PeriodicModelAverager ModelAverager ABC r Base all model averagers Args process_group The process group used all-reduce If ` ` None ` ` default process group which created func ` torch distributed init_process_group ` will used default ` ` None ` ` __init__ process_group Optional dist ProcessGroup = None process_group = process_group process_group None _not_none dist group WORLD step = abstractmethod average_parameters params raise NotImplementedError PeriodicModelAverager ModelAverager r Averages parameters periodically after warm-up stage This can used running ` post-local SGD https arxiv org abs ` _ running ` ~torch nn DistributedDataParallel ` DDP using subgroups created meth ` ~torch distributed new_subgroups ` Args period int The number steps per model averaging Usually period should greater than ` ` ` ` reduce communication cost Otherwise only DDP needs used warmup_steps int The number warm-up steps During stage model averaging skipped process_group The process group used all-reduce If ` ` None ` ` default process group which created func ` torch distributed init_process_group ` will used default ` ` None ` ` Example xdoctest +SKIP undefined variables torch torch distributed dist torch distributed algorithms ddp_comm_hooks post_localSGD_hook post_localSGD torch distributed algorithms model_averaging averagers averagers torch nn nn dist init_process_group nccl rank=rank world_size= torch cuda set_device rank module = nn Linear bias=False cuda model = nn parallel DistributedDataParallel module device_ids= rank output_device=rank Register post-localSGD communication hook state = PostLocalSGDState process_group=None subgroup=None start_localSGD_iter= model register_comm_hook state post_localSGD_hook In first steps run global gradient averaging like normal DDP every step After steps run model averaging every steps Note ` ` warmup_steps ` ` must same ` ` start_localSGD_iter ` ` used ` ` PostLocalSGDState ` ` averager = averagers PeriodicModelAverager period= warmup_steps= step range optimizer zero_grad loss = loss_fn output labels loss backward optimizer step Will average model parameters globally every steps Thus inter-node communication only occurs every iterations after initial ` ` warmup_steps ` ` period averager average_parameters model parameters __init__ period warmup_steps= process_group Optional dist ProcessGroup = None super __init__ process_group warmup_steps raise ValueError Arg ` ` warmup_steps ` ` must non-negative number warmup_steps = warmup_steps period raise ValueError Arg ` ` period ` ` must positive value period == warnings warn When period no need use model averaging because communication cost all-reducing parameters will no less than cost all-reducing gradients DistributedDataParallel backward pass Therefore only DistributedDataParallel should used case stacklevel= period = period average_parameters params Union Iterable torch nn Parameter Iterable dict str torch nn Parameter Averages parameters parameter groups optimizer ` ` step ` ` no less than ` ` warmup_steps ` ` Can divided ` ` period ` ` where ` ` step ` ` increased each iteration training loop Args params The parameters model parameter groups optimizer step = warmup_steps step - warmup_steps period == utils average_parameters_or_parameter_groups params _not_none process_group step +=