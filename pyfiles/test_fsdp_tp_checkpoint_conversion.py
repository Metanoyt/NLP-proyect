Owner s oncall distributed torch torch distributed checkpoint dist_cp torch distributed _shard sharded_tensor ShardedTensor torch distributed _state_dict_utils _all_gather_sharded_tensor torch distributed device_mesh init_device_mesh torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp fully_sharded_data_parallel StateDictType torch distributed tensor DTensor Replicate torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase MLPModule skip_if_lt_x_gpu with_comms torch testing _internal distributed checkpoint_utils with_temp_dir TODO modularize test add test checkpoint conversion both direction TestFsdpTpCheckpointConversion DTensorTestBase with_comms skip_if_lt_x_gpu with_temp_dir test_fsdp_to_tp CHECKPOINT_DIR = temp_dir model = MLPModule device_type rank create FSDP wrapped model fsdp_model = FSDP model use_orig_params=True FSDP set_state_dict_type fsdp_model StateDictType SHARDED_STATE_DICT fsdp_state_dict = fsdp_model state_dict save fsdp_state_dict storage dist_cp save state_dict=fsdp_state_dict storage_writer=dist_cp FileSystemWriter CHECKPOINT_DIR create TP wrapped model mesh_shape = world_size device_mesh = init_device_mesh device_type mesh_shape model = MLPModule device_type rank Parallelize module based given Parallel Style parallelize_plan = net ColwiseParallel net RowwiseParallel tp_model = parallelize_module model device_mesh parallelize_plan optimizer = torch optim SGD tp_model parameters lr= Update parameters so tp_model state_dict will different fsdp_model state_dict torch manual_seed inp = torch rand rank output = tp_model inp output sum backward optimizer step tp_state_dict = tp_model state_dict Check parameters indeed different prior loading fsdp_item tp_item zip fsdp_state_dict items tp_state_dict items fsdp_k fsdp_v = fsdp_item tp_k tp_v = tp_item assertEqual fsdp_k tp_k isinstance fsdp_v ShardedTensor isinstance tp_v DTensor fsdp_redistributed = _all_gather_sharded_tensor fsdp_v tp_redistributed = tp_v redistribute device_mesh placements= Replicate to_local assertNotEqual fsdp_redistributed tp_redistributed dist_cp load state_dict=tp_state_dict storage_reader=dist_cp FileSystemReader CHECKPOINT_DIR tp_model load_state_dict tp_state_dict Check parameters equal after loading fsdp_item tp_item zip fsdp_state_dict items tp_state_dict items fsdp_k fsdp_v = fsdp_item tp_k tp_v = tp_item assertEqual fsdp_k tp_k isinstance fsdp_v ShardedTensor isinstance tp_v DTensor fsdp_redistributed = _all_gather_sharded_tensor fsdp_v tp_redistributed = tp_v redistribute device_mesh placements= Replicate to_local assertEqual fsdp_redistributed tp_redistributed __name__ == __main__ run_tests