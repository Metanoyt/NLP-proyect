Owner s oncall distributed contextlib copy functools itertools unittest collections defaultdict collections abc Callable Iterable typing Any Optional Union torch torch distributed dist torch nn nn torch distributed _composable checkpoint replicate torch distributed algorithms _checkpoint checkpoint_wrapper _CHECKPOINT_PREFIX apply_activation_checkpointing torch distributed device_mesh DeviceMesh torch distributed fsdp CPUOffloadPolicy FSDPModule fully_shard OffloadPolicy register_fsdp_forward_method share_comm_ctx torch distributed fsdp _fully_shard _fsdp_collectives foreach_all_gather foreach_reduce torch distributed tensor DTensor init_device_mesh Shard torch distributed tensor debug CommDebugMode torch testing _internal common_distributed skip_if_lt_x_gpu skip_if_rocm_arch_multiprocess torch testing _internal common_fsdp check_sharded_parity compiled_fsdp_test FSDPTest FSDPTestMultiThread MLP MLPStack patch_all_gather patch_foreach_all_gather patch_foreach_reduce patch_reduce_scatter torch testing _internal common_utils get_cycles_per_ms MI _ARCH run_tests TEST_HPU TEST_XPU wrapSwapTensorsTest xfailIf torch testing _internal distributed _tensor common_dtensor ModelArgs Transformer TransformerBlock c d_ops = torch ops c d funcol = torch ops c d_functional torch testing _internal common_fsdp get_devtype device_type = torch device get_devtype TestFullyShardForwardInputs FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu test_root_move_forward_input_to_device device = torch device device_type type ParamlessModule nn Module forward x torch Tensor ys tuple torch Tensor Check FSDP moved inputs GPU including recursing into tuple data structure assert x device == device f Expects device got x device assert ys device == device f Expects device got ys device assert ys device == device f Expects device got ys device y = ys + ys x + y + model = ParamlessModule device fully_shard model device x = torch randn ys = torch randn torch randn assertEqual x device torch device cpu assertEqual ys device torch device cpu assertEqual ys device torch device cpu model x ys TestFullyShardRegisteredParams FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu test_param_registration_after_forward Tests parameter registration after forward device = torch device device_type type Single FSDP group reshard_after_forward True False None torch manual_seed model = MLP device Since seed per process per thread we broadcast ensure same parameters across ranks param model parameters dist broadcast param src= ref_model = copy deepcopy model fully_shard model reshard_after_forward=reshard_after_forward root only inp = torch randn device=device_type type _assert_dtensor_params model parameters _assert_same_params model parameters ref_model parameters model inp reshard_after_forward _assert_dtensor_params model parameters _assert_tensor_params model parameters _assert_same_params model parameters ref_model parameters model reshard however we can manually reshard _assert_dtensor_params model parameters _assert_same_params model parameters ref_model parameters Multiple FSDP groups reshard_after_forward True False None torch manual_seed model = nn Sequential MLP device MLP device param model parameters dist broadcast param src= ref_model = copy deepcopy model fully_shard model in_proj reshard_after_forward=reshard_after_forward fully_shard model out_proj reshard_after_forward=reshard_after_forward fully_shard model reshard_after_forward=reshard_after_forward _assert_dtensor_params model parameters _assert_same_params model parameters ref_model parameters model inp non_root_params = list model in_proj parameters + list model out_proj parameters root_params = list set model parameters - set non_root_params reshard_after_forward None _assert_dtensor_params non_root_params _assert_tensor_params root_params reshard_after_forward _assert_dtensor_params non_root_params _assert_dtensor_params root_params _assert_tensor_params non_root_params _assert_tensor_params root_params _assert_same_params model parameters ref_model parameters module model modules isinstance module FSDPModule module reshard however we can manually reshard _assert_dtensor_params model parameters _assert_same_params model parameters ref_model parameters skip_if_lt_x_gpu test_param_registration_after_backward Tests parameter registration after backward device = torch device device_type type Single FSDP group reshard_after_forward True False model = MLP device fully_shard model reshard_after_forward=reshard_after_forward root only inp = torch randn device=device_type type _assert_dtensor_params model parameters model inp sum backward _assert_dtensor_params model parameters Multiple FSDP groups reshard_after_forward True False model = MLP device fully_shard model in_proj reshard_after_forward=reshard_after_forward fully_shard model out_proj reshard_after_forward=reshard_after_forward fully_shard model reshard_after_forward=reshard_after_forward _assert_dtensor_params model parameters model inp sum backward _assert_dtensor_params model parameters _assert_tensor_params params Iterable nn Parameter need iterate over list multiple times params = list params assertGreater len params param params assertNotIsInstance param DTensor assertIsInstance param torch Tensor _assert_dtensor_params params Iterable nn Parameter params = list params assertGreater len params param params assertIsInstance param DTensor _assert_same_params params Iterable nn Parameter ref_params Iterable nn Parameter params ref_params = list params list ref_params assertEqual len params len ref_params param ref_param zip params ref_params isinstance param DTensor param = param full_tensor assertEqual param shape ref_param shape assertEqual param ref_param TestFullyShardCastAfterInit FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu wrapSwapTensorsTest True test_to_float _after_init Tests user can cast module float after init NOTE Test fp instead lower precision dtype like bf better numerics The important part changing dtype torch manual_seed mlp_dim device dtype = device_type torch float model = MLP mlp_dim device=device param model parameters dist broadcast param src= ref_model = copy deepcopy model dtype replicate ref_model ref_optim = torch optim Adam ref_model parameters lr= e- module model in_proj model out_proj model fully_shard module model dtype param model parameters assertEqual param dtype dtype assertEqual param to_local dtype dtype assertEqual param _spec tensor_meta dtype dtype optim = torch optim Adam model parameters lr= e- foreach=True check_sharded_parity ref_model model torch manual_seed + rank + inp = torch randn mlp_dim device=device_type type dtype=dtype iter_idx range losses list torch Tensor = _model ref_model model losses append _model inp sum losses - backward assertEqual losses losses check_sharded_parity ref_model model param model parameters assertEqual param dtype dtype assertEqual param to_local dtype dtype assertEqual param _spec tensor_meta dtype dtype assertEqual param grad dtype dtype assertEqual param grad to_local dtype dtype assertEqual param grad _spec tensor_meta dtype dtype _optim ref_optim optim _optim step _optim zero_grad set_to_none= iter_idx == TestFullyShard DTrainingCore FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_train_parity_single_group_shard_dim Tests train parity DDP single FSDP group when sharding parameters dim- run_subtests lin_shapes use_shard_placement_fn False _test_train_parity_single_group skip_if_lt_x_gpu test_train_parity_single_group_shard_largest_dim Tests train parity DDP single FSDP group when sharding parameters their largest dim run_subtests Sharding nonzero dim requires even sharding lin_shapes use_shard_placement_fn True _test_train_parity_single_group _test_train_parity_single_group lin_shapes list tuple int int use_shard_placement_fn bool torch manual_seed model = nn Sequential nn Linear lin_shapes nn ReLU nn Linear lin_shapes ref_model = copy deepcopy model device_type replicate ref_model device_ids= rank ref_optim = torch optim Adam ref_model parameters lr= e- _shard_placement_fn param nn Parameter - Optional Shard Shard param shape index max param shape shard_placement_fn = _shard_placement_fn use_shard_placement_fn None fully_shard model shard_placement_fn=shard_placement_fn optim = torch optim Adam model parameters lr= e- torch manual_seed + rank + inp = torch randn lin_shapes device=device_type type iter_idx range losses list torch Tensor = _model _optim ref_model ref_optim model optim _optim zero_grad set_to_none= iter_idx == losses append _model inp sum losses - backward _optim step assertEqual losses losses skip_if_lt_x_gpu unittest skipIf TEST_HPU TEST_XPU Sleep kernel supported HPU XPU compiled_fsdp_test compile_compute_on_module=Transformer test_train_parity_multi_group Tests train parity against DDP when using multiple parameter groups communication communication computation overlap plus memory reduction run_subtests reshard_after_forward True False test_device_type device_type type offload_policy OffloadPolicy delay_after_forward False True delay_before_all_gather False True delay_before_reduce_scatter False True delay_before_optim False True unshard_async_op False _test_train_parity_multi_group skip_if_lt_x_gpu unittest skipIf TEST_HPU TEST_XPU sleep kernel supported HPU XPU test_train_parity_multi_group_cpu_offload_eager Tests train parity against DDP when using multiple parameter groups communication CPU offloading run_subtests reshard_after_forward True save CI time offload_policy CPUOffloadPolicy pin_memory=True CPUOffloadPolicy pin_memory=False test_device_type device_type type delay_after_forward False True delay_before_all_gather False True delay_before_reduce_scatter False True delay_before_optim False True unshard_async_op False _test_train_parity_multi_group skip_if_lt_x_gpu unittest skipIf TEST_HPU TEST_XPU sleep kernel supported HPU XPU compiled_fsdp_test compile_compute_on_module=Transformer test_train_parity_multi_group_unshard_async_op Tests train parity against DDP when using multiple parameter groups communication setting ` ` unshard_async_op=True ` ` run_subtests reshard_after_forward True test_device_type device_type type offload_policy OffloadPolicy delay_after_forward False True delay_before_all_gather False True delay_before_reduce_scatter False True delay_before_optim False True unshard_async_op True _test_train_parity_multi_group _test_train_parity_multi_group reshard_after_forward Union bool int offload_policy OffloadPolicy test_device_type str delay_after_forward bool delay_before_all_gather bool delay_before_reduce_scatter bool delay_before_optim bool unshard_async_op bool Only test individual delays all four delays save test time delay_after_forward + delay_before_all_gather + delay_before_reduce_scatter + delay_before_optim assert test_device_type cuda hpu xpu cpu f test_device_type torch manual_seed vocab_size = model_args = ModelArgs n_layers= n_heads= vocab_size=vocab_size max_seq_len= dropout_p= model = Transformer model_args ref_model = copy deepcopy model test_device_type == device_type type replicate ref_model device_type device_ids= rank gloo_pg = dist new_group backend= gloo replicate ref_model process_group=gloo_pg ref_optim = torch optim Adam ref_model parameters lr= e- mesh = init_device_mesh test_device_type world_size fully_shard_fn = functools partial fully_shard mesh=mesh reshard_after_forward=reshard_after_forward offload_policy=offload_policy module model modules isinstance module TransformerBlock fully_shard_fn module fully_shard_fn model unshard_async_op model _set_unshard_async_op unshard_async_op optim = torch optim Adam model parameters lr= e- delay_in_ms = orig_all_gather = dist all_gather_into_tensor orig_reduce_scatter = dist reduce_scatter_tensor delayed_all_gather args kwargs torch get_device_module device_type _sleep int delay_in_ms get_cycles_per_ms orig_all_gather args kwargs delayed_reduce_scatter args kwargs torch get_device_module device_type _sleep int delay_in_ms get_cycles_per_ms orig_reduce_scatter args kwargs torch manual_seed + rank + patch_all_gather_ctx = patch_all_gather delayed_all_gather delay_before_all_gather contextlib nullcontext patch_reduce_scatter_ctx = patch_reduce_scatter delayed_reduce_scatter delay_before_reduce_scatter contextlib nullcontext patch_all_gather_ctx patch_reduce_scatter_ctx iter_idx range inp = torch randint vocab_size device=device_type losses list torch Tensor = _model _optim ref_model ref_optim model optim _optim zero_grad set_to_none= iter_idx == losses append _model inp sum _model model delay_after_forward torch get_device_module test_device_type _sleep int delay_in_ms get_cycles_per_ms losses - backward _model model delay_before_optim torch get_device_module test_device_type _sleep int delay_in_ms get_cycles_per_ms _optim step assertEqual losses losses skip_if_lt_x_gpu unittest skipIf TEST_XPU Sleep supported XPU test_non_root_forward_backward Tests running forward backward through root then through non-root The non-root needs synchronize streams queue callback torch manual_seed lin_dim = model = nn Sequential MLP lin_dim torch device cpu _ range ref_model = copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters lr= e- mlp model fully_shard mlp fully_shard model optim = torch optim Adam model parameters lr= e- foreach=True torch manual_seed + rank inp = torch randn lin_dim device=device_type ref_root_loss = ref_model inp sum ref_root_loss backward param ref_model parameters dist all_reduce param grad param grad detach div_ world_size ref_optim step ref_optim zero_grad ref_nonroot_loss = ref_model inp sum ref_nonroot_loss backward param ref_model parameters param grad None dist all_reduce param grad param grad detach div_ world_size ref_optim step root_loss = model inp sum root_loss backward torch get_device_module device_type _sleep int get_cycles_per_ms optim step optim zero_grad nonroot_loss = model inp sum nonroot_loss backward optim step assertEqual ref_root_loss root_loss assertEqual ref_nonroot_loss nonroot_loss assertEqual ref_model inp sum model inp sum skip_if_lt_x_gpu test_multi_forward_module Tests parity DDP when running module participates multiple times forward run_subtests reshard_after_forward True False _test_multi_forward_module _test_multi_forward_module reshard_after_forward Union bool int MultiForwardModule nn Module __init__ device torch device super __init__ inner = nn Linear device=device outer = nn Linear device=device forward x i = inner x j = inner x outer i + j torch manual_seed model = MultiForwardModule device=device_type type ref_model = copy deepcopy model replicate ref_model device_ids= rank ref_optim = torch optim Adam ref_model parameters lr= e- fully_shard model inner fully_shard model optim = torch optim Adam model parameters lr= e- torch manual_seed + rank inp = torch randn device=device_type type iter_idx range losses list torch Tensor = _model _optim ref_model ref_optim model optim _optim zero_grad set_to_none= iter_idx == losses append _model inp sum losses - backward _optim step assertEqual losses losses skip_if_lt_x_gpu test_explicit_prefetching torch manual_seed model_args = ModelArgs n_layers= dropout_p= model = Transformer model_args ref_model = replicate copy deepcopy model device_type ref_optim = torch optim AdamW ref_model parameters lr= e- layer itertools chain model layers model fully_shard layer optim = torch optim AdamW model parameters lr= e- num_to_forward_prefetch = num_to_backward_prefetch = i layer enumerate model layers i = len model layers - num_to_forward_prefetch break layers_to_prefetch = model layers i + j j range num_to_forward_prefetch + layer set_modules_to_forward_prefetch layers_to_prefetch i layer enumerate model layers i num_to_backward_prefetch continue layers_to_prefetch = model layers i - j j range num_to_backward_prefetch + layer set_modules_to_backward_prefetch layers_to_prefetch torch manual_seed + rank inp = torch randint model_args vocab_size device=device_type type _ range losses list torch Tensor = _model _optim ref_model ref_optim model optim _optim zero_grad losses append _model inp sum losses - backward _optim step assertEqual losses losses skip_if_lt_x_gpu unittest skipIf TEST_HPU TEST_XPU Sleep supported HPU XPU test_post_optim_event torch manual_seed model_args = ModelArgs dropout_p= model = Transformer model_args ref_model = replicate copy deepcopy model device_type type ref_optim = torch optim AdamW ref_model parameters lr= e- layer itertools chain model layers model fully_shard layer optim = torch optim AdamW model parameters lr= e- step_post_hook fsdp_module FSDPModule opt torch optim Optimizer args kwargs - None post_optim_event = torch get_device_module device_type current_stream record_event fsdp_module set_post_optim_event post_optim_event optim register_step_post_hook functools partial step_post_hook model torch manual_seed + rank inp = torch randint model_args vocab_size device=device_type type Track all losses check equality end avoid CPU sync point after each iteration ref_losses list torch Tensor = losses list torch Tensor = _ range ref_optim zero_grad ref_losses append ref_model inp sum ref_losses - backward ref_optim step _ range optim zero_grad losses append model inp sum losses - backward optim step Sleep after optimizer step allow CPU run ahead into next iteration s forward exercising post-optim stream sync torch get_device_module device_type _sleep int get_cycles_per_ms ref_loss loss zip ref_losses losses assertEqual ref_loss loss TestFullyShard DTrainingCompose FSDPTest property world_size - int Since these tests run larger transformer model they may see some numeric drift GPUs min torch get_device_module device_type device_count skip_if_lt_x_gpu compiled_fsdp_test compile_compute_on_module=Transformer xfailIf TEST_XPU https github com intel torch-xpu-ops issues test_train_parity_with_activation_checkpointing Tests train parity against DDP when composing activation checkpointing run_subtests reshard_after_forward True False checkpoint_impl composable utils wrapper module_grouping block mem_eff mem_eff_weight_tied _test_train_parity_with_activation_checkpointing _test_train_parity_with_activation_checkpointing reshard_after_forward Union bool int checkpoint_impl str module_grouping str assert checkpoint_impl composable utils wrapper testing_compile = fully_shard = torch distributed fsdp fully_shard testing_compile checkpoint_impl == composable torch manual_seed vocab_size = torch device device_type model_args = ModelArgs n_layers= n_heads= vocab_size=vocab_size max_seq_len= dropout_p= checkpoint_activations= checkpoint_impl == utils For mem-efficient module grouping we separate embeddings output projection which does support weight tying weight_tying=module_grouping = mem_eff model = Transformer model_args ref_model = replicate copy deepcopy model device_ids= rank ref_optim = torch optim Adam ref_model parameters lr= e- Apply activation checkpointing prefixes_to_ignore = checkpoint_impl == wrapper prefixes_to_ignore = _CHECKPOINT_PREFIX apply_activation_checkpointing model check_fn=lambda m isinstance m TransformerBlock checkpoint_impl == composable module model modules isinstance module TransformerBlock checkpoint module Apply FSDP fsdp_kwargs = reshard_after_forward reshard_after_forward module_grouping == mem_eff assert model_args n_layers == fully_shard model layers fsdp_kwargs fully_shard model layers model layers fsdp_kwargs fully_shard model tok_embeddings model pos_embeddings fsdp_kwargs Embedding weights needed embedding backward model tok_embeddings set_unshard_in_backward False fully_shard model norm model output fsdp_kwargs module_grouping == mem_eff_weight_tied fully_shard model tok_embeddings model output fsdp_kwargs layer model layers fully_shard layer fsdp_kwargs module_grouping == block layer model layers fully_shard layer fsdp_kwargs raise NotImplementedError f Unknown module grouping module_grouping fully_shard model fsdp_kwargs optim = torch optim Adam model parameters lr= e- torch manual_seed + rank Reuse same input across iterations avoid loss explosion trying learn random inputs inp = torch randint vocab_size device=device_type type check_sharded_parity ref_model model prefixes_to_ignore=prefixes_to_ignore iter_idx range losses list torch Tensor = _model ref_model model torch manual_seed iter_idx + dropout determinism losses append _model inp sum losses - backward testing_compile check_sharded_parity ref_model model prefixes_to_ignore=prefixes_to_ignore assertEqual losses losses _optim ref_optim optim _optim step _optim zero_grad set_to_none= iter_idx == testing_compile check_sharded_parity ref_model model prefixes_to_ignore=prefixes_to_ignore TestFullyShardShardPlacementFnMultiProcess FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_train_parity_shard_placement_fn_shard_largest_dim torch manual_seed model_args = ModelArgs n_layers= dropout_p= model = Transformer model_args ref_model = copy deepcopy model device_type ref_optim = torch optim AdamW ref_model parameters lr= e- shard_placement_fn param nn Parameter - Optional Shard Shard param shape index max param shape layer model layers fully_shard layer shard_placement_fn=shard_placement_fn fully_shard model shard_placement_fn=shard_placement_fn optim = torch optim AdamW model parameters lr= e- param ref_param zip model parameters ref_model parameters full_param = param full_tensor assertEqual full_param ref_param torch manual_seed + rank inp = torch randint model_args vocab_size device=device_type type _ range ref_loss = ref_model inp sum loss = model inp sum assertEqual ref_loss loss ref_loss backward loss backward param ref_model parameters param grad None dist all_reduce param grad op=dist ReduceOp AVG ref_optim step optim step ref_optim zero_grad optim zero_grad param ref_param zip model parameters ref_model parameters full_param = param full_tensor assertEqual full_param ref_param TestFullyShardShardPlacementFnMultiThread FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu test_shard_placement_fn_contiguous_params_grads dim = model = MLP dim=dim shard_placement_fn param nn Parameter - Optional Shard param ndim Shard Shard fully_shard model in_proj shard_placement_fn=shard_placement_fn fully_shard model out_proj shard_placement_fn=shard_placement_fn fully_shard model shard_placement_fn=shard_placement_fn assert_contiguous_params module nn Module args Any param module parameters assertTrue param is_contiguous model in_proj register_forward_pre_hook assert_contiguous_params model out_proj register_forward_pre_hook assert_contiguous_params param model parameters assertTrue param is_contiguous assertTrue param to_local is_contiguous inp = torch randn dim device=device_type type model inp sum backward param model parameters assertTrue param is_contiguous assertTrue param to_local is_contiguous assertTrue param grad is_contiguous assertTrue param grad to_local is_contiguous TestFullyShardSharedParams FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_train_parity_with_shared_params run_subtests reshard_after_forward False True use_activation_checkpointing False True _test_train_shared_params _test_train_shared_params reshard_after_forward bool use_activation_checkpointing bool torch manual_seed model_args = ModelArgs n_layers= dropout_p= weight_tying=True model = Transformer model_args ref_model = copy deepcopy model device_type replicate ref_model device_ids= rank ref_optim = torch optim Adam ref_model parameters lr= e- module model modules isinstance module TransformerBlock use_activation_checkpointing checkpoint module fully_shard module reshard_after_forward=reshard_after_forward fully_shard model reshard_after_forward=reshard_after_forward optim = torch optim Adam model parameters lr= e- torch manual_seed + rank + iter_idx range inp = torch randint model_args vocab_size device=device_type type losses list torch Tensor = _model _optim ref_model ref_optim model optim _optim zero_grad set_to_none= iter_idx == losses append _model inp sum losses - backward _optim step assertEqual losses losses TestFullyShardGradientAccumulation FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_gradient_accumulation Tests gradient accumulation without gradient reduction without resharding after backward meshes = init_device_mesh device_type type world_size always test FSDP world_size == test HSDP too enough GPUs shard_size replicate_size = meshes append init_device_mesh device_type type replicate_size shard_size mesh_dim_names= dp_replicate dp_shard run_subtests mesh meshes reshard_after_forward True False all disable reduce-scatter all modules root_only disable reduce-scatter root s linear only some_mlps disable reduce-scatter some MLPs mode all root_only some_mlps reshard_after_backward False True offload_policy OffloadPolicy CPUOffloadPolicy For HSDP only ` True ` reduce-scatter only no all-reduce each microbatch until last microbatch ` False ` neither reduce-scatter nor all-reduce each microbatch until last microbatch reduce_scatter_only False True _test_gradient_accumulation _test_gradient_accumulation mesh DeviceMesh reshard_after_forward Union bool int mode str reshard_after_backward bool offload_policy OffloadPolicy reduce_scatter_only bool HSDP reshard_after_backward reshard_after_forward False mode == some_mlps isinstance offload_policy CPUOffloadPolicy reshard_after_forward True mesh ndim = reduce_scatter_only skip since common applicable torch manual_seed batch_size lin_dim num_mlps num_microbatches = mode == some_mlps num_mlps_to_disable_reduce_scatter = modules = nn Linear lin_dim lin_dim modules extend MLP lin_dim _ range num_mlps model = nn Sequential modules ref_model = copy deepcopy model device_type fully_shard_fn = functools partial fully_shard mesh=mesh reshard_after_forward=reshard_after_forward offload_policy=offload_policy mlp model fully_shard_fn mlp fully_shard_fn model root gets st linear ref_optim = torch optim Adam ref_model parameters lr= e- optim = torch optim Adam model parameters lr= e- set_grad_sync_flag module nn Module is_last_microbatch bool recurse bool = True reduce_scatter_only module set_requires_all_reduce is_last_microbatch recurse=recurse module set_requires_gradient_sync is_last_microbatch recurse=recurse set_backward_flags _model nn Module is_last_microbatch bool mode == all set_grad_sync_flag _model is_last_microbatch reshard_after_backward _model set_reshard_after_backward is_last_microbatch mode == some_mlps mlp model + num_mlps_to_disable_reduce_scatter set_grad_sync_flag mlp is_last_microbatch reshard_after_backward mlp set_reshard_after_backward is_last_microbatch mode == root_only set_grad_sync_flag model is_last_microbatch recurse=False reshard_after_backward model set_reshard_after_backward is_last_microbatch recurse=False torch manual_seed + rank + iter_idx range comm_count_list = microbatch_idx range num_microbatches is_last_microbatch = microbatch_idx == num_microbatches - set_backward_flags model is_last_microbatch inp = torch randn batch_size lin_dim device=device_type type losses list torch Tensor = _model ref_model model CommDebugMode comm_mode losses append _model inp sum losses - backward comm_count_list append comm_mode get_comm_counts assertEqual losses losses comm_counts = defaultdict int comm_count_dict comm_count_list collective count comm_count_dict items comm_counts collective += count all_gather_count = comm_counts c d_ops _allgather_base_ reduce_scatter_count = comm_counts c d_ops _reduce_scatter_base_ all_reduce_count = comm_counts c d_ops allreduce_ Expect one reduce-scatter per MLP plus one root s linear last microbatch expected_reduce_scatter_count = num_mlps + mode == some_mlps Expect additional reduce-scatters non-disabled MLPs root s linear expected_reduce_scatter_count += num_mlps - num_mlps_to_disable_reduce_scatter + num_microbatches - mode == root_only Expect additional reduce-scatters all MLPs expected_reduce_scatter_count += num_mlps num_microbatches - expected_all_reduce_count = expected_reduce_scatter_count mesh ndim == reduce_scatter_only Specially HSDP only reduce-scattering all-reducing until last microbatch expect one reduce-scatter per MLP plus root per microbatch expected_reduce_scatter_count = num_mlps + num_microbatches assertEqual reduce_scatter_count expected_reduce_scatter_count assertEqual all_reduce_count expected_all_reduce_count Expect one all-gather per MLP plus one root s linear first microbatch s forward expected_all_gather_count = num_mlps + reshard_after_forward False ` True ` ` ` expected_all_gather_count += num_mlps + Multiply number microbatches since these all-gathers run every microbatch expected_all_gather_count = num_microbatches reshard_after_backward ` reshard_after_forward=False ` expected_all_gather_count = num_microbatches mode == all ` reshard_after_forward backward=False ` Only reshard parameters after last microbatch s backward so there should any more all-gathers pass mode == root_only ` reshard_after_forward backward=False ` The MLPs should still contribute all-gathers each microbatch forward expected_all_gather_count += num_mlps num_microbatches - assertEqual all_gather_count expected_all_gather_count param ref_model parameters param grad None dist all_reduce param grad op=dist ReduceOp AVG check_sharded_parity ref_model model _optim optim ref_optim _optim step When ` set_to_none=False ` we exercising mixing gradient accumulation without communication _optim zero_grad set_to_none= iter_idx skip_if_lt_x_gpu test_ f b_microbatching run_subtests use_explicit_unshard False True reshard_after_backward False True _test_ f b_microbatching _test_ f b_microbatching use_explicit_unshard bool reshard_after_backward bool torch manual_seed model_args = ModelArgs dropout_p= model = Transformer model_args ref_model = copy deepcopy model device_type ref_optim = torch optim AdamW ref_model parameters lr= e- module model modules isinstance module TransformerBlock fully_shard module reshard_after_forward=False fully_shard model reshard_after_forward=False optim = torch optim AdamW model parameters lr= e- num_microbatches = local_batch_size = torch manual_seed + rank + inps = torch randint model_args vocab_size local_batch_size device=device_type type _ range num_microbatches Before pipelining we may prefer issue all all-gathers ahead time increase overlap opportunity no difference parameter memory usage since we do reshard after forward use_explicit_unshard module model modules isinstance module FSDPModule module unshard async_op=True Emulate f b pipeline schedule only reduce gradients last microbatch losses list torch Tensor = ref_losses list torch Tensor = inp_idx inp enumerate inps is_last_microbatch = inp_idx == num_microbatches - model set_requires_gradient_sync is_last_microbatch model set_is_last_backward is_last_microbatch reshard_after_backward model set_reshard_after_backward is_last_microbatch losses append model inp sum losses - backward ref_losses append ref_model inp sum ref_losses - backward param ref_model parameters dist all_reduce param grad op=dist ReduceOp AVG loss ref_loss zip losses ref_losses assertEqual loss ref_loss optim step ref_optim step check_sharded_parity ref_model model TestFullyShardNDTraining FSDPTest property world_size - int min torch get_device_module device_type device_count init_global_mesh - DeviceMesh Prefer test = GPUs GPUs use -way TP dp_size = world_size pp_size = world_size init_device_mesh device_type type pp_size dp_size world_size dp_size pp_size mesh_dim_names= pp dp tp skip_if_rocm_arch_multiprocess MI _ARCH skip_if_lt_x_gpu test_ d_mlp_with_nd_mesh global_mesh = init_global_mesh run_subtests reshard_after_forward False True use_activation_checkpointing False True mlp_dim foreach False functools partial _test_ d_mlp_with_nd_mesh global_mesh _test_ d_mlp_with_nd_mesh global_mesh DeviceMesh reshard_after_forward bool use_activation_checkpointing bool mlp_dim int foreach bool global_mesh = init_global_mesh _ dp_mesh tp_mesh = global_mesh pp global_mesh dp global_mesh tp dp_pg = dp_mesh get_group used ` replicate ` torch manual_seed model = MLPStack mlp_dim ref_model = copy deepcopy model device_type replicate ref_model device_ids= rank process_group=dp_pg ref_optim = torch optim Adam ref_model parameters lr= e- foreach=foreach model parallelize tp_mesh dp_mesh use_activation_checkpointing reshard_after_forward=reshard_after_forward optim = torch optim Adam model parameters lr= e- foreach=foreach torch manual_seed + dp_pg rank + device = device_type iter_idx range inp = torch randn mlp_dim device=device losses list torch Tensor = _model _optim ref_model ref_optim model optim _optim zero_grad set_to_none= iter_idx == losses append _model inp sum losses - backward _optim step assertEqual losses losses _ p model named_parameters assertIsInstance p DTensor assertEqual p device_mesh ndim assertEqual len p placements assertEqual p device_mesh mesh_dim_names dp tp TestFullyShardHSDP DTraining FSDPTest property world_size - int min torch get_device_module device_type device_count init_global_mesh - DeviceMesh init_device_mesh device_type type mesh_dim_names= dp_replicate dp_shard tp skip_if_lt_x_gpu test_ d_mlp_with_nd_mesh global_mesh = init_global_mesh run_subtests reshard_after_forward False True use_activation_checkpointing False True mlp_dim foreach False functools partial _test_ d_mlp_with_nd_mesh global_mesh _test_ d_mlp_with_nd_mesh global_mesh DeviceMesh reshard_after_forward bool use_activation_checkpointing bool mlp_dim int foreach bool global_mesh = init_global_mesh dp_mesh tp_mesh = global_mesh dp_replicate dp_shard global_mesh tp dp_pg = dp_mesh _flatten get_group used ` replicate ` torch manual_seed model = MLPStack mlp_dim ref_model = copy deepcopy model device_type replicate ref_model device_ids= rank process_group=dp_pg ref_optim = torch optim Adam ref_model parameters lr= e- foreach=foreach model parallelize tp_mesh dp_mesh use_activation_checkpointing reshard_after_forward=reshard_after_forward Checking parameters match orig model critical validate full_tensor correctly replicates strided-sharded layers ref_p p zip ref_model parameters model parameters assertIsInstance p DTensor assertEqual ref_p p full_tensor optim = torch optim Adam model parameters lr= e- foreach=foreach torch manual_seed + dp_pg rank + device = device_type iter_idx range inp = torch randn mlp_dim device=device losses list torch Tensor = _model _optim ref_model ref_optim model optim _optim zero_grad set_to_none= iter_idx == losses append _model inp sum losses - backward _optim step assertEqual losses losses _ p model named_parameters assertIsInstance p DTensor assertEqual p device_mesh ndim assertEqual len p placements assertEqual p device_mesh mesh_dim_names dp_replicate dp_shard tp TestFullyShardHSDPTraining FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_train_parity_hsdp shard_size = world_size replicate_size = world_size shard_size global_mesh = init_device_mesh device_type type replicate_size shard_size mesh_dim_names= dp_replicate dp_shard run_subtests reshard_after_forward False True use_activation_checkpointing False True mlp_dim sync_gradients_at_last_batch True False offload_policy CPUOffloadPolicy pin_memory=True CPUOffloadPolicy pin_memory=False functools partial _test_train_parity_hsdp global_mesh _test_train_parity_hsdp global_mesh DeviceMesh reshard_after_forward bool use_activation_checkpointing bool mlp_dim int sync_gradients_at_last_batch bool offload_policy CPUOffloadPolicy torch manual_seed model = nn Sequential nn LayerNorm mlp_dim bias=False MLP mlp_dim dim_multiplier= MLP mlp_dim MLP mlp_dim dim_multiplier= ref_model = copy deepcopy model device_type replicate ref_model device_ids= rank ref_optim = torch optim Adam ref_model parameters lr= e- mlp model use_activation_checkpointing checkpoint mlp fully_shard mlp mesh=global_mesh reshard_after_forward=reshard_after_forward offload_policy=offload_policy fully_shard model mesh=global_mesh reshard_after_forward=reshard_after_forward offload_policy=offload_policy optim = torch optim Adam model parameters lr= e- check_sharded_parity ref_model model torch manual_seed + rank + device = device_type num_microbatches = iter_idx range microbatch_idx range num_microbatches is_last_microbatch = microbatch_idx == num_microbatches - sync_gradients_at_last_batch model set_requires_gradient_sync is_last_microbatch inp = torch randn mlp_dim device=device losses list torch Tensor = _model _optim ref_model ref_optim model optim losses append _model inp sum losses - backward assertEqual losses losses check_sharded_parity ref_model model _model _optim ref_model ref_optim model optim _optim step _optim zero_grad set_to_none= iter_idx == check_sharded_parity ref_model model TestFullyShardCustomForwardMethod FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_register_fsdp_forward_method Based https github com pytorch pytorch issues VisionTransformer nn Module __init__ - None super __init__ patch_proj = nn Conv d kernel_size= stride= forward_features imgs torch Tensor - torch Tensor patch_proj imgs flatten transpose forward imgs torch Tensor - torch Tensor forward_features imgs sum dim= Model nn Module __init__ - None super __init__ vit projector = VisionTransformer nn Linear forward imgs torch Tensor - torch Tensor Run ` vit forward_features ` which ` forward ` patch_embeddings = vit forward_features imgs projector patch_embeddings torch manual_seed model = Model ref_model = copy deepcopy model device_type fully_shard model vit fully_shard model projector fully_shard model register_fsdp_forward_method model vit forward_features torch manual_seed + rank + inp = torch randn device=device_type type ref_loss = ref_model inp sum loss = model inp sum assertEqual ref_loss loss ref_loss backward loss backward param ref_model parameters dist all_reduce param grad op=dist ReduceOp AVG check_sharded_parity ref_model model TestFullyShardShareCommContext FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_share_comm_context torch manual_seed n_layers = lin_dim = model = nn Sequential MLP lin_dim torch device cpu _ range n_layers ref_model = copy deepcopy model device_type layer model fully_shard layer layer _get_fsdp_state _lazy_init share_comm_ctx list model torch manual_seed + rank + inp = torch randn lin_dim device=device_type type ref_loss = ref_model inp sum all_gather_streams = set reduce_scatter_streams = set torch distributed fsdp _fully_shard _fsdp_api AllGather ReduceScatter torch distributed fsdp _fully_shard _fsdp_param FSDPParam orig_foreach_all_gather = foreach_all_gather foreach_all_gather_with_assert fsdp_params list FSDPParam group dist ProcessGroup async_op bool all_gather_copy_in_stream torch Stream all_gather_stream torch Stream device torch device all_gather_comm AllGather nonlocal all_gather_streams all_gather_streams add all_gather_stream orig_foreach_all_gather fsdp_params group async_op all_gather_copy_in_stream all_gather_stream device all_gather_comm orig_foreach_reduce = foreach_reduce torch no_grad foreach_reduce_with_assert fsdp_params list FSDPParam unsharded_grads list torch Tensor reduce_scatter_group dist ProcessGroup reduce_scatter_stream torch Stream reduce_scatter_comm ReduceScatter orig_dtype Optional torch dtype reduce_dtype Optional torch dtype device torch device gradient_divide_factor Optional float all_reduce_group Optional dist ProcessGroup ` None ` iff HSDP all_reduce_stream torch Stream all_reduce_grads bool partial_reduce_output Optional torch Tensor only used HSDP all_reduce_hook Optional Callable torch Tensor None force_sum_reduction_for_comms bool = False nonlocal reduce_scatter_streams reduce_scatter_streams add reduce_scatter_stream orig_foreach_reduce fsdp_params unsharded_grads reduce_scatter_group reduce_scatter_stream reduce_scatter_comm orig_dtype reduce_dtype device gradient_divide_factor all_reduce_group all_reduce_stream all_reduce_grads partial_reduce_output all_reduce_hook force_sum_reduction_for_comms patch_foreach_all_gather foreach_all_gather_with_assert patch_foreach_reduce foreach_reduce_with_assert loss = model inp sum assertEqual ref_loss loss ref_loss backward loss backward param ref_model parameters dist all_reduce param grad op=dist ReduceOp AVG assertEqual len all_gather_streams assertEqual len reduce_scatter_streams check_sharded_parity ref_model model TestFullyShardWorldSize FSDPTest property world_size - int skip_if_lt_x_gpu test_train_parity_single_worldsize Tests train parity DDP single FSDP group when sharding parameters dim- run_subtests lin_shapes use_shard_placement_fn False _test_train_parity_single_group _test_train_parity_single_group lin_shapes list tuple int int use_shard_placement_fn bool torch manual_seed model = nn Sequential nn Linear lin_shapes nn ReLU nn Linear lin_shapes ref_model = copy deepcopy model device_type replicate ref_model device_ids= rank ref_optim = torch optim Adam ref_model parameters lr= e- _shard_placement_fn param nn Parameter - Optional Shard Shard param shape index max param shape shard_placement_fn = _shard_placement_fn use_shard_placement_fn None fully_shard model shard_placement_fn=shard_placement_fn optim = torch optim Adam model parameters lr= e- torch manual_seed + rank + inp = torch randn lin_shapes device=device_type type iter_idx range losses list torch Tensor = ref_optim zero_grad set_to_none= iter_idx == losses append ref_model inp sum losses - backward ref_optim step optim zero_grad set_to_none= iter_idx == comm_mode = CommDebugMode comm_mode losses append model inp sum losses - backward assertEqual comm_mode get_total_counts optim step assertEqual losses losses __name__ == __main__ run_tests