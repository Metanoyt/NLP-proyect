mypy allow-untyped-defs __future__ annotations logging typing Optional TYPE_CHECKING TypedDict torch torch _inductor codegen rocm ck_conv_template CKGroupedConvFwdTemplate config ir lowering add_layout_constraint constrain_to_fx_strides lowerings L register_lowering select_algorithm autotune_select_algorithm ExternKernelChoice SymbolicGridFn TritonTemplate utils is_ones is_zeros pad_listlike sympy_product use_ck_conv_template use_triton_template virtualized V TYPE_CHECKING collections abc Sequence ir TensorBox log = logging getLogger __name__ aten = torch ops aten SymbolicGridFn conv d_grid n c h w meta cdiv cdiv n h w meta BLOCK_M cdiv c meta BLOCK_N meta GROUPS SymbolicGridFn conv d_grid n c d h w meta cdiv cdiv n d h w meta BLOCK_M cdiv c meta BLOCK_N meta GROUPS LOOP_BODY_ D = idx_x_h = i - PADDING_H + idx_y_h STRIDE_H idx_x_w = j - PADDING_W + idx_y_w STRIDE_W idx_x_c = tl arange BLOCK_K + k x_ptrs = x_base + idx_x_h stride_xh None + idx_x_w stride_xw None + idx_x_c stride_xc None mask_x = idx_n BATCH None idx_x_h = None idx_x_h IN_H None idx_x_w = None idx_x_w IN_W None idx_x_c GROUP_IN_C None matrix_x = tl load x_ptrs mask=mask_x other= w_ptrs = w_base + idx_x_c stride_wc_in None + i stride_wh + j stride_ww mask_w = idx_x_c None GROUP_IN_C idx_y_c None GROUP_OUT_C matrix_w = tl load w_ptrs mask=mask_w other= acc += tl dot matrix_x matrix_w allow_tf =ALLOW_TF This relatively simple conv implementation can likely improved Many alternate conv versions can found here https github com pytorch torchdynamo pull conv d_template = TritonTemplate name= convolution d grid=conv d_grid source=r def_kernel X W Tensor dimensions BATCH = size X IN_C = size X IN_H = size X IN_W = size X OUT_C = size None OUT_H = size None OUT_W = size None Strides stride_xn = stride X stride_xc = stride X stride_xh = stride X stride_xw = stride X stride_wc_out = stride W stride_wc_in = stride W stride_wh = stride W stride_ww = stride W nhw = tl program_id INDEX_DTYPE BLOCK_M + tl arange BLOCK_M idx_y_w = nhw OUT_W nh = nhw OUT_W idx_y_h = nh OUT_H idx_n = nh OUT_H idx_y_c = tl program_id INDEX_DTYPE BLOCK_N + tl arange BLOCK_N GROUPS == group = GROUP_IN_C = IN_C GROUP_OUT_C = OUT_C group = tl program_id INDEX_DTYPE GROUP_IN_C = IN_C GROUPS GROUP_OUT_C = OUT_C GROUPS endif x_base = X + group stride_xc GROUP_IN_C + idx_n stride_xn None w_base = W + group stride_wc_out GROUP_OUT_C + idx_y_c stride_wc_out None acc = tl zeros BLOCK_M BLOCK_N dtype=tl float UNROLL i range KERNEL_H j range KERNEL_W i = i j = j k range GROUP_IN_C BLOCK_K + LOOP_BODY_ D + endfor endfor Could simplified slightly slower i range KERNEL_H j range KERNEL_W k range GROUP_IN_C BLOCK_K BLOCK_K_COUNT = GROUP_IN_C + BLOCK_K - BLOCK_K ijk range KERNEL_H KERNEL_W BLOCK_K_COUNT k = ijk BLOCK_K_COUNT BLOCK_K ij = ijk BLOCK_K_COUNT i = ij KERNEL_W j = ij KERNEL_W + LOOP_BODY_ D + endif mask = idx_n BATCH None idx_y_h OUT_H None idx_y_w OUT_W None idx_y_c GROUP_OUT_C None idx_n = idx_n None idx_c = idx_y_c None + group GROUP_OUT_C idx_h = idx_y_h None idx_w = idx_y_w None inductor generates suffix store_output idx_n idx_c idx_h idx_w acc mask val_shape= BLOCK_M BLOCK_N LOOP_BODY_ D = idx_x_d = d - PADDING_D + idx_y_d STRIDE_D idx_x_h = i - PADDING_H + idx_y_h STRIDE_H idx_x_w = j - PADDING_W + idx_y_w STRIDE_W idx_x_c = tl arange BLOCK_K + k x_ptrs = x_base + idx_x_d stride_xd None + idx_x_h stride_xh None + idx_x_w stride_xw None + idx_x_c stride_xc None mask_x = idx_n BATCH None idx_x_d = None idx_x_d IN_D None idx_x_h = None idx_x_h IN_H None idx_x_w = None idx_x_w IN_W None idx_x_c GROUP_IN_C None matrix_x = tl load x_ptrs mask=mask_x other= w_ptrs = w_base + idx_x_c stride_wc_in None + d stride_wd + i stride_wh + j stride_ww mask_w = idx_x_c None GROUP_IN_C idx_y_c None GROUP_OUT_C matrix_w = tl load w_ptrs mask=mask_w other= acc += tl dot matrix_x matrix_w allow_tf =ALLOW_TF conv d_template = TritonTemplate name= convolution d grid=conv d_grid source=r def_kernel X W Tensor dimensions BATCH = size X IN_C = size X IN_D = size X IN_H = size X IN_W = size X OUT_C = size None OUT_D = size None OUT_H = size None OUT_W = size None Strides stride_xn = stride X stride_xc = stride X stride_xd = stride X stride_xh = stride X stride_xw = stride X stride_wc_out = stride W stride_wc_in = stride W stride_wd = stride W stride_wh = stride W stride_ww = stride W ndhw = tl program_id INDEX_DTYPE BLOCK_M + tl arange BLOCK_M idx_y_w = ndhw OUT_W ndh = ndhw OUT_W idx_y_h = ndh OUT_H nd = ndh OUT_H idx_y_d = nd OUT_D idx_n = nd OUT_D idx_y_c = tl program_id INDEX_DTYPE BLOCK_N + tl arange BLOCK_N GROUPS == group = GROUP_IN_C = IN_C GROUP_OUT_C = OUT_C group = tl program_id INDEX_DTYPE GROUP_IN_C = IN_C GROUPS GROUP_OUT_C = OUT_C GROUPS endif x_base = X + group stride_xc GROUP_IN_C + idx_n stride_xn None w_base = W + group stride_wc_out GROUP_OUT_C + idx_y_c stride_wc_out None acc = tl zeros BLOCK_M BLOCK_N dtype=tl float UNROLL d range KERNEL_D i range KERNEL_H j range KERNEL_W d = d i = i j = j k range GROUP_IN_C BLOCK_K + LOOP_BODY_ D + endfor endfor endfor Could simplified slightly slower d range KERNEL_D i range KERNEL_H j range KERNEL_W k range GROUP_IN_C BLOCK_K BLOCK_K_COUNT = GROUP_IN_C + BLOCK_K - BLOCK_K dijk range KERNEL_D KERNEL_H KERNEL_W BLOCK_K_COUNT k = dijk BLOCK_K_COUNT BLOCK_K dij = dijk BLOCK_K_COUNT j = dij KERNEL_W di = dij KERNEL_W i = di KERNEL_H d = di KERNEL_H + LOOP_BODY_ D + endif mask = idx_n BATCH None idx_y_d OUT_D None idx_y_h OUT_H None idx_y_w OUT_W None idx_y_c GROUP_OUT_C None idx_n = idx_n None idx_c = idx_y_c None + group GROUP_OUT_C idx_d = idx_y_d None idx_h = idx_y_h None idx_w = idx_y_w None inductor generates suffix store_output idx_n idx_c idx_d idx_h idx_w acc mask val_shape= BLOCK_M BLOCK_N aten_convolution = ExternKernelChoice torch convolution convolution has_out_variant=False op_overload=aten convolution default conv x _via_mm x w out w = torch squeeze torch squeeze w - - torch matmul x permute w permute out=out permute aten_conv x _via_mm = ExternKernelChoice conv x _via_mm None ConvLayoutParams TypedDict stride tuple int padding tuple int dilation tuple int transposed bool output_padding tuple int groups int conv_layout x TensorBox weight TensorBox bias Optional TensorBox stride Sequence int padding tuple int dilation tuple int transposed bool output_padding tuple int groups int - ir Layout Determine output layout convolution V graph fake_mode output = torch ops aten convolution ir ir_node_to_tensor x guard_shape=True ir ir_node_to_tensor weight guard_shape=True ir ir_node_to_tensor bias guard_shape=True V graph sizevars size_hints stride type ignore arg-type V graph sizevars size_hints padding type ignore arg-type V graph sizevars size_hints dilation type ignore arg-type transposed V graph sizevars size_hints output_padding type ignore arg-type groups sizes = ir convert_shape_to_inductor output size stride = ir convert_shape_to_inductor output stride type ignore assignment ir FixedLayout x get_device_or_error x get_dtype sizes stride channels_last_order rank order = list reversed range rank order insert order pop - order convert_ x _conv_to_mm x weight bias special case x convolution which actually just matmul rank = len weight get_size _ range rank - weight = L aten squeeze weight dim=- weight = L aten permute weight x = ir ExternKernel require_stride_order x channels_last_order rank x_permute = list range rank x_permute append x_permute pop x = L aten permute x x_permute sizes in_chan = x get_size x = L aten reshape x sympy_product sizes in_chan bias None result = L aten mm x weight result = L aten addmm bias x weight result = L aten reshape result sizes - result_permute = list range rank result_permute insert result_permute pop - L aten permute result result_permute register_lowering aten convolution convolution x TensorBox weight TensorBox bias Optional TensorBox stride Sequence int padding Sequence int dilation Sequence int transposed bool output_padding Sequence int groups int stride = tuple stride padding = tuple padding dilation = tuple dilation output_padding = tuple output_padding isinstance groups int groups = V graph sizevars guard_int groups assert isinstance groups int Need use hint triton template since template does work dynamic shape No need guard_int dilation output_padding since template only used when dilation output_padding stride = tuple V graph sizevars guard_int_seq stride padding = tuple V graph sizevars guard_int_seq padding kwargs ConvLayoutParams = stride stride padding padding dilation dilation transposed transposed output_padding output_padding groups groups device_type = ir get_device_type x len x get_size == len weight get_size - add batch dimension simplify rest function L aten squeeze convolution L aten expand x x get_size weight bias kwargs dim= out_chan in_chan kernel_shape = V graph sizevars guard_int_seq weight get_size Always convert conv D D Intel GPU Only conv D can converted channel last layout which have much better performance len x get_size == len kernel_shape == device_type == xpu kwargs update stride + stride padding + padding dilation + dilation output_padding + output_padding N C L - N C L x = L aten unsqueeze x dim= weight = L aten unsqueeze weight dim= L aten squeeze convolution x weight bias kwargs dim= ndim = len kernel_shape stride = pad_listlike stride ndim padding = pad_listlike padding ndim dilation = pad_listlike dilation ndim output_padding = pad_listlike output_padding ndim channels_last_conv V graph layout_opt ndim == True layout = conv_layout x weight None kwargs req_stride_order = ir get_stride_order V graph sizevars size_hints layout stride req_stride_order == ir NHWC_STRIDE_ORDER autotuning_gemm = config max_autotune config max_autotune_gemm config conv_ x _as_mm autotuning_gemm channels_last_conv is_ones kernel_shape is_ones stride is_zeros padding is_ones dilation transposed is_zeros output_padding groups == V graph sizevars statically_known_gt sympy_product x get_size convert_ x _conv_to_mm x weight bias bias None device_type = cpu peel off bias cudnn slower result = convolution x weight None kwargs L aten add result L aten view bias result get_size + ndim x realize weight realize ndim can convolution models such demucs TODO check s beneficial convert Conv d Conv d then apply channels last V graph layout_opt ndim == V graph num_channels_last_conv += x = ir ExternKernel require_channels_last x type ignore assignment TODO maybe we can convert weights channels last just once before running model weight = ir ExternKernel require_channels_last weight type ignore assignment layout = conv_layout x weight None kwargs layout = conv_layout x weight None kwargs req_stride_order = ir get_stride_order V graph sizevars size_hints layout stride x = ir ExternKernel require_stride_order x req_stride_order type ignore assignment weight = ir ExternKernel require_stride_order weight req_stride_order type ignore assignment ordered_kwargs_for_cpp_kernel = stride padding dilation transposed output_padding groups bias None args = x weight kwargs bias = None type ignore typeddict-unknown-key ordered_kwargs_for_cpp_kernel insert bias args = x weight bias bias realize bias freeze_layout V graph sizevars guard_int_seq bias get_size choices = torch _inductor utils _use_conv_autotune_backend ATEN choices = aten_convolution bind args layout ordered_kwargs_for_cpp_kernel kwargs torch _inductor utils _use_conv_autotune_backend TRITON use_triton_template layout templates only support these is_ones dilation transposed is_zeros output_padding there some odd models where check fails e g shufflenet_v _x _ V graph sizevars statically_known_equals in_chan groups x get_size type ignore arg-type is_ones kernel_shape is_ones stride is_zeros padding groups == choices append aten_conv x _via_mm bind args layout conv_configs = V choices get_conv_configs device_type dtype_size = x get_dtype itemsize cfg conv_configs sympy_product x get_size x get_size out_chan in_chan dtype_size=dtype_size ndim == conv d_template maybe_append_choice choices input_nodes= x weight layout=layout KERNEL_H=kernel_shape KERNEL_W=kernel_shape STRIDE_H=stride STRIDE_W=stride PADDING_H=padding PADDING_W=padding GROUPS=groups TODO jansel try unroll bigger kernels once fixed https github com triton-lang triton issues UNROLL=is_ones kernel_shape ALLOW_TF =torch backends cudnn allow_tf num_stages=cfg num_stages num_warps=cfg num_warps cfg kwargs ndim == conv d_template maybe_append_choice choices input_nodes= x weight layout=layout KERNEL_D=kernel_shape KERNEL_H=kernel_shape KERNEL_W=kernel_shape STRIDE_D=stride STRIDE_H=stride STRIDE_W=stride PADDING_D=padding PADDING_H=padding PADDING_W=padding GROUPS=groups TODO jansel try unroll bigger kernels once fixed https github com triton-lang triton issues UNROLL=is_ones kernel_shape ALLOW_TF =torch backends cudnn allow_tf num_stages=cfg num_stages num_warps=cfg num_warps cfg kwargs use_ck_conv_template layout CKGroupedConvFwdTemplate add_ck_conv_choices choices layout input_nodes= x weight + bias bias None tuple stride=stride padding=padding dilation=dilation groups=groups n_spatial_dimensions=ndim autotune_select_algorithm convolution choices args layout register_lowering aten _convolution _convolution x weight bias stride padding dilation transposed output_padding groups benchmark deterministic cudnn_enabled allow_tf convolution x weight bias stride padding dilation transposed output_padding groups constrain_conv_to_fx_strides fx_node args kwargs assert fx_node target torch ops aten convolution default V graph layout_opt args kwargs constrain_to_fx_strides fx_node args kwargs add_layout_constraint aten convolution constrain_conv_to_fx_strides