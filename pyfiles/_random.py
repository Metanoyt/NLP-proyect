mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates contextlib warnings logging getLogger typing Optional Union torch torch distributed device_mesh _get_device_handle DeviceMesh torch distributed tensor _dtensor_spec DTensorSpec torch distributed tensor placement_types Shard logger = getLogger __name__ __all__ = is_rng_supported_mesh manual_seed OffsetBasedRNGTracker _rng_tracker Optional _RNGStateTracker = None is_rng_supported_mesh device_mesh DeviceMesh - bool Checks current device ` ` device_mesh ` ` supports DTensor s random APIs Currently DTensor Random APIs only supports cuda cuda-like devices We suggest users call API test availability before using our random APIs Args device_mesh ` DeviceMesh ` The device mesh which we check random ops APIs supported Returns A bool value True ` ` device_mesh ` ` supports DTensor Random APIs False otherwise warning Currently we only support correct RNG cuda cuda-like devices device_handle = _get_device_handle device_mesh device_type device_handle hasattr device_handle set_rng_state True TODO Logs way too much warnings warn f DTensor random operators may have complete support device_mesh device_type device mesh stacklevel= False manual_seed seed int device_mesh DeviceMesh - None Sets seed generating random numbers calling rank Args seed int The desired seed device_mesh ` DeviceMesh ` The device mesh set seed It required ` ` device_mesh ` ` include calling rank This ensure SPMD region maintains synchronous RNG state which means no ranks should initialized values other than ` ` seed ` ` Returns None warning func ` manual_seed ` does check ` ` seed ` ` value correctness Users must ensure their own value passed desired ` ` seed ` ` ranks within ` ` device_mesh ` ` If ` ` device_mesh ` ` sub-mesh calling rank part ` ` manual_seed ` ` will throw error Current implementation only supports GPU device mesh is_rng_supported_mesh device_mesh warnings warn DTensor manual_seed may have complete support f device_mesh device_type device mesh stacklevel= TODO deprecate API also need ensure we disable broadcast PP case s currently bundled together API See torchtitan distributed utils py set_determinism warnings warn DTensor manual_seed deprecated since DTensor no longer maintains separate copy generator state Use ` torch manual_seed ` instead Note we still need ensure setting ` run_state_sync=False ` support pp case instantiate RNG tracker haven t By default DTensor uses OffsetBasedRNGTracker perform random operators global _rng_tracker _rng_tracker _rng_tracker = OffsetBasedRNGTracker device_mesh run_state_sync=False device_mesh get_coordinate None raise RuntimeError manual_seed requires current rank part device mesh otherwise DTensor RNG state rank will initialized behavior DTensor random ops undefined DTensor no longer maintains copy rng state manual seed dtensor same thing manual seed torch torch manual_seed seed _PhiloxState Convenience accessor interpreting packed bits seed uint offset uint philox state which some reason actually exposed size- uint tensor The state always moved cpu since necessary CPU before applying back generator __init__ state torch Tensor _state = state cpu property state _state property offset - int int _state view dtype=torch int item offset setter offset offset int - None offset_tensor = torch tensor offset dtype=torch uint device= cpu view torch uint _state = offset_tensor property seed - int int _state view dtype=torch int item seed setter seed seed int - None seed_tensor = torch tensor seed dtype=torch uint device= cpu view torch uint _state = seed_tensor _RNGStateTracker _RNGStateTracker stores Random Number Generator RNG state ByteTensor object dict mapping corresponding tag each state tensor It also provides set convenient utility methods help access modify state tensors The most important interface _distribute_region which will used when DTensor executes random op operator calls RNG __init__ device torch device pyrefly ignore read-only _device = device _device_handle = _get_device_handle _device type _device_handle _device_handle is_available raise RuntimeError f __class__ __name__ instantiation requires presence f device type device couldn t find _use_distribute_region = True property distribute_region_enabled - bool _use_distribute_region distribute_region_enabled setter distribute_region_enabled value - None _use_distribute_region = value _distribute_region spec DTensorSpec generator Optional torch Generator = None pass _manual_seed parallel_seed int - None pass OffsetBasedRNGTracker _RNGStateTracker This subclass ` ` _RNGStateTracker ` ` defines default policy how RNG states should shared synchronized among all ranks respect semantics DTensor random operators note _RNGStateTracker only supports cuda cuda-like device __init__ device_mesh DeviceMesh run_state_sync bool = True super __init__ _resolve_device device_mesh=device_mesh assert _device_handle None DTensor RNG tracker so far only supports CUDA CUDA-like devices _device type == cpu raise RuntimeError f __class__ __name__ instantiation requires presence f CUDA CUDA-like XPU device Got _device type instead rng_state = _get_device_state run_state_sync synchronize RNG state using rank s current one torch distributed broadcast rng_state my_rng_state = _get_device_state all my_rng_state == rng_state logger warning DTensor synchronizing RNG states every rank state rank This behavior deprecated Please call ` torch manual_seed ` every rank participates SPMD DTensor Operations same seed If using Pipeline Parallelism each pipeling state would use different seed all ranks belonging one pipeline stage would use same seed _set_device_state rng_state _get_device_state - torch Tensor _device type == hpu _device_handle set_rng_ctx philox rng_state = _device_handle get_rng_state _device _device type == hpu _device_handle unset_rng_ctx philox rng_state _set_device_state state torch Tensor It seems underlying generator wants cpu tensor dtensor code expects ` _get_device_state ` convert device tensor probably because we may use our backend comms sync debug now we just convert back cpu here make sure always works _device type == hpu _device_handle set_rng_ctx philox _device_handle set_rng_state state cpu _device type == hpu _device_handle unset_rng_ctx philox contextlib contextmanager _distribute_region spec DTensorSpec generator Optional torch Generator = None generator None This little hacky any user-passed generator we store its state under unique key because we need keep copy because its easiest way make work existing set get APIs We also ensure we remove rng_states after each _distribute_region state = _PhiloxState generator get_state state = _PhiloxState _get_device_state distribute_region_enabled _device type == hpu _device_handle set_rng_ctx philox old_offset = state offset _set_pre_op_offset state spec torch random fork_rng devices= _device device_type=self _device type assert _device_handle None _device_handle set_rng_state state state try yield execute region code finally update offset synchronize among ranks _set_post_op_offset state spec old_offset _device type == hpu _device_handle unset_rng_ctx philox yield generator None ensure we propagate state advancement back user s RNG so its visible impacts any future usage RNG dtensor non-dtensor b drop our own cache so user updates seed value their rng uses DTensor again we always use latest value generator set_state state state _set_device_state state state _set_pre_op_offset state _PhiloxState spec DTensorSpec - None Set starting RNG offset current device s local shard before actual op execution The pre_op_offset value should start current RNG offset increment size local shard until reaches size whole DTensor For different ranks hold same DTensor shard their pre_op_offset will same Args state ` Tensor ` The generator state modify spec ` DTensorSpec ` spec DTensor object which we prepare offset running random ops Returns None warning Note current implementation does consider DTensor s continguity Example take DTensor shape example Assume DTensor placed device mesh placements Shard Replicate Shard mesh ` ` spec mesh get_coordinate ` ` provides coordinate current rank mesh For example coordinate rank Another concept introduce besides rank coordinate shard coordinate Each rank holds local shard DTensor In example DTensor partitioned into shards The first shard has replicas rank coord rank coord have replica each That being said local shard rank rank correspond same shard DTensor To denote each DTensor shard we use shard coordinate example will tuple i j where shard i j has slice DTensor i i + j j + = i = j Once we have rank coordinate shard coordinate we can calculate each rank what shard DTensor rank holds help dim_map The dim_map above DTensor so shard coordinate rank rank coord x y z simply z x taking rank_coord dim_map rank_coord dim_map Following calculation rank rank holds shard coord rank rank holds shard coord rank rank holds shard coord rank rank holds shard coord The last value calculate before obtaining starting offset shard linear index The starting offset each rank will its shard_linear_index local_tensor_numel dtensor_shape = spec shape mesh = spec mesh note dim_map does allow double sharding which FSDP fully_shard +TP case Replace custom logic dim_map once we support dim_map list Union int list int = - spec ndim i placement enumerate spec placements isinstance placement Shard shard_dim = placement dim dim_map shard_dim == - dim_map shard_dim = i mesh_dim_list = dim_map shard_dim assert isinstance mesh_dim_list list mesh_dim_list append i Compute shard coordinate The coordinate each tensor dim tuple idx range If DTensor partitioned its dim i into n shards current rank holds j-th then its shard coordinate will idx=j range=n dim i mesh_coordinate = mesh get_coordinate assert mesh_coordinate None mesh_size = mesh shape shard_idx_by_dim = total_num_shards_by_dim = total number shards each tensor dim mesh_dim dim_map shard_idx = total_num_shards = tensor dim sharded more than mesh dim isinstance mesh_dim list rank_coord = mesh_coordinate d d mesh_dim num_shards = mesh_size d d mesh_dim compute shard idx total number shards idx size zip rank_coord num_shards shard_idx = shard_idx size + idx total_num_shards = size shard_idx_by_dim append shard_idx total_num_shards_by_dim append total_num_shards compute shard linear index shard_linear_idx = _calc_shard_linear_idx shard_idx_by_dim total_num_shards_by_dim compute starting offset using first shard s size local_size_on_rank_ = list dtensor_shape idx placement enumerate spec placements isinstance placement Shard mesh_dim_size = mesh size idx shard_dim = placement dim local_size_on_rank_ shard_dim _ = placement _local_shard_size_and_offset dtensor_shape shard_dim mesh_dim_size torch distributed tensor _ops utils prod local_size = prod local_size_on_rank_ get current RNG offset current_offset = state offset pytorch offset must multiple source aten src ATen cuda CUDAGeneratorImpl cpp offset_incr = shard_linear_idx local_size + state offset = current_offset + offset_incr _set_post_op_offset state _PhiloxState spec DTensorSpec old_offset int - None Sets RNG synchronized state after running local random op Every rank should set its RNG offset ` old_offset + DTensor numel ` where old_offset offset before calling ` set_pre_op_offset ` i e offset before running DTensor random ops Args state ` Tensor ` The generator state modify spec ` DTensorSpec ` spec DTensor object which we post-process offset running random ops Returns None dtensor_shape = spec shape torch distributed tensor _ops utils prod numel = prod dtensor_shape pytorch offset must multiple source aten src ATen cuda CUDAGeneratorImpl cpp numel = numel + state offset = old_offset + numel _calc_shard_linear_idx shard_coord list int shard_size list int - int compute shard linear index shard_linear_idx = shard_coord_stride = idx size zip reversed shard_coord reversed shard_size shard_linear_idx += idx shard_coord_stride shard_coord_stride = size shard_linear_idx _resolve_device device_mesh DeviceMesh - torch device device_type = device_mesh device_type device_handle = _get_device_handle device_type assert device_handle None device_idx = device_mesh get_rank device_handle device_count torch device f device_type device_idx d