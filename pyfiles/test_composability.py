Owner s oncall distributed copy torch torch nn nn torch nn functional F torch distributed device_mesh init_device_mesh torch distributed fsdp fully_shard MixedPrecisionPolicy torch distributed fsdp _fully_shard _fsdp_param ShardedState torch distributed pipelining PipelineStage torch distributed pipelining schedules _Action _ComputationType _PipelineScheduleRuntime PipelineScheduleSingle Schedule F B ScheduleGPipe ScheduleInterleaved F B ScheduleInterleavedZeroBubble ScheduleLoopedBFS torch distributed tensor DTensor torch nn parallel DistributedDataParallel DDP torch testing _internal common_cuda TEST_MULTIGPU torch testing _internal common_distributed MultiProcContinuousTest requires_nccl skip_if_lt_x_gpu torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests skip_but_pass_in_sandcastle_if TEST_WITH_ROCM device_type = cuda MLP Layer MLPModule torch nn Module __init__ d_hid int super __init__ net = torch nn Linear d_hid d_hid relu = torch nn ReLU net = torch nn Linear d_hid d_hid init_weights init_weights ensure proper init otherwise gradient tests will more likely get zero grad values torch nn init kaiming_uniform_ net weight mode= fan_in nonlinearity= relu torch nn init kaiming_uniform_ net weight mode= fan_in nonlinearity= relu forward x x = net x x = relu x x = net x x MLPModuleEven torch nn Module __init__ d_hid int super __init__ net = nn Linear d_hid d_hid net = nn Linear d_hid d_hid net = nn Linear d_hid d_hid init_weights init_weights torch nn init kaiming_uniform_ net weight mode= fan_in nonlinearity= relu torch nn init kaiming_uniform_ net weight mode= fan_in nonlinearity= relu torch nn init kaiming_uniform_ net weight mode= fan_in nonlinearity= relu forward x x = F relu net x x = F relu net x x = F relu net x x loss_fn y target scale= e- Scale loss simulate small learning rate avoid exploding grads torch nn functional cross_entropy y target scale ComposabilityTest MultiProcContinuousTest classmethod backend_str cls - str Testing NCCL backend nccl property device - torch device torch device device_type rank _rand_microbatches dp_mesh num_microbatches dim dtype=torch float full = torch rand num_microbatches dim device=self device dtype=dtype _ range dp_mesh size local = full dp_mesh get_local_rank local_mb = local i reshape dim i range num_microbatches full local local_mb build pipeline stage _build_pp_stage pp_group full_model total_layers apply_dp stage_idx num_stages divide model e g layers number stages layers_per_stage = total_layers num_stages assert layers_per_stage num_stages == total_layers offset so validation code can match partial layer back orig model offset = stage_idx layers_per_stage partial_model = nn Sequential full_model offset stage_idx + layers_per_stage partial_model device dp_model = apply_dp partial_model stage = PipelineStage dp_model stage_idx num_stages device group=pp_group stage offset _build_pp_schedule ScheduleClass num_microbatches pp_group full_model total_layers apply_dp loss_fn scale_grads=True issubclass ScheduleClass PipelineScheduleSingle pipeline_stage offset = _build_pp_stage pp_group full_model total_layers apply_dp pp_group rank pp_group size partial_models = pipeline_stage submod offsets = offset pipeline_schedule = ScheduleClass pipeline_stage n_microbatches=num_microbatches loss_fn=loss_fn scale_grads=scale_grads n_virtual = num_stages = pp_group size n_virtual stages = offsets = i range n_virtual stage offset = _build_pp_stage pp_group full_model total_layers apply_dp pp_group rank + n_virtual i num_stages stages append stage offsets append offset partial_models = pipeline_stage submod pipeline_stage stages pipeline_schedule = ScheduleClass stages n_microbatches=num_microbatches loss_fn=loss_fn scale_grads=scale_grads pipeline_schedule partial_models offsets requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if TEST_MULTIGPU Test requires + GPUs parametrize ScheduleClass ScheduleGPipe ScheduleInterleaved F B ScheduleInterleavedZeroBubble test_pp_ddp ScheduleClass ScheduleClass == ScheduleInterleavedZeroBubble TODO DDP + InterleavedZeroBubble currently supported due issue DDP reducer triggering https github com pytorch pytorch issues torch get_device_module device_type set_device device mesh_shape = world_size mesh_dim_names = dp pp device_mesh = init_device_mesh cuda mesh_shape=mesh_shape mesh_dim_names=mesh_dim_names pp_group = device_mesh pp get_group dp_mesh = device_mesh dp create entire model total_layers = num_microbatches = dim = full_model = nn ModuleList MLPModule dim _ range total_layers ref_model = nn Sequential copy deepcopy full_model ref_model device Prepare inputs inputs input_local _ = _rand_microbatches dp_mesh num_microbatches dim targets target_local _ = _rand_microbatches dp_mesh num_microbatches dim apply_dp partial_model DDP partial_model process_group=dp_mesh get_group Build pipeline stages apply data parallelism attach schedule pipeline_schedule partial_models offsets = _build_pp_schedule ScheduleClass num_microbatches pp_group full_model total_layers apply_dp loss_fn Run pipeline pp_group rank == pipeline_schedule step input_local pipeline_schedule step target=target_local Ref model runs different inputs accumulating grads across them ensures we detect DDP all-reduce becomes no-op sim_dp_rank range dp_mesh size loss_fn ref_model inputs sim_dp_rank targets sim_dp_rank backward ref_model torch float p ref_model parameters p grad = p grad torch float p grad = dp_mesh size Validate whichever weights we have locally match part our local full ref model ref_parameters = dict ref_model named_parameters partial_model offset zip partial_models offsets name p partial_model named_parameters parts = name split remove DDP module prefix FSDP doesn t have one parts = str int parts + offset name = join parts ref_p = ref_parameters name torch testing assert_close p grad ref_p grad requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if TEST_MULTIGPU Test requires + GPUs parametrize dp_type FSDP FSDP_MP parametrize ScheduleClass Schedule F B ScheduleInterleaved F B ScheduleLoopedBFS ScheduleInterleavedZeroBubble test_pp_fsdp dp_type ScheduleClass TEST_WITH_ROCM torch get_device_module device_type set_device device mesh_shape = world_size mesh_dim_names = dp pp device_mesh = init_device_mesh cuda mesh_shape=mesh_shape mesh_dim_names=mesh_dim_names pp_group = device_mesh pp get_group dp_mesh = device_mesh dp fsdp_mixed-precision dtype mp_dtype = torch bfloat dp_type == FSDP_MP torch float create entire model total_layers = num_microbatches = dim = full_model = nn ModuleList MLPModule dim _ range total_layers ref_model = nn Sequential copy deepcopy full_model ref_model device dp_type == FSDP_MP ref_model dtype=mp_dtype Prepare inputs inputs input_local _ = _rand_microbatches dp_mesh num_microbatches dim dtype=mp_dtype targets target_local _ = _rand_microbatches dp_mesh num_microbatches dim dtype=mp_dtype Apply FSDP stage module apply_dp partial_model mp_policy = MixedPrecisionPolicy param_dtype=mp_dtype reduce_dtype=torch float fsdp_config = mesh dp_mesh mp_policy mp_policy layer partial_model children fully_shard layer fsdp_config reshard_after_forward=False fully_shard partial_model fsdp_config Build pipeline stages apply data parallelism attach schedule pipeline_schedule partial_models offsets = _build_pp_schedule ScheduleClass num_microbatches pp_group full_model total_layers apply_dp loss_fn Run pipeline pp_group rank == pipeline_schedule step input_local pipeline_schedule step target=target_local m partial_models p m parameters assert p grad None introduce race condition FSDP s reduce-scatter which could corrupt gradients pipelining does properly synchronize FSDP p grad div_ p grad mul_ Ref model runs different inputs accumulating grads across them ensures we detect FSDP reduce becomes no-op fsdp case we use one these inputs each DP rank sim_dp_rank range dp_mesh size loss_fn ref_model inputs sim_dp_rank targets sim_dp_rank backward ref_model torch float p ref_model parameters p grad = p grad torch float p grad = dp_mesh size Validate whichever weights we have locally match part our local full ref model we force FSDP s grads all-gathered full_tensor make simpler ref_parameters = dict ref_model named_parameters partial_model offset zip partial_models offsets name p partial_model named_parameters parts = name split parts = str int parts + offset name = join parts ref_p = ref_parameters name assertTrue isinstance p grad DTensor torch testing assert_close p grad full_tensor ref_p grad atol= e- rtol= e- requires_nccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if TEST_MULTIGPU Test requires + GPUs parametrize dp_type FSDP FSDP_MP test_pp_fsdp_unshard_reshard_runtime dp_type Test FSDP UNSHARD RESHARD functionality using _PipelineScheduleRuntime custom schedules TEST_WITH_ROCM torch get_device_module device_type set_device device mesh_shape = world_size mesh_dim_names = dp pp device_mesh = init_device_mesh cuda mesh_shape=mesh_shape mesh_dim_names=mesh_dim_names pp_group = device_mesh pp get_group dp_mesh = device_mesh dp fsdp_mixed-precision dtype mp_dtype = torch bfloat dp_type == FSDP_MP torch float total_layers = dim = full_model = nn ModuleList MLPModule dim _ range total_layers apply_dp partial_model mp_policy = MixedPrecisionPolicy param_dtype=mp_dtype reduce_dtype=torch float fsdp_config = mesh dp_mesh mp_policy mp_policy layer partial_model children fully_shard layer fsdp_config reshard_after_forward=False fully_shard partial_model fsdp_config Build pipeline stages num_stages = pp_group size layers_per_stage = total_layers num_stages stage_idx = pp_group rank offset = stage_idx layers_per_stage partial_model = nn Sequential full_model offset stage_idx + layers_per_stage partial_model device fsdp_model = apply_dp partial_model distributed_state = fully_shard state fsdp_model distributed_state _lazy_init stage = PipelineStage fsdp_model stage_idx num_stages device group=pp_group Helper function check FSDP sharding state check_fsdp_unsharded_state module expected_unsharded=False Check FSDP parameters expected sharding state distributed_state = fully_shard state module unsharded_count = total_fsdp_params = state distributed_state _state_ctx all_states state _fsdp_param_group group = state _fsdp_param_group fsdp_param group fsdp_params total_fsdp_params += fsdp_param sharded_state == ShardedState UNSHARDED unsharded_count += expected_unsharded assertEqual unsharded_count total_fsdp_params f Expected all total_fsdp_params FSDP parameters unsharded f only unsharded_count unsharded assertEqual unsharded_count f Expected all FSDP parameters sharded f unsharded_count out total_fsdp_params unsharded total_fsdp_params Return whether we found any FSDP parameters Test initial state - should sharded has_fsdp = check_fsdp_unsharded_state stage submod expected_unsharded=False has_fsdp skipTest No FSDP parameters found model create_schedule computation_types microbatch_index=None schedule = _Action stage_index= stage only stage computation_type=comp_type microbatch_index=microbatch_index comp_type == _ComputationType FORWARD None comp_type computation_types schedule unshard_schedule = create_schedule _ComputationType UNSHARD _ComputationType FORWARD microbatch_index= unshard_reshard_schedule = create_schedule _ComputationType UNSHARD _ComputationType FORWARD _ComputationType RESHARD microbatch_index= Test Run UNSHARD + RESHARD schedule runtime = _PipelineScheduleRuntime stage n_microbatches= loss_fn=None scale_grads=False runtime pipeline_order_with_comms = unshard_reshard_schedule dummy_input = torch randn dim device=self device dtype=mp_dtype runtime step dummy_input Verify parameters now sharded again check_fsdp_unsharded_state stage submod expected_unsharded=False Test Run UNSHARD only schedule runtime pipeline_order_with_comms = unshard_schedule runtime step dummy_input Verify parameters still sharded check_fsdp_unsharded_state stage submod expected_unsharded=False instantiate_parametrized_tests ComposabilityTest __name__ == __main__ run_tests