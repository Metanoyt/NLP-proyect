__future__ annotations sympy sympy S torch _prims_common BoolLike FloatLike IntLike ` ` torch fx experimental symbolic_shapes ` ` provides interfaces interacting our symbolic shapes reasoning system used heavily torch compile Although generally considered public API when writing framework code PyTorch well extensions PyTorch e g custom operator implementations you may need make use these APIs setup dynamic shapes support appropriately abc atexit collections dis functools hashlib inspect itertools logging math operator os re sys threading traceback collections Counter defaultdict collections abc Callable Generator Iterator Mapping Sequence contextlib _GeneratorContextManager contextmanager dataclasses asdict dataclass field enum Enum typing Any cast Generic NamedTuple NoReturn Optional TYPE_CHECKING TypeAlias TypeGuard TypeVar Union typing_extensions deprecated ParamSpec torch torch fx torch fx traceback fx_traceback torch utils _pytree pytree NB The sym_ functions used via getattr must imported here torch SymBool SymFloat SymInt torch _C _functorch get_unwrapped is_batchedtensor torch _guards ShapeGuard SLoc Source TracingContext torch _logging dtrace_structured LazyString structured trace_structured torch _subclasses meta_utils is_sparse_any torch _utils_internal signpost_event torch fx experimental _config config torch fx experimental recording FakeTensorMeta record_shapeenv_event replay_shape_env_events shape_env_check_state_equal ShapeEnvEvent torch fx experimental sym_node SymNode SymTypes torch types py_sym_types torch utils _ordered_set OrderedSet torch utils _python_dispatch is_traceable_wrapper_subclass torch utils _sympy functions Application CeilToInt CleanDiv FloorDiv FloorToInt IntTrueDiv IsNonOverlappingAndDenseIndicator Max Mod PythonMod TruncToInt torch utils _sympy numbers int_oo torch utils _sympy printers CppPrinter PythonPrinter torch utils _sympy singleton_int SingletonInt torch utils _sympy solve try_solve torch utils _sympy symbol make_symbol symbol_is_type SymT torch utils _sympy value_ranges bound_sympy SymPyValueRangeAnalysis ValueRangeError ValueRanges torch utils _traceback CapturedTraceback format_frame TYPE_CHECKING types torch Tensor torch _dynamo source TensorPropertySource torch _subclasses fake_tensor FakeTensor torch types BoolLikeType FloatLikeType IntLikeType InputList = list DimList = list log = logging getLogger __name__ GuardOnDataDependentSymNode RuntimeError cond sympy Basic __init__ cond sympy Basic args Any - None super __init__ args cond = cond PendingUnbackedSymbolNotFound RuntimeError pass aten = torch _ops ops aten type ignore has-type __all__ = guard_or_false guard_or_true has_symbolic_sizes_strides create_contiguous ShapeEnv is_concrete_int is_concrete_float is_concrete_bool has_static_value guard_int guard_float guard_scalar canonicalize_bool_expr hint_int SYMPY_INTERP free_symbols is_symbol_binding_fx_node is_nested_int SHAPEENV_EVENT_KEY CURRENT_NODE_KEY has_free_symbols has_free_unbacked_symbols sym_and sym_eq sym_or SymbolicContext StatelessSymbolicContext StatefulSymbolicContext SubclassSymbolicContext SymIntSymbolicContext TrackedFake statically_known_true statically_known_false guard_size_oblivious check_consistent compute_unbacked_bindings ConvertIntKey rebind_unbacked resolve_unbacked_bindings is_accessor_node ValueRangesSLoc SymIntEqByExpr Specialization FX node metadata keys symbolic shape FX graph SHAPEENV_EVENT_KEY = shapeenv_event CURRENT_NODE_KEY = current_node log_lru_cache_stats wrapped_f functools _lru_cache_wrapper object - None log debug lru_cache_stats s s wrapped_f __name__ type ignore attr-defined wrapped_f cumulative_cache_info type ignore attr-defined Note about Sympy Expr SympyBoolean Basic typing Sympy hierarchy Basic Expr SympyBoolean Relational Notably Expr SympyBoolean related So use Basic when expression could denote int float OR bool otherwise use more specific Expr int float SympyBoolean bool In obscure Meta only situations sympy logic boolalg doesn t exist runtime So make sure only type checker evaluates alias Xref https www internalfb com diff D SympyBoolean TypeAlias = sympy logic boolalg Boolean _T = TypeVar _T _SympyT = TypeVar _SympyT sympy Expr SympyBoolean sympy Basic SymIntEqByExpr This wrapper around SymInt which has alternative semantics equality pickling Specifically instead erroring guarding we instead will hash compare equality based underlying sympy expression e g s s will always compare False NB This does NOT do fancy analysis maybe_evaluate_static does we can only reason through equalities occur because expressions canonicalize same expression via regular simplification staticmethod _extract val Union torch SymInt int - sympy Expr isinstance val torch SymInt val node expr sympy Integer val __init__ val Union torch SymInt int - None val sympy Expr = SymIntEqByExpr _extract val __repr__ - str repr val __eq__ other object - bool assert isinstance other SymIntEqByExpr val == other val __hash__ - int hash val _nested_int_aware_sort tup tuple IntLikeType int - tuple int IntLikeType int Order nested ints their coefficients here order nested ints after non-nested-ints tup node nested_int_coeff tup is_nested_int tup tup Wrapper lru_cache reports statistics process end lru_cache maxsize Optional int - Callable Callable _T functools _lru_cache_wrapper _T inner f Callable _T - functools _lru_cache_wrapper _T wrapped_f = functools lru_cache maxsize f old_cache_clear = wrapped_f cache_clear prev_hits = prev_misses = TODO There s ref-cycle here wrapped_f - cumulative_cache_info - wrapped_f cannot solved weakref wrapped_f weakref able some versions Python cumulative_cache_info - functools _CacheInfo cur = wrapped_f cache_info functools _CacheInfo prev_hits + cur hits prev_misses + cur misses cur maxsize cur currsize new_cache_clear - None nonlocal prev_hits prev_misses cur = wrapped_f cache_info prev_hits += cur hits prev_misses += cur misses old_cache_clear wrapped_f cache_clear = new_cache_clear type ignore attr-defined method-assign wrapped_f cumulative_cache_info = cumulative_cache_info type ignore attr-defined method-assign log isEnabledFor logging DEBUG atexit register log_lru_cache_stats wrapped_f type ignore arg-type wrapped_f inner These modules contain generic code interacting ShapeEnv which unlikely identify particular interesting guard statement lru_cache None uninteresting_files - set str torch _compile torch _dynamo eval_frame torch _inductor sizevars torch _library custom_ops torch _library fake_impl torch _logging torch _subclasses fake_tensor torch _subclasses meta_utils torch export _trace mods = sys modules __name__ torch export _trace torch fx experimental recording torch fx experimental sym_node torch fx interpreter torch fx _symbolic_trace torch torch _compile torch _dynamo eval_frame torch _inductor sizevars torch _library custom_ops torch _library fake_impl torch _subclasses meta_utils torch _subclasses fake_tensor torch _logging _internal torch _logging structured torch _dynamo guards inspect getfile m m mods &#124; torch _dynamo guards uninteresting_files &#124; string ConstraintViolationError RuntimeError pass has_symbolic_sizes_strides elem torch Tensor - bool elem _has_symbolic_sizes_strides Int TypeAlias = Union torch SymInt int create_contiguous shape Sequence Int - list Int strides list Int = dim reversed shape - strides append dim strides - type ignore operator list reversed strides hint_int Union torch SymInt int fallback Optional int = None - int Retrieve hint int based underlying real values observed runtime If no hint available e g because data dependent shapes fallback None use instead otherwise raise error isinstance torch SymInt node require_hint fallback assert type int Scalar TypeAlias = Union torch SymInt torch SymFloat torch SymBool int float bool has_hint Scalar - bool isinstance SymTypes node has_hint True is_concrete_int IntLikeType - bool Utility check underlying object SymInt concrete value Also returns true integer passed Args SymInt int Object test int assert isinstance SymInt int isinstance int True isinstance node expr sympy core numbers Integer True False is_concrete_float FloatLikeType - bool r Utility check underlying object SymInt concrete value Also returns true integer passed Args SymInt float Object test float assert isinstance SymFloat float isinstance float True isinstance node expr sympy core numbers Float True False is_concrete_bool BoolLikeType - bool Utility check underlying object SymBool concrete value Also returns true integer passed Args SymBool bool Object test bool assert isinstance SymBool bool isinstance bool True isinstance node expr sympy logic boolalg BooleanTrue sympy logic boolalg BooleanFalse True False has_static_value Union SymBool SymFloat SymInt bool float int - bool User-code friendly utility check value static dynamic Returns true given constant symbolic expression fixed value Args Union SymBool SymFloat SymInt bool float int Object test assert isinstance BoolLike + FloatLike + IntLike isinstance BoolLike is_concrete_bool type ignore arg-type isinstance FloatLike is_concrete_float type ignore arg-type isinstance IntLike is_concrete_int type ignore arg-type True assert isinstance py_sym_types node shape_env bound_sympy node expr is_singleton type ignore union-attr guard_size_oblivious expr Union torch SymBool bool - bool Perform guard symbolic boolean expression size oblivious way This typically used when non-oblivious test would result guard data dependent value which we don t know value compile time When guard tested way we may diverge behavior how regular PyTorch semantics would treat For more information see https github com pytorch pytorch pull isinstance expr torch SymBool expr node guard_size_oblivious assert isinstance expr bool expr expr check_consistent new _T old _T - None Test two meta values typically either Tensor SymInt have same values e g after retracing If we don t understand quantities question we ll just skip consistency check TODO do boolean equality test too see https github com pytorch pytorch issues scalar_types = torch SymInt torch SymFloat int float isinstance new torch Tensor assert isinstance old torch Tensor torch _check old dim == new dim lambda f old shape = new shape old = new Do manually so each individual test irrefutable TODO should helper maybe sym_eq That gives us compound expression I m sure simplifies right now i j zip old shape new shape torch _check i == j lambda f old shape = new shape old = new NB bool subclass int isinstance new scalar_types isinstance new bool assert isinstance old scalar_types isinstance old bool f old = new torch _check old == new lambda f old = new old = new resolve_unbacked_bindings shape_env Optional ShapeEnv bindings Optional dict sympy Symbol pytree KeyPath - Optional dict sympy Symbol pytree KeyPath When we do fake tensor prop we oftentimes will allocate new unbacked symints We then run proxy tensor mode which populates node meta unbacked_bindings these new symints To ensure consistency we use PropagateUnbackedSymInts rename unbacked bindings their old ones But all node metas still using old bindings before renaming This function helps post facto apply any renamings discovered PropogateUnbackedSymInts pass bindings None None assert shape_env None shape_env unbacked_renamings get k k v k v bindings items Result TypeAlias = Union torch Tensor tuple torch Tensor rebind_unbacked shape_env Optional ShapeEnv n torch fx Node result Result - None Suppose we retracing pre-existing FX graph previously had fake tensor propagation therefore unbacked SymInts When we retrace we re-propagate fake tensors which results new unbacked SymInts When happens we need tell shape environment about equivalence old new unbacked SymInts Pass us old torch fx Node which has old binding information new result which we can extract new unbacked SymInts out Inputs never need rebinding n op == placeholder bindings = resolve_unbacked_bindings shape_env n meta get unbacked_bindings assert shape_env None raw_u path bindings items u = pytree key_get result path Sometimes things previously unbacked bindings become constants There two situations can happen First you might have runtime assert causes constant-ification In case binding itself will still unbacked symbol because we will only force constant later fake tensor propagation In case u SymInt we still do all our work normal But second might fake tensor propagation DIRECTLY converted unbacked SymInt into constant This happens more rarely we have identified two situations can validly occur - If you have tensor_version operator these initially allocated unbacked SymInts after AOTAutograd they get forced specialized specific values In case there no reason do runtime asserts them just hack properly keep track them start - If you have item call constant tensor result item call constant we do need runtime asserts symbol In https github com pytorch pytorch issues we have case where initial trace program we unable determine torch tensor constant then subsequent passes cause torch tensor become constant then unbacked symbol goes poof In all these cases no longer necessary generate deferred runtime asserts since other subsystems e g constant-ification pass ensure quantity now truly static cannot change runtime So s OK discard these situations There one more hazard re https github com pytorch pytorch issues problem you can end up dangling unbacked symbols exist ShapeEnv never bound anywhere You might like invariant unbacked symbols never get lost But we do have invariant so do try enforce isinstance u int float log info rebind_unbacked discard s s s - s n target raw_u path u continue We only care about rebinding unbacked things u node hint None continue raw_u = u node expr Simplify SymBool binding isinstance raw_u sympy Piecewise len raw_u args == raw_u _args = cast tuple sympy Basic sympy Basic raw_u args raw_u _args == isinstance eq = raw_u _args sympy Eq isinstance new_raw_u = eq lhs sympy Symbol shape_env var_to_range new_raw_u issubset ValueRanges eq rhs == cast tuple sympy Basic sympy Basic raw_u args == True This what pattern match above testing repacked = _sympy_cast_symbool_to_symint_guardless sympy Eq new_raw_u assert repacked == raw_u f repacked = raw_u Cancel to_int to_bool x This sound because x raw_u = new_raw_u isinstance raw_u sympy Symbol assert raw_u free_symbols f should have been constant got raw_u continue The old new could same you improperly hit memo while retracing Make sure you updated FakeTensorMode epoch assert raw_u = raw_u f raw_u possible memo disaster Reuse OLD symbol name shape_env _rename_unbacked_to raw_u raw_u NB You could try expand cover more cases simply detecting whenever you have int output bit dangerous case someone adds function returns int mutating So manually whitelist now is_accessor_node node torch fx Node - bool Helper function determine node trying access symbolic integer such size stride offset item Currently primarily only used DCE pass figure out purity Dynamo only exercised condition node op == call_method isinstance node args torch fx Node isinstance node args meta get example_value torch Tensor node target size stride storage_offset item True node op == call_function node target torch ops aten sym_size torch ops aten sym_size default torch ops aten sym_size int torch ops aten sym_stride torch ops aten sym_stride default torch ops aten sym_stride int torch ops aten sym_storage_offset torch ops aten sym_storage_offset default torch ops aten sym_numel default True False canonicalize_bool_expr expr _T - _T Canonicalize boolean expression transforming into lt le inequality moving all non-constant terms rhs We canonicalize And Ors Not via cnf then canonicalize their subexpr recursively nb sympy Rel canonical good enough https github com sympy sympy issues Args expr sympy Expr Expression canonicalize Canonicalise inequality transforming into lt le inequality moving all non-constant terms rhs We canonicalise And Ors Not via cnf nb Relational canonical sympy broken https github com sympy sympy issues isinstance expr sympy Rel sympy And sympy Or sympy Not sympy Eq sympy Ne expr isinstance expr sympy And sympy Or sympy Not expr = sympy logic boolalg to_cnf expr _canonicalize_bool_expr_impl expr type ignore arg-type return-value _sympy_from_args cls type Union sympy Add sympy Mul args list sympy Expr sort bool = True is_commutative Optional bool = None - sympy Expr Create sympy expression list arguments optimizing performance This function creates sympy Add Mul expression list arguments while avoiding expensive operations like flattening It handles sorting arguments appropriately based expression type Args cls The sympy create Add Mul args List sympy expressions combine sort Whether sort arguments default True is_commutative Whether operation commutative default None Returns A sympy expression type cls combining all arguments Raises ValueError If cls sympy Add sympy Mul args cls identity type ignore union-attr These args already canonical form so we avoid calling Add args avoid expensive Add flatten operation sort cls sympy Add sort_fn = sympy core add _addsort cls sympy Mul sort_fn = sympy core mul _mulsort raise ValueError f Unknown cls cls we don t support non commutative sort assert is_commutative True args is_Number rest = args sort_fn rest cls _from_args args + rest is_commutative=is_commutative type ignore attr-defined args = args copy sort_fn args cls _from_args args is_commutative=is_commutative type ignore attr-defined args already sorted we create directly cls _from_args args is_commutative=is_commutative type ignore attr-defined _canonicalize_bool_expr_impl expr SympyBoolean - SympyBoolean After canonicalization we guaranteed have eliminated Ge Gt relations rewriting them Le Lt respectively isinstance expr sympy And sympy Or type expr map canonicalize_bool_expr expr args opposite = sympy Gt sympy Lt sympy Ge sympy Le t Union type Any isinstance expr tuple opposite keys rhs = expr lhs - expr rhs type ignore attr-defined t = opposite type expr type ignore index assert isinstance expr sympy Lt sympy Le sympy Eq sympy Ne rhs = expr rhs - expr lhs t = type expr is_neg t sympy Expr - bool t is_Number t is_negative isinstance t sympy Mul t args is_Number t args is_negative lhs = S Zero rhs = _reduce_to_lowest_terms rhs isinstance rhs sympy Add pos = neg = term rhs args is_neg term neg append -term pos append term these already sorted rhs = _sympy_from_args sympy Add pos sort=False is_commutative=True terms changed so needs sorting lhs = _sympy_from_args sympy Add neg sort=True is_commutative=True is_neg rhs lhs == lhs rhs = -rhs S Zero We don t have evaluate here because lhs rhs came Boolean already simplified t lhs rhs evaluate=False _reduce_to_lowest_terms expr sympy Expr - sympy Expr Eliminates any integer factor given expression E g x + y reduces x + y Useful when expression == = integer_coefficient x sympy Expr - int x is_Integer abs int x x is_Mul If one args Mul Integer first arg eg args x y == x y abs int x args x args is_Integer type ignore call-overload div_by_factor x sympy Expr factor int - sympy Expr x is_Integer x factor x is_Mul x args = factor args = x args sympy Integer factor x args Mul _from_args require canonical list args so we remove first arg x args factor args = list x args _sympy_from_args sympy Mul args is_commutative=x is_commutative raise AssertionError f illegal arg div_by_factor x expr is_Add atoms = cast Sequence sympy Expr expr args factor = functools reduce math gcd map integer_coefficient atoms factor == expr pyrefly ignore bad-argument-type atoms = div_by_factor x factor x atoms _sympy_from_args sympy Add atoms sort=True is_commutative=expr is_commutative expr is_Integer S One expr is_Mul div_by_factor expr integer_coefficient expr expr is_nested_int s IntLikeType - TypeGuard SymInt isinstance s torch SymInt s node is_nested_int IterateExprsAtom TypeAlias = Union SymInt SymFloat SymBool int float bool sympy Basic torch Tensor IterateExprs TypeAlias = Union IterateExprsAtom Sequence IterateExprsAtom _iterate_exprs val IterateExprs - Iterator sympy Basic Recursively iterate through value yield all sympy expressions contained within This function traverses various data structures tensors lists tuples etc extracts any symbolic expressions they contain It s used operations like finding free symbols complex nested structures Args val The value extract sympy expressions Can symbolic type SymInt SymFloat SymBool sympy expression primitive type int float bool container tuple list sparse tensor regular tensor None torch Generator Yields sympy Basic Each sympy expression found value Raises AssertionError If value unsupported type This almost close enough implement terms _iterate_nodes except needs handle ` list sympy Basic ` which _iterate_nodes can t handle isinstance val SymTypes This allow applies jagged layout NestedTensor case nested ints symbolic is_symbolic val yield val node expr isinstance val SymNode yield val expr isinstance val sympy Basic yield val isinstance val int float bool pass isinstance val tuple list s val yield _iterate_exprs s is_sparse_any val yield _iterate_exprs val size isinstance val torch Tensor yield _iterate_exprs val size yield _iterate_exprs val stride yield _iterate_exprs val storage_offset val None pass see Note Generator arguments AOTDispatcher isinstance val torch Generator pass raise AssertionError f cannot extract sympy expressions val type val _iterate_nodes val Any - Iterator SymNode Recursively iterate through value yield all SymNodes contained within isinstance val SymNode yield val isinstance val py_sym_types This allow applies jagged layout NestedTensor case nested ints symbolic is_symbolic val yield val node isinstance val tuple list torch Size s val yield _iterate_nodes s isinstance val torch Tensor yield _iterate_nodes val size is_sparse_any val yield _iterate_nodes val stride yield _iterate_nodes val storage_offset free_symbols val IterateExprs - OrderedSet sympy Symbol Recursively collect all free symbols value This function traverses various data structures tensors lists tuples etc extracts all sympy symbols contained within them It s useful finding all symbolic variables complex nested structure depends Args val The value extract symbols Can symbolic type SymInt SymFloat SymBool container tuple list tensor None Returns OrderedSet sympy Symbol An ordered set all free symbols found value val None OrderedSet itr = _iterate_exprs val we need least call union so we hand code identity try first_expr = next itr except StopIteration OrderedSet TODO Apparently returning OrderedSet here breaks python test distributed tensor test_dtensor_compile py TestDTensorCompile test_dtensor_dynamic first_expr free_symbols union e free_symbols e itr type ignore return-value has_free_symbols val IterateExprs - bool Faster version bool free_symbols val all e is_number e is_Boolean e _iterate_exprs val has_free_unbacked_symbols x IterateExprs - bool Faster version bool free_unbacked_symbols val sympy core traversal iterargs s _iterate_exprs x arg iterargs s arg is_Symbol symbol_is_type arg SymT UNBACKED_INT SymT UNBACKED_FLOAT True False free_unbacked_symbols x IterateExprs - OrderedSet sympy Symbol Like free_symbols filtered only report unbacked symbols NB keep synced is_unbacked_symint OrderedSet s s free_symbols x symbol_is_type s SymT UNBACKED_INT SymT UNBACKED_FLOAT _free_non_source_unbacked_symbols x IterateExprs unbacked_inputs OrderedSet sympy Symbol - OrderedSet sympy Symbol Unbacked symbols inputs graph These symbols originated data-dependent operations opposed mark_unbacked calls unbacked_symbols = free_unbacked_symbols x non_source_symbols = unbacked_symbols - unbacked_inputs non_source_symbols WARNING Don t use Dynamo produced graphs they don t have meta setup is_symbol_binding_fx_node node torch fx Node - Optional sympy Symbol Check given FX node symbol binding node A symbol binding node one has SymInt value its meta contains sympy Symbol expression either placeholder node contains unbacked symbols Args node torch fx Node The FX node check Returns Optional sympy Symbol The sympy Symbol node symbol binding node None otherwise val node meta isinstance node meta val torch SymInt isinstance node meta val node expr sympy Symbol node op == placeholder free_unbacked_symbols node meta val node expr node meta val node expr None find_symbol_binding_fx_nodes graph torch fx Graph - dict sympy Symbol torch fx Node Find all nodes FX graph bind sympy Symbols This function scans through all nodes given FX graph identifies nodes bind sympy Symbols typically placeholder nodes SymInt values When multiple nodes bind same symbol only first occurrence kept Args graph The FX graph search symbol binding nodes Returns A dictionary mapping sympy Symbols their binding FX nodes r = NB Prefer first occurrence symbol node graph nodes s = is_symbol_binding_fx_node node None s r r s = node r dataclass frozen=True Specialization This used multi-graph compilation contexts where we generate multiple specialized graphs dispatch appropriate one runtime This allows us optimize trade-off between performance generality creating specialized versions common patterns e g x shape == while maintaining general fallback source TensorPropertySource check_fn Callable Analogous ConvertIntSource dataclass frozen=True ConvertIntKey __str__ - str cast_symbool_to_symint_guardless get b bool - IntLikeType Get int value bool cast_symbool_to_symint_guardless b dataclass frozen=True CallMethodKey name str __str__ - str f name get o Any - Any Call method object getattr o name dataclass frozen=True InnerTensorKey inner_name str __str__ - str f inner_name get o Any - Any Get inner tensor attribute getattr o inner_name dataclass frozen=True DivideByKey divisor IntLikeType __str__ - str f __floordiv__ divisor get o int - int Divide object divisor o divisor _free_unbacked_symbols_with_path object path pytree KeyPath real Optional object = None shape_env Optional ShapeEnv = None pending Optional set sympy Symbol = None simplify bool = False - dict sympy Symbol pytree KeyPath Recursively traverses structure find unbacked symbols their access paths This function walks through tensors lists tuples symbolic values locate unbacked symbols pending set returns mapping those symbols their access paths structure Args The object traverse tensor list tuple SymInt etc path The current path object tree real Optional real tensor corresponding fake tensor being traversed shape_env Optional ShapeEnv register unbacked values pending Set unbacked symbols look will modified in-place simplify Whether use simplified expressions Returns A dictionary mapping unbacked symbols their access paths go = functools partial _free_unbacked_symbols_with_path shape_env=shape_env pending=pending simplify=simplify expr s Union SymInt SymFloat SymBool - sympy Expr simplify s node expr When called compute_unbacked_bindings NB Intentionally access _expr expr do want simplification s node _expr pending None pending = set r = isinstance tuple list NB real apparently always tuple list here python test inductor test_torchinductor py CpuTests test_index_propagation_nested_indirect_indexing_cpu i range len r update go i path + pytree SequenceKey i real=real i real None None type ignore index is_traceable_wrapper_subclass TODO Determine correct attrs _ = __tensor_flatten__ attr attrs sub = getattr attr r update go sub path + InnerTensorKey attr isinstance torch Tensor is_batchedtensor unwrapped_tensor = get_unwrapped r update go unwrapped_tensor path isinstance torch Tensor is_batchedtensor torch _subclasses fake_tensor FakeTensor assert isinstance FakeTensor r update go size path + CallMethodKey size real=a real_tensor size real_tensor None None layout torch sparse_csr torch sparse_csc torch sparse_bsr torch sparse_bsc r update go stride path + CallMethodKey stride real=a real_tensor stride real_tensor None None r update go storage_offset path + CallMethodKey storage_offset real= real_tensor storage_offset real_tensor None None isinstance torch SymInt torch SymFloat isinstance s = expr sympy Symbol s pending r s = path shape_env real None assert isinstance real int float shape_env set_unbacked_var_to_val s real pending remove s When unbacked SymInt perfectly divisible integer constant we replace integer constant improve reasoning capabilities However synthetic examples then possible factor never explicitly allocated Fortunately we can compute division isinstance torch SymInt isinstance s = expr sympy Mul len s args == isinstance lhs = s args sympy Integer sympy Symbol isinstance rhs = s args sympy Symbol support exactly one unbacked now rhs pending ^ lhs pending support constant coefficient backed symbolic coefficient isinstance coeff = lhs lhs pending rhs sympy Integer shape_env coeff shape_env var_to_val _symint_wrap s sympy Symbol - SymInt shape_env create_symintnode type ignore union-attr s hint=int shape_env var_to_val s type ignore union-attr source=shape_env var_to_sources get s None type ignore union-attr unbacked = lhs lhs pending rhs divisor IntLikeType = int coeff shape_env isinstance coeff sympy Integer _symint_wrap coeff TODO DivideByKey needs test divisibility runtime r unbacked = path + DivideByKey divisor real None assert isinstance real int val = real int coeff isinstance coeff sympy Integer CleanDiv real coeff shape_env shape_env set_unbacked_var_to_val unbacked val pending remove unbacked The annoyance here arises fact SymBool allocated allocating SymInt then testing s equal one So you have complicated binding site logic isinstance torch SymBool isinstance s = expr sympy Eq This must match create_unbacked_symbool EXACTLY isinstance s lhs sympy Symbol s rhs == s lhs pending r s lhs = path + ConvertIntKey real None assert type real bool shape_env shape_env set_unbacked_var_to_val s int real pending remove s lhs r compute_unbacked_bindings shape_env Optional ShapeEnv example_value object old_example_value Optional object = None peek bool = False - Optional dict sympy Symbol pytree KeyPath After having run fake tensor propagation producing example_value result traverse example_value looking freshly bound unbacked symbols record their paths later It error we have allocated unbacked SymInt cannot found example_value NB means you have multi-output function you must call tuple tensor output you cannot wait The peek parameter lets you check out what bindings without changing affected list This primarily useful ensuring unbacked_var_to_val promptly populated when propagate_real_tensors shape_env None None fs = shape_env pending_fresh_unbacked_symbols pending = set fs pending None peek log info compute_unbacked_bindings s fs fs clear symbol_to_path = _free_unbacked_symbols_with_path example_value shape_env=shape_env pending=pending simplify=False peek pending extra = repr example_value stride example_value storage_offset isinstance example_value torch Tensor raise PendingUnbackedSymbolNotFound f Pending unbacked symbols pending returned outputs example_value extra \n Did you accidentally call new_dynamic_size item more times than you needed your fake implementation \n For more help see https docs google com document d RWrH- wLEpzR kCS gGBNen_-Fs- PVbWWFE AcgeWE edit Why do we have do some rebinding here If original FX node wasn t binding site because you had memo hit post translation you aren t memo hit anymore there s now new binding site we know because s same FX node value actually same they re just obviously equal anymore The logic here written carefully because unlike bind_unbacked case we guaranteed have symbol old_sym If we have symbol do regular rename unbacked we don t we need specially eliminate fresh unbacked symbol NB we trusting memoization correct we don t need generate new runtime assert This load bearing repropagation can happen after we ve frozen runtime asserts old_example_value None keypath symbol_to_path values old_sym = pytree key_get old_example_value keypath new_sym = pytree key_get example_value keypath isinstance new_sym SymTypes isinstance new_s = new_sym node expr sympy Symbol isinstance old_sym SymTypes old_s = old_sym node expr = new_s If old_s unbacked_symbol we assume original unbacked symbol replaced backed symbol old_s This can happen when node reuses original symbol due memoi original symbol gets replaced backed symbol When happens we just replace new_s old_s because we know value same isinstance old_s sympy Symbol free_unbacked_symbols old_s shape_env _rename_unbacked_to new_s old_s shape_env _eliminate_unbacked new_s old_s isinstance old_sym SymTypes shape_env _eliminate_unbacked new_s sympy sympify old_sym symbol_to_path Note guard_or_ The following two functions common utilities used while defining unbacked semantics various framework code Those would used situations you prefer guard know result expression over guarding case you hit data dependent error you ok just returning true false When use If you can use higher level combinator prefer using those instead they definitely safe modulo short-circuiting It can used program would behave equivalently _guard_or returned true false Many inductor optimizations fall bracket example Finally s even OK program wouldn t behave equivalently so long change semantics preserving It can semantics preserving program errors more cases than did previously otherwise behaves identically changes some quantity way doesn t matter e g strides often fall bucket Specialize general case add runtime assertion would fail during runtime conditions general case satisfied Examples assuming expand reshape inputs - assuming non-broadcasting path _guard_or BoolLikeType default bool - bool Try guard data dependent error encountered just default isinstance SymBool assert isinstance bool backed_size_oblivious True we treat backed unbacked here torch fx experimental _config backed_size_oblivious result = _static_eval_sym_bool result result None default shape_env = getattr node shape_env None xla symnode path shape_env None guard_bool sym_node = node r = sym_node shape_env evaluate_sym_node sym_node size_oblivious=False fallback_value=default bool r guard_or_false BoolLikeType - bool Try guard data dependent error encountered just false _guard_or False guard_or_true BoolLikeType - bool Try guard data dependent error encountered just true _guard_or True _static_eval_sym_bool x SymBool - Optional bool assert isinstance x SymBool expr = x node expr try Shape env access inside try purpose xla symnode does have its attributes shape_env = x node shape_env simplified = shape_env _maybe_evaluate_static expr simplified None bool simplified None except Exception log debug Could simplify s expr None statically_known_false x BoolLikeType - bool Returns True x can simplified constant False If x cannot evaluated static we False note This function doesn t introduce new guards so expression may end up evaluating False runtime even function returns False Args x bool SymBool The expression try statically evaluating isinstance x SymBool assert isinstance x bool x result = _static_eval_sym_bool x result None False result statically_known_true x BoolLikeType - bool Returns True x can simplified constant true note This function doesn t introduce new guards so expression may end up evaluating true runtime even function returns False Args x bool SymBool The expression try statically evaluating isinstance x SymBool assert isinstance x bool x result = _static_eval_sym_bool x result None False result sym_and x BoolLikeType others BoolLikeType - BoolLikeType symbolic expressions without bool casting len others == x y others x = operator and_ x y x sym_eq x _T y _T - BoolLikeType Like == when run list tuple will recursively test equality use sym_and join results together without guarding isinstance x tuple list isinstance y list tuple len x = len y False functools reduce operator and_ map sym_eq x y True isinstance x int torch SymInt isinstance y int torch SymInt x == y raise AssertionError f unexpected sym_eq between type x type y sym_or x BoolLikeType others BoolLikeType - BoolLikeType symbolic expressions without bool casting len others == x y others x = operator or_ x y x guard_scalar Union SymBool SymInt SymFloat int bool float - Union bool int float Guard scalar value which can symbolic concrete boolean integer float This function dispatches appropriate guard function based type input Args A symbolic concrete scalar value bool int float Returns The concrete value after guarding Raises AssertionError If input recognized scalar type isinstance SymBool bool guard_bool isinstance SymInt int guard_int isinstance SymFloat float guard_float raise AssertionError f unrecognized scalar _advise_is_size SymInt - None Don t use directly use torch _check_is_size instead This softer version _constrain_range_for_size min= max=Inf Instead forcibly constraining variable erroring we failed constrain will simply advise us size constrained some way We will always defer runtime assert constraint we cannot prove compile-time we we only sometimes learn useful extra information compile-time information This contrast constrain_range_for_size where you don t call fresh unbacked symint chances we will choke TODO Make Dynamo handle appropriately seen Dynamo-ed code Right now only really used code AOTAutograd trace through so big problem isn t supported principle all code should Dynamo able too TODO I didn t support min max because I didn t have use case where actually helped In principle we can support just makes implementation below more complicated This must always succeed because sole allowed caller _check_is_size responsible expect_true ing This assert triggers expensive sym compute do do until its cheap assert = NB s important constrain range size hinted SymInts because only unsound will immediately trip our asserts hints have consistent static analysis If you somehow have unbounded SymInt later constrains will inconsistent range isinstance SymInt isinstance node SymNode isinstance node expr sympy Symbol node shape_env is_unbacked_symint node expr _constrain_range_for_size _advise_is_bounded SymInt upper_bound IntLikeType - None isinstance SymInt isinstance node SymNode isinstance node expr sympy Symbol node shape_env is_unbacked_symint node expr isinstance upper_bound int TODO relax node shape_env _constrain_is_bounded node expr upper_bound _constrain_range_for_size SymInt min Optional int = None max Optional int = None - None This function NOT INTENDED used itself isinstance SymFloat SymBool raise ValueError Constraining SymFloat SymBool nyi assert isinstance SymInt can only constrain range SymInt assert isinstance node expr sympy Symbol f constraining non-Symbols NYI node shape_env _constrain_range_for_size node expr min max inclusive both ways constrain_range SymInt min Optional int max Optional int = None - None Applies constraint passed SymInt must lie between min-max inclusive-inclusive WITHOUT introducing guard SymInt meaning can used unbacked SymInts If min max None we assume dimension unbounded direction Repeated application constrain_range intersects ranges This fairly low level API doesn t have lot safety guarantees TODO provide higher level APIs Currently we use API following circumstance when we allocate unbacked SymInt denoting integer quantity which data dependent we ordinarily do know anything about what values may take This means any sort guard will immediately fail However many cases we know something about unbacked SymInt example we know nonzero x size must = We use constrain_range narrow possible range declaring negative symbols impossible This permits definitely answer True queries like nnz = even we don t know what actual hinted value nnz In fact we actually use constrain_range unsoundly discharge common guards unbacked SymInt produced nonzero we will also assume equal even though these perfectly possible values runtime because we generally expect graphs valid N= also valid N= min None min = -int_oo max None max = int_oo max min raise ValueError Maximum value constrain_as_size can t less than specified min value f received min= min max= max isinstance int min = = max raise ValueError f Invalid value range min max node shape_env _constrain_range node expr min max constrain_unify torch SymInt b torch SymInt - None Given two SymInts constrain them so they must equal NB will work SymInts represent nontrivial expressions yet isinstance SymInt isinstance b SymInt assert == b shape_env = b node shape_env shape_env = node shape_env shape_env _constrain_unify b Assume boolean true purposes subsequent symbolic reasoning This will keep track corresponding runtime checks verify result upheld either regular guard special set asserts which triggered when unbacked SymInt allocated DO NOT use function these cases - This inappropriate branching conditions where both true false result valid programs We will always assume condition evaluates true so will never possible trace false condition when you use For true branching unbacked SymInts you must use torch cond you incorrectly use expect_true case you will make false branch unreachable we will simply assume only true branch ever exercised - This inappropriate situations where you know some other system invariant guarantees property holds since you don t really need insert runtime check case Use something like constrain_range case This API has hitch To avoid having reimplement error reporting capabilities function CAN False The invariant surrounding code must raise error when function returns False This quite low level so we recommend using other functions like check which enforce more intuitive way By way name nod __builtin_expect macro which used similarly unlike __builtin_expect you MUST fail unlikely branch I think expect good name recent versions C++ replaced likely which weaker accurate function expect_true BoolLikeType skip int = - bool isinstance SymBool TODO check perf implications frame = inspect currentframe _ range skip + always run loop least once frame None break frame = frame f_back node expect_true frame f_code co_filename frame frame f_lineno frame assert type bool guard_bool BoolLikeType - bool isinstance SymBool node guard_bool NB uses Python backtrace assert type bool guard_int IntLikeType - int isinstance SymInt node guard_int NB uses Python backtrace assert type int guard_float FloatLikeType - float isinstance SymFloat node guard_float NB uses Python backtrace assert isinstance float Given GraphModule all FakeTensors all placeholders fx_placeholder_vals gm torch fx GraphModule - list object n meta val n gm graph nodes n op == placeholder fx_placeholder_targets gm torch fx GraphModule - list str n target n gm graph nodes n op == placeholder Given GraphModule arguments run evaluate guards its associated ShapeEnv satisfied passed arguments This WILL check duck sizing eval_guards gm torch fx GraphModule args Tensor ignore_static bool = True - bool assert gm shape_env None gm shape_env evaluate_guards_for_args type ignore operator union-attr fx_placeholder_vals gm args ignore_static=ignore_static bind_symbols gm torch fx GraphModule args Tensor - dict sympy Symbol int assert gm shape_env None gm shape_env bind_symbols fx_placeholder_vals gm args type ignore operator union-attr DimDynamic Enum Controls how perform symbol allocation dimension It always sound default DYNAMIC policies DUCK STATIC can result better trace-time compile-time performance they reduce number allocated symbols generally make your graph more static NB If we notice you ve applied constraint dimension we will force DYNAMIC simplicity DimDynamic controlled variety higher level UX features Currently - In eager mode default policy DUCK - The default changed STATIC assume_static_by_default - An individual dim marked DYNAMIC you mark_dynamic_dim - In export mode default policy STATIC - An individual dim marked DYNAMIC you specify dynamic_shapes passed export Treat dimension symbolically DYNAMIC = Treat dimension symbolically its hint matches another dynamic dimension unify two symbols duck sizing DUCK = Treat dimension statically based its hint STATIC = Treat dimension size-like unbacked SIZE_LIKE_UNBACKED = Infer strides stride If size static strides will static well INFER_STRIDE = Like SIZE_LIKE_UNBACKED there s hint OBLIVIOUS_SIZE = NB These constraints affect both clients backends given some constraint C client must pass inputs satisfy constraint while backend must introduce guards BEYOND constraint For clarity we document implications both sides both client backend NB These constraints single dimension In principle we could also have multi-dimension constraints our guess actually useful so we supporting right now NB Strict constraints typically only suitable export eager backend like inductor may validly introduce extra discretionary guards improve performance code A StrictMinMaxConstraint would brittle under future optimizations performed inductor we don t guarantee eager code StrictMinMaxConstraint will keep working future dataclass frozen=True Constraint warn_only bool dataclass frozen=True StrictMinMaxConstraint Constraint For clients size dimension must within vr which specifies lower upper bound inclusive-inclusive AND must non-negative should see NB below For backends there must any guards dimension which implied given lower upper bound Regardless lower bound backend can assume size non-negative An unbounded StrictMinMaxConstraint can thought strict version RelaxedUnspecConstraint NB Export will often unsoundly assume graph works even though trace time we assumed size The idea we produce graph works range values will OK N= too vr ValueRanges render source Source - str Format constrain equation TODO better printing -oo oo f vr lower = source name = vr upper dataclass frozen=True RelaxedUnspecConstraint Constraint For clients no explicit constraint constraint whatever implicitly inferred guards tracing For backends there must exist least TWO possible values size dimension which satisfy guards dimension In other words constraint helps us distinguish between we don t care dimension specializes versus dimension must unspecialized However constraint doesn t say very much about what specialization permitted example we guard size being even would still acceptable under unspec constraint This makes RelaxedUnspecConstraint useful eager mode where your backend compiler may add constraints otherwise dynamic dimensions we can t assert there NO guards brittle because compilers should able add extra constraints If you want assert there no guards use StrictMinMaxConstraint unbounded ValueRanges render source Source - str f RelaxedUnspecConstraint source name NB None here indicates client constraint whatever implicitly inferred guards tracing backend can add whatever guards wants including fully specializing value DimConstraint = Union StrictMinMaxConstraint RelaxedUnspecConstraint None dataclass frozen=True EqualityConstraint Constraint Represent decide various kinds equality constraints between input sources A source pair pair input sources dynamic dimensions specified equal We represent ` source_pairs ` union-find forest so we can efficiently check whether two such sources transitively equal A derived equality relates input source expression over root The root can another input source corresponding some dynamic dimension phantom symbol does directly represent any dynamic dimension We represent ` derived_equalities ` involving input sources transitively-closed map so we can efficiently check whether input source transitively equal given expression over another input source NOTE In contrast easy decide whether input source transitively equal given expression over phantom symbol such expressions already canonical form so problem reduces symbolic expression equality source_pairs list tuple Source Source derived_equalities list tuple Source Union Source sympy Symbol Callable sympy Expr sympy Expr phantom_symbols list sympy Symbol relaxed_sources set Source _parents dict Source Source = field init=False _defs dict Source sympy Expr = field init=False __post_init__ - None Pre-processing answer queries ` is_equal ` ` is_derived ` below Example Suppose we given source_pairs = b b = c derived_equalities d = c + e = d - We first construct union find source_pairs _parents = b c Then we compute canonical symbolic expressions recursively applying derived_equalities until we bottom out _defs = d c + e c + - aka c _parents map input sources input sources where conceptually these directed edges union-find forest _parents dict Source Source = object __setattr__ _parents _parents _defs map input sources canonical symbolic expressions i e unary expressions symbols corresponds regular Dims i e derived Dims _defs dict Source sympy Expr = object __setattr__ _defs _defs source source source_pairs preprocess into union-find forest _union _find source _find source source root fn derived_equalities preprocess into transitively-closed map NOTE avik we reuse union-find forest canonicalizing input sources isinstance root sympy Symbol sympy Integer _defs _find source = fn root _defs _find source = fn _rewrite root _find source Source - Source chase edges find root equivalence source _parents _find _parents source source _union root Source root Source - None merge two equivalence classes adding edge one root other root = root _parents root = root _rewrite src Source - sympy Expr always represent given source root its equivalence src = _find src src _defs simply look up definition exists NOTE avik This works because definitions always transitively-closed otherwise we would have do recursive rewriting _defs src otherwise create symbol representing source sympy Symbol src name is_equal source Source source Source - bool check whether source source have same root relaxed src = _find source relaxed_sources src = _find source relaxed_sources src == src check whether source derived equal source is_derived source source lambda x x is_derived src Source symbol_src Source fn Callable sympy Expr sympy Expr - bool check whether both src symbol_src have same definition _rewrite src == fn _rewrite symbol_src _assert_symbol_context symbolic_context object - TypeGuard SymbolicContext assert isinstance symbolic_context SymbolicContext Invalid symbolic_context object assert type symbolic_context SymbolicContext Illegal usage symbolic_context ABC True _is_supported_equivalence expr sympy Expr - bool Currently supported Dim ops linear expressions integer coefficients So check expr only contains + ints single occurrence symbol See also documentation dynamic_shapes _DerivedDim isinstance expr sympy Add sympy Mul len expr args False lhs rhs = expr args _is_supported_equivalence lhs isinstance rhs sympy Integer isinstance lhs sympy Integer _is_supported_equivalence rhs isinstance expr sympy Symbol _has_uninterpretable_sympy_function expr sympy Basic - bool Add functions our sympy interpreter can t reify into FX nodes expr has torch utils _sympy functions ToFloat torch utils _sympy functions TruncToInt torch utils _sympy functions CeilToInt dataclass frozen=True SymbolicContext Data structure specifying how we should create symbols ` ` create_symbolic_sizes_strides_storage_offset ` ` e g should they static dynamic This abstract base because we probably going add another version says use exactly these SymInts don t allocate fresh symbols dataclass frozen=True SymIntSymbolicContext SymbolicContext Data structure specifying any constraints SymInt input constraint DimConstraint _P = ParamSpec _P _T = TypeVar _T dataclass frozen=True StatelessSymbolicContext SymbolicContext Generic _P _T Create symbols ` ` create_symbolic_sizes_strides_storage_offset ` ` via symbolic_context determination given ` ` DimDynamic ` ` ` ` DimConstraint ` ` This will cause fresh symbols allocated dynamic_sizes DimList DimDynamic dynamic_strides DimList DimDynamic = None type ignore assignment constraint_sizes DimList DimConstraint = None type ignore assignment constraint_strides DimList DimConstraint = None type ignore assignment specialize_on Optional list list Callable _P _T = None If tensor view should populated base It contains information how allocate symbols when recursively fakeifying base during view fake-ification view_base_context Optional SymbolicContext = None TODO add storage offset stride symbolic_context __post_init__ - None specialize_on None object __setattr__ specialize_on len dynamic_sizes dynamic_strides None object __setattr__ dynamic_strides DimDynamic INFER_STRIDE len dynamic_sizes constraint_sizes None object __setattr__ constraint_sizes None len dynamic_sizes constraint_strides None object __setattr__ constraint_strides None len dynamic_sizes assert all stride DimDynamic INFER_STRIDE DimDynamic DYNAMIC DimDynamic DUCK stride dynamic_strides note Tensor Fakification Symbol Caching As time note dynamo creates fresh fake tensor mode backends The reason we do because there certain classes operations namely metadata mutations change tensor size stride etc This means fake tensor state end dynamo trace different than fake tensor state beginning trace Backends like aot_autograd need fresh fake tensor correctly track metadata mutation view relationships etc As we create new fake mode we also lose memoization comes Rather than transfer memoization cache we instead transfer shape env However comes nuance - dynamo selective how makes symbolic shapes Due strategies automatic dynamic constraints policy which dims dynamic nuanced varies across recompilations In order preserve symbolic decisions made during dynamo tensor fakification we pass StatefulSymbolicContext creation time This object tracked per tensor TracingContext The lifecycle object should match lifecycle original dynamo tracked tensor safe reuse object many times necessary create fake tensor Fake tensors created new fake modes should produce same exact symbols original providing same shape_env used TODO voz Shape env validation dataclass frozen=True StatefulSymbolicContext StatelessSymbolicContext Create symbols ` ` create_symbolic_sizes_strides_storage_offset ` ` via symbolic_context determination given cache Source Symbol A cache hit will reuse stored symbol cache miss will write cache This behaves like StatelessSymbolicContext except cache supersedes other values - dynamic_sizes constraint_sizes will read we cache hit It cache owner s responsibility maintain lifecycle cache respect different shape_envs clearing etc tensor_source Source = None type ignore assignment Why keyed int first That integer actually id shape_env This cache short-circuits symbol creation we must store per shape env Now while tracing invariants single shape env per tracing context every new frame gets new shape_env So where would we have multiple shape envs The answer lies recording When we replaying replay_shape_env_events invoked creates new shape_env Replaying events against new shape_env will cause fail unknown symbols symbols cached here will skip creation never get recorded var_to_val etc TODO voz consider weakref shape_env here shape_env_to_source_to_symbol_cache dict int dict str sympy Expr = None type ignore assignment __post_init__ - None super __post_init__ The None default annoying required because dataclass limitations assert tensor_source None shape_env_to_source_to_symbol_cache object __setattr__ shape_env_to_source_to_symbol_cache dataclass frozen=True SubclassSymbolicContext StatefulSymbolicContext The correct symbolic context given inner tensor traceable tensor subclass may differ outer symbolic context This structure allows flexibility inner symbolic contexts mapped via attr - symbolic context inner_contexts dict str SymbolicContext = None type ignore assignment __post_init__ - None super __post_init__ inner_contexts None pyrefly ignore bad-assignment inner_contexts = dataclass TrackedFake Tracks sources all fake tensors we wrap Dynamo Used shape guard computation fake Union FakeTensor SymInt source Source symbolic_context Optional SymbolicContext __hash__ - int hash fake source name __eq__ other object - bool isinstance other TrackedFake fake other fake source name == other source name False is_symbolic val Union int SymInt float SymFloat bool SymBool - TypeGuard Union SymInt SymFloat SymBool isinstance val int float bool False val node is_symbolic IndicatorTypes = IsNonOverlappingAndDenseIndicator _expandsums args list sympy Expr - tuple sympy Expr bool Expand products sums into sums products This function takes list sympy expressions separates them into additive expressions those is_Add=True other expressions It then computes distributive product expanding a+b c+d into c + d + b c + b d Args args A list sympy expressions expand Returns A tuple containing - The expanded expression sympy Expr - A boolean indicating whether expansion occurred True multiple additive expressions present there least one additive one other expression adds other = arg args arg is_Add adds append arg other append arg result = sympy Mul other add adds result = b b itertools product result add args result = sympy Add result result len adds len adds len other _fast_expand expr _SympyT - _SympyT A faster implementation sympy s expand function common cases This function expands expressions like a+b ^n a+b c+d into sums products avoids expensive checks features sympy s full expand implementation It only recreates objects when necessary avoid expensive operations Args expr A sympy expression expand Returns The expanded expression The expand algorithm sympy slow due all features supports For eg e^ -x x- x+ expanded x- e^x + e^x x x positive e^ -x x-e^ -x x+ x negative We do implement such features here avoid expensive checks We also make sure we only re-create objects any args changed avoid expensive checks when re-creating objects new_args = _fast_expand arg arg expr args type ignore arg-type pyrefly ignore missing-attribute any arg new_arg arg new_arg zip expr args new_args pyrefly ignore missing-attribute _fast_expand expr func new_args pyrefly ignore missing-attribute expr is_Pow base sympy Expr exp sympy Expr base exp = expr args type ignore assignment exp is_Integer base is_Add exp sympy expand_multinomial expr deep=False exp S One sympy expand_multinomial S One expr deep=False pyrefly ignore missing-attribute expr is_Mul num list sympy Expr = den list sympy Expr = pyrefly ignore missing-attribute arg expr args arg is_Pow arg args == - den append S One arg type ignore operator arg-type num append arg type ignore arg-type num num_changed = _expandsums num den den_changed = _expandsums den num_changed den_changed num den expr lru_cache safe_expand r _SympyT - _SympyT Expand given symbolic expression recursively rewriting product sums into sum products product being either multiplication exponentiation NOTE using intermediate expression may prevent simplification down line e g we eagerly expand ` + b ^ ` into ` a^ + ab + b^ ` we won t able simplify ` a^ + ab + b^ + b ` easily hasattr r expand try _fast_expand r except RecursionError log warning RecursionError _fast_expand s r r r _SymbolInfo NamedTuple k sympy Symbol vr Optional ValueRanges val Optional sympy Integer is_size_like bool lru_cache None _maybe_evaluate_static_worker expr _SympyT NB tuple ensure can LRU cached symbol_info tuple _SymbolInfo unbacked_only bool size_oblivious bool - Optional _SympyT This variant ShapeEnv _maybe_evaluate_static has no dependence ShapeEnv thus can cached indefinitely It does heavy lifting static evaluation including nontrivial reliance Sympy simplification occurs when we reallocate symbols Simplify making use value range lower bound new_shape_env = new_range_env = idx sinfo enumerate symbol_info k vr val is_size_like = sinfo isinstance val SingletonInt Skip var_ranges logic SingletonInt which only used jagged layout NestedTensors today continue assert vr None size_oblivious is_size_like lower = max vr lower Clamping size-oblivious some quantity below sys maxsize helps us determine f u = sys maxsize which test looking sys maxsize sentinel you don t really want worry about unbacked SymInts This similar flavor where size oblivious omits changes semantics benign way upper = min vr upper Excluding very upper bound can helpful upper lower upper = upper - This bit dodgy what means there size-like unbacked symbol whose upper bound This causes problems lower = upper vr = ValueRanges lower upper lower = vr lower Don t do anything we don t have nontrivial lower bound Also don t do anything we asked only simplify unbacked SymInt lower -int_oo unbacked_only val None vr is_int new_range_env k = vr continue The goal take our symbols which have various lower bounds reallocate them into new symbols which exactly positive e g we have s inf we want turn into ess inf where s = ess + This gives most information sympy subsequent simplifications Positive means = Positive - means = Positive + lower - means = lower The new symbol s too low so when we substitute we have increase offset conversely new variables have have their value range bounds adjusted well s = sympy Symbol f evaluate_static_shape_ idx positive=True integer=True Note Offset might fraction e g aten split Tensor shapes always integers Sympy might give unexpected results when comparing integer non-integer Therefore we cast offset int here For example shape_ = sympy Symbol shape_ positive=True integer=True expr = sympy Eq shape_ - expr xreplace False offset = int lower - new_shape_env k = s + offset new_range_env s = SymPyValueRangeAnalysis add vr -offset TODO remove try catch esp unbacked_only try pyrefly ignore missing-attribute new_expr = expr xreplace new_shape_env except RecursionError log warning RecursionError sympy xreplace s s expr new_shape_env None We need canonicalize after expand we may have something like ` + b = ` sympy will simplify The two appeareances will then make value ranges analysis give lose bounds new_expr = canonicalize_bool_expr safe_expand new_expr new_expr is_number new_expr Check range can solve statically out = bound_sympy new_expr new_range_env out is_singleton out lower new_expr unbacked_only None error - NoReturn raise AssertionError shouldn t hit TODO Deduplicate torch _prims_common __init__ py eval_is_non_overlapping_and_dense sizes Sequence int strides Sequence int - int int guard_bool _eval_is_non_overlapping_and_dense sizes strides _eval_is_non_overlapping_and_dense sizes Sequence int strides Sequence int - bool Evaluates whether tensor given sizes strides non-overlapping dense A tensor non-overlapping there s no memory location belongs more than one element A tensor dense all elements stored memory without gaps Args sizes Sequence dimension sizes tensor strides Sequence strides tensor Returns True tensor non-overlapping dense False otherwise dim = len sizes Short-circuits tensors rank one which non-overlapping dense their stride one element tensor dim == strides == sizes Checks there exists permutation strides s t tensor would contiguous Sorts length stride pairs stride lengths_and_strides = sorted zip sizes strides key=operator itemgetter Unlike C++ code we don t move size dimensions end So we have keep going code expected_stride = length stride lengths_and_strides length == continue stride = expected_stride False expected_stride = length True _sympy_cast_symbool_to_symint_guardless x SympyBoolean - sympy Expr sympy Piecewise x True cast_symbool_to_symint_guardless symbool Union bool torch SymBool - Union int torch SymInt Converts SymBool bool SymInt int without introducing guards This function maps True False preserving symbolic nature input when s SymBool Unlike regular casting which might introduce guards function performs conversion without adding any guards Args symbool A boolean value either concrete bool symbolic SymBool Returns The corresponding integer value True False either concrete int symbolic SymInt isinstance symbool bool symbool int_sym = _sympy_cast_symbool_to_symint_guardless symbool node expr symbool node shape_env create_symintnode int_sym hint=int symbool node require_hint has_hint symbool None SYMPY_INTERP = IsNonOverlappingAndDenseIndicator eval_is_non_overlapping_and_dense cast_symbool_to_symint_guardless cast_symbool_to_symint_guardless math math torch torch _lru_cache fn Callable _T maxsize Optional int = None - functools _lru_cache_wrapper _T Wrapper around lru_cache clears when new info about shapes has been updated Use lru_cache output always same regardless constraints we know now i e evaluate_expr Use _lru_cache otherwise Also note depends _update_version_counter being called shape environment whenever constraints updated otherwise cache will cleared fn_cache = lru_cache maxsize fn prior_version = config validate_shape_env_version_key prior_key = None functools wraps fn wrapper ShapeEnv args Any kwargs Any - _T nonlocal prior_version prior_key prior_key None prior_key = _get_key prior_version = _version_counter fn_cache cache_clear prior_version = _version_counter prior_key = _get_key assert prior_key == _get_key ShapeEnv cache key changed without version being updated fn_cache args kwargs functools wraps fn wrapper ShapeEnv args Any kwargs Any - _T type ignore misc nonlocal prior_version prior_version = _version_counter fn_cache cache_clear prior_version = _version_counter fn_cache args kwargs wrapper cache_clear = fn_cache cache_clear type ignore attr-defined wrapper cache_info = fn_cache cache_info type ignore attr-defined wrapper type ignore return-value dataclass frozen=True RuntimeAssert This pretty similar ShapeGuard also comes message exclusively used things MUST true unlike guards which can evaluate False which case you just choose use particular specialization expr SympyBoolean msg str = field repr=False stack CapturedTraceback = field repr=False Used printing SymExprs compile_fx SymExprPrinter PythonPrinter _print_Float expr sympy Float - str str float expr _ShapeGuardPrinter abc ABC Abstract base printers convert symbolic expressions string representations This provides common functionality printing symbolic expressions special handling symbols represent tensor shapes strides etc Subclasses implement specific formatting different output languages Args symbol_to_source Mapping sympy symbols their source objects source_ref Function convert source its string representation var_to_sources Mapping sympy symbols their source objects error reporting __init__ symbol_to_source Mapping sympy Symbol list Source source_ref Callable Source str var_to_sources Mapping sympy Symbol list Source - None symbol_to_source = symbol_to_source source_ref = source_ref var_to_sources = var_to_sources super __init__ _print_Float expr sympy Float - str Convert sympy Float Python float string representation str float expr _print_Symbol expr sympy Symbol - str Convert sympy Symbol its source representation This method looks up symbol symbol_to_source mapping returns string representation its first source If symbol symbol_to_source which can happen when symbols appear guard expressions through simplification substitution falls back var_to_sources Args expr The sympy Symbol convert Returns String representation symbol s source Raises AssertionError If symbol found either mapping assert isinstance expr sympy Symbol str type expr Try symbol_to_source first fall back var_to_sources found source = symbol_to_source get expr print_source source source = var_to_sources get expr print_source source repr_sources src Mapping sympy Symbol list Source - str repr symbol s name s sources symbol sources src items raise RuntimeError f expr repr_sources symbol_to_source f repr_sources var_to_sources This could due issue described https github com pytorch pytorch pull abc abstractmethod print_source source Source - str Convert source object its string representation Args source The source object convert Returns String representation source abc abstractmethod doprint expr sympy Expr - str Convert sympy expression its string representation Args expr The sympy expression convert Returns String representation expression ShapeGuardPythonPrinter _ShapeGuardPrinter PythonPrinter Python printer shape guards extends base ShapeGuardPrinter This provides functionality print symbolic expressions Python code caching improve performance when printing same expressions multiple times It handles printing sources expressions according Python syntax Args args Arguments passed parent classes __init__ args Any - None super __init__ args _print_cache dict sympy Expr str = print_source source Source - str Convert source object its string representation using source_ref function Args source The source object convert Returns String representation source source_ref source doprint expr sympy Expr - str Convert sympy expression its Python string representation caching This method first checks expression already cache If found returns cached result otherwise delegates PythonPrinter s doprint method caches result Args expr The sympy expression convert Returns String representation expression Python syntax val = _print_cache get expr None val None val res = PythonPrinter doprint expr _print_cache expr = res res deprecated ` torch fx experimental symbolic_shapes ShapeGuardPrinter ` deprecated please use ` torch fx experimental symbolic_shapes ShapeGuardPythonPrinter ` instead category=FutureWarning ShapeGuardPrinter ShapeGuardPythonPrinter pass _ShapeGuardCppPrinter _ShapeGuardPrinter CppPrinter __init__ args Any - None all_symbols set str = set source_to_symbol dict Source sympy Symbol = super __init__ args print_source source Source - str source source_to_symbol source_to_symbol source name source_name = source name mangled_name = re sub ^ - a-zA-Z_ + _ source_name old_mangled_name = mangled_name count = while mangled_name all_symbols mangled_name = f old_mangled_name _ count count += source_to_symbol source = sympy Symbol mangled_name all_symbols add mangled_name mangled_name doprint expr sympy Expr - str CppPrinter doprint expr A dataclass storing shape guards dataclass frozen=True _ShapeGuardsHelper exprs list str A dataclass storing C++ expressions helper variables dataclass frozen=True _CppShapeGuardsHelper _ShapeGuardsHelper source_to_symbol dict Source sympy Symbol LoggingShapeGuardPrinter ShapeGuardPythonPrinter __init__ var_to_sources Mapping sympy Symbol list Source super __init__ var_to_sources lambda n n name var_to_sources DynamicDimConstraintPrinter PythonPrinter Printer dynamic dim constraints - Instead symbol s_k prints its source t size i - Instead Eq _ _ Mod _ _ etc prints _ == _ _ _ etc We use suggest code specifying dynamic dim constraints __init__ symbol_to_source dict sympy Symbol list Source source_name_to_debug_name Mapping str str super __init__ symbol_to_source = symbol_to_source source_name_to_debug_name = source_name_to_debug_name _print_Symbol expr sympy Symbol - str assert isinstance expr sympy Symbol str type expr assert symbol_to_source get expr f Unknown symbol expr created constraints solver symbol_to_source expr name DimConstraints Custom solver system constraints symbolic dimensions Solutions static values simplified dynamic constraints __init__ symbol_to_source dict sympy Symbol list Source var_to_val Mapping sympy Symbol sympy Integer marked_dynamic set sympy Symbol source_name_to_debug_name Mapping str str - None We try solve systems inequalities free variable _univariate_inequalities dict sympy Symbol set SympyBoolean = defaultdict set Among them we prioritize solving free variable has equalities NOTE _symbols_with_equalities always subset _univariate_inequalities keys removing symbol former = removing latter _symbols_with_equalities set sympy Symbol = set A solution free variable equalities becomes substitution We use these substitutions simplify other constraints NOTE removing symbol _symbols_with_equalities = adding _substitutions _substitutions dict sympy Symbol sympy Integer = In general constraints may have operations Of course can expressed terms Our inequality solver can handle So we need transform them away We do so using values variables hints evaluate For soundness we record additional congruence guards solve them separately _var_to_val Mapping sympy Symbol sympy Integer = var_to_val _congruences defaultdict sympy Symbol set sympy Expr = defaultdict set We do try directly solve inequalities free variables NOTE free variables these inequalities cannot also _substitutions _multivariate_inequalities set SympyBoolean = set We park external equalities between free variables here _symbolic_equivalences list tuple Source sympy Expr = Solutions come two forms - static specializations - dynamic inequalities congruences _static_results set str = set _dynamic_results set str = set printer solutions _dcp = DynamicDimConstraintPrinter symbol_to_source source_name_to_debug_name inconsistencies found substituting concrete values static solutions _inconsistencies list str = symbols marked dynamic _marked_dynamic = marked_dynamic track supported sympy functions subtract list all sympy functions _supported_sympy_functions set sympy Function = Application Mod PythonMod FloorDiv _enumerate_sympy_functions rewrite_with_congruences s sympy Symbol expr _SympyT - _SympyT Eliminate expressions form b d b d while adding congruences form b d == k This leaves rational operators particular form b d our inequality solver can handle We solve added congruences separately using our congruence solver see below mod_handler args sympy Expr - sympy Expr Suppose we have expression form b d free variable s Using value s hint we can evaluate b d value k Then we can rewrite b d k while adding guard b d == k NOTE avik This abstraction provably sound general incomplete It complete IFF original expression always evaluates constant value i e does vary s In other words - solutions s rewritten expression guaranteed also solutions s original expression - while may possible find solutions s original expression solutions rewritten expression case original expression cannot evaluate same value all solutions s Should we worried about incompleteness No because following reasons It unblocks dramatic simplification would otherwise possible current tech i e don t let perfect enemy good We already have tradition using hints add guards compiler making progress We have yet seen counterexample arise practice In particular any congruence guards we generate simplify seem form b d == k where k constant Here s theoretical counterexample s s + == s - satisfied all s = With any hint say s = k we d rewrite s s + == k - But substituting we would then get k - == s - thus s = k only constant solution base divisor = args base divisor = rewrite_with_congruences s base rewrite_with_congruences s divisor mod_reduced = base xreplace _var_to_val divisor xreplace _var_to_val congruence = base - mod_reduced divisor congruence = _congruences s add congruence mod_reduced floor_div_handler args sympy Expr - sympy Expr Suppose we have expression form b d free variable s Using value s we can evaluate b d value k Then we can rewrite b d b - k d while adding guard b d == k NOTE avik This exactly equivalent rewriting b d b - b d d eliminating b d above base divisor = args base divisor = rewrite_with_congruences s base rewrite_with_congruences s divisor mod_reduced = base xreplace _var_to_val divisor xreplace _var_to_val congruence = base - mod_reduced divisor congruence = _congruences s add congruence NB Must CleanDiv needs regular sympy division so inequality solver works This sort problematic is_integer tests though haha base - mod_reduced divisor pyrefly ignore missing-attribute expr has Mod pyrefly ignore missing-attribute expr = expr replace Mod mod_handler - - - - - - - - so negative arguments should OK pyrefly ignore missing-attribute expr has PythonMod pyrefly ignore missing-attribute expr = expr replace PythonMod mod_handler pyrefly ignore missing-attribute expr has FloorDiv pyrefly ignore missing-attribute expr = expr replace FloorDiv floor_div_handler expr _enumerate_sympy_functions - None module = torch utils _sympy functions all_functions = set attr dir module isinstance func = getattr module attr sympy FunctionClass all_functions add func _unsupported_sympy_functions = all_functions difference _supported_sympy_functions _has_unsupported_sympy_function expr sympy Basic - bool Tracks list sympy Functions export solver doesn t know how handle expr has _unsupported_sympy_functions add expr SympyBoolean - bool Add expression set constraints Return whether expression trivial constraint i e obvious tautology expr == sympy true True orig_expr = expr orig_reduced = orig_expr xreplace _var_to_val TODO avik https github com pytorch pytorch issues It possible ` expr ` will fail consistency check because precision errors Specifically substituting its free symbols their concrete values we might end up comparing floats Until we have fix issue we delay raising such failures See solve orig_reduced == sympy false _inconsistencies append f orig_expr inconsistent isinstance expr sympy Ne sympy Or sympy And _has_unsupported_sympy_function expr we re going do anything useful these so drop them False free_symbols = expr free_symbols assert free_symbols f Did expect constraint no free variables expr len free_symbols multivariate record move _multivariate_inequalities add expr univariate can solve these immediately s = next iter free_symbols eliminate see documentation ` rewrite_with_congruences ` above old_n_congruences = len _congruences s expr = rewrite_with_congruences s expr new_n_congruences = len _congruences s expr == sympy true old_n_congruences == new_n_congruences reduced = expr xreplace _var_to_val reduced == sympy false _inconsistencies append f expr obtained rewriting orig_expr congruences inconsistent isinstance expr sympy Eq special status symbols have equalities see ` solve ` below _symbols_with_equalities add s _univariate_inequalities s add expr False add_equality source Source expr sympy Expr - None Add equality constraint expr is_number specialization right here _static_results add f source name == expr these will resolve either specializations dynamic equality constraints _symbolic_equivalences append source expr _reduce_congruences - dict sympy Symbol set sympy Expr reduced_congruences dict sympy Symbol set sympy Expr = s congruences _congruences items remainder_modulus_pairs = congruences_to_check = set congruence congruences base divisor = congruence args We given congruence form base divisor == free variable s So - we transform into equation form base = divisor tmp - we solve equation s get linear solution free variable tmp tmp = sympy Symbol reduce_congruences_tmp integer=True symbol solution = sympy solve_linear base - divisor tmp symbols= s See https docs sympy org latest modules solvers solvers html#sympy solvers solvers solve_linear how interpret results s == symbol This means solution form s = modulus tmp + remainder modulus remainder = sympy polys polytools div solution tmp isinstance modulus sympy Integer isinstance remainder sympy Integer Make sure = remainder = modulus remainder = remainder modulus remainder_modulus_pairs append remainder modulus continue This means we did get unique solution equation No problem we will check congruences_to_check add congruence Finally we solve congruence s such s = r_i mod m_i each r_i m_i The solution will congruence form s = r mod m NOTE avik Since given m_i may pairwise coprime we can t just use CRT remainder_modulus_pairs remainder modulus = sympy ntheory modular solve_congruence remainder_modulus_pairs reduced_congruences s = s - remainder modulus substitution = s modulus sympy Symbol tmp integer=True + remainder reduced_congruences s update congruence congruence congruences_to_check sympy checksol congruence substitution reduced_congruences s = congruences_to_check reduced_congruences _raise_inconsistencies - None _inconsistencies msg = \n join _inconsistencies _inconsistencies clear raise ValueError f The following inconsistencies found \n msg solve - None Solve system constraint equations find simplified constraints _raise_inconsistencies long there symbols equalities solve them NOTE avik guaranteed terminate #iterations = #symbols while _symbols_with_equalities s = _symbols_with_equalities pop exprs = _univariate_inequalities pop s solution = sympy solvers inequalities reduce_inequalities exprs s isinstance solution sympy And solution = next arg arg solution args isinstance arg sympy Eq solution assert isinstance solution sympy Eq f Expected equality constraint s got solution symbol val = solution args assert symbol == s f Expected constraint s instead symbol because univariate solution specialization _static_results add f _dcp symbol_to_source s name == val add substitution simplify other constraints _substitutions s = val type ignore assignment simplify multivariate inequalities some them will now become univariate multivariate_inequalities = _multivariate_inequalities _multivariate_inequalities = set expr multivariate_inequalities add expr xreplace s _substitutions s _raise_inconsistencies solve linear congruences NOTE avik We do need solve them symbols have already been specialized reduced_congruences = _reduce_congruences s congruences reduced_congruences items congruence congruences any congruence cannot checked becomes dynamic constraint well s _substitutions sympy checksol congruence s _substitutions s _is_supported_congruence congruence base divisor = congruence args tmp_name = _ + str _dcp source_name_to_debug_name get _dcp symbol_to_source s name _dcp symbol_to_source s name tmp = sympy Symbol tmp_name integer=True torch _dynamo source ConstantSource _dcp symbol_to_source tmp = ConstantSource tmp_name r = try_solve sympy Eq base divisor tmp s assert r None _dynamic_results add _dcp doprint sympy Eq s r remaining symbols have only pure inequalities no equalities s exprs _univariate_inequalities items try solution = sympy solvers inequalities reduce_inequalities exprs s because univariate solution dynamic range constraint isinstance solution sympy Or solution = next iter arg arg solution args arg xreplace _var_to_val isinstance solution sympy And arg solution args _dynamic_results add _dcp doprint arg _dynamic_results add _dcp doprint solution except NotImplementedError AssertionError log warning Failed reduce inequalities exc_info=True expr exprs _dynamic_results add _dcp doprint expr simplify symbolic equivalences some them will now become specializations symbolic_equivalences = _symbolic_equivalences _symbolic_equivalences = source expr symbolic_equivalences add_equality source expr xreplace _substitutions remaining symbolic equivalences become dynamic equality constraints source expr _symbolic_equivalences _dynamic_results add f source name == _dcp doprint expr classmethod _is_supported_congruence cls congruence sympy Expr - bool base divisor = congruence args Congruences can currently expressed supported Dim ops form x + b == where x Dim b constants This allows us derive x b y - some Dim y See also documentation dynamic_shapes _DerivedDim isinstance base sympy Add lhs rhs = base args cond = isinstance lhs sympy Symbol isinstance rhs sympy Integer isinstance lhs sympy Integer isinstance rhs sympy Symbol cond = isinstance base sympy Symbol cond = cond isinstance divisor sympy Integer cond forced_specializations - dict str sympy Expr Returns dictionary names symbols their specialized value debug_name src Source - str name = src name _dcp source_name_to_debug_name f _dcp source_name_to_debug_name name = name name debug_name _dcp symbol_to_source s val s val _substitutions items s _marked_dynamic _is_derived_dim dim object - TypeGuard torch export dynamic_shapes _DerivedDim isinstance dim torch export dynamic_shapes _DerivedDim _is_dim dim object - TypeGuard torch export dynamic_shapes Dim isinstance dim torch export dynamic_shapes Dim isinstance dim torch export dynamic_shapes _DerivedDim _process_derived_dim_roots results dict str dict str Any name_to_dim dict str Any - None Here we resolve concerns derived dims suggested fixes newly introduced roots root swapping Newly introduced roots appear modulo guards e g Mod dx = suggests dx derived dim equal _dx introducing new root _dx Currently final suggested fixes handle correctly we can get intermediate results look like dy eq dx + dx eq _dx + min max routine prettifies unifying single root making each suggestion either derived dim min max range both With suggested fixes derived dims roots can swapped e g dx dx - - dy + dy Here we don t want print out attached name since leads messages like dx - = Dim dx - Instead we evaluate new root value remove results its derivations First we find all original roots specified dynamic_shapes found values results i e used computing suggesting fix values These original roots suppose ` dx ` either specialized unchanged refined swapped expressed derived dim If any first cases happen we suggest ` dx ` s value results remove suggestions derivations ` dx ` assuming derived relation valid If swapped we find new root use fix evaluate ` dx ` s new value then do same ` dx ` s derivations Assuming originally specified derived relations correct valid because relations plain wrong e g input shape = spec dx dx - produce_guards will catch crash before hand relations numerically correct do match emitted guard example forward x y x reshape - + y guard s = s inputs = torch randn torch randn dx = Dim dx min= max= dynamic_shapes= x dx y dx + matches values op then leads linear equations produce_guards able solve unique solution dx = specialize b export constraint solver will raise issue due range constraints unique solution means all values range satisfy guard also force specializations torch export dynamic_shapes Dim _check_same_range c Mapping str int dim object - bool returns True c dim both min max ranges same values _is_dim dim min c max c dim min c get min == dim min == c get min type ignore attr-defined let pass analysis min = specified min = dim max == c get max int_oo type ignore attr-defined newly introduced roots part we handle adding newly introduced roots these arise guards like x shape == leading suggested fixes like dx = _dx extract _dx find appropriate min max values before we have something like dx eq _dx+ min max dy dx+ dz dx+ we want instead _dx min max dx _dx+ dy _dx+ dz _dx+ introduced_roots dict str str = map new root - old root k c list results items eq c isinstance c eq sympy Expr derived dim root = next iter c eq free_symbols str root name_to_dim introduced_roots str root = k calculate necessary min max modulus remainder = sympy polys polytools div c eq root c_min = c get min min_ = math ceil c_min - remainder modulus c_max = c get max int_oo max_ = math floor c_max - remainder modulus create result dim results str root = min min_ max max_ name_to_dim str root = Dim str root min=min_ max=max_ remove old root min max bounds c pop min None c pop max None alter derivations depend old root unify new root e g dx= _dx+ dy=dx+ - dy= _dx+ old_root introduced_roots values c results values eq c isinstance c eq sympy Expr str symbol = next iter c eq free_symbols == old_root derived dim root = old_root new_root_expr = results str old_root eq dx= _dx+ new_expr = c eq subs symbol new_root_expr dy= _dx+ + c eq = new_expr root swapping collect all original roots used calculating values suggested fixes consists dx min max - dx refined root dim dy dx + - dx root suggested fix modified_roots set str = set k c results items k name_to_dim _dynamo export may handle source directly continue _is_dim name_to_dim k min c max c case modified_roots add k eq c isinstance c eq sympy Expr case root = next iter c eq free_symbols assert root None modified_roots add str root exclude newly introduced roots we ve already processed these modified_roots = modified_roots difference introduced_roots evaluate new value each root now either unchanged refined new range specialized concrete value modified_root_values dict str dict str Any = mroot modified_roots swapped_root = True mroot results c = results mroot min c max c isinstance range c eq int specialized here original root root Dim concrete value results derived dim swapped we handle below _check_same_range c name_to_dim mroot ignore unchanged modified_root_values mroot = c swapped_root = False swapped_root original root has been swapped results means new root range had specialized original root would have too find new root solve original root s range k c results items k name_to_dim continue dim = name_to_dim k dim __class__ __name__ == _DerivedDim dim root __name__ == mroot only look min max root otherwise root would have specialized min c max c expr = sympy sympify k s = next iter expr free_symbols result = min try_solve sympy Eq expr c min s type ignore arg-type index max try_solve sympy Eq expr c max s type ignore arg-type index _check_same_range result name_to_dim mroot type ignore index arg-type ignore unchanged modified_root_values mroot = result type ignore index break filter out results where key derived dim e g dx - we only want suggest fixes root avoid derived names also remove anything modified_roots since we either add new modified values after have decided they unchanged k list results keys k name_to_dim continue _is_derived_dim name_to_dim k k modified_roots del results k update results modified root values now results has following properties - only contains original roots keys - each root now either specialized refined derived another original root results update modified_root_values prettify_results original_signature inspect Signature dynamic_shapes Union dict str Any tuple Any list Any constraint_violation_error object forced_specializations dict str str - str Format message constraint violation errors torch export dynamic_shapes _get_dim_name_mapping _dcp source_name_to_debug_name nothing do transform s str inverse bool = False - str k v _dcp source_name_to_debug_name items s = s replace k v inverse s replace v k s results defaultdict str dict str Any = defaultdict dict dynamic_shapes None dynamic_shapes = flip op str - str op == = = op == = = op == op == assert op == == op relation_with_digit expr str op str digit int - None op == = results expr max = digit op == results expr max = digit - op == = results expr min = digit op == results expr min = digit + assert op == == results expr eq = digit retrieve dynamic shapes name_to_dim = _get_dim_name_mapping dynamic_shapes s _static_results union _dynamic_results t = transform s t == s continue left op right = re split r == &#124; = &#124; = &#124; &#124; t op = op strip op == == left == right continue right isdigit relation_with_digit left op int right left isdigit relation_with_digit right flip op int left assert op == == t try results left eq = sympy sympify right except TypeError rhs source linked Dim name pass order forced specializations based name forced_specializations = k forced_specializations k k sorted forced_specializations keys key=lambda x x split = buf = forced_specializations debug_names = set k forced_specializations dim = name_to_dim k split = _is_derived_dim dim debug_names add dim root __name__ type ignore attr-defined debug_names add dim __name__ buf += f Specializations unexpectedly required join sorted debug_names For more information run TORCH_LOGS= +dynamic \n s val forced_specializations items buf += f - solving guards generated s resulted specialized value val \n _process_derived_dim_roots results name_to_dim dims = others = order results source name results = k results k k sorted results keys key=lambda x transform x inverse=True k c results items eq c other = c eq isinstance other int others append f k = other _is_supported_equivalence other others append f k = other min_ = c get min None min_ == min_ = None max_ = c get max None min_ None max_ None dims append f k = Dim k min= min_ max= max_ min_ None dims append f k = Dim k min= min_ max_ None dims append f k = Dim k max= max_ dims append f k = Dim k results will get filtered out no new suggestions can happen guards too complex case don t suggest fix dims others buf += \nSuggested fixes \n buf += \n join dims + others buf TLS = threading local dataclass frozen=True ShapeEnvSettings Encapsulates all shape env settings could potentially affect FakeTensor dispatch Used when creating dispatch cache keys allow_scalar_outputs bool allow_dynamic_output_shape_ops bool assume_static_by_default bool specialize_zero_one bool duck_shape bool prefer_deferred_runtime_asserts_over_guards bool trace_asserts bool dataclass ValueRangesSLoc Locations guards triggered lower upper bound lower SLoc upper SLoc contextmanager _suppress_guards shape_env ShapeEnv - Iterator None shape_env _suppress_guards_enter try yield finally shape_env _suppress_guards_exit dataclass _FrameLocalResult loc Optional str = None locals dict str Any = field default_factory=dict symbols dict str str = field default_factory=dict ShapeEnv This wrapper over actual __init__ function Where add new constructor parameter ShapeEnv ===================================================== This __init__ function should used only parameters related event recording These parameters we don t wish pass down road new ShapeEnv instances created replaying events If you wish add parameter constructor ShapeEnv unrelated event recording do so _init function __init__ should_record_events Optional bool = None tracked_fakes Optional list Any = None kwargs Any - None _init kwargs Disable event recording when replaying kwargs should_record_events = False torch fx experimental validator translation_validation_enabled _translation_validation_enabled = translation_validation_enabled If specified enable event recording both - Translation validation - Translation validation bisection disabled should_record_events = should_record_events should_record_events None _translation_validation_enabled config translation_validation_no_bisect Enable event recording check both - It should record events - The recording check enabled check_recorded_events = should_record_events config check_shape_env_recorded_events This will make sure we only record top-level function call is_recording = False Keep track list tracked fakes tracked_fakes = tracked_fakes List events reconstructing ShapeEnv arbitrary points time events list ShapeEnvEvent = ShapeEnvEvent ShapeEnv kwargs=kwargs should_record_events FakeTensor per-ShapeEnv operation cache This used caching operations contain symbolic shapes which have guards ShapeEnv so ShapeEnv-dependent NOTE It s important SymNodes cache have their ShapeEnv stripped otherwise you end up cycles which can only cleaned GC fake_tensor_cache dict torch _subclasses fake_tensor _DispatchCacheKey torch _subclasses fake_tensor _DispatchCacheEntry = Pro-tip you add new field ShapeEnv affects some accept tests Accept their output EXPECTTEST_ACCEPT= python test dynamo test_dynamic_shapes py -k test_shape_env_equal _init allow_scalar_outputs bool = True allow_dynamic_output_shape_ops bool = True NB These legacy configuration help us make good choices when constraint dynamic dims explicitly passed us Ideally we will fix all call sites explicit have implicit choices apparently pretty involved assume_static_by_default bool = False Note - On specialization The following options affect decisions we make about eager specialization Disabling them will increase trace time we do more symbolic reasoning can also harm quality generated code because inductor may able specialize bounds being equal -- although we later respecialize because guard your code may just good before When True eagerly specialize input sizes which have specialize_zero_one bool = True When True assume input sizes which have same size symbolically equal duck_shape Optional bool = None For debugging co_fields Optional dict str str = None When True whenever safe we will generate deferred runtime assert instead guard whenever we know expression must True otherwise would error even backed SymInts where we could ostensibly unconditionally generate guards This useful export where preventing error checking sizes showing up guards helpful since these guards some sense overly pedantic See also https github com pytorch pytorch issues prefer_deferred_runtime_asserts_over_guards bool = False XXX Add any new settings could affect FakeTensor evaluation torch _subclasses fake_tensor _ShapeEnvSettings trace_asserts bool = False - None duck_shape None duck_shape = config use_duck_shape settings = ShapeEnvSettings Not directly used ShapeEnv indirectly used FakeTensor allow_scalar_outputs=allow_scalar_outputs allow_dynamic_output_shape_ops=allow_dynamic_output_shape_ops End assume_static_by_default=assume_static_by_default specialize_zero_one=specialize_zero_one duck_shape=duck_shape prefer_deferred_runtime_asserts_over_guards=prefer_deferred_runtime_asserts_over_guards trace_asserts=trace_asserts guards list ShapeGuard = axioms dict sympy Expr sympy Expr = A set ids have already been allocated This used when we allocate symbol ids using hash source names ensure we don t have collisions via linear probing unique_ids set int = set Maps symbolic ints their original concrete values Currently populated tensors var_to_val dict sympy Symbol sympy Integer = Like var_to_val only set when propagate_real_tensors Used last resort avoid GuardOnDataDependent error unbacked_var_to_val dict sympy Symbol sympy Integer = Like above used exclusively OBLIVIOUS_SIZE These potentially could put together I am sure writing out logic individually before abstracting oblivious_var_to_val dict sympy Symbol sympy Integer = Maps symbolic ints their min max range These ranges conservative int MUST fall range range may contain ints which may actually appear practice var_to_range dict sympy Symbol ValueRanges = var_to_range_sloc dict sympy Symbol ValueRangesSLoc = source_name_to_debug_name dict str str = var_to_sources dict sympy Symbol list Source = A set unbacked symbols inputs i e data dependent unbacked_inputs OrderedSet sympy Symbol = OrderedSet var_to_stack dict sympy Symbol CapturedTraceback = var_to_hint_override dict sympy Symbol int = Maps source original symbol assigned source_to_var dict str sympy Symbol = Maps sympy ints expressions representing them Populated equality guards i e shape == b shape replacements dict sympy Symbol sympy Expr = The sloc guard triggered replacement added replacements_slocs dict sympy Symbol SLoc = unbacked_renamings dict sympy Symbol sympy Symbol = Set holds b expressions evaluate divisible set sympy Expr = set Set holds size-like symbols When we perform size-oblivious tests these can assumed = size_like set sympy Symbol = set Duck-shaping says two input tensors have same size they get assigned same symbolic variable val_to_var dict int sympy Symbol = unbacked_symfloat_counter = unbacked_symint_counter = Similar guards these MUST evaluate true can only evaluated runtime midway through i e they always involve unbacked symints For efficiency reasons we index following way Suppose you have runtime assert i + i = s We pick most recently allocated symbol source expression add assert list symbol e g i i + i = s We access runtime asserts two situations - When we guarding expression we will attempt statically evaluate case unbacked SymInts can simplify away If we have runtime assert we may able discharge guard entirely We only need attempt runtime asserts mention freevars expression question - When we performing codegen Inductor eager when finalizing export FX graph we need know what extra runtime asserts insert Whenever unbacked SymInt comes into scope all runtime asserts involving become eligible insertion so long all their other free unbacked symbols also scope We technically can handle any choice key kicking inexpressible asserts next unbacked symbol wait we choose latest key assert will only show up moment when we can actually codegen deferred_runtime_asserts dict Optional sympy Symbol list RuntimeAssert = This exists so we can efficiently invalidate cache s used part cache key otherwise we d have iterate through deferred_runtime_asserts compute its length num_deferred_runtime_asserts = log = log log info create_env frozen = False runtime_asserts_frozen = False dim_constraints Optional DimConstraints = None counter Counter str = collections Counter Mapping sympy Symbol number guards which mention symbol symbol_guard_counter Counter sympy Symbol = collections Counter A selection important fields co_field solely used signpost_event co_fields = co_fields co_fields Whenever we allocate fresh unbacked Symbol we add pending list Unbacked symbol allocation can occur unpredictable points during meta tensor propagation some point we have know what binding site unbacked symbol computed when we actually place node graph The important thing we always actually handle every unaccounted unbacked symbol so list helps us keep track them then make sure they all accounted We could potentially give rise errors earlier lexically scoping when we do propagation only allowing unbacked symbols allocated point time However inconvenient do Dynamo because fake tensor propagation far when we analyze binding sites set_example_value so we do more mutatey way NB fresh unbacked symbols NEVER get substitutions applied them they binding sites pending_fresh_unbacked_symbols list sympy Symbol = Version counter used invalidate cached values _prev_cache_key = _get_key _version_counter = Each time divisible changed should set True set _update_version_counter _resimplify_floor_div_axioms = True Cache FX nodes Maps already built node tuple node s target list arguments This drastically reduces size FX graph avoiding duplicated nodes fx_node_cache dict tuple Callable tuple Any torch fx Node = source_to_symbol dict str sympy Symbol = Suppose you want replace unbacked symbol another unbacked symbol This error prone because you can cause references unbacked symbols time travel backwards E g u = x item use u u = y item u = z item torch _check u == u + u If you replace u u + u then use u now references u u prior them actually being bound runtime To control we track order unbacked symbols allocated only allow substitutions they respect dependency order unbacked symbol can only substituted unbacked symbols come before order This also imposes ordering unbacked symbol binding sites themselves you allowed reorder unbacked symbol bindings At moment tracked we potentially could track IR level using higher order operator something like effect token tracking unbacked_alloc_order dict sympy Symbol int = specialization_stacks dict Source traceback StackSummary = trace_asserts = trace_asserts specializations OrderedSet Specialization = OrderedSet torch fx experimental validator translation_validation_enabled _translation_validation_enabled = translation_validation_enabled _translation_validation_enabled torch fx experimental validator TranslationValidator validator = TranslationValidator graph = torch fx Graph Create output graph start inserting before This needed when deepcopy -ing object graph inserting_before graph output None Mapping each node name node itself This useful matching FX node recorded ShapeEnv graph FX node ShapeEnv we running event Whenever you add node graph you must add mapping variable Otherwise built FX graph replayed ShapeEnv will valid name_to_node dict str torch fx Node = property allow_scalar_outputs - bool settings allow_scalar_outputs property allow_dynamic_output_shape_ops - bool settings allow_dynamic_output_shape_ops property assume_static_by_default - bool settings assume_static_by_default property specialize_zero_one - bool settings specialize_zero_one property duck_shape - bool settings duck_shape property prefer_deferred_runtime_asserts_over_guards - bool settings prefer_deferred_runtime_asserts_over_guards contextmanager patch_source_specialization source Source check_fn Callable sympy Symbol sympy Expr - Iterator None Temporarily add symbol-level axioms ShapeEnv This useful when you want fork have parallel universes ShapeEnvs For example we use when doing multi-graph compile so we can support various graphs varying levels specializations This context manager allows temporarily adding constraints shape environment based specialization function applied symbol associated source Args source The source symbol specialize check_fn A function takes sympy Symbol returns sympy expression representing constraint specialization applied name = source name sym = source_to_var name expr = check_fn SymInt SymNode sym int None node _expr new_axioms = dict get_implications simplify expr added_replacements = axiom new_axioms isinstance axiom sympy Eq isinstance axiom lhs sympy Symbol isinstance axiom rhs sympy Integer axiom lhs replacements replacements axiom lhs = axiom rhs added_replacements axiom lhs = axiom rhs axioms update new_axioms We need freeze ShapeEnv because any additional modification ShapeEnv will cause unsoundness subsequent specialization calls frozen = True try yield finally k new_axioms axioms pop k None k added_replacements replacements pop k None frozen = False check_equal other ShapeEnv - None Compare another ShapeEnv equivalence ShapeEnv fields relevant outcome ShapeEnv produce_guards call - Debugging variables - Translation validation related variables - Events recording related variables non_state_variable_names = counter log var_to_stack fx_node_cache graph validator check_recorded_events should_record_events is_recording tracked_fakes events source_name_to_debug_name _prev_cache_key _version_counter dim_constraints source locations OK diverge var_to_range_sloc replacements_slocs _resimplify_floor_div_axioms _expr_sym_node_id specialization_stacks Mapping value each to-be-compared field into values should actually compared You should modify example field holds state debugging information e g ShapeGuard holds actual guard sympy Expr stack when added set guards In order compare we throw away stack information map_value key str value Any - Any key == guards Transform list ShapeGuard into list expressions g expr g value key == deferred_runtime_asserts Transform list RuntimeAsserts into list expressions s ra expr ra ras s ras value items key == name_to_node Compare just set keys same set value keys key symbol_guard_counter pending_fresh_unbacked_symbols fake_tensor_cache Skip comparisons None value shape_env_check_state_equal other non_state_variable_names map_value _snapshot_tracked_fakes - Optional list Any tracked_fakes None None torch _dynamo variables builder TrackedFake maybe_transform_fake fake TrackedFake - TrackedFake inner_fake = fake fake isinstance fake fake torch SymInt torch SymFloat FakeTensorMeta from_fake fake fake Even though TrackedFake accepts either Union SymInt FakeTensor here we give FakeTensorMeta two reasons all information we need when recording ShapeEnvEvents works even each TrackedFake changes its metadata TrackedFake inner_fake fake source fake symbolic_context type ignore arg-type maybe_transform_fake fake fake tracked_fakes _last_event_index - int len events - contextmanager _recording - Iterator None is_recording = True try yield finally is_recording = False record_shapeenv_event _eliminate_unbacked orig_s sympy Symbol new_s sympy Expr - None _set_replacement orig_s new_s eliminate_unbacked record_shapeenv_event set_unbacked_var_to_val k sympy Symbol v int - None Used only when propagate_real_tensors registers value unbacked symbol which can used last resort resolve hints log info set_unbacked_var_to_val s = s k v unbacked_var_to_val k = sympy sympify v Unlike set_replacement records shapeenv event record_shapeenv_event _rename_unbacked_to orig_s sympy Symbol new_s sympy Symbol - None assert isinstance orig_s sympy Symbol orig_s assert isinstance new_s sympy Symbol new_s assert free_unbacked_symbols new_s new_s assert free_unbacked_symbols orig_s orig_s dest = replacements get orig_s dest None assert free_unbacked_symbols dest f orig_s - dest _set_replacement orig_s new_s rename_unbacked_to unbacked_renamings orig_s = new_s dest None _set_replacement new_s dest rename_unbacked_to_dest record_shapeenv_event _constrain_is_bounded sympy Symbol upper_bound int - None TODO Do something nontrivial when upper_bound expression pass record_shapeenv_event _constrain_range_for_size sympy Symbol min Optional int = None max Optional int = None - None min None min = max None max = int_oo max min raise ValueError Maximum value constrain_as_size can t less than specified min value f received min= min max= max constrain_symbol_range compiler_min=min compiler_max=max size_like add record_shapeenv_event _constrain_range sympy Expr min int max int - None isinstance sympy Integer min = int = max raise ValueRangeError f Invalid value int range min max TODO Shouldn t we install guard symbol backed Or semantics unchecked assert actually something useful Might better restrict only unbacked SymInt isinstance sympy Symbol constrain_symbol_range compiler_min=min compiler_max=max record_shapeenv_event _constrain_unify SymInt b SymInt - None Given two SymInts constrain them so they must equal NB will work SymInts represent nontrivial expressions yet TODO does install deferred runtime assert yet TODO Maybe dedupe _maybe_guard_rel Update Feb extra important do doesn t handle unbacked replacements properly nor does generate deferred runtime asserts isinstance SymInt isinstance b SymInt assert == b assert isinstance b node expr sympy Symbol constraining non-Symbols NYI assert b node shape_env replacements b node expr = sympy Integer TODO Actually we can support long one them symbol NB We can t actually do unification our operators injective assert isinstance node expr sympy Symbol constraining non-Symbols NYI assert node shape_env isinstance b SymInt replacements node expr = sympy Integer b assert node shape_env b node shape_env assert isinstance b node expr sympy Symbol constraining non-Symbols NYI new_var = _find node expr replacements b node expr = new_var _ignore_fresh_unbacked_symbols_tls - bool getattr TLS ignore_fresh_unbacked_symbols False record_shapeenv_event _ignore_fresh_unbacked_symbols_set b bool - bool prev = _ignore_fresh_unbacked_symbols_tls TLS ignore_fresh_unbacked_symbols = b prev contextmanager ignore_fresh_unbacked_symbols - Iterator None Indicates newly allocated unbacked SymInts being discarded prev = _ignore_fresh_unbacked_symbols_set True try yield finally _ignore_fresh_unbacked_symbols_set prev record_shapeenv_event freeze - None Freeze ShapeEnv stop accumulating guards A frozen ShapeEnv will ignore any further guards generated only emit warning which may lead accuracy problems frozen = True record_shapeenv_event freeze_runtime_asserts - None Freeze ShapeEnv stop adding deferred runtime asserts We will error you try install new runtime assert when frozen This would indicate lowering violation perhaps something we know statically already True we checking again way clearly dischargeable prefer_deferred_runtime_asserts_over_guards = False runtime_asserts_frozen = True _create_symbol_for_source source Source - Optional sympy Symbol _translation_validation_enabled None srcname = source name source source_to_symbol source_to_symbol srcname = sympy Symbol srcname integer=True source_to_symbol srcname _add_z var symbol sympy Symbol type type - None _translation_validation_enabled validator add_var symbol type _add_target_expr expr SympyBoolean - None _translation_validation_enabled validator add_target_expr expr _add_assertion expr SympyBoolean - None _translation_validation_enabled validator add_assertion expr _check_translation_validate - None _translation_validation_enabled validator validate record_shapeenv_event _create_fx_call_function op Callable args tuple - tuple Optional torch fx Node bool Cache tuple order avoid duplicated nodes node_key = op args Flags whether returned node cached fresh = False _translation_validation_enabled node_key fx_node_cache Presence None arguments implies we should ignore operation any None args We check we mixing SymNode should ignored fx_node None those should fx_node None assert all isinstance torch fx Node args None fresh fresh = True If translation validation enabled all arguments must have its own FX node assert all None args f missing arg FX graph op __name__ args node = fx_node_cache node_key = graph call_function op args name_to_node node name = node fx_node_cache get node_key None fresh _create_fx_placeholder_and_z var symbol sympy Symbol type type - Optional torch fx Node _translation_validation_enabled None node_key = graph placeholder symbol Check we haven t added symbol already If so skip placeholder creation generates invalid Python code node_key fx_node_cache Add Z variable according type _add_z var symbol type Create FX placeholder out mangled name mangled_name = re sub r ^a-zA-Z - _ re sub r symbol name node = fx_node_cache node_key = graph placeholder mangled_name name_to_node node name = node Attach symbol placeholder so we can retrieve Z variable later node meta symbol = symbol fx_node_cache node_key _remove_fx_node node Optional torch fx Node - None _translation_validation_enabled node None name_to_node pop node name graph erase_node node _add_fx_node_metadata node torch fx Node - None torch _dynamo utils get_current_node should_record_events node meta SHAPEENV_EVENT_KEY = _last_event_index node meta CURRENT_NODE_KEY = get_current_node staticmethod _suppress_guards_tls - bool getattr TLS suppress_guards False record_shapeenv_event _suppress_guards_enter - None hasattr TLS suppress_guards_stack TLS suppress_guards_stack = old = _suppress_guards_tls TLS suppress_guards_stack append old TLS suppress_guards = True record_shapeenv_event _suppress_guards_exit - None old = TLS suppress_guards_stack pop len TLS suppress_guards_stack False TLS suppress_guards = old suppress_guards - _GeneratorContextManager None Context manager ignore all guards generated inside _suppress_guards _get_key - tuple int int int int Defines current state guards we ve accumulated ShapeEnv Determines when we need invalidate our cache len replacements len divisible num_deferred_runtime_asserts len unbacked_var_to_val _update_version_counter - None change shape env effects divisible set _resimplify_floor_div_axioms This used trigger resimplication FloorDiv CleanDivs implication inside function resimplify_floor_div len divisible = _prev_cache_key _resimplify_floor_div_axioms = True The shape environment queried orders magnitude more often than changed so we summarise cache key into linearly increasing version counter which cheaper check _lru_cache Only update version counter state actually changed cur_key = _get_key _prev_cache_key = cur_key _prev_cache_key = cur_key _version_counter += _produce_dyn_sizes ex_size Sequence IntLikeType source Source symbolic_context SymbolicContext - list sympy Expr _produce_dyn_sizes_from_int_tuple tuple ex_size source symbolic_context _produce_dyn_sizes_from_int_tuple tensor_size Sequence IntLikeType source Source symbolic_context SymbolicContext hint_overrides Optional dict int int = None - list sympy Expr assert all is_symbolic val val tensor_size f Expect size plain tuple ints got tensor_size torch _dynamo source TensorProperty TensorPropertySource hint_overrides hint_overrides = _assert_symbol_context symbolic_context dynamic_dims = symbolic_context dynamic_sizes type ignore attr-defined constraint_dims = symbolic_context constraint_sizes type ignore attr-defined size = i val enumerate tensor_size sym = create_symbol hint_overrides get i val TensorPropertySource source TensorProperty SIZE i dynamic_dims i constraint_dims i do_not_specialize_zero_one=config backed_size_oblivious symbolic_context=symbolic_context isinstance symbolic_context StatelessSymbolicContext symbolic_context specialize_on specialization symbolic_context specialize_on i specializations add Specialization TensorPropertySource source TensorProperty SIZE i specialization config backed_size_oblivious isinstance sym sympy Symbol could static symbol_is_type sym SymT SIZE size_like add sym size append sym size create_symbolic_sizes_strides_storage_offset ex torch Tensor source Source symbolic_context Optional SymbolicContext = None - tuple tuple IntLikeType tuple IntLikeType IntLikeType Returns list symbolic sizes strides given tensor We try our best express stride terms sizes so introduce new symbolic variables ex_size = tuple _maybe_specialize_sym_int_with_hint sz sz ex size ex_stride = tuple _maybe_specialize_sym_int_with_hint sd sd ex stride ex_storage_offset = _maybe_specialize_sym_int_with_hint ex storage_offset _create_symbolic_sizes_strides_storage_offset ex_size ex_stride ex_storage_offset _is_dim_dynamic ex i i range ex dim source symbolic_context=symbolic_context Dynamo may want wrap FakeTensors SymInt sizes up e g make_fx opt_f tracing_mode= symbolic We create symbols shape_env using backed hints behind SymInt Case when SymInt backed dynamo can proceed FakeTensors have concrete shape produce_guards will trigger specializations outer stuff Case when SymInt unbacked we will throw data dependent error require_hint It s probably good now s important note approach has implications original shape_env when checking guards different order Example --------- Consider function opt_f shown below torch compile opt_f x bool y Tensor x == True y + torch randn y Depending sequence calls we might install two different sets guards opt_f False y - x == False always works any size y opt_f True y - Triggers recompilation results guards like - x == True y size == - y size == x == True The order checking guards matters In specific example If True branch guard check precedes False branch True branch y size check precedes x == True we may have unnecessary shape specialization y _maybe_specialize_sym_int_with_hint maybe_sym IntLikeType - IntLikeType assert isinstance maybe_sym int torch SymInt is_symbolic maybe_sym assert maybe_sym node shape_env expect symbol created shape env other than current one maybe_sym node require_hint maybe_sym record_shapeenv_event _create_symbolic_sizes_strides_storage_offset NB SymInt allowed here due nested int normally you don t actually pass true symbolic sizes function ex_size Sequence IntLikeType ex_stride Sequence IntLikeType ex_storage_offset IntLikeType is_dim_dynamic Sequence bool source Source symbolic_context Optional SymbolicContext = None hint_overrides Optional dict int int = None - tuple tuple IntLikeType tuple IntLikeType IntLikeType dim = len ex_size hint_overrides hint_overrides = Reimplement legacy behavior symbolic_context None constraint_sizes list DimConstraint = None dim constraint_strides list DimConstraint = None dim dynamic_dims = dynamic_strides = i range dim NB This encapsulation breaking Legacy behavior bad is_dim_dynamic i r = DimDynamic DYNAMIC assume_static_by_default r = DimDynamic STATIC r = DimDynamic DUCK dynamic_dims append r dynamic_strides append r dynamic_dims = DimDynamic DUCK dim dynamic_strides = DimDynamic INFER_STRIDE dim symbolic_context None - set one symbolic_context = StatelessSymbolicContext dynamic_sizes=dynamic_dims dynamic_strides=dynamic_strides constraint_sizes=constraint_sizes constraint_strides=constraint_strides We got StatelessSymbolicContext _assert_symbol_context symbolic_context constraint_sizes = symbolic_context constraint_sizes type ignore attr-defined constraint_strides = symbolic_context constraint_strides type ignore attr-defined dynamic_sizes = symbolic_context dynamic_sizes type ignore attr-defined dynamic_strides = symbolic_context dynamic_strides type ignore attr-defined TODO make configurable outside symbolic_context we made symbolic_context decision here where all sizes static we going specialize all inner strides offset too We don t have do arguably we should ALWAYS allow dynamic offset cheap TODO This should DYNAMIC using DUCK BC dynamic_offset = DimDynamic STATIC all r == DimDynamic STATIC r dynamic_sizes DimDynamic DUCK are_sizes_static = all r == DimDynamic STATIC r dynamic_sizes assert len dynamic_sizes == dim f len dynamic_sizes = dim assert len dynamic_strides == dim f len dynamic_sizes = dim assert len constraint_sizes == dim assert len constraint_strides == dim torch _dynamo source TensorProperty TensorPropertySource size list sympy Expr = _produce_dyn_sizes_from_int_tuple ex_size source symbolic_context hint_overrides=hint_overrides stride = _compute_symbolic_stride source size ex_size ex_stride dynamic_strides constraint_strides are_sizes_static symbolic_context sym_sizes = create_symintnode sym hint=hint_overrides get i hint source=TensorPropertySource source TensorProperty SIZE i i sym hint enumerate zip size ex_size i sym enumerate sym_sizes isinstance sym torch SymInt i hint_overrides var_to_hint_override sym node expr = hint_overrides i sym_stride = i stride_expr enumerate stride NB Don t duck size stride instead use expression we computed assert stride_expr None sym_stride append create_symintnode stride_expr hint=ex_stride i source=TensorPropertySource source TensorProperty STRIDE i sym_storage_offset = create_symintnode create_symbol ex_storage_offset TensorPropertySource source TensorProperty STORAGE_OFFSET dynamic_dim=dynamic_offset constraint_dim=None symbolic_context=symbolic_context hint=ex_storage_offset source=TensorPropertySource source TensorProperty STORAGE_OFFSET tuple sym_sizes tuple sym_stride sym_storage_offset _compute_symbolic_stride source Source size Sequence sympy Expr ex_size Sequence IntLikeType ex_stride Sequence IntLikeType dynamic_strides Sequence DimDynamic constraint_strides Sequence Optional Union StrictMinMaxConstraint RelaxedUnspecConstraint are_sizes_static bool symbolic_context SymbolicContext - list sympy Expr torch _dynamo source TensorProperty TensorPropertySource stride list Optional sympy Expr = None len size candidates dict IntLikeType sympy Expr = iterate over unbound strides val ascending order index descending tie breaker since cases like we want fill right most stride first val_list = val -i i val enumerate ex_stride val_list sort key=_nested_int_aware_sort val neg_i val_list i = -neg_i contiguous_stride = i = len ex_stride - ex_stride i == ex_size i + ex_stride i + val contiguous_stride out_stride = sympy Integer val dynamic_stride = dynamic_strides i dynamic_stride == DimDynamic INFER_STRIDE val candidates Set stride candidate only DimDynamic INFER_STRIDE out_stride = candidates val Set INFER_STRIDE STATIC DUCK depending sizes dyn_stride = dynamic_stride dynamic_stride == DimDynamic INFER_STRIDE dyn_stride = DimDynamic STATIC are_sizes_static DimDynamic DUCK out_stride = create_symbol val TensorPropertySource source TensorProperty STRIDE i dynamic_dim=dyn_stride constraint_dim=constraint_strides i symbolic_context=symbolic_context stride i = out_stride candidates ex_size i val = size i out_stride assert all x None x stride stride record_shapeenv_event create_symintnode sym sympy Expr hint Optional int source Optional Source = None - IntLikeType Create SymInt value symbolic expression If you know what current hint value SymInt created pass into hint Otherwise pass None we will make our best guess _translation_validation_enabled source None Create new symbol source symbol = _create_symbol_for_source source assert symbol None Create new FX placeholder Z variable symbol fx_node = _create_fx_placeholder_and_z var symbol int Add equality assertion newly created symbol sym _add_assertion sympy Eq symbol sym fx_node = None out IntLikeType isinstance sym sympy Integer hint None assert int sym == hint out = int sym How can occur When we mark_unbacked we end up real tensor has hints all sizes we MUST NOT create SymNode hint because we re hiding hint our eyes unbacked Symbol And fact hint compute may inconsistent size oblivious tests free_unbacked_symbols sym hint = None out = SymInt SymNode sym int hint fx_node=fx_node out record_shapeenv_event create_symfloatnode sym sympy Expr hint Optional int &#124; float &#124; bool source Optional Source = None - FloatLikeType Create SymFloat value symbolic expression _translation_validation_enabled source None Create new symbol source symbol = _create_symbol_for_source source assert symbol None Create new FX placeholder Z variable symbol fx_node = _create_fx_placeholder_and_z var symbol float Add equality assertion newly created symbol sym _add_assertion sympy Eq symbol sym fx_node = None out FloatLikeType isinstance sym sympy Float hint None assert float sym == hint out = float sym You could give same treatment SymInt above you supported mark_unbacked float s kind strange thing do though because floats don t get specialization anyway free_unbacked_symbols sym assert hint None sym out = SymFloat SymNode sym float hint fx_node=fx_node out record_shapeenv_event create_unspecified_symint_and_symbol value int source Source dynamic_dim DimDynamic - IntLikeType Create SymInt wrapping new unspecified symbol create_symintnode create_unspecified_symbol value source=source dynamic_dim=dynamic_dim hint=value source=source create_symboolnode sym sympy Expr - SymBool Create SymBool object sympy boolean expression This function only being used serialization so we do track validation SymBool SymNode sym bool None _log_create_unbacked_symbol prefix str symbol sympy Symbol vr ValueRanges source Optional Source = None sym_node Optional SymNode = None - None is_debug = config extended_debug_create_symbol None str symbol config extended_debug_create_symbol split sloc Union str SLoc source None sloc maybe_extra_debug = _get_stack_summary is_debug sloc maybe_extra_debug = source name log info s s s s s s prefix symbol vr lower vr upper sloc maybe_extra_debug stack_info=is_debug trace_structured create_unbacked_symbol metadata_fn=lambda symbol str symbol node_id id sym_node vr f vr lower vr upper user_stack structured get_user_stack stack structured get_framework_stack record_shapeenv_event create_unbacked_symfloat - SymFloat Create symbolic float without hint value symbol sympy Symbol = make_symbol SymT UNBACKED_FLOAT unbacked_symfloat_counter unbacked_symfloat_counter += counter create_unbacked_symbol += _ignore_fresh_unbacked_symbols_tls pending_fresh_unbacked_symbols append symbol var_to_stack symbol = CapturedTraceback extract skip= vr = var_to_range symbol = ValueRanges unknown assert vr is_float sloc = _get_sloc var_to_range_sloc symbol = ValueRangesSLoc sloc sloc Create new FX placeholder Z variable symbol fx_node = _create_fx_placeholder_and_z var symbol float sym_node = SymNode symbol float None fx_node=fx_node _log_create_unbacked_symbol create_unbacked_symfloat symbol vr sym_node=sym_node SymFloat sym_node record_shapeenv_event create_unbacked_symint source Optional Source = None - SymInt Create symbolic integer without hint value symbol sympy Symbol = make_symbol SymT UNBACKED_INT unbacked_symint_counter integer=True unbacked_symint_counter += _ignore_fresh_unbacked_symbols_tls pending_fresh_unbacked_symbols append symbol counter create_unbacked_symbol += var_to_stack symbol = CapturedTraceback extract skip= vr = var_to_range symbol = _default_unspecified_value_range assert vr is_int sloc = _get_sloc var_to_range_sloc symbol = ValueRangesSLoc sloc sloc Create new FX placeholder Z variable symbol fx_node = _create_fx_placeholder_and_z var symbol int sym_node = SymNode symbol int None fx_node=fx_node _log_create_unbacked_symbol create_unbacked_symint symbol vr source sym_node=sym_node SymInt sym_node is_unbacked_symint symbol sympy Symbol - bool Check sympy symbol matches naming convention unbacked symbols symbol_is_type symbol SymT UNBACKED_INT record_shapeenv_event create_unbacked_symbool - SymBool Create symbolic boolean without hint value symbol sympy Symbol = make_symbol SymT UNBACKED_INT unbacked_symint_counter integer=True unbacked_symint_counter += _ignore_fresh_unbacked_symbols_tls pending_fresh_unbacked_symbols append symbol counter create_unbacked_symbol += var_to_stack symbol = CapturedTraceback extract skip= vr = var_to_range symbol = ValueRanges assert vr is_int sloc = _get_sloc default value range unbacked SymBool var_to_range_sloc symbol = ValueRangesSLoc sloc sloc Create new FX placeholder Z variable symbol fx_node = _create_fx_placeholder_and_z var symbol bool sym_node = SymNode sympy Eq symbol bool None fx_node=fx_node _log_create_unbacked_symbol create_unbacked_symbool symbol vr sym_node=sym_node SymBool sym_node record_shapeenv_event create_unspecified_symbol val Union int SymInt float SymFloat source Source dynamic_dim DimDynamic = DimDynamic DUCK constraint_dim DimConstraint = None NB includes None symbolic_context Optional StatelessSymbolicContext = None - sympy Expr Create symbol unspecified value Compared standard symbols we do assume value positive nor do we specialze zero one values positive None unspecified symbols since we can t assume will neither positive nor negative We don t want specialize zero one val unspecified symbol so we can always get new symbol despite val create_symbol val source dynamic_dim constraint_dim positive=None do_not_specialize_zero_one=True symbolic_context=symbolic_context record_shapeenv_event create_symbol val int source Source dynamic_dim DimDynamic = DimDynamic DUCK constraint_dim DimConstraint = None NB includes None positive Optional bool = True do_not_specialize_zero_one bool = False symbolic_context Optional StatelessSymbolicContext = None - sympy Expr Create new symbol which tracked ShapeEnv check constraint_dim actually static integer isinstance constraint_dim StrictMinMaxConstraint constraint_dim vr lower == constraint_dim vr upper dynamic_dim = DimDynamic STATIC constraint_dim vr lower = val raise ConstraintViolationError f Static shape constraint constraint_dim vr lower does match input size val f source name symbolic_context torch _dynamo source TensorPropertySource assert isinstance source TensorPropertySource TODO storage_offset handling assert source idx None symbolic_context dynamic_sizes source idx = dynamic_dim symbolic_context constraint_sizes source idx = None constraint_dim = None see note Tensor Fakification Symbol Caching source_name = source name isinstance symbolic_context StatefulSymbolicContext id symbolic_context shape_env_to_source_to_symbol_cache symbolic_context shape_env_to_source_to_symbol_cache id = isinstance symbolic_context StatefulSymbolicContext source_name source_name symbolic_context shape_env_to_source_to_symbol_cache id symbolic_context shape_env_to_source_to_symbol_cache id source_name dynamic_dim DimDynamic SIZE_LIKE_UNBACKED DimDynamic OBLIVIOUS_SIZE out = create_unbacked_symint source node expr _constrain_range_for_size out unbacked_inputs add out isinstance symbolic_context StatefulSymbolicContext source_name symbolic_context shape_env_to_source_to_symbol_cache id source_name = out dynamic_dim DimDynamic OBLIVIOUS_SIZE oblivious_var_to_val out = val out do_not_specialize_zero_one specialize_zero_one = False specialize_zero_one = specialize_zero_one assert isinstance source Source f type source source assert positive val f positive set negative value val It s always sound allocate symbol DYNAMIC If user constrained symbol force symbolic_context DYNAMIC because our constraint code will do weird stuff e g s duck shaped constraint_dim None dynamic_dim = DimDynamic DYNAMIC dynamic_dim DimDynamic STATIC out = sympy Integer val isinstance symbolic_context StatefulSymbolicContext source_name symbolic_context shape_env_to_source_to_symbol_cache id source_name = out out dynamic_dim DimDynamic DUCK duck_shape can used globally turn off duck shaping even requested duck = duck_shape dynamic_dim DimDynamic DYNAMIC duck = False raise AssertionError f unhandled dynamic_dim dynamic_dim sloc = _get_sloc val specialize_zero_one val == sympy S Zero sympy S One duck val val_to_var If we re duck shaping we always create new symbol Even we re duck shaping we haven t seen particular value before we also create new symbol symbol_id = _generate_unique_id source name type val int is_nested_int val sympy_expr = make_symbol SymT SIZE symbol_id positive=positive integer=True sympy_expr = make_symbol SymT FLOAT symbol_id positive=positive real=True source_to_var source_name = sympy_expr We always associate vars vals isinstance val int var_to_val sympy_expr = sympy Integer val isinstance val float var_to_val sympy_expr = sympy Float val Only used jagged layout nested tensors var_to_val sympy_expr = SingletonInt val node nested_int coeff=val node nested_int_coeff Do appending later because we always want populate var_to_sources sympy_expr = Create Z variable new symbol _add_z var sympy_expr int duck Make sure reuse symbol subsequent duck shaping pyrefly ignore unsupported-operation val_to_var val = sympy_expr isinstance val int positive Add assertions newly created symbols _add_assertion sympy_expr Apply default range which assumes zero-one var_to_range sympy_expr = _default_value_range do_not_specialize_zero_one var_to_range_sloc sympy_expr = ValueRangesSLoc _get_sloc user code shown first use value -- guard itself due user code due specialization framework avoid specialization try torch _dynamo decorators mark_unbacked tensor dim specialize_zero_one None sloc var_to_range sympy_expr = _default_unspecified_value_range var_to_range_sloc sympy_expr = ValueRangesSLoc sloc sloc Small performance optimization we have min-max constraint we can proactively narrow range isinstance constraint_dim StrictMinMaxConstraint assert duck _update_var_to_range sympy_expr constraint_dim vr is_constraint=True vr = var_to_range sympy_expr assert vr is_int val vr raise ConstraintViolationError f val range vr lower vr upper range_str = f vr lower vr upper isinstance val float var_to_range sympy_expr = vr = ValueRanges -sympy oo sympy oo var_to_range_sloc sympy_expr = ValueRangesSLoc sloc sloc range_str = f vr lower vr upper assert vr is_float Skip var_range logic SingletonInt Only used jagged layout nested tensors range_str = r = sympy_expr is_debug = config extended_debug_create_symbol None str sympy_expr config extended_debug_create_symbol split maybe_more_info = is_debug os getenv TORCHDYNAMO_EXTENDED_ADVICE maybe_more_info = more info run f TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL= sympy_expr suppress message run TORCHDYNAMO_EXTENDED_ADVICE= sloc maybe_extra_debug = _get_stack_summary is_debug log info create_symbol s = s s s s s s sympy_expr val source name range_str sloc maybe_more_info maybe_extra_debug stack_info=is_debug trace_structured create_symbol metadata_fn=lambda symbol str sympy_expr val repr val vr range_str source source name user_stack structured from_traceback TracingContext extract_stack stack structured from_traceback CapturedTraceback extract skip= summary counter create_symbol += This implements duck-shaping input sizes match assigned same symint r = val_to_var val source_to_var source_name = r log debug create_symbol s duck sized s r source name isinstance r sympy Symbol r_sources = var_to_sources r r_sources append source source is_ephemeral r_sources is_ephemeral prefer non-ephemeral source first since may guarded later r_sources r_sources - = r_sources - r_sources This ensures we get zeros symbol_guard_counts which makes some queries simpler since we will accumulate mass way symbol_guard_counter r = isinstance symbolic_context StatefulSymbolicContext source_name symbolic_context shape_env_to_source_to_symbol_cache id source_name = r r add_var_to_val expr sympy Symbol val int - None Adds new symbol symbolic environment log debug add_var_to_val s s expr val stack_info=True assert expr var_to_val f expr already exists var_to_val expr = sympy Integer val _debug_name source Source - str src_name = source name source_name_to_debug_name get src_name src_name _render_range_for_constraint_violation source Source c Union StrictMinMaxConstraint RelaxedUnspecConstraint - str isinstance c StrictMinMaxConstraint lower upper = c vr lower c vr upper default = _default_value_range lower = default lower lower = None upper = default upper upper = None c_render = f _debug_name source = source name specified range lower None upper None c_render += f lower = _debug_name source = upper lower None upper None c_render += f _debug_name source = upper lower None upper None c_render += f lower = _debug_name source c_render c render source produce_guards args Any kwargs Any - list str Like produce_guards_verbose only returns non-verbose python guard expressions no verbose guards produced produce_guards_verbose args kwargs langs= python exprs produce_guards_verbose placeholders Sequence FakeTensor sources Sequence Source source_ref Callable Source str = lambda n n name guards Optional list ShapeGuard = None input_contexts Optional DimList SymbolicContext = None Encodes user-specified input shape equations form s = s s = fn s See docs EqualityConstraint details encoding equalities_inputs Optional EqualityConstraint = None _simplified bool = False Indicates we should produce guards known static values ignore_static bool = True langs tuple str = python verbose_python - list _ShapeGuardsHelper Generates list guards strings which when evaluated context defines tensors all sources returns True False depending guards list evaluated True Primarily used Dynamo also helpful manual testing guards see evaluate_guards_for_args For convenience testing source allowed str which case we will assume LocalSource simplified lets you omit duck sizing equality guards This useful testing when you don t care about boilerplate guards may helpful user output too careful though some equality guards nontrivial It would nice get simplified output print them too It s private because s intended normal use Returns guards python python verbose comments verbose default log info produce_guards Check we get same ShapeEnv state replaying recorded events This will create new ShapeEnv instance call all recorded function calls new instance Finally will check whether new instance has equal state It s important we do beginning function since modifies dim_constraints through its execution Changes happen method aren t interesting since function call we wish reproduce end If we wish simply reproduce ShapeEnv instances even after call method should also recorded check_recorded_events shape_env = replay_shape_env_events events check_equal shape_env assert len placeholders == len sources f len placeholders = len sources Tensorlike = torch Tensor FakeTensorMeta _create_no_constraints_context t Tensor - StatelessSymbolicContext StatelessSymbolicContext Ignored only constraints part relevant below dynamic_sizes= DimDynamic DYNAMIC t dim dynamic_strides= DimDynamic INFER_STRIDE t dim constraint_sizes= None t dim constraint_strides= None t dim Expand optional inputs verify invariants upheld input_contexts None pyrefly ignore bad-assignment input_contexts = pyrefly ignore bad-argument-type _create_no_constraints_context t isinstance t Tensorlike None t placeholders assert len input_contexts == len placeholders i t context enumerate zip placeholders input_contexts isinstance t Tensorlike context None pyrefly ignore bad-argument-type input_contexts i = _create_no_constraints_context t assert isinstance t SymInt int SymFloat float assert isinstance context list It took lot sweat figure out algorithm here Let s explain how works The ShapeEnv lifecycle looks something like - For each input you either generate fresh Sympy symbol s represent its value binding site you reuse some preexisting symbol expression skipping symbol allocation e g duck sizing preexisting symbol expressing stride multiplication separate stride size Naively you might expect bind fresh Sympy symbol every input fairly wasteful most these symbols immediately simplify away you don t eagerly specialize e g symbols you end up very complicated expressions optimizable practice - You perform some compute these symbols occasionally introducing guards boolean expressions these symbols In particular whenever we guard equality _maybe_guard_rel we can simplify shapes e g when s == s we can now replace all occurrences s s Sometimes boolean expression evaluation doesn t introduce guard guard already entailed simplifications we have applied - In end you have bunch replacements saying how simplify shapes bunch guards all equality guards trivial because they re covered replacements From ShapeEnv we must generate Python expression when evaluated set inputs tells us whether these boolean expressions would have evaluated same way However we cannot easily compute we elide recording boolean expressions when we think they vacuously true Thus we seek approximation we must generate expression true would have produced equivalent ShapeEnv which would answer guard expressions same way Our notion equivalence bit subtle For example consider ShapeEnv created input size versus no other guards Duck sizing would generate s s first case s s second We do NOT assume size variables disjoint so fact graph assumes input could s s subsumes s s setting s == s vice versa However consider analogous case versus Duck sizing generates s s graph does NOT subsume graph because we assume any size variables NOT make simplifications according e g we queried s == we would immediately False without returning guard So perhaps easier flip things their head guard expressions we generate here say what simplifications valid what Below we explain each guard expressions we generate TODO Make more efficient binding all size stride offsets locals before performing tests them torch _dynamo source TensorProperty TensorPropertySource Actual codegen must delayed we don t necessarily know what symbol mapping input_guards = symbol_to_source dict sympy Symbol list Source = collections defaultdict list symbol_to_constraints defaultdict sympy Symbol set Constraint = collections defaultdict set constraint_violations list tuple bool str Callable str = printers list _ShapeGuardPrinter = py_printer = ShapeGuardPythonPrinter symbol_to_source source_ref var_to_sources lang langs lang python verbose_python printers append py_printer lang == cpp printers append _ShapeGuardCppPrinter symbol_to_source source_ref var_to_sources raise NotImplementedError f Unknown lang lang record_constraint_violation warn_only bool debug_name str msg str hint Optional Callable str = None - None constraint_violations append warn_only debug_name lambda f msg hint hint msg is_dim src object - TypeGuard TensorPropertySource isinstance src TensorPropertySource src prop TensorProperty SIZE equalities_inputs source_index = i src enumerate sources source_index src name = i get_expression tensor_dim_src Source - sympy Expr fake = placeholders source_index tensor_dim_src base name type ignore attr-defined assert tensor_dim_src idx None type ignore attr-defined symint = fake shape tensor_dim_src idx type ignore attr-defined isinstance symint torch SymInt symint node expr assert type symint int f Expected int got type symint sympy Integer symint src src equalities_inputs source_pairs expr expr = get_expression src get_expression src type ignore Check whether given input shape values satisfy specified equation s = s - Raise when equation violated given input shape values - Otherwise issue guard constrain them concrete_val = evaluate_expr sympy Eq expr expr concrete_val raise ConstraintViolationError f src name = expr isinstance expr int expr xreplace var_to_val equal f src name = expr isinstance expr int expr xreplace var_to_val srcEq root fn equalities_inputs derived_equalities expr = get_expression srcEq recall root either phantom symbol input source isinstance root sympy Symbol expr debug_name = root var_to_sources root name isinstance root sympy Integer expr debug_name = root str root expr debug_name = get_expression root _debug_name root expr _ = fn expr Check whether given input shape values satisfy specified equation s = fn s - Raise when equation violated given input shape values - Otherwise issue guard constrain them concrete_val = evaluate_expr sympy Eq expr expr _ concrete_val raise ConstraintViolationError f Expected input srcEq name equal f fn sympy Symbol debug_name f where debug_name = expr xreplace var_to_val f got expr xreplace var_to_val phantom_symbol equalities_inputs phantom_symbols isinstance phantom_symbol sympy Symbol we created additional phantom symbols input shape dimensions symbol_to_source phantom_symbol extend var_to_sources phantom_symbol How do we know what value s Fresh variables can only bound inputs so there MUST some other input which binds variable If there no such input error our system We record where all symbols come help you diagnose why those symbols didn t occur In fact generally speaking only possible outermost user ShapeEnv evaluate guards because some inputs may available inner levels For example Dynamo can guard tensors never actually become graph arguments they pruned In case only Dynamo knows about these arguments track_symint source Source val IntLikeType constraint DimConstraint = None - None log debug track_symint s s s LazyString source name val constraint assert isinstance val SymInt is_symbolic val isinstance val SymInt val node maybe_as_int None val = val node maybe_as_int isinstance val SymInt s = val node expr isinstance s sympy Symbol symbol_to_source s append source constraint None isinstance constraint RelaxedUnspecConstraint symbol_to_constraints s add constraint constraint_violated = False isinstance constraint StrictMinMaxConstraint try inferring ranges expr s sym_vrs = x var_to_range get x None x s free_symbols any vr None vr sym_vrs values some free symbols s don t have ranges constraint_violated = True isinstance constraint RelaxedUnspecConstraint s is_number i = int s Don t complain about specialization we expect have compile case anyway i constraint_violated = True constraint_violated assert constraint None hint s sympy Expr - str sexpr = py_printer doprint s f sexpr var_with_range = _render_range_for_constraint_violation source constraint msg = f Not all values var_with_range valid because f _debug_name source inferred equal record_constraint_violation constraint warn_only _debug_name source msg hint=functools partial hint s input_guards append source s s = sympy Integer val input_guards append source s constraint_violated = False isinstance constraint StrictMinMaxConstraint s == constraint vr lower == constraint vr upper allow static constraints constraint_violated = True isinstance constraint RelaxedUnspecConstraint Don t complain about specialization we expect have compile case anyway val constraint_violated = True constraint_violated assert constraint None var_with_range = _render_range_for_constraint_violation source constraint user_stack = specialization_stacks get source None msg = f You marked _debug_name source dynamic your code f specialized constant val If you re using mark_dynamic f either remove use maybe_mark_dynamic If you re using Dim DYNAMIC f replace either Dim STATIC Dim AUTO + \n\nUser stack \n + join user_stack format user_stack record_constraint_violation constraint warn_only _debug_name source msg track_symfloat source Source val FloatLikeType - None log debug track_symfloat s s LazyString source name val assert isinstance val SymFloat is_symbolic val isinstance val SymFloat val node maybe_as_float None val = val node maybe_as_float isinstance val SymFloat s = val node expr isinstance s sympy Symbol symbol_to_source s append source input_guards append source s s = sympy Float val input_guards append source s pyrefly ignore no-matching-overload t source context zip placeholders sources input_contexts isinstance source str torch _dynamo source LocalSource source = LocalSource source assert isinstance source Source t None continue isinstance t SymInt int constraint = None context None getattr context constraint None track_symint source t constraint continue isinstance t SymFloat float track_symfloat source t continue assert isinstance t Tensorlike is_traceable_wrapper_subclass t torch _dynamo source AttrSource assert isinstance context SubclassSymbolicContext For subclasses we need track symints BOTH outer inner tensors TODO type better sources_tensors_constraints list tuple Source Any Any Any = source t context constraint_sizes context constraint_strides attrs _ = t __tensor_flatten__ attr attrs inner_t = getattr t attr inner_context = context inner_contexts attr sources_tensors_constraints append AttrSource source attr inner_t inner_context constraint_sizes type ignore attr-defined inner_context constraint_strides type ignore attr-defined sources_tensors_constraints = source t context constraint_sizes context constraint_strides type ignore attr-defined src curr_t constraint_size constraint_stride sources_tensors_constraints is_sparse_any curr_t i ss enumerate curr_t size property_source = TensorPropertySource src TensorProperty SIZE i track_symint property_source ss constraint_size i i ss enumerate curr_t size property_source = TensorPropertySource src TensorProperty SIZE i track_symint property_source ss constraint_size i i ss enumerate curr_t stride property_source = TensorPropertySource src TensorProperty STRIDE i track_symint property_source ss constraint_stride i track_symint TensorPropertySource src TensorProperty STORAGE_OFFSET curr_t storage_offset Every input must equal final simplified symbolic expression stored placeholder Given placeholder s s we have input we must show s == s == This does lot work covers duck sizing equality guards all_exprs list list str = _ langs dim_constraints = DimConstraints symbol_to_source var_to_val set symbol_to_constraints keys source_name_to_debug_name _simplified source expr input_guards srcname = source name _translation_validation_enabled Ignore sources turned into SymInts srcname source_to_symbol _add_target_expr sympy Eq source_to_symbol srcname expr Small optimization isinstance expr sympy Symbol symbol_to_source get expr source == symbol_to_source expr continue This logic excludes static values found tensors guarding because dynamo s check_tensor_fn does see guards cpp However non tensor sources we still need guard here ignore_static isinstance source TensorPropertySource expr is_number log debug Skipping guard s f source_ref source == expr continue is_dim source dim_constraints add_equality source expr exprs printer lang zip all_exprs printers langs res = f printer print_source source == printer doprint expr lang == verbose_python s = source_to_var get srcname None source = var_to_sources s res = f res duck sizing added equality because these f variables had same size var_to_val s avoid specialization set torch fx experimental _config use_duck_shape = False sloc = replacements_slocs get s None res = f res sloc res = f res unknown var s please file bug res = f res unknown source srcname please file bug exprs append res isinstance source TensorPropertySource source prop TensorProperty SIZE equalities_inputs len expr free_symbols == symbol = next iter expr free_symbols isinstance expr sympy Symbol expr symbol_to_constraints equalities_inputs is_equal source symbol_to_source expr msg = f The values _debug_name source = source name f _debug_name symbol_to_source expr = symbol_to_source expr name must always equal record_constraint_violation equalities_inputs warn_only _debug_name source msg isinstance expr sympy Symbol symbol symbol_to_constraints equalities_inputs is_derived source symbol_to_source symbol lambda x expr xreplace symbol x src = symbol_to_source symbol msg = f The values _debug_name source = source name must always related f values _debug_name src = src name f _debug_name source = expr xreplace symbol sympy sympify _debug_name src record_constraint_violation equalities_inputs warn_only _debug_name source msg NB Not necessary report constraint violations here constraints guaranteed symbols we ve already caught constants non-atomic expressions so we only have relational constraints we don t support those moment Every guard must evaluate True remember many guards like s == s because trivial due simplification issued = set issue_guard guard ShapeGuard - None expr = simplify guard expr Avoid re-issuing same guard expr issued issued add expr try is_trivial = False any is_dim source s expr free_symbols source symbol_to_source s assert dim_constraints None is_trivial = dim_constraints add expr exprs printer lang zip all_exprs printers langs guard_expr = printer doprint expr lang == verbose_python guard_expr = f guard_expr guard sloc exprs append guard_expr _add_target_expr expr A non-relational constraint single sizevar can violate constraint is_trivial len expr free_symbols == symbol = next iter expr free_symbols source = symbol_to_source symbol constraints = symbol_to_constraints symbol c constraints isinstance c StrictMinMaxConstraint var_with_range = _render_range_for_constraint_violation source c msg = f Not all values var_with_range f satisfy generated guard py_printer doprint expr record_constraint_violation c warn_only _debug_name source msg isinstance c RelaxedUnspecConstraint This fine we allow guards here long didn t constrain one value we don t actually know depends our ValueRanges reasoning capability pass raise AssertionError f unrecognized constraint c except Exception log warning Failing guard allocated s guard sloc raise First issue all guards This removes all checks follow bounds We could simply emit those also bounds = size when necessary guard guards guards None guards _maybe_evaluate_static guard expr axioms= size_oblivious=guard size_oblivious None continue issue_guard guard Because there guards export s constraint solver can suggest good fixes we may have deferred runtime asserts produce_guards alone won t do anything e g divisiblity guards we want send runtime asserts export s constraint solver too These will still stay graph asserts export s constraint solver can decide whether do anything them i e raise error provide suggested fixes decide s out scope leave runtime assert graph ra deferred_runtime_asserts get None _maybe_evaluate_static ra expr axioms= None continue expr = simplify ra expr dim_constraints add expr Every symbol must within its value range handles specialization too symbol sources symbol_to_source items r = var_to_range get symbol r None continue vr_sloc = var_to_range_sloc symbol assert sources bounds = rf = source_ref sources verbose_expr = r lower -sympy oo -int_oo any is_dim source source sources dim_constraints add sympy Ge symbol r lower Only print lower bound simplified mode default _simplified r lower = _default_value_range lower bounds append sympy Le r lower symbol evaluate=False verbose_expr = f r lower = rf vr_sloc lower r upper sympy oo int_oo any is_dim source source sources dim_constraints add sympy Le symbol r upper nontrivial upper bound always interesting bounds append sympy Le symbol r upper evaluate=False verbose_expr verbose_expr = f r lower = rf = r upper vr_sloc lower vr_sloc upper verbose_expr = f rf = r upper vr_sloc upper bounds bound = sympy And bounds evaluate=False exprs printer lang zip all_exprs printers langs lang == verbose_python exprs append verbose_expr exprs append printer doprint bound NB verbose_exprs done above Check constraints constraints = symbol_to_constraints symbol c constraints isinstance c StrictMinMaxConstraint TODO With int_oo I think condition noop now c vr _default_value_range issubset r source = sources expr = sympy And sympy Le r lower symbol sympy Le symbol r upper guard_expr = py_printer doprint expr var_with_range = _render_range_for_constraint_violation source c msg = f Not all values var_with_range satisfy generated guard guard_expr record_constraint_violation c warn_only _debug_name source msg We NaN specialize which means similar specialization we should assume float NOT nan This load bearing you have something like equality guard nan will play merry hell reasoning symbol_is_type symbol SymT FLOAT res = f math isnan py_printer print_source sources exprs printer lang zip all_exprs printers langs lang == verbose_python exprs append f res implicit guard float input due NaN specialization framework lang == python exprs append res lang == cpp exprs append f ~std isnan printer print_source sources raise NotImplementedError f Unimplemented lang lang constraint_violations warn_msgs list str = error_msgs list str = debug_names = set warn_only debug_name msg_cb constraint_violations warn_only str_msg = f len warn_msgs + msg_cb warn_msgs append str_msg str_msg = f - msg_cb error_msgs append str_msg pyrefly ignore bad-argument-type debug_names add debug_name len error_msgs debug_names_str = join sorted debug_names err = \n join error_msgs raise ConstraintViolationError f Constraints violated debug_names_str For more information run TORCH_LOGS= +dynamic \n f err len warn_msgs log debug s Warning only constraints violated len warn_msgs signpost_event dynamic produce_guards co_fields counter num_guards len all_exprs free_symbols sum v symbol_to_source values v The keys meaningless aggregate perspective so don t include them Biggest first symbol_guard_counts sorted symbol_guard_counter values reverse=True _translation_validation_enabled torch fx experimental validator PopulateValidator Add all deferred runtime assertions these technically handled produce_guards we need put them target set ras deferred_runtime_asserts values ra ras _add_target_expr ra expr Add value range bound guards all symbols no trivial bounds Reason _maybe_evaluate_static may eliminate guards based refined value ranges sym vr var_to_range items vr lower -sympy oo -int_oo _add_target_expr sympy Le vr lower sym vr upper sympy oo int_oo _add_target_expr sympy Le sym vr upper Before validating populate input validator built FX graph fx_traceback preserve_node_meta PopulateValidator graph validator run Only run translation validation when we passing custom guards guards None _check_translation_validate helpers list _ShapeGuardsHelper = exprs printer lang zip all_exprs printers langs lang == cpp assert isinstance printer _ShapeGuardCppPrinter helpers append _CppShapeGuardsHelper exprs printer source_to_symbol helpers append _ShapeGuardsHelper exprs helpers produce_guards_expression placeholders Sequence Union SymInt FakeTensor guards Optional list ShapeGuard = None ignore_static bool = True - Optional str Expected used evaluate_guards_expression Produces guards given placeholders returns string expression evaluated evaluate_guards_expression given concrete values placeholders torch _dynamo source LocalSource arg_names = f t i i range len placeholders produced_guards = produce_guards placeholders LocalSource arg_names guards=guards ignore_static=ignore_static produced_guards join produced_guards None evaluate_symexpr code str - Union int float bool To used compile_fx evaluate symexprs args = str e val e val var_to_val items eval code SYMPY_INTERP args deserialize_symexpr code str - Union SymInt SymFloat SymBool To used compile_fx deserialize symexprs args = str e SymInt SymNode e int int val fx_node=None e val var_to_val items eval code SYMPY_INTERP args evaluate_guards_expression code str args Sequence object - bool Expected used produce_guards_expression Evaluates expression generated produce_guards_expression given concrete args arg_names = f t i i range len args eval code SYMPY_INTERP L dict zip arg_names args evaluate_guards_for_args placeholders Sequence FakeTensor args Sequence Tensor ignore_static bool = True - bool Generate guards graph s placeholder values evaluate guards args code = produce_guards_expression placeholders ignore_static=ignore_static code evaluate_guards_expression code args True get_pruned_guards symints Sequence torch SymInt - list ShapeGuard Get list guards pruned so only provides guards reference symints passed input pyrefly ignore bad-assignment symints = s node expr s symints isinstance s node expr sympy Symbol guards = g g guards all s symints s g expr free_symbols guards bind_symbols placeholders Sequence FakeTensor args Sequence Tensor - dict sympy Symbol int Given paired list placeholders fake tensors symbolic sizes concrete arguments regular tensors real sizes returns dictionary mapping each symbol its real value So example you have placeholder size s s binding will give you s s This guaranteed bind ALL symbols ShapeEnv we can t bind symbol doesn t occur any placeholder symbols already have replacements won t get bindings This little duplicative evaluate_guards s different enough seemed cleanest make another copy This assumes guards already checked though s cheap we ll check shenanigans bindings dict sympy Symbol int = bind_symint arg object val object - None isinstance val SymInt assert isinstance arg int s = val node expr isinstance s sympy Symbol s bindings assert bindings s == arg f bindings s = arg bindings s = arg isinstance -s sympy Symbol -s bindings assert bindings -s == -arg f bindings -s = -arg bindings -s = -arg t arg zip placeholders args t None continue isinstance t SymInt bind_symint arg t continue assert isinstance t torch Tensor i s enumerate t size bind_symint arg size i s i s enumerate t stride bind_symint arg stride i s bind_symint arg storage_offset t storage_offset bindings get_nontrivial_guards - list SympyBoolean Returns list guard expressions aren t statically known i e trivial simplify guard expr guard guards _maybe_evaluate_static guard expr axioms= size_oblivious=guard size_oblivious None format_guards verbose bool = False - str Format shape env s guard expressions optional traceback info verbose \n join f - guard expr + str guard sloc verbose guard guards bound_sympy expr sympy Expr size_oblivious bool = False - ValueRanges Given sympy expression computes ValueRanges bound what values can TODO maybe s guaranteed x var_to_range var_to_range = x var_to_range get x None x expr free_symbols size_oblivious Clamp values size-like variables NB discarding old upper bound intentional per https github com pytorch pytorch pull x size_like var_to_range keys var_to_range x None NB do NOT set upper we re using solely determine we can do size-like replacement upper bound irrelevant here var_to_range x = ValueRanges int_oo bound_sympy expr var_to_range type ignore arg-type _lru_cache get_axioms symbols Optional tuple sympy Symbol = None compute_hint bool = False - tuple SympyBoolean Given symbols expression returns all runtime asserts have those symbols concatenated all guards If symbols None returns all runtime asserts all guards symbols None runtime_asserts = r expr rs deferred_runtime_asserts values r rs runtime_asserts = r expr s symbols s var_to_val r deferred_runtime_asserts get s guards Iterator SympyBoolean = g expr g guards axioms Iterator SympyBoolean = itertools chain guards runtime_asserts compute_hint axioms = canonicalize_bool_expr xreplace var_to_val axioms tuple dict fromkeys axioms keys lru_cache None get_implications e SympyBoolean - tuple tuple SympyBoolean sympy logic boolalg BooleanAtom Given expression returns list predicates follow equiv dict SympyBoolean sympy logic boolalg BooleanAtom = add_expr expr SympyBoolean - None expr = canonicalize_bool_expr expr isinstance expr sympy Eq sympy Ne No need canonicalize TODO We could further canonicalize Eq ordering lhs rhs somehow With we could remove need commutativity part opposite = sympy Eq isinstance expr sympy Ne sympy Ne Commutativity == = equiv type expr expr lhs expr rhs evaluate=False = sympy true equiv type expr expr rhs expr lhs evaluate=False = sympy true equiv opposite expr lhs expr rhs evaluate=False = sympy false equiv opposite expr rhs expr lhs evaluate=False = sympy false Expr negation equiv expr = sympy true we do pass evaluate=False like others purpose here we want b =b ~ b equiv canonicalize_bool_expr sympy Not expr = sympy false add_expr e Other relational expressions expression implies isinstance e sympy Eq add_expr sympy Le e lhs e rhs evaluate=False add_expr sympy Ge e lhs e rhs evaluate=False isinstance e sympy Lt add_expr sympy Le e lhs e rhs evaluate=False add_expr sympy Ne e lhs e rhs evaluate=False e lhs is_integer e rhs is_integer type ignore attr-defined add_expr sympy Le e lhs e rhs - evaluate=False isinstance e sympy Le add_expr sympy Lt e lhs e rhs + evaluate=False tuple equiv items _lru_cache _maybe_evaluate_static expr sympy Basic unbacked_only bool = False compute_hint bool = False size_oblivious bool = False axioms Optional tuple SympyBoolean = None var_to_range Optional tuple tuple sympy Symbol ValueRanges = None - Optional sympy Basic Tries evaluate expr without introducing guards If unbacked_only == True then we only do substitutions unbacked SymInts leaving regular hinted integers alone This could result expression still contains backed SymInts which you could then potentially guard Use compute_hint == True you trying compute non-binding hint particular hint values backed unbacked SymInts e g s happens run compute_hint will substitute s axioms compute hint NYE assert compute_hint axioms expr = simplify expr size_oblivious compute_hint expr = expr xreplace var_to_val xreplace unbacked_var_to_val expr = canonicalize_bool_expr expr resimplify_floor_div axioms dict sympy Expr sympy Expr - None _resimplify_floor_div_axioms _resimplify_floor_div_axioms = False new_items = k v list axioms items A FloorDiv implications could have became CleanDiv point due new facts shapeEnv This handles such issue its ideal This only expression simplification depends global state shape env TODO try get rid CleanDiv since breaks invariant s simplifications sympy expressions only depend expression itself k has FloorDiv new_items update simplify k v axioms update new_items Pattern matching axioms None resimplify_floor_div axioms subst = axioms subst = e axioms e free_symbols issubset expr free_symbols subst update dict get_implications simplify e resimplify_floor_div subst expr = expr xreplace subst TODO compute hint might have gotten broken here fs = expr free_symbols fs expr is_number expr is_Boolean expr var_to_range None var_ranges = var_to_range var_ranges = dict var_to_range symbol_info = tuple _SymbolInfo s var_ranges get s var_to_val get s s size_like s sorted fs key=str TODO speed up sort r = _maybe_evaluate_static_worker expr symbol_info unbacked_only size_oblivious r _lru_cache replace expr _SympyT - _SympyT Apply symbol replacements any symbols given expression replacements = pyrefly ignore missing-attribute s expr free_symbols r = _find s Micro-optimization only do replacements r s different Otherwise xreplace no-op will trigger expensive assumption queries expr has relational node r is_Symbol r = s replacements s = r replacements pyrefly ignore missing-attribute safe_expand expr xreplace replacements expr _lru_cache _update_divisible - None new_divisible = set k divisible res = replace k res is_number new_divisible add k divisible = new_divisible _update_version_counter _lru_cache simplify expr _SympyT size_oblivious bool = False - _SympyT Use known constraints replacements simplify given expr expr = safe_expand expr expr = replace expr Simplify max x x when x = max x commonly introduced expression when creating contiguous strides size_oblivious min_max_replacements = atom expr atoms Max type ignore has-type len atom args continue b = atom args b == b == b = b == _maybe_evaluate_static sympy Ge b min_max_replacements atom = b == _maybe_evaluate_static sympy Ge b min_max_replacements atom = b min_max_replacements expr = expr xreplace min_max_replacements expr has TruncToInt trunc_replacements = atom expr atoms TruncToInt isinstance atom args IntTrueDiv base divisor = atom args args base divisor == trunc_replacements atom = CleanDiv base divisor TruncToInt IntTrueDiv b == FloorDiv b trunc_replacements atom = FloorDiv base divisor trunc_replacements expr = expr xreplace trunc_replacements TODO would seem pass necessary given below replacement nested FloorDivs non-recursive replacement doesn t work recursive makes hard look up divisibility because existing divisibility info has FloorDiv now just do separate pass catch common nested case expr has FloorDiv _update_divisible div_replacements = atom expr atoms FloorDiv base divisor = atom args isinstance divisor FloorDiv base divisor = divisor args replace Mod base divisor divisible base == base replace Mod base divisor divisible div_replacements atom = divisor div_replacements expr = expr xreplace div_replacements expr = safe_expand expr expr has FloorDiv div_replacements = pows = expr atoms sympy Pow rationals = expr atoms sympy Rational difference expr atoms sympy Integer fd expr atoms FloorDiv base divisor = fd args replace Mod base divisor divisible div_replacements fd = CleanDiv base divisor div_replacements new_expr = expr xreplace div_replacements new_expr = safe_expand new_expr new_pows = new_expr atoms sympy Pow new_rationals = new_expr atoms sympy Rational difference new_expr atoms sympy Integer divisions simplified away new_pows issubset pows new_rationals issubset rationals expr = new_expr expr TODO overload allow_none literal lru_cache size_hint expr sympy Basic allow_none bool = False - Optional sympy Basic Gets size hint given expression underlying shapes we had Does introduce guard so only use when you can guarantee your code still valid arbitrary shapes such optimization decisions result_expr = safe_expand expr xreplace var_to_val result_expr is_number torch utils _sympy singleton_int SingletonInt isinstance result_expr SingletonInt None r = _maybe_evaluate_static result_expr compute_hint=True r None r allow_none None oblivious_var_to_val See https github com pytorch pytorch issues #issuecomment- correct_hint = result_expr xreplace oblivious_var_to_val counterfactual_hint = result_expr xreplace k max v k v oblivious_var_to_val items correct_hint free_symbols counterfactual_hint free_symbols correct_hint == counterfactual_hint log info oblivious_size hit s - s expr correct_hint correct_hint log info oblivious_size counterfactual failed s - s = s expr correct_hint counterfactual_hint log info oblivious_size miss s - s counterfactual s expr correct_hint counterfactual_hint unbacked_var_to_val unsound_expr = result_expr xreplace unbacked_var_to_val unsound_expr free_symbols log warning propagate_real_tensors size_hint s - s expr unsound_expr trace_structured propagate_real_tensors metadata_fn=lambda expr repr expr result repr unsound_expr stack structured from_traceback CapturedTraceback extract skip= summary guard_or_defer_runtime_assert sympy Eq result_expr unsound_expr f propagate_real_tensors result_expr == unsound_expr unsound_expr raise _make_data_dependent_error result_expr expr result_expr NB keep sync size_hint lru_cache has_hint expr sympy Expr - bool result_expr = safe_expand expr xreplace var_to_val result_expr is_number _maybe_evaluate_static result_expr None _make_data_dependent_error expr sympy Basic unhinted_expr sympy Basic expr_sym_node_id Optional int = None - GuardOnDataDependentSymNode TODO Dynamo context having user code having name local will much better size_like_symbols = s expr free_symbols stacktrace = join var_to_stack s format log debug Data dependent variable s allocated \n s s stacktrace s size_like size_like_symbols append s size_oblivious_result_msg = sloc maybe_extra_debug = _get_stack_summary True expr is_integer type ignore attr-defined desc = Could extract specialized integer data-dependent expression desc = Could guard data-dependent expression size_oblivious_result_msg = consider using data-dependent friendly APIs such guard_or_false guard_or_true statically_known_true msg = f desc expr unhinted unhinted_expr f Size-like symbols join map str size_like_symbols none \n\n f size_oblivious_result_msg \n f Caused sloc \n For more information run TORCH_LOGS= dynamic \n For extended logs when we create symbols also add f TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL= join map str expr free_symbols \n If you suspect guard triggered C++ add TORCHDYNAMO_EXTENDED_DEBUG_CPP= \n For more debugging help see https docs google com document d HSuTTVvYH pTew Rtpeu Ht nQEFTYhAX Ypa_xJs edit usp=sharing\n + maybe_extra_debug TODO Help text about how use our runtime tests fix problem dtrace_structured guard_on_data_dependent_error metadata_fn=lambda expr repr expr unhinted_expr repr unhinted_expr expr_id _expr_sym_node_id stack structured from_traceback CapturedTraceback extract skip= summary GuardOnDataDependentSymNode expr msg _update_var_to_range symbol sympy Symbol vr ValueRanges vr_sloc Optional ValueRangesSLoc = None is_constraint bool = False - None lower upper = vr lower vr upper If we have size-like unbacked SymInt refuse refine range less than two This because when we intersect range inf size oblivious tests range would unsatisfiable In other words once you have size-like unbacked SymInt we can never learn exactly zero one because we would now give inconsistent results all size oblivous tests upper symbol size_like vr = ValueRanges lower Updates range guards corresponding each bound symbol symbol var_to_range log debug _update_var_to_range s = s new symbol vr var_to_range symbol = vr vr_sloc None sloc = _get_sloc vr_sloc = ValueRangesSLoc sloc sloc var_to_range_sloc symbol = vr_sloc old = var_to_range symbol new = old vr new = old vr_sloc None sloc = _get_sloc vr_sloc = ValueRangesSLoc sloc sloc new lower = old lower var_to_range_sloc symbol lower = vr_sloc lower new upper = old upper var_to_range_sloc symbol upper = vr_sloc upper var_to_range symbol = new log debug _update_var_to_range s = s update symbol new v = var_to_val get symbol None r = var_to_range symbol v r For constraint failure delay later TODO Rework all constraint logic very duplicative regular reasoning is_constraint assert v r f v r _set_replacement sympy Symbol tgt sympy Expr msg str - None Adds updates replacement symbol Use instead ` replacements = tgt ` tgt == replacements get None tgt free_symbols Precondition == tgt assert isinstance sympy Symbol prefer_deferred_runtime_asserts_over_guards _is_supported_equivalence tgt continuing leads placeholder shapes having complex expressions we can t resolve Handles nested tensor symbolic variables which don t have var_to_range bounds tgt_bound = None var_to_range src_bound = var_to_range First refine value range based computed value range tgt This always OK do even we decide do substitution end This might no-op already has tighter bound tgt_bound = bound_sympy tgt _update_var_to_range tgt_bound Next check we can update range free symbols tgt based range But only do - source bound non-trivially improves over what we get out existing bounds - replacement univariate we can invert tgt expression tgt_bound issubset src_bound len tgt free_symbols == b = next iter tgt free_symbols Try invert equality r = try_solve sympy Eq tgt b floordiv_inequality=False r None log debug set_replacement solve s s == s gives s b tgt r The solution here can non-integral example we have s = s then s = s What we would like do calculated bounds arbitrary precision then requantize bound integers when we done rat_b_bound = bound_sympy r b_bound = ValueRanges CeilToInt rat_b_bound lower FloorToInt rat_b_bound upper _update_var_to_range b b_bound var_to_range_sloc tgt_bound = bound_sympy tgt assert tgt_bound issubset src_bound f tgt_bound= subset src_bound= TODO Should we propagate size-like-ness Pros u size-like intuitively u == u should cause u become size-like Cons u size-like what about u - == u You CAN T propagate case because what u == then u negative clearly isn t size So minimum any f x whose value range isn t inf given x inf cannot propagate size-like-ness But there many situations where you could imagine u going size-like actually you just didn t have refined enough value range u Since even innocuous looking arithmetic operations can destroy size-like-ness s best propagate all force user annotate necessary Compromise we preserve size-like-ness only exact equality nothing size_like isinstance tgt sympy Symbol size_like add tgt isinstance tgt sympy Symbol tgt size_like size_like add Now decide we will do substitution - If source has non-trivial range only substitute we preserve range Note we may have propagated src_range free variables tgt when tgt univariate we could find inverse which helps us achieve This ensures we never forget about user defined ranges even they end up being defined composite formulas like s + s - If variable unbacked only substitute substitution would preserve bounds also under size-like-ness conditions tgt_bound issubset src_bound log debug skipped set_replacement s = s s s subset s tgt msg tgt_bound src_bound size_like tgt_bound_so = bound_sympy tgt size_oblivious=True src_bound_so = bound_sympy size_oblivious=True tgt_bound_so issubset src_bound_so log debug skipped set_replacement s = s s s subset s size-oblivious conditions tgt msg tgt_bound_so src_bound_so isinstance tgt sympy Integer sympy Float specializing constant which likely unexpected unless you specified dynamic=True user_tb = TracingContext extract_stack trace_structured symbolic_shape_specialization metadata_fn=lambda symbol repr sources s name s var_to_sources get value repr tgt reason msg stack structured from_traceback CapturedTraceback extract skip= summary user_stack structured from_traceback user_tb user_tb None source var_to_sources get user_tb specialization_stacks source = user_tb config print_specializations log warning Specializing s s var_to_sources name tgt log debug SPECIALIZATION stack_info=True log info set_replacement s = s s s tgt msg tgt_bound replacements = tgt NB replacement may get refined user will find FIRST one most useful TODO Maybe we could consider tracking all them replacements_slocs replacements_slocs = _get_sloc _update_version_counter When specializing == tgt equality should also conveyed Z case expression uses _add_target_expr sympy Eq tgt evaluate=False _add_divisible expr sympy Expr - None divisible add expr _update_version_counter _lru_cache record_shapeenv_event _find sympy Symbol - sympy Expr Implements DSU-like algorithm find variable represents Also handles transitive non-identity replacements b + c c d replacements res = replacements cur_replace = s _find s s res free_symbols replaced changed = replacements _xreplace cur_replace changed _set_replacement replaced find replacements lru_cache _maybe_guard_rel expr sympy Expr - None The relational guard guarded true Use information simplify shapes i e == b == isinstance expr sympy And arg expr args _maybe_guard_rel arg isinstance expr sympy Rel A good example what goes wrong you don t do python test functorch test_aotdispatch py -k test_aot_autograd_symbolic_module_exhaustive_nn_LazyConv d_cpu_float isinstance expr sympy Ne free = list expr free_symbols assert len free f The expression should static point expr In case really gnarly expression we don t blow up len free Prioritize unbacked symints solving ordering them last Prefer simplify out lexicographically higher symbols i e simplify out s over s NB unfortunately isn t strictly equivalent simplifying out newer symbols Prefer simplify out symbols ephemeral sources _smart_symbol_sort x sympy Symbol - tuple int int str has_only_ephemeral_sources = x var_to_sources all s is_ephemeral s var_to_sources x NB size_hint int sympy Expr do use int_oo here hint_size = size_hint x allow_none=True hint_size None size = sys maxsize symbol_is_type x SymT SIZE assert isinstance hint_size sympy Expr size = int hint_size size = sys maxsize name = x name puts ephemeral sourced symbols first when sorting reverse has_only_ephemeral_sources size name free = sorted free key=_smart_symbol_sort reverse=True type ignore attr-defined lhs = expr lhs rhs = expr rhs _refine_ranges expr The rest stuff equality only isinstance expr sympy Eq expr has Mod try floor_div_atoms = lhs atoms FloorDiv union rhs atoms FloorDiv len floor_div_atoms any divisor = floor_div_atoms raise NotImplementedError Never replace unbacked symbols other unbacked symbols function arguments ex mark_unbacked symbols fine replace other unbacked those coming item calls This error prone because you can cause references unbacked symbols time travel backwards E g u = x item use u u = y item u = z item torch _check u == u + u If you replace u u + u then use u now references u u prior them actually being bound runtime It s pretty inconvenient setup control dependencies substitutions so ban entirely trivial_solve lhs sympy Expr rhs sympy Expr - bool isinstance lhs sympy Symbol free_unbacked_symbols lhs _free_non_source_unbacked_symbols rhs unbacked_inputs True symbol_is_type lhs SymT FLOAT True TODO Maybe trivial solutions int should also done False short-circuit when no solving needed trivial_solve lhs rhs _set_replacement lhs _find rhs trivial_lhs trivial_solve rhs lhs _set_replacement rhs _find lhs trivial_rhs r = try_solve expr free floordiv_inequality=False r None all t is_integer t sympy preorder_traversal r new_var = _find r ok = len free_unbacked_symbols new_var == ok _set_replacement free new_var solve except NotImplementedError pass expr has Mod mod_expr = next iter expr atoms Mod try r = try_solve expr mod_expr floordiv_inequality=False r None r == _add_divisible mod_expr This little bit extra logic make things like torch empty i q view c - q work out p q = mod_expr args isinstance q sympy Number isinstance p sympy Mul len p args == c i = p args Given Mod c i q == isinstance c sympy Number isinstance i sympy Symbol is_unbacked_symint i We have Mod i q c == which means we can rewrite i q gcd q c i d = q sympy gcd q c TODO CleanDiv i = create_unbacked_symint node expr Propagate value ranges It doesn t really matter we use truediv floordiv because we have established divisibility _update_var_to_range i SymPyValueRangeAnalysis floordiv var_to_range i ValueRanges wrap d Propagate hints real tensor tracing i unbacked_var_to_val set_unbacked_var_to_val i unbacked_var_to_val i d Propagate size-like-ness i size_like size_like add i _set_replacement i d i divisibility except NotImplementedError pass See Note - On specialization _default_value_range do_not_specialize_zero_one bool = False - ValueRanges lower = do_not_specialize_zero_one specialize_zero_one ValueRanges lower int_oo _default_unspecified_value_range - ValueRanges ValueRanges unknown_int _lru_cache _simplify_floor_div expr sympy Expr - sympy Expr floor_divs = tuple expr atoms FloorDiv we expect floor_divs exact thus add guards exact floordivs even tracing doesn t require them otherwise fd reversed floor_divs base divisor = fd args mod_expr = Mod base divisor eq_expr = sympy Eq mod_expr add necessary mod guards evaluate_expr eq_expr simplify expr We re about add guard runtime assert check ShapeEnv frozen so issue warning _check_frozen expr sympy Basic concrete_val sympy Basic - None frozen counter ignored_backward_guard += signpost_event dynamic evaluate_expr_frozen co_fields ignored_guard f expr == concrete_val no version = original state signpost expected version = dynamic backwards eagerly compiled version log info Ignored guard s == s could result accuracy problems expr concrete_val only print stack trace when debug mode e g TORCH_LOGS= dynamic stack_info=log getEffectiveLevel logging WARNING _get_user_frame - Optional types FrameType frame = inspect currentframe while frame None frame f_code co_filename uninteresting_files frame frame = frame f_back frame _get_stack_summary is_debug bool = False framework_loc Optional str = None - tuple SLoc str floc Optional Union str traceback FrameSummary = framework_loc floc None frame = _get_user_frame try frame None floc = traceback FrameSummary frame f_code co_filename frame f_lineno frame f_code co_name finally del frame NB stack truncated s fine because main stack_info will give you rest info you need maybe_user_loc = None user_tb = TracingContext extract_stack user_tb idx = len user_tb - while idx user_tb idx filename uninteresting_files idx -= maybe_user_loc = format_frame user_tb idx line=True maybe_extra_debug = is_debug user_tb maybe_extra_debug = \nUser Stack most recent call last \n + snipped see stack below prefix \n + join traceback format_list user_tb is_debug config extended_debug_cpp cpp_stack = CapturedTraceback extract cpp=True maybe_extra_debug += \nC++ stack trace \n + join cpp_stack format is_debug maybe_extra_debug += \nFor C++ stack trace run TORCHDYNAMO_EXTENDED_DEBUG_CPP= SLoc floc maybe_user_loc maybe_extra_debug Pass framework_loc override framework location info _get_sloc framework_loc Optional str = None - SLoc sloc _ = _get_stack_summary framework_loc=framework_loc sloc _generate_unique_id source_name str - int attempt = int hashlib sha source_name encode hexdigest while attempt unique_ids attempt += unique_ids add attempt attempt _find_frame_locals - _FrameLocalResult Given current user code frame finds relevant lines code values symbolic locals free symbols involved frame_locals dict str Any = frame_symbols dict str str = frame = _find_user_code_frame None frame f_code co_filename == string _FrameLocalResult find bytecode instructions relevant frame instructions = list dis Bytecode frame f_code co_lines offset = inspect getsourcelines frame f_code start end cur = None None None pyrefly ignore bad-assignment i instr enumerate instructions instr starts_line None cur = instr starts_line cur = frame f_lineno continue start None start = end = i end = i start None end None no instructions found _FrameLocalResult track involved locals free symbols go x Any - Optional str isinstance x torch Tensor y x size go y y x stride go y go x storage_offset f Tensor shape x size f stride x stride f storage_offset x storage_offset isinstance x SymBool SymInt SymFloat s x node expr free_symbols str s frame_symbols type ignore operator continue s var_to_sources frame_symbols str s = var_to_sources s name type ignore assignment str x None go through instructions seeing linenos involved locals last_lineno = frame f_lineno instr instructions start end + lineno = instr starts_line None last_lineno = max last_lineno lineno isinstance instr argval str instr argval frame f_locals flat_locals = pytree tree_flatten frame f_locals instr argval frame_locals instr argval = go flat_local flat_local flat_locals store LOC locs = co_lines frame f_lineno - offset last_lineno + - offset locs _FrameLocalResult indent = len locs - len locs lstrip frame_loc = join loc indent loc locs strip type ignore assignment _FrameLocalResult loc=frame_loc locals=frame_locals symbols=frame_symbols _log_guard prefix str g SympyBoolean forcing_spec bool - None dtrace_structured guard_added metadata_fn=lambda expr str g prefix prefix expr_node_id _expr_sym_node_id user_stack structured get_user_stack stack structured get_framework_stack symbol_to_sources str v k k v source_to_var items v g free_symbols frame_locals asdict _find_frame_locals trace_structured guard_added_fast metadata_fn=lambda expr str g user_stack structured from_traceback TracingContext extract_stack stack structured from_traceback CapturedTraceback extract skip= summary log isEnabledFor logging INFO str_g = str g is_debug = config extended_debug_guard_added None str_g == config extended_debug_guard_added sloc maybe_extra_debug = _get_stack_summary is_debug maybe_more_info = is_debug maybe_more_info = more info run f TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED= str_g log info s s guard added s s s prefix forcing_spec f prefix forcing_spec str_g sloc maybe_more_info maybe_extra_debug stack_info=is_debug A local variable evaluate_expr stored avoid using lru_cache top since does effect results When needed its read directly _expr_sym_node_id Optional int = None evaluate_sym_node sym_node SymNode size_oblivious bool = False fallback_value Optional bool = None - sympy Basic Given SymNode evaluates sym_node expr adding guards necessary _expr_sym_node_id = id sym_node evaluate_expr sym_node expr sym_node hint sym_node fx_node size_oblivious fallback_value=fallback_value _is_python_assert - bool Check boolean used assertion bytecode pattern assertions pretty stable Python -- ported minimal changes torch fx proxy py Bytecode pattern ` assert ` statements TO_BOOL COMPARE_OP Only Python = POP_JUMP_IF_TRUE LOAD_ASSERTION_ERROR RAISE_VARARGS frame = _get_user_frame assert frame None insts = list dis get_instructions frame f_code sys version_info = For Python = instructions can - bytes long bisect bisect_left cur = bisect_left insts frame f_lasti key=lambda x x offset For Python = instructions always bytes cur = frame f_lasti sys version_info = insts cur opname TO_BOOL COMPARE_OP Peek instruction further cur += inst = insts cur inst opname == POP_JUMP_IF_TRUE inst arg None first = insts cur + starts_with_assert = first opname == LOAD_GLOBAL first argval == AssertionError first opname == LOAD_ASSERTION_ERROR starts_with_assert insts cur + opname == RAISE_VARARGS True False _log_real_tensor_propagation orig_expr sympy Basic unsound_result sympy Basic - None log warning propagate_real_tensors evaluate_expr s - s orig_expr unsound_result trace_structured propagate_real_tensors metadata_fn=lambda expr repr orig_expr result repr unsound_result stack structured from_traceback CapturedTraceback extract skip= summary dtrace_structured propagate_real_tensors_provenance metadata_fn=lambda expr repr orig_expr result repr unsound_result expr_node_id _expr_sym_node_id user_stack structured get_user_stack stack structured get_framework_stack symbol_to_sources str v k k v source_to_var items v orig_expr free_symbols frame_locals asdict _find_frame_locals evaluate_expr orig_expr sympy Basic hint Optional Union int bool float = None fx_node Optional torch fx Node = None size_oblivious bool = False fallback_value Optional bool = None forcing_spec bool = False - sympy Basic Given expression evaluates adding guards necessary When fallback_value None function fallback_value instead failing data dependent error Add extra state evaluate_expr depends suppress_guards_tls = ShapeEnv _suppress_guards_tls _inner_evaluate_expr orig_expr hint fx_node size_oblivious forcing_spec suppress_guards_tls fallback_value lru_cache record_shapeenv_event save_tracked_fakes=True name= evaluate_expr _inner_evaluate_expr orig_expr sympy Basic hint Optional Union int bool float fx_node Optional torch fx Node size_oblivious bool forcing_spec bool _suppress_guards_tls bool fallback_value Optional bool = None - sympy Basic try _evaluate_expr orig_expr hint fx_node size_oblivious fallback_value forcing_spec=forcing_spec except Exception e isinstance e GuardOnDataDependentSymNode pass log warning failed during evaluate_expr s hint= s size_oblivious= s forcing_spec= s orig_expr hint size_oblivious forcing_spec raise _log_suppressed_dde SymBool assumed_value bool - None sloc extra = _get_stack_summary True log info could evaluate s due data dependency assumed s no runtime assertions s s assumed_value sloc extra _evaluate_expr orig_expr sympy Basic hint Optional Union bool int float = None fx_node Optional torch fx Node = None size_oblivious bool = False fallback_value Optional bool = None forcing_spec bool = False - sympy Basic TODO split conjunctions evaluate them separately isinstance orig_expr sympy logic boolalg BooleanTrue sympy logic boolalg BooleanFalse orig_expr Don t track one Because cache inside function cache only lasts invocation function call functools cache compute_concrete_val - sympy Basic hint None This only ever called expressions WITHOUT unbacked symbols r = size_hint orig_expr assert r None r sympy sympify hint concrete_val Optional sympy Basic Check translation_validation set corresponding fx_node None guard should suppressed guard doesn t contain backed symfloat symbols since z can t handle floats fallback_value none If all above check we create FX node representing actual expression guarded node = None fresh = False _translation_validation_enabled fx_node None _suppress_guards_tls size_oblivious any symbol_is_type s SymT FLOAT s orig_expr free_symbols fallback_value None TODO does even worked unbacked think concrete_val = compute_concrete_val concrete_val sympy true node fresh = _create_fx_call_function torch _assert fx_node concrete_val sympy false neg _ = _create_fx_call_function operator not_ fx_node node fresh = _create_fx_call_function torch _assert neg eql _ = _create_fx_call_function operator eq fx_node concrete_val node fresh = _create_fx_call_function torch _assert eql assert node None If fresh node we have remember event index corresponds assertion node Reason so given assertion node we can replay ShapeEnv events until point where assertion node freshly created fresh _add_fx_node_metadata node After creating FX node corresponding orig_expr we must make sure no error will raised until end function Reason translation validation may become invalid otherwise If error raised before end function we remove FX node inserted re-raise error guard = None try orig_expr is_number log debug eval s trivial orig_expr hint None isinstance hint bool assert orig_expr == hint f orig_expr = hint assert sympy Eq orig_expr hint f orig_expr = hint orig_expr expr = orig_expr static_expr = _maybe_evaluate_static expr size_oblivious=size_oblivious static_expr None log debug eval s == s statically known f size_oblivious orig_expr size_oblivious size_oblivious static_expr size_oblivious config backed_size_oblivious hint None TODO maybe reconcile use counterfactual hints unbacked case assert static_expr == hint f static_expr = hint static_expr transmute_into_runtime_assert = False concrete_val = None expr free_symbols = var_to_val keys TODO dedupe _maybe_evaluate_static Attempt eliminate unbacked SymInt new_expr = _maybe_evaluate_static expr unbacked_only=True assert new_expr None new_expr free_symbols = var_to_val keys ok = False fallback_value set when guard_or_true guard_or_false used ok fallback_value None _log_suppressed_dde orig_expr fallback_value fallback_value oblivious_var_to_val will defined iff we have sizes DimDynamic OBLIVIOUS_SIZE type See https github com pytorch pytorch issues #issuecomment- oblivious_var_to_val correct_hint = orig_expr xreplace oblivious_var_to_val free_symbols counterfactual_hint = orig_expr xreplace k max v k v oblivious_var_to_val items free_symbols correct_hint == counterfactual_hint TODO better logging log info oblivious_size s - s passed counterfactual orig_expr correct_hint concrete_val = correct_hint NB do NOT transmute into runtime assert ok = True unbacked_var_to_val None iff propagate_real_tensors propagate_real_tensors we check example values generate unsound_result they pass we add runtime assertions continue ok unbacked_var_to_val unsound_result = orig_expr xreplace unbacked_var_to_val xreplace var_to_val free_symbols _log_real_tensor_propagation orig_expr unsound_result transmute_into_runtime_assert = True concrete_val = unsound_result ok = True Check coming python assert statement so convert runtime assertion instead failing ok trace_asserts _is_python_assert concrete_val = sympy true transmute_into_runtime_assert = True ok = True ok raise _make_data_dependent_error expr xreplace var_to_val expr expr_sym_node_id=self _expr_sym_node_id expr = new_expr concrete_val None concrete_val = compute_concrete_val _check_frozen expr concrete_val config inject_EVALUATE_EXPR_flip_equality_TESTING_ONLY isinstance hint bool isinstance expr sympy Eq sympy Ne expr = sympy Not expr Turn into boolean expression no longer need consult concrete_val concrete_val sympy true g = cast SympyBoolean expr concrete_val sympy false g = sympy Not expr g = sympy Eq expr concrete_val type ignore arg-type transmute_into_runtime_assert guard_or_defer_runtime_assert g f propagate_real_tensors orig_expr == concrete_val concrete_val _suppress_guards_tls _log_guard eval g forcing_spec=forcing_spec TODO If we successfully eliminate symbol via equality actually necessary save guard equality we will implicitly generate guard when we match input against symbol Probably easiest way implement have maybe_guard_rel bool saying subsumed guard therefore guard no longer necessary _maybe_guard_rel g torch compiler is_exporting prefer_deferred_runtime_asserts_over_guards s fine defer simple guards here without checking _maybe_guard_rel call above will set replacements possible so result here will statically known guard_or_defer_runtime_assert g f evaluate_expr orig_expr point we ve evaluated concrete expr value have flipped negated guard necessary Now we know what guard defer runtime assert guard = ShapeGuard g _get_sloc size_oblivious=size_oblivious guards append guard axioms update dict get_implications simplify g _log_guard eval guard suppressed g forcing_spec=forcing_spec except Exception fresh _remove_fx_node node raise _suppress_guards_tls guard None we might have deferred runtime assert s g free_symbols symbol_guard_counter s += Forcing_spec avoid infinite recursion forcing_spec config symbol_guard_limit_before_specialize None symbol_guard_counter s config symbol_guard_limit_before_specialize Force specialization log info symbol_guard_limit_before_specialize= s exceeded s config symbol_guard_limit_before_specialize s evaluate_expr s forcing_spec=True concrete_val cleanup - None Break reference cycles This destroys stacks If you really want keep them we just need some way break references code objects s var_to_stack values s cleanup ras deferred_runtime_asserts values ra ras ra stack cleanup lru_cache record_shapeenv_event save_tracked_fakes=True guard_or_defer_runtime_assert orig_expr SympyBoolean msg str fx_node Optional torch fx Node = None - bool Adds guard orig_expr True we can fall back adding assert checked runtime Args orig_expr sympy Expr Boolean expression assert true msg str Message display assertion failure fx_node Optional torch fx Node node ` ` graph ` ` corresponding expression applicable expr = orig_expr TODO split conjunctions evaluate them separately static_expr = _maybe_evaluate_static expr static_expr None log debug runtime_assert s == s statically known orig_expr static_expr TODO assert bool static_expr bool static_expr Attempt eliminate unbacked SymInt new_expr = _maybe_evaluate_static expr unbacked_only=True assert new_expr None prefer_deferred_runtime_asserts_over_guards new_expr free_symbols = var_to_val keys Do normal guard evaluate_expr new_expr fx_node=fx_node NB Don t use new_expr expr could contain gunk like shape which we don t want guard _translation_validation_enabled fx_node None _suppress_guards_tls node fresh = _create_fx_call_function torch _assert fx_node assert node None fresh _add_fx_node_metadata node _suppress_guards_tls _log_guard runtime_assert orig_expr forcing_spec=False If you re here because assert read Note Backwards runtime asserts torch _inductor graph py runtime_asserts_frozen log debug runtime_asserts_frozen then got s expr _check_frozen expr sympy true eliminate symbols equality tests refine ranges _maybe_guard_rel expr canonicalise remove equations trivially equal orig_expr = expr expr = canonicalize_bool_expr expr stack = CapturedTraceback extract skip= ra = RuntimeAssert expr msg stack TODO Do way less janky than int s name cands = sorted s s expr free_symbols symbol_is_type s SymT UNBACKED_INT key=lambda s int s name Is None when prefer_deferred_runtime_asserts_over_guards=True guard question has no unbacked SymInts front ix = cands - cands None deferred_runtime_asserts setdefault ix append ra axioms update dict get_implications simplify expr num_deferred_runtime_asserts += _update_version_counter _log_guard runtime_assert guard suppressed orig_expr forcing_spec=False True Refines ranges variables present guard This function tries refine range variables inside guard reasoning about Specifically when guard sympy Relational operation It does mainly things Tries isolate variable left-hand side Compute value range right-hand side Update value range variable better _refine_ranges expr SympyBoolean - None expr = simplify expr symbol expr free_symbols assert isinstance symbol sympy Symbol isinstance var_to_val get symbol None SingletonInt Skip var_to_range logic SingletonInt which only used jagged layout NestedTensors today continue r = try_solve expr symbol r None symbol is_integer r is_integer Range refinement only supports integer symbols now There lots SymPy bugs when comes comparing reals integers so we skip now continue r_expr rhs = r vr = var_to_range symbol lower upper = vr lower vr upper rhs_vr = bound_sympy rhs var_to_range Let s suppose we have preexisting range x Now we issue guard x y where range y Then lower = rhs_vr lower = therefore refinement can happen refining x since x must greater than y lowest y could sympy Eq may update both lower upper bounds sympy G t e may update lower bound only sympy L t e may update upper bound only lower = rhs_vr lower isinstance r_expr sympy Eq sympy Ge sympy Gt Strictly greater relations allow us refine bit more since x y implies lower bound x y + lower = rhs_vr lower + int isinstance r_expr sympy Gt upper = rhs_vr upper isinstance r_expr sympy Eq sympy Le sympy Lt upper = rhs_vr upper - int isinstance r_expr sympy Lt Do nothing new value range no better than what we already have vr == ValueRanges lower upper continue Updates range guards corresponding each bound symbol _update_var_to_range symbol ValueRanges lower upper If range refined singleton set replacement var_to_range symbol is_singleton _set_replacement symbol var_to_range symbol lower range_refined_to_singleton Clears cache since update can change result _maybe_evaluate_static cache_clear lru_cache maxsize=None record_shapeenv_event constrain_symbol_range s sympy Symbol compiler_min int compiler_max int - None upd_vr = ValueRanges compiler_min compiler_max old_vr = var_to_range get s ValueRanges unknown _update_var_to_range s upd_vr new_vr = var_to_range s = old_vr log info constrain_symbol_range s s s s new_vr lower new_vr upper _is_int expr object - bool isinstance expr SymInt expr node expr is_number WARNING This legacy DO NOT USE _is_dim_dynamic t torch Tensor d int - bool hasattr t _dynamo_dynamic_indices d t _dynamo_dynamic_indices PropagateUnbackedSymInts torch fx Interpreter run_node n torch fx Node - Result Run FX node propagating unbacked Symbol bindings new fake tensor torch _guards detect_fake_mode result = super run_node n fake_mode = detect_fake_mode assert fake_mode None rebind_unbacked fake_mode shape_env n result result _find_user_code_frame - Optional types FrameType frame = inspect currentframe while frame None frame f_code co_filename startswith os path dirname inspect getfile torch + os path sep break frame = frame f_back frame _blame_user_code e Exception frame types FrameType - None frame_summary = traceback FrameSummary frame f_code co_filename frame f_lineno frame f_code co_name msg = e args msg += \n\nThe following call raised error \n + join traceback StackSummary from_list frame_summary format e args = msg _PythonMsgPrinter PythonPrinter Util printer replaces sympy symbols their source-level names renders sympy relational operators e g Eq Ne Ge Le inline i e == = __init__ src_map dict str list str - None super __init__ src_map = src_map _print_Symbol sym sympy Symbol - str src_map sym name _suggest_torch_checks e GuardOnDataDependentSymNode src_map defaultdict str list str - None Enhances GuardOnDataDependentSymNode error suggested fixes using torch _check This function analyzes condition caused data-dependent error generates user-friendly suggestions fixing adding appropriate torch _check calls It handles special cases like non-negative checks specific recommendations Args e The GuardOnDataDependentSymNode error enhance suggestions src_map A mapping symbol names their corresponding source-level variable names Returns None Modifies error message in-place updating e args extract unresolved condition unbacked symints error cond = e cond diff = join s name s cond free_symbols s name src_map diff log warning Unable find user code corresponding s diff printer = _PythonMsgPrinter src_map msg = e args msg += \nTo fix error insert one following checks before call not_cond_str = printer doprint sympy Not cond suggested fixes resolve ` cond ` tell compiler assume either ` cond ` its negation user will need select which suggested_fixes = f torch _check printer doprint cond f torch _check not_cond_str i fix enumerate suggested_fixes msg += f \n i + fix src_mapped = join f ` s ` join src_map s s sorted s name s cond free_symbols msg += f \n\n These suggested fixes derived replacing src_mapped cond its negation e args = msg _suggest_fixes_for_data_dependent_error_non_strict e GuardOnDataDependentSymNode - None Given raised data-dependent error add following error message closest user code location raised error suggested fixes error terms live variables location walk stack up data-dependent error until non-torch frame found frame = _find_user_code_frame frame None add frame info error message _blame_user_code e frame map symbol names reachable via frame locals their source-level names src_map = defaultdict list var val frame f_locals items try tree_leaves_with_path = pytree tree_leaves_with_path val except ValueError log warning pytree tree_leaves_with_path failed value type s local variable s type val var continue figure out how access any symbol inside ` val ` through ` var ` path leaf tree_leaves_with_path name = var + pytree keystr path isinstance leaf torch SymInt src_map str leaf node expr append name isinstance leaf torch Tensor i dim enumerate leaf shape isinstance dim torch SymInt src_map str dim node expr append f name shape i add suggested torch check s based ` src_map ` error message replacing unbacked symints unresolved condition error isinstance e cond sympy logic boolalg Boolean _suggest_torch_checks e src_map contextmanager _remove_effect_token_unbacked_bindings node torch fx Node - Generator None None None Temporarily modifies unbacked_bindings node s metadata removing first element each path which corresponds effect token This used when processing nodes have effect tokens first element their unbacked_bindings paths The context manager ensures original bindings restored after operation complete Args node The FX node whose unbacked_bindings will temporarily modified Yields None old_bindings = node meta get unbacked_bindings Remove extra layer effect token new_bindings = k path path path k path old_bindings items node meta unbacked_bindings = new_bindings try yield finally node meta unbacked_bindings = old_bindings This helper function used passes insert runtime assertions graph When accessing expressions representing input placeholders we do apply replacements since those inputs should seen assertions use them inserted The only replacement we apply unbacked renaming _get_placeholder_expr sym_node SymNode - sympy Expr shape_env = sym_node shape_env result = sym_node _expr result shape_env unbacked_renamings shape_env unbacked_renamings result result