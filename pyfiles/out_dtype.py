mypy allow-untyped-defs torch torch utils _pytree pytree torch _C DispatchKey torch _higher_order_ops utils autograd_not_implemented torch _ops HigherOrderOperator torch _prims_common elementwise_dtypes ELEMENTWISE_TYPE_PROMOTION_KIND torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor disable_proxy_modes_tracing maybe_handle_decomp ProxyTorchDispatchMode track_tensor_tree TODO figure out more generic approach ALLOWABLE_OPS = torch ops aten linear default torch ops aten mm default torch ops aten conv d default torch ops aten convolution default torch ops aten mul Tensor torch ops aten mul Scalar torch ops aten div Tensor torch ops aten div Scalar OutDtypeOperator HigherOrderOperator The out_dtype operator takes existing ATen functional operator ` out_dtype ` argument arguments original operator executes original operator returns Tensor ` out_dtype ` precision This operator does mandate compute precision so allows representation opinionated about exact implementation The general implementation all operators will following Promote inputs dtypes based default PyTorch dtype promotion rules using dtypes all input Tensors Scalars ` out_dtype ` arugument Execute operator Cast output ` out_dtype ` __init__ - None super __init__ out_dtype __call__ op output_dtype args isinstance op torch _ops OpOverload raise ValueError out_dtype s first argument must OpOverload op _schema is_mutable raise ValueError out_dtype s first argument needs functional operator len op _schema returns == isinstance op _schema returns type torch TensorType raise ValueError out_dtype s can only apply ops single tensor f Instead got r type r op _schema returns op ALLOWABLE_OPS raise ValueError f out_dtype only allows following operators ALLOWABLE_OPS res = super __call__ op output_dtype args res out_dtype = OutDtypeOperator trace_out_dtype proxy_mode func_overload op output_dtype args NB Long-term we should put decomposition logic into ProxyTorchDispatchMode so people do need call maybe_handle_decomp all HigherOrderOp proxy implementations r = maybe_handle_decomp proxy_mode func_overload op output_dtype args r NotImplemented r disable_proxy_modes_tracing This simplified implementation operator just tracing Actual implementation may also first promote arguments out = op args dtype=output_dtype node_args = op output_dtype args proxy_args = pytree tree_map proxy_mode tracer unwrap_proxy node_args out_proxy = proxy_mode tracer create_proxy call_function func_overload proxy_args name= out_dtype track_tensor_tree out out_proxy constant=None tracer=proxy_mode tracer out_dtype py_impl DispatchKey CompositeExplicitAutograd out_dtype_dense op torch _ops OpOverload output_dtype torch dtype args is_int_mm op output_dtype args torch _int_mm args out_dtype_fallback op output_dtype args is_int_mm op output_dtype args op torch ops aten mm default output_dtype == torch int len args == args dtype == torch int args dtype == torch int args is_cuda args is_xpu args is_cuda args is_xpu out_dtype_fallback op output_dtype args flat_inputs = pytree arg_tree_leaves args + torch ones dtype=output_dtype promote_dtype torch dtype = elementwise_dtypes flat_inputs type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT casted_args = pytree tree_map_only torch Tensor lambda arg arg dtype=promote_dtype args res = op casted_args dtype=output_dtype res out_dtype py_autograd_impl autograd_not_implemented out_dtype deferred_error=True out_dtype py_impl ProxyTorchDispatchMode out_dtype_proxy mode ProxyTorchDispatchMode op torch _ops OpOverload output_dtype torch dtype args trace_out_dtype mode out_dtype op output_dtype args out_dtype py_impl FakeTensorMode out_dtype_fake_tensor_mode mode FakeTensorMode op torch _ops OpOverload output_dtype torch dtype args mode out_dtype_dense op output_dtype args out_dtype py_functionalize_impl out_dtype_func ctx op output_dtype args unwrapped_args = tuple ctx unwrap_tensors arg arg args ctx redispatch_to_next res = out_dtype op output_dtype unwrapped_args ctx wrap_tensors res