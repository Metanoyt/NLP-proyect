mypy allow-untyped-defs os threading queue Empty EmptyQueue Queue torch _lazy device_context get_device_context ClosureHandler __init__ - None pass run closure Run closure function Args closure callable function run closure __call__ closures closure closures run closure AsyncClosureHandler ClosureHandler Handler Asynchronous Step Closures Args max_queue_size The maximum length closure queue after which training loop will block until closures evaluated By default reasonable limit maximum queue This value can set using ` XLA_MAX_ASYNC_QUEUE ` environment variable __init__ max_queue_size= super __init__ _closure_queue Queue = Queue int os environ get LTC_MAX_ASYNC_QUEUE max_queue_size _closure_exception Queue = Queue _closure_lock = threading Lock _closure_event_loop_finished = threading Event _closure_event_loop = None start_event_loop Start closure event loop started _closure_event_loop None event_loop Run loop until closure event set closure queue empty while True try closure = _closure_queue get block=True timeout= closure _closure_queue task_done except EmptyQueue _closure_lock _closure_queue empty _closure_event_loop_finished set except Exception e _closure_exception put e _closure_event_loop = threading Thread target=event_loop pyrefly ignore bad-assignment _closure_event_loop start pyrefly ignore missing-attribute run closure _closure_lock _closure_queue put closure block=True _closure_event_loop None _closure_event_loop is_alive try e = _closure_exception get block=False raise RuntimeError Cannot run asynchronous closure due previously raised exception e except EmptyQueue _closure_event_loop = None start_event_loop add_step_closure closure args= run_async=False Adds closure list ones run end step Many times during model training there need print report print console post tensorboard etc information which require content intermediary tensors inspected Inspecting different tensors content different points model code requires many executions typically causes performance issues Adding step closure will ensure will run after barrier when all live tensors will already materialized device data Live tensors which will include ones captured closure arguments So using ` add_step_closure ` will ensure single execution will performed even when multiple closures queued requiring multiple tensors inspected Step closures will run sequentially order they have been queued Note even though using API execution will optimized advised throttle printing reporting events once every N steps Args closure callable The function called args tuple The arguments passed closure run_async If True run closure asynchronously devctx = get_device_context closures_type = async_step_closures run_async step_closures step_closures = getattr devctx closures_type None step_closures None step_closures = setattr devctx closures_type step_closures step_closures append lambda a=args closure run_step_closures devctx = get_device_context async_step_closures = getattr devctx async_step_closures None async_step_closures None devctx async_step_closures = type ignore attr-defined async_closure_handler = getattr devctx async_closure_handler None async_closure_handler None async_closure_handler = AsyncClosureHandler devctx async_closure_handler = async_closure_handler type ignore attr-defined async_closure_handler async_step_closures step_closures = getattr devctx step_closures None step_closures None devctx step_closures = type ignore attr-defined closure_handler = getattr devctx closure_handler None closure_handler None closure_handler = ClosureHandler devctx closure_handler = closure_handler type ignore attr-defined closure_handler step_closures devctx