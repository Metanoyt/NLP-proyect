__future__ annotations itertools abc ABC dataclasses dataclass typing Any torchgen api dispatcher dispatcher torchgen api lazy getValueT isValueType LazyArgument LazyIrProperties LazyIrSchema tensorListValueT torchgen api translate translate torchgen api types BaseCType Binding deviceT DispatcherSignature kernel_signature NativeSignature OptionalCType VectorCType torchgen context method_with_native_function torchgen dest lazy_ts_lowering ts_lowering_body torchgen model Argument BackendIndex BackendMetadata BaseTy BaseType FunctionSchema ListType NativeFunction NativeFunctionsGroup node_ctor_arg_rvalue_string arg LazyArgument - str Given LazyArgument generate c++ string materializing rvalue arg passing into lazy Node constructor TODO Matching CType seems wrong should matching Type isValueType arg lazy_type isinstance arg lazy_type BaseCType arg is_wrapped_scalar f node_ arg name arg lazy_type type tensorListValueT f lazy_ arg name _tensorlist arg is_symint_or_list f GetSymIntValue arg name f lazy_ arg name - GetIrValue isinstance arg lazy_type OptionalCType arg is_symint_or_list TODO I don t understand when you should put lazy_ name f arg name std make_optional GetSymIntValue arg name std nullopt arg is_wrapped_scalar f node_ arg name f lazy_ arg name f std make_optional lazy_ arg name - GetIrValue std nullopt raise AssertionError f TODO sure there other valid types handle here arg lazy_type NB here because right now we aren t treating SymInt value type when we do needs move above NB we cannot test arg lazy_type we ve already specified int _t so we cannot distinguish between SymInt int _t isinstance arg orig_type ListType arg orig_type elem == BaseType BaseTy SymInt arg symint f GetSymIntArrayRefValue arg name f std vector int _t arg name begin arg name end isinstance arg lazy_type VectorCType isinstance arg lazy_type elem BaseCType f std vector arg lazy_type elem type arg name begin arg name end isinstance arg lazy_type OptionalCType isinstance arg lazy_type elem VectorCType isinstance arg lazy_type elem elem BaseCType f torch lazy ToOptionalVector arg lazy_type elem elem type arg name f arg name node_ctor_inputs schema LazyIrSchema - str Produce formatted string arguments passed into constructor node node_ctor_values = node_ctor_arg_rvalue_string arg arg schema filtered_args join node_ctor_values gen_fallback_code schema LazyIrSchema sig DispatcherSignature &#124; NativeSignature overload_name str - str Generate code falls back eager conditioned predicate dispatcher_sig = DispatcherSignature from_schema schema func exprs = translate sig arguments dispatcher_sig arguments fallback_args = \n join expr exprs len overload_name aten_op_str = f ATEN_OP schema aten_name overload_name aten_op_str = f ATEN_OP schema aten_name f force_eager_fallback aten_symbol schema native call_fallback_fn_symint ltc_eager_fallback aten_op_str call fallback_args aten_symbol schema LazyIrSchema - str missing_interned_strings = sigmoid_backward schema aten_name missing_interned_strings f c Symbol fromQualString aten schema aten_name schema aten_name startswith f aten schema aten_name schema aten_name converts all tensor-like arguments meta tensors Returns string containing all logic does conversions context used translate all relevant bindings convert_to_meta_tensors sig DispatcherSignature - tuple str list Binding context list Binding = unwrapped_tensor_args list str = arg sig arguments isinstance arg argument Argument arg argument type is_tensor_like unwrapped_name = f arg name _meta unwrapped_tensor_args append f auto unwrapped_name = to_meta arg name context append arg with_name unwrapped_name context append arg unwrap_tensor_args_str = \n join unwrapped_tensor_args unwrap_tensor_args_str context dataclass frozen=True GenLazyIR ABC backend_index BackendIndex backend_name str node_base str use_lazy_shape bool method_with_native_function __call__ f NativeFunctionsGroup &#124; NativeFunction - list str func = f functional func isinstance f NativeFunctionsGroup f func metadata = backend_index get_kernel f functional isinstance f NativeFunctionsGroup f schema = LazyIrSchema func symint=metadata None metadata supports_symint gen schema there no lowering functionality generated unless IR base subclassed implemented backend-specific node lowering_function schema LazyIrSchema - str create_function schema LazyIrSchema node_ctor_args str - str can_be_reused_function schema LazyIrSchema node_ctor_args str - str f bool CanBeReused node_ctor_args const false node_base_ctor_call schema LazyIrSchema - str value_args = schema filtered_args values=True scalars=False backends can customize way node base constructor called long all its arguments can generated information available schema base_ctor_value_args_list = arg value_args isinstance arg lazy_type BaseCType VectorCType base_ctor_value_args_list append f arg name isinstance arg lazy_type OptionalCType base_ctor_value_args_list append f arg name value_or kNullValue raise AssertionError f Unsupported type arg lazy_type - add support necessary base_ctor_value_args = join base_ctor_value_args_list scalar_args = schema filtered_args values=False scalars=True Shape construction Conditionally build shape depending specified shape property schema properties ShapePrecompute shape_ctor_arg = std move shapes schema properties ShapeCompute shape_args = name value_args shape_args extend name scalar_args shape_ctor_arg = f compute_shape_ schema name join shape_args schema properties ShapeCache shape_args = f operand i i range len value_args shape_args extend name scalar_args shape_ctor_arg = f compute_shape_ schema name join shape_args shape_ctor_arg = scalar_hashes = join f name scalar_args f node_base schema node_name ClassOpKind OpList base_ctor_value_args shape_ctor_arg num_outputs len schema returns torch lazy MHash scalar_hashes gen schema LazyIrSchema - list str opkind = schema opkind aten_symbol schema now we just want one IR decl soon after also method defs we use functional version out inplace all_args = schema filtered_args scalar_args = schema filtered_args values=False scalars=True ctor_args = f const i lazy_type cpp_type i name i all_args reuse_ctor_args = join ctor_args use_lazy_shape schema properties ShapePrecompute ctor_args append std vector torch lazy Shape shapes node_ctor_args = join ctor_args scalar_initializers = \n join This code just special casing mapping string_view - strings f name name has_value std make_optional std string name std nullopt lazy_type cpp_type == std optional c string_view f name name scalar_args len scalar_initializers scalar_initializers = f \n scalar_initializers scalar_decls = \n join f std string name lazy_type cpp_type == c string_view f std optional std string name lazy_type cpp_type == std optional c string_view f lazy_type cpp_type name scalar_args optional_values = arg name arg schema filtered_args values=True scalars=False isinstance arg lazy_type OptionalCType has_optional_decls = \n join f bool has_ value value optional_values has_optional_defs = \n join f has_ value = value value optional_values members_to_string = arg scalar_args isinstance arg lazy_type OptionalCType value = f arg name value arg is_generator value = torch Generator members_to_string append f arg name has_value ss arg name = value ss arg name =null members_to_string append f ss arg name = arg name members_to_string_str = \n join members_to_string f \ schema node_name public node_base public static torch lazy OpKind ClassOpKind torch lazy OpKind opkind schema node_name node_ctor_args node_base_ctor_call schema scalar_initializers has_optional_defs std string ToString const override std stringstream ss ss node_base ToString members_to_string_str ss str create_function schema reuse_ctor_args can_be_reused_function schema reuse_ctor_args lowering_function schema scalar_decls has_optional_decls dataclass frozen=True GenTSLazyIR GenLazyIR lowering_function schema LazyIrSchema - str signature = torch lazy TSOpVector Lower std shared_ptr torch jit GraphFunction function torch lazy TSLoweringContext loctx const override schema properties LowerDeclOnly f signature schema properties Lower f signature ts_lowering_body schema create_function schema LazyIrSchema node_ctor_args str - str signature = f static NodePtr Create node_ctor_args schema properties CreateFnDeclOnly f signature schema properties CreateFn f signature ReuseOrMakeNode schema node_name data can_be_reused_function schema LazyIrSchema node_ctor_args str - str signature = f bool CanBeReused node_ctor_args const schema properties CanBeReusedDeclOnly f signature schema properties CanBeReused value_comparison = arg itertools chain schema positional_values schema keyword_values isinstance arg lazy_type OptionalCType value_comparison append f nullable_operand i++ == arg name value_or kNullValue value_comparison append f operand i++ == arg name arg itertools chain schema positional_scalars schema keyword_scalars isinstance arg lazy_type OptionalCType value_comparison append f this- arg name arg name &#124; &#124; this- arg name arg name this- arg name == arg name value_comparison append f this- arg name == arg name value_comparison_str = \n join value_comparison f signature size_t i = value_comparison_str dataclass frozen=True GenLazyNativeFuncDefinition class_method_name str backend_index BackendIndex tensor_class str gen_forced_fallback_code bool backend_namespace str get_tensorlist str get_tensor_or_wrap_number str try_get_tensor str metrics_counter str create_tensor str create_from_first_tensor bool create_aten_from_ltc_tensor str tuple_aten_from_ltc_tensors str lazy_tensor_ptr str get_device_fn str lazy_tensor_decls func NativeFunction schema LazyIrSchema - str value_args = schema filtered_args values=True scalars=False Generates lazy_ name variables LazyTensors wrapping input tensors lazy_tensor_decls list str = arg value_args arg is_wrapped_scalar isinstance arg lazy_type OptionalCType lazy_tensor_decls append f auto node_ arg name = arg name std make_optional torch lazy LazyGraphExecutor Get - GetIrValueForScalarFromCodegen arg name common_device std nullopt lazy_tensor_decls append f auto node_ arg name = torch lazy LazyGraphExecutor Get - GetIrValueForScalarFromCodegen arg name common_device arg is_symint_or_list continue values extracted isValueType isinstance arg lazy_type BaseCType arg lazy_type type tensorListValueT lazy_tensor_decls append f auto lazy_ arg name _tensorlist = f backend_namespace get_tensorlist arg name lazy_tensor_decls append f lazy_tensor_ptr lazy_ arg name = f backend_namespace get_tensor_or_wrap_number arg name common_device isinstance arg lazy_type OptionalCType assert arg lazy_type elem == BaseCType getValueT arg lazy_type elem TODO alanwaketan Maybe we want apply GetLtcTensorOrCreateForWrappedNumber here hold until we encounter real world example lazy_tensor_decls append f lazy_tensor_ptr lazy_ arg name = f backend_namespace try_get_tensor arg name value_or Tensor raise AssertionError f TODO sure there other valid types handle here arg lazy_type \n join lazy_tensor_decls force_eager_fallback func NativeFunction schema LazyIrSchema metadata BackendMetadata sig DispatcherSignature &#124; NativeSignature - str gen_forced_fallback_code gen_fallback_code schema sig overload_name=func func name overload_name metrics func NativeFunction schema LazyIrSchema - str f metrics_counter get_device func NativeFunction schema LazyIrSchema - str value_args = schema filtered_args values=True scalars=False scalar_args = schema filtered_args values=False scalars=True value_types_names = f name value_args is_wrapped_scalar optional_device = OptionalCType BaseCType deviceT optional_devices = name scalar_args lazy_type == optional_device assert len value_types_names len optional_devices Expected least one Value Device type get_device_str = f get_device_fn join value_types_names + optional_devices f auto common_device = get_device_str TORCH_INTERNAL_ASSERT common_device shape_inference func NativeFunction schema LazyIrSchema - str metadata = backend_index get_kernel func assert metadata None all_args = schema filtered_args returns_length = len schema returns call meta kernel exists compute output shape dtype our IR Note Generated LTC Shape Functions LTC uses meta tensors core do shape inference when possible otherwise we generate shape function declaration needs manually implemented How do we detect which ops eligible use meta tensors In general we should able use meta tensors just structured operators also composite operators implemented terms structured kernels We don t currently have way knowing codegen time which ops implemented way This case all view view_copy operators however so we re going use them specifically all view_copy ops instead manually writing shape rules all them is_view_copy_op = view_copy func tags is_structured = func structured func structured_delegate None is_structured is_view_copy_op meta_out = std vector torch lazy Shape shapes torch lazy Shape out_meta scalar_type out_meta sizes vec returns_length this_shape i int - str f torch lazy Shape std get i out_meta scalar_type std get i out_meta sizes vec shapes_str = join this_shape i i range returns_length meta_out = std vector torch lazy Shape shapes + shapes_str + Convert tensor args meta device call We can t pass input tensors directly because they functional wrappers If any meta kernels call tensor op redispatch we don t want hit functionalize kernels Even meta functions might redispatch e g they call into view ops dispatcher_sig = DispatcherSignature from_schema func func meta_conversion_str meta_call_ctx = convert_to_meta_tensors dispatcher_sig meta_call_args = e expr e translate meta_call_ctx dispatcher_sig arguments method=False is_view_copy_op view_copy ops always have CompositeExplicitAutogradNonFunctional kernel assert func has_composite_explicit_autograd_non_functional_kernel dispatch_ns = compositeexplicitautogradnonfunctional dispatch_ns = meta aten_name = schema aten_name TODO trolling func func has_symint metadata supports_symint aten_name += _symint shape_str = f \ meta_conversion_str auto out_meta = dispatch_ns aten_name join meta_call_args meta_out shape_sig = ComputeShapeSignature metadata kernel func symint=metadata supports_symint shape_str = f auto shapes = shape_sig shape_call shape_str += f TORCH_INTERNAL_ASSERT shapes size == returns_length Calculating which dimensions symbolic func_schema_str = aten + str func func shape_str += f torch lazy symbolicShapeEnabled std vector torch jit IValue inputs = join str name all_args const char schema_str = func_schema_str applySymbolicShapesOnLT schema_str inputs shapes shape_str build_ir_node func NativeFunction schema LazyIrSchema - str node_ctor_input_str = node_ctor_inputs schema f torch lazy NodePtr node = torch lazy ReuseNode schema node_name node_ctor_input_str node shape_inference func schema node = torch lazy MakeNode schema node_name node_ctor_input_str std move shapes CacheNode node create_lazy_tensor first_tensor_name str &#124; None = None - str xla uses instance method tensor creation time being create_from_first_tensor TODO whc remove XLA switches using static method creation assert first_tensor_name None Requires first tensor create lazy tensor f first_tensor_name create_tensor f backend_namespace create_tensor return_aten_tensor func NativeFunction schema LazyIrSchema - str returns_length = len schema returns value_args = schema filtered_args values=True scalars=False value_types_names = f name value_args is_wrapped_scalar first_tensor_name = value_types_names len value_types_names None bridge_str = f auto result = create_aten_from_ltc_tensor create_lazy_tensor first_tensor_name std move node common_device returns_length assert len value_types_names Code below assumes there least one tensor arg bridge_str = f std vector lazy_tensor_ptr lazy_tensors int i = i returns_length i++ lazy_tensors push_back create_lazy_tensor first_tensor_name getValueT node i common_device auto result = tuple_aten_from_ltc_tensors returns_length lazy_tensors schema name name inplace func func is_out_fn assert returns_length == We assumed there no such case where op in-place variant f has tuple outputs got tuple len returns_length bridge_str = f lazy_ first_tensor_name - SetInPlaceIrValue node auto result = first_tensor_name bridge_str += result bridge_str method_with_native_function __call__ func NativeFunction - list str sig = kernel_signature func backend_index metadata = backend_index get_kernel func assert metadata None schema = LazyIrSchema func func symint=metadata supports_symint f \ sig decl name=f class_method_name metadata kernel force_eager_fallback func schema metadata sig metrics func schema get_device func schema lazy_tensor_decls func schema build_ir_node func schema return_aten_tensor func schema \n ComputeShapeSignature Here we use base name suffix signature avoid generating in-place variants __init__ kernel_name str f NativeFunction symint bool - None __schema = LazyIrSchema f func symint=symint __dispatch_args = join decl dispatcher arguments f func symint=symint __call_args = join f arg name arg __schema filtered_args generator=True __kernel_name = kernel_name __decl_suffix - str f __kernel_name __dispatch_args __call_suffix - str f __kernel_name __call_args property shape_decl - str f TORCH_API std vector torch lazy Shape compute_shape_ __decl_suffix property shape_call - str f torch lazy compute_shape_ __call_suffix dataclass frozen=True GenLazyShapeInferenceDefinition backend_index BackendIndex tensor_class str method_with_native_function __call__ f NativeFunction - list str metadata = backend_index get_kernel f assert metadata None See Note Generated LTC Shape Functions is_view_copy_op = view_copy f tags is_structured = f structured f structured_delegate None is_structured is_view_copy_op shape_sig = ComputeShapeSignature metadata kernel f symint=metadata supports_symint \n join f shape_sig shape_decl generate_non_native_lazy_ir_nodes non_native list dict str Any gen_lazy_ir GenLazyIR - list str Generate non-native lazy IR node classes nodes = op non_native Set default properties Non-Native IRs properties = LazyIrProperties ShapeCache CanBeReused LowerDeclOnly p op get properties setattr properties p True non-native assumed want symint bindings you wrote symint schema = LazyIrSchema FunctionSchema parse op func properties symint=True schema opkind = op get opkind nodes append gen_lazy_ir gen schema nodes