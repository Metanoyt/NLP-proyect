Nodes represent definition value our graph operators builtins inspect logging operator types collections abc Callable Iterable Mapping Sequence typing Any Optional TYPE_CHECKING TypeAlias Union typing_extensions ParamSpec TypeVar torch torch _C _fx_map_aggregate _fx_map_arg _NodeBase torch fx operator_schemas ArgsKwargsPair normalize_function normalize_module torch utils _dtype_abbrs dtype_abbrs _ops ops _ops _compatibility compatibility TYPE_CHECKING graph Graph __all__ = Node map_arg map_aggregate has_side_effect log = logging getLogger __name__ BaseArgumentTypes = Union str int float bool complex torch dtype torch Tensor torch device torch memory_format torch layout torch _ops OpOverload torch SymInt torch SymBool torch SymFloat base_types = BaseArgumentTypes __args__ type ignore attr-defined Target TypeAlias = Union Callable Any str Argument = Optional Union tuple Argument Sequence Argument Mapping str Argument slice Slice Argument Argument Argument slice templated type typing range Node BaseArgumentTypes pyrefly ignore invalid-annotation ArgumentT = TypeVar ArgumentT bound=Argument _P = ParamSpec _P _R = TypeVar _R _legal_ops = dict fromkeys placeholder call_method call_module call_function get_attr output root Dynamo unable trace global set Callable __contains__ See https github com pytorch pytorch issues Since we only have handful ops so switch list callables _side_effectful_need_to_be_preserved_pre_dispatch list Callable Any = torch _C _set_grad_enabled torch amp _enter_autocast torch amp _exit_autocast TODO Either refactor into functions dce functional graphs dce all graphs add logic correctly mark all inplace ops side effectful _side_effectful_functions set Callable Any = torch _assert torch _assert_async _ops aten _assert_async msg _ops aten _assert_scalar default _ops aten _assert_tensor_metadata default _ops aten sym_constrain_range default _ops aten sym_constrain_range_for_size default _ops profiler _record_function_enter _ops profiler _record_function_enter_new _ops profiler _record_function_exit _ops inductor accumulate_grad_ default operator setitem _side_effectful_need_to_be_preserved_pre_dispatch hasattr _ops inductor resize_storage_bytes_ _side_effectful_functions add _ops inductor resize_storage_bytes_ default compatibility is_backward_compatible=False has_side_effect fn Callable _P _R - Callable _P _R _side_effectful_functions add fn fn fixed master WAR _find_module_of_method orig_method Callable Any - str name = orig_method __name__ module = orig_method __module__ module None module guess torch torch nn functional getattr guess name None orig_method guess __name__ raise RuntimeError f cannot find module orig_method Borrowed CPython typing module https github com python cpython blob f dc c d fee efaf d e bdf e Lib typing py#L _type_repr obj object - str Return repr object special-casing types internal helper If obj type we shorter version than default type __repr__ based module qualified name which typically enough uniquely identify type For everything we fall back repr obj Extension If we don t ignore GenericAlias then ` list int ` will print simply list isinstance obj type isinstance obj types GenericAlias obj __module__ == builtins obj __qualname__ f obj __module__ obj __qualname__ obj isinstance obj types FunctionType obj __name__ repr obj _get_qualified_name func Callable Any - str things like getattr just appear builtins getattr builtins func __name__ None func func __name__ torch Tensor fn isinstance func types MethodDescriptorType types WrapperDescriptorType func getattr torch Tensor func __name__ None func __module__ == torch _tensor __name__ func __qualname__ == f Tensor func __name__ f torch Tensor func __name__ name = func __name__ name == lambda For lambdas try get their defining name module try name = inspect getsource func split = strip except Exception e raise RuntimeError Unable represent lambda e module = _find_module_of_method func module = module replace torch _ops torch ops WAR bug how torch ops assigns module Fixup segment_reduce mismatch module == torch name == segment_reduce name = _ + name f module name _format_arg arg object max_list_len float = float inf - str hasattr arg _custom_fx_repr_fn arg _custom_fx_repr_fn isinstance arg list items = join _format_arg idx enumerate arg idx max_list_len maybe_len = len arg max_list_len + f total_len= len arg f items maybe_len isinstance arg tuple items = join _format_arg idx enumerate arg idx max_list_len maybe_len = len arg max_list_len + f total_len= len arg maybe_comma = len arg == f items maybe_comma maybe_len isinstance arg dict items_str = join f k _format_arg v k v arg items f items_str isinstance arg Node + str arg str arg compatibility is_backward_compatible=True Node _NodeBase ` ` Node ` ` data structure represents individual operations within ` ` Graph ` ` For most part Nodes represent callsites various entities such operators methods Modules some exceptions include nodes specify function inputs outputs Each ` ` Node ` ` has function specified its ` ` op ` ` property The ` ` Node ` ` semantics each value ` ` op ` ` follows - ` ` placeholder ` ` represents function input The ` ` name ` ` attribute specifies name value will take ` ` target ` ` similarly name argument ` ` args ` ` holds either nothing single argument denoting default parameter function input ` ` kwargs ` ` don t-care Placeholders correspond function parameters e g ` ` x ` ` graph printout - ` ` get_attr ` ` retrieves parameter module hierarchy ` ` name ` ` similarly name result fetch assigned ` ` target ` ` fully-qualified name parameter s position module hierarchy ` ` args ` ` ` ` kwargs ` ` don t-care - ` ` call_function ` ` applies free function some values ` ` name ` ` similarly name value assign ` ` target ` ` function applied ` ` args ` ` ` ` kwargs ` ` represent arguments function following Python calling convention - ` ` call_module ` ` applies module module hierarchy s ` ` forward ` ` method given arguments ` ` name ` ` previous ` ` target ` ` fully-qualified name module module hierarchy call ` ` args ` ` ` ` kwargs ` ` represent arguments invoke module excluding argument - ` ` call_method ` ` calls method value ` ` name ` ` similar ` ` target ` ` string name method apply ` ` ` ` argument ` ` args ` ` ` ` kwargs ` ` represent arguments invoke module including argument - ` ` output ` ` contains output traced function its ` ` args ` ` attribute This corresponds statement Graph printout _args tuple Argument _kwargs dict str Argument graph Graph unique name value being created name str kind operation = placeholder &#124; call_method &#124; call_module &#124; call_function &#124; get_attr op str method module function name method module function attr being invoked e g add layer torch add target Target All ` Node ` -valued inputs Key Node value don t-care The public API ` all_input_nodes ` private attribute should accessed directly _input_nodes dict Node None All nodes use value produced Node Note one user may correspond several uses e g node ` ` x + x ` ` would appear once here represents two uses Is dict act ordered set Keys significant value dont-care users dict Node None Type expression representing output value node This should contain same Type objects would appear type annotations function inputs outputs For placeholder nodes value will used type-annotate generated function parameters For node value will used type-annotate generated function type Note special case ` ` ` ` does produce value s more notation Thus value describes type args ` ` ` ` node type Optional Any _sort_key Any If set use fn print node _repr_fn Optional Callable Node str Dictionary store metadata passes need do their transformations This metadata preserved across node copies meta dict str Any compatibility is_backward_compatible=True __init__ graph Graph name str op str target Target args tuple Argument kwargs dict str Argument return_type Optional Any = None - None Instantiate instance ` ` Node ` ` Note most often you want use Graph APIs i e ` ` Graph call_module ` ` ` ` Graph call_method ` ` etc rather than instantiating ` ` Node ` ` directly Args graph Graph The ` ` Graph ` ` which ` ` Node ` ` should belong name str The name which output ` ` Node ` ` should assigned op str The opcode ` ` Node ` ` Can one placeholder call_method call_module call_function get_attr output target Target The target op should call See broader ` ` Node ` ` docstring more details args Tuple Argument The args passed ` ` target ` ` kwargs Dict str Argument The kwargs passed ` ` target ` ` return_type Optional Any The python type expression representing type output node This field can used annotation values generated code other types analyses op == call_function callable target raise ValueError f Node graph = graph name = name target target has type torch typename target Callable expected assert op _legal_ops isinstance target str raise ValueError f Node graph = graph name = name target target has type torch typename target str expected super __init__ graph name op target return_type _update_args_kwargs args kwargs __getstate__ - dict str Any __dict__ graph graph name name op op target target type target _sort_key _sort_key _args _args _kwargs _kwargs _erased _erased _prev _prev _next _next _input_nodes _input_nodes users users _repr_fn _repr_fn meta meta __setstate__ state dict str Any - None k v state items setattr k v property next - Node Returns next ` ` Node ` ` linked list Nodes Returns The next ` ` Node ` ` linked list Nodes _next property prev - Node Returns previous ` ` Node ` ` linked list Nodes Returns The previous ` ` Node ` ` linked list Nodes _prev compatibility is_backward_compatible=True prepend x Node - None Insert x before node list nodes graph Example Before p - bx - x - ax After p - x - bx - ax Args x Node The node put before node Must member same graph pyrefly ignore missing-attribute _prepend x compatibility is_backward_compatible=True append x Node - None Insert ` ` x ` ` after node list nodes graph Equivalent ` ` next prepend x ` ` Args x Node The node put after node Must member same graph pyrefly ignore missing-attribute _next _prepend x property args - tuple Argument The tuple arguments ` ` Node ` ` The interpretation arguments depends node s opcode See ` Node ` docstring more information Assignment property allowed All accounting uses users updated automatically assignment _args args setter args tuple Argument - None Set tuple arguments Node The interpretation arguments depends node s opcode See ` ` fx Graph ` ` docstring more information DO NOT CALL ` _update_args_kwargs ` directly The correct way set ` args ` via direct assignment i e ` node args = new_args ` _update_args_kwargs _kwargs property kwargs - dict str Argument The dict keyword arguments ` ` Node ` ` The interpretation arguments depends node s opcode See ` Node ` docstring more information Assignment property allowed All accounting uses users updated automatically assignment _kwargs kwargs setter kwargs k dict str Argument - None Set dict kwargs Node The interpretation arguments depends node s opcode See ` ` fx Graph ` ` docstring more information DO NOT CALL ` _update_args_kwargs ` directly The correct way set ` args ` via direct assignment i e ` node kwargs = new_kwargs ` _update_args_kwargs _args k property all_input_nodes - list Node Return all Nodes inputs Node This equivalent iterating over ` ` args ` ` ` ` kwargs ` ` only collecting values Nodes Returns List ` ` Nodes ` ` appear ` ` args ` ` ` ` kwargs ` ` ` ` Node ` ` order list _input_nodes keys compatibility is_backward_compatible=True update_arg idx int arg Argument - None Update existing positional argument contain new value ` ` arg ` ` After calling ` ` args idx == arg ` ` Args idx int The index into ` ` args ` ` element update arg Argument The new argument value write into ` ` args ` ` args = list args args idx = arg args = tuple args compatibility is_backward_compatible=True insert_arg idx int arg Argument - None Insert positional argument argument list given index Args idx int The index element ` ` args ` ` inserted before arg Argument The new argument value insert into ` ` args ` ` assert = idx = len args insert_args index must between len args args_left = args idx args_right = args idx _args = args_left + arg + args_right _new_input_nodes dict Node None = _fx_map_arg arg _new_input_nodes setdefault new_use _new_input_nodes keys new_use _input_nodes _input_nodes setdefault new_use new_use users setdefault compatibility is_backward_compatible=True update_kwarg key str arg Argument - None Update existing keyword argument contain new value ` ` arg ` ` After calling ` ` kwargs key == arg ` ` Args key str The key ` ` kwargs ` ` element update arg Argument The new argument value write into ` ` kwargs ` ` kwargs = kwargs key arg property stack_trace - Optional str Return Python stack trace recorded during tracing any When traced fx Tracer property usually populated ` Tracer create_proxy ` To record stack traces during tracing debug purposes set ` record_stack_traces = True ` ` Tracer ` instance When traced dynamo property will populated default ` OutputGraph create_proxy ` stack_trace would have innermost frame end string meta get stack_trace None stack_trace setter stack_trace trace Optional str - None meta stack_trace = trace __repr__ - str _repr_fn _repr_fn name staticmethod _pretty_print_target target object - str Make target printouts more user-friendly builtins will printed ` builtins xyz ` operators will printed ` operator xyz ` other callables will printed qualified name e g torch add isinstance target str target hasattr target __module__ name = getattr target __name__ None name None Just defensive we don t have ` __name__ ` get qualname Not sure happens any members ` operator ` ` builtins ` This fallback path good since e g things ` operator ` have ` _operator ` their __module__ TODO THIS IS BROKEN _get_qualified_name calls ` __name__ ` _get_qualified_name target type ignore arg-type target __module__ == builtins f builtins name target __module__ == _operator f operator name _get_qualified_name target type ignore arg-type compatibility is_backward_compatible=True format_node placeholder_names Optional list str = None maybe_return_typename Optional list str = None include_tensor_metadata bool = False - Optional str Return descriptive string representation ` ` ` ` This method can used no arguments debugging utility This function also used internally ` ` __str__ ` ` method ` ` Graph ` ` Together strings ` ` placeholder_names ` ` ` ` maybe_return_typename ` ` make up signature autogenerated ` ` forward ` ` function Graph s surrounding GraphModule ` ` placeholder_names ` ` ` ` maybe_return_typename ` ` should used otherwise Args placeholder_names A list will store formatted strings representing placeholders generated ` ` forward ` ` function Internal use only maybe_return_typename A single-element list will store formatted string representing output generated ` ` forward ` ` function Internal use only include_tensor_metadata Whether include tensor metadata Returns str If we re using ` ` format_node ` ` internal helper ` ` __str__ ` ` method ` ` Graph ` ` ` ` ` ` placeholder Node ` ` None ` ` Otherwise descriptive string representation current Node op == placeholder assert isinstance target str arg_str = target arg_str += arg_str + f _type_repr type type placeholder_names placeholder_names append arg_str None maybe_typename = f _type_repr type type default_val = default= + str args + args f name maybe_typename num_users= len users = op target= target default_val op == get_attr maybe_typename = f _type_repr type type None f name maybe_typename num_users= len users = f op target= _pretty_print_target target op == output type maybe_return_typename maybe_return_typename = f - _type_repr type f args stringify_shape shape Iterable - str f join str x x shape meta_val = meta get val meta get tensor_meta meta get example_value None type_annotation = include_tensor_metadata isinstance meta_val torch Tensor meta_val layout torch sparse_csc torch sparse_csr stride_annotation = f stringify_shape meta_val stride device_annotation = f meta_val device type_annotation = f Tensor dtype_abbrs meta_val dtype stringify_shape meta_val shape f stride_annotation device_annotation type_annotation = f _type_repr type type None f name type_annotation num_users= len users = f op target= _pretty_print_target target f args = _format_arg args kwargs = _format_arg kwargs compatibility is_backward_compatible=True replace_all_uses_with replace_with Node delete_user_cb Optional Callable Node bool = None propagate_meta bool = False - list Node Replace all uses ` ` ` ` Graph Node ` ` replace_with ` ` Args replace_with Node The node replace all uses ` ` ` ` delete_user_cb Callable Callback called determine whether given user node should removed propagate_meta bool Whether copy all properties meta field original node onto replacement node For safety only valid do replacement node doesn t already have existing meta field Returns The list Nodes which change made propagate_meta assert len replace_with meta == Called node replace_all_uses_with replace_with propagate_meta=True replace_with already has meta keys k v meta items replace_with meta k = v to_process = users replace_hooks = getattr graph owning_module _replace_hooks None result = use_node to_process delete_user_cb None delete_user_cb use_node continue result append use_node replace_hooks replace_hook replace_hooks replace_hook old=self new=replace_with name user=use_node pyrefly ignore missing-attribute use_node _replace_input_with replace_with type ignore attr-defined result compatibility is_backward_compatible=False is_impure impure_random bool = True - bool Returns whether op impure i e its op placeholder output call_function call_module which impure Args impure_random bool Whether treat rand op impure Returns bool If op impure op placeholder output True op == call_function schema = getattr target _schema None schema None schema is_mutable impure since mutates inputs True impure_random getattr target _nondeterministic_seeded False impure since mutates RNG state True Handle Python random functions don t have _nondeterministic_seeded still affect global RNG state issue These should impure regardless impure_random setting maintain consistency between eager compiled execution _random_functions = torch rand torch randn torch randint torch randperm torch rand_like torch randn_like torch randint_like torch normal torch poisson torch bernoulli torch multinomial target _random_functions All random operations impure ensure consistent behavior between eager compiled execution regardless generator usage True target _side_effectful_functions subgraph_has_impure_ops module torch fx GraphModule - bool Return True GraphModule type subgraph contains any impure op False assert isinstance module torch fx GraphModule caller should only pass GraphModule subgraph_has_impure_ops check node module graph nodes node op == call_function node is_impure impure_random True node op == call_module submodule = module get_submodule node target isinstance submodule torch fx GraphModule subgraph_has_impure_ops submodule False Check impure module op == call_module assert graph owning_module None graph owning_module set purity check target_mod = graph owning_module get_submodule target assert target_mod None f Did find expected submodule target target isinstance target_mod torch fx GraphModule subgraph_has_impure_ops target_mod getattr target_mod _is_impure False False compatibility is_backward_compatible=False normalized_arguments root torch nn Module arg_types Optional tuple Any = None kwarg_types Optional dict str Any = None normalize_to_only_use_kwargs bool = False - Optional ArgsKwargsPair Returns normalized arguments Python targets This means ` args kwargs ` will matched up module functional s signature exclusively kwargs positional order ` normalize_to_only_use_kwargs ` true Also populates default values Does support positional-only parameters varargs parameters Supports module calls May require ` arg_types ` ` kwarg_types ` order disambiguate overloads Args root torch nn Module Module upon which resolve module targets arg_types Optional Tuple Any Tuple arg types args kwarg_types Optional Dict str Any Dict arg types kwargs normalize_to_only_use_kwargs bool Whether normalize only use kwargs Returns Returns NamedTuple ArgsKwargsPair ` None ` successful op == call_function assert callable target normalize_function target args type ignore arg-type kwargs arg_types kwarg_types normalize_to_only_use_kwargs=normalize_to_only_use_kwargs op == call_module assert isinstance target str normalize_module root target args type ignore arg-type kwargs normalize_to_only_use_kwargs=normalize_to_only_use_kwargs None compatibility is_backward_compatible=True replace_input_with old_input Node new_input Node - None Loop through input nodes ` ` ` ` replace all instances ` ` old_input ` ` ` ` new_input ` ` Args old_input Node The old input node replaced new_input Node The new input node replace ` ` old_input ` ` m = graph owning_module getattr m _replace_hooks None replace_hook m _replace_hooks replace_hook old=old_input new=new_input name user=self pyrefly ignore missing-attribute _replace_input_with old_input new_input type ignore attr-defined _rename candidate str - None candidate == name name = graph _graph_namespace create_name candidate None name = name graph _graph_namespace _rename_object name __setattr__ name str value Any - None name == name hasattr name m = graph owning_module getattr m _replace_hooks None assert isinstance value str user users replace_hook m _replace_hooks replace_hook old=self new=value user=user update = False hasattr name hasattr graph _find_nodes_lookup_table graph _find_nodes_lookup_table update = True graph _find_nodes_lookup_table remove object __setattr__ name value update graph _find_nodes_lookup_table insert compatibility is_backward_compatible=True map_arg ArgumentT fn Callable Node Argument - ArgumentT Apply fn recursively each Node appearing arg arg may list tuple slice dict string keys value will have same type structure assert callable fn torch fx map_arg fn fn must callable _fx_map_arg fn compatibility is_backward_compatible=True map_aggregate ArgumentT fn Callable Argument Argument - ArgumentT Apply fn recursively each object appearing arg arg may list tuple slice dict string keys value will have same type structure _fx_map_aggregate fn