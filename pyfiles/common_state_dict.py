mypy allow-untyped-defs Owner s oncall distributed copy itertools chain typing Any torch torch nn nn torch distributed _sharded_tensor ShardedTensor torch distributed _state_dict_utils _gather_state_dict torch distributed checkpoint state_dict _PG _STATE set_state_dict StateDictOptions torch distributed tensor DTensor VerifyStateDictMixin _compare_tensor orig_tensor dist_tensor offload_to_cpu=False isinstance dist_tensor DTensor ShardedTensor dist_tensor = _gather_state_dict mykey dist_tensor pop mykey offload_to_cpu orig_tensor = orig_tensor cpu dist_tensor = dist_tensor cpu assertTrue isinstance dist_tensor torch Tensor assertTrue torch allclose orig_tensor dist_tensor _verify_msd msd dict str Any dist_msd dict str Any options StateDictOptions = StateDictOptions offload_to_cpu=False - None options ignore_frozen_params assertEqual len msd len dist_msd fqn param msd items dist_param = dist_msd get fqn options ignore_frozen_params assertIsNotNone dist_param f fqn= try _compare_tensor param dist_param offload_to_cpu except AssertionError e raise AssertionError f fqn has mismatched value param dist_param e dist_param None assertFalse param requires_grad f fqn= _verify_osd model nn Module optim torch optim Optimizer osd dict str Any dist_osd dict str Any - None params = list chain from_iterable g params g optim param_groups param_pid_mapping = dict zip params range len params strict=True fqn_pid_mapping = fqn param model named_parameters pid = param_pid_mapping param fqn_pid_mapping fqn = pid fqn_pid_mapping pid = fqn Check optimizer_state_dict state assertEqual len osd _STATE len dist_osd _STATE pid states osd _STATE items fqn = fqn_pid_mapping pid dist_states = dist_osd _STATE get fqn None assertIsNotNone dist_states fqn assertEqual len states len dist_states key state states items dist_state = states get key None assertIsNotNone dist_state _compare_tensor state dist_state Check optimizer_state_dict param_group old_dist_osd_pg = dist_osd _PG len osd _PG = len dist_osd _PG assertTrue len dist_osd _PG len osd _PG new_pg = copy deepcopy dist_osd _PG new_pg params = dist_group dist_osd _PG new_pg params extend dist_group params dist_osd _PG = new_pg assertEqual len osd _PG len dist_osd _PG group dist_group zip osd _PG dist_osd _PG strict=True assertEqual len group len dist_group key value group items Below doesn t work because param_groups can have None values dist_value = dist_group get key None assertIsNotNone dist_value dist_group group dist_value = dist_group key key == params fqns = fqn_pid_mapping pid pid value assertEqual sorted fqns sorted dist_value assertEqual value dist_value dist_osd _PG = old_dist_osd_pg _verify_osd_by_load model nn Module optim torch optim Optimizer new_optim torch optim Optimizer dist_osd dict str Any - None new_dist_osd = _gather_state_dict dist_osd set_state_dict model optimizers=new_optim model_state_dict= optim_state_dict=new_dist_osd assertEqual optim state_dict new_optim state_dict FusionEmbedding nn Module __init__ vocab_size int fusion_vocab_size int embed_dim int - None super __init__ embedding = nn Embedding vocab_size embed_dim fusion_embedding = nn Embedding fusion_vocab_size embed_dim FusionEmbeddingWithHook nn Module __init__ vocab_size int fusion_vocab_size int embed_dim int - None super __init__ embedding = nn Embedding vocab_size embed_dim fusion_embedding = nn Embedding fusion_vocab_size embed_dim _register_state_dict_hook FusionEmbeddingWithHook _state_dict_hook _register_load_state_dict_pre_hook FusionEmbeddingWithHook _load_state_dict_hook with_module=True _state_dict_hook destination prefix keep_vars Remove embedding original embedding state_dict name This keeps original state dict name embedding before fusing FusionEmbedding key = prefix + embedding weight new_key = prefix + weight destination new_key = destination key del destination key _load_state_dict_hook state_dict prefix args kwargs Apply extra embedding prefix state_dict key account FusionEmbedding wrapping state_dict key = prefix + weight new_key = prefix + embedding weight state_dict new_key = state_dict key del state_dict key FusionEmbeddingWithModifier FusionEmbeddingWithHook _fqn_modifiers private function contract between DSD When users change state_dict keys they need provide mapping new key original key This used ensure consistency between state_dict keys fqn _fqn_modifiers - dict str str weight embedding