subprocess sys torch pip_install package subprocess check_call sys executable -m pip install package try transformers AutoModelForCausalLM AutoTokenizer WhisperForConditionalGeneration WhisperProcessor except ModuleNotFoundError print Installing HuggingFace Transformers pip_install git+https github com huggingface transformers git#egg=transformers finally transformers AutoModelForCausalLM AutoTokenizer WhisperForConditionalGeneration WhisperProcessor Benchmark staticmethod get_model_and_inputs model_name device raise NotImplementedError get_model_and_inputs implemented WhisperBenchmark Benchmark SAMPLE_RATE = DURATION = seconds staticmethod get_model_and_inputs model_name device processor = WhisperProcessor from_pretrained model_name model = WhisperForConditionalGeneration from_pretrained model_name device model config forced_decoder_ids = None model generation_config do_sample = False model generation_config temperature = num_samples = int WhisperBenchmark DURATION WhisperBenchmark SAMPLE_RATE audio = torch randn num_samples inputs = dict processor audio sampling_rate=WhisperBenchmark SAMPLE_RATE return_tensors= pt inputs input_features = inputs input_features device decoder_start_token = model config decoder_start_token_id inputs decoder_input_ids = torch tensor decoder_start_token device=device model inputs TextGenerationBenchmark Benchmark INPUT_LENGTH = OUTPUT_LENGTH = staticmethod get_model_and_inputs model_name device tokenizer = AutoTokenizer from_pretrained model_name model = AutoModelForCausalLM from_pretrained model_name device_map=device model eval model generation_config do_sample = False model generation_config use_cache = True model generation_config cache_implementation = static model generation_config max_new_tokens = TextGenerationBenchmark OUTPUT_LENGTH model generation_config pad_token_id = tokenizer eos_token_id model generation_config temperature = vocab_size = tokenizer vocab_size input_ids = torch randint low= high=vocab_size size= TextGenerationBenchmark INPUT_LENGTH device=device dtype=torch long example_inputs = input_ids input_ids model example_inputs HF_LLM_MODELS dict str Benchmark = meta-llama Llama- - B TextGenerationBenchmark google gemma- - b TextGenerationBenchmark google gemma- - b-it TextGenerationBenchmark openai whisper-tiny WhisperBenchmark Qwen Qwen - B TextGenerationBenchmark mistralai Mistral- B-Instruct-v TextGenerationBenchmark openai gpt-oss- b TextGenerationBenchmark