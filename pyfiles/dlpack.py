typing Any Optional torch enum torch _C _to_dlpack to_dlpack torch types Device _Device __all__ = DLDeviceType from_dlpack DLDeviceType enum IntEnum Enums DLPack specification aten src ATen dlpack h kDLCPU = kDLCUDA = kDLCUDAHost = kDLOpenCL = kDLVulkan = kDLMetal = kDLVPI = kDLROCM = kDLROCMHost = kDLExtDev = kDLCUDAManaged = kDLOneAPI = kDLWebGPU = kDLHexagon = kDLMAIA = torch _C _add_docstr to_dlpack r to_dlpack tensor - PyCapsule Returns opaque object DLPack capsule representing tensor note ` ` to_dlpack ` ` legacy DLPack interface The capsule returns cannot used anything Python other than use input ` ` from_dlpack ` ` The more idiomatic use DLPack call ` ` from_dlpack ` ` directly tensor object - works when object has ` ` __dlpack__ ` ` method which PyTorch most other libraries indeed have now warning Only call ` ` from_dlpack ` ` once per capsule produced ` ` to_dlpack ` ` Behavior when capsule consumed multiple times undefined Args tensor tensor exported The DLPack capsule shares tensor s memory TODO add typing Protocol able tell Mypy only objects __dlpack__ __dlpack_device__ methods accepted from_dlpack ext_tensor Any device Optional _Device = None copy Optional bool = None - torch Tensor from_dlpack ext_tensor - Tensor Converts tensor external library into ` ` torch Tensor ` ` The returned PyTorch tensor will share memory input tensor which may have come another library Note in-place operations will therefore also affect data input tensor This may lead unexpected issues e g other libraries may have read-only flags immutable data structures so user should only do they know sure fine Args ext_tensor object ` ` __dlpack__ ` ` attribute DLPack capsule The tensor DLPack capsule convert If ` ` ext_tensor ` ` tensor ndarray object must support ` ` __dlpack__ ` ` protocol i e have ` ` ext_tensor __dlpack__ ` ` method Otherwise ` ` ext_tensor ` ` may DLPack capsule which opaque ` ` PyCapsule ` ` instance typically produced ` ` to_dlpack ` ` function method device torch device str None An optional PyTorch device specifying where place new tensor If None default new tensor will same device ` ` ext_tensor ` ` copy bool None An optional boolean indicating whether copy ` ` ` ` If None PyTorch will copy only necessary Examples torch utils dlpack t = torch arange Convert tensor directly supported PyTorch = t = torch from_dlpack t t = - show memory shared t tensor - - t tensor - - The old-style DLPack usage intermediate capsule object capsule = torch utils dlpack to_dlpack t capsule capsule object dltensor t = torch from_dlpack capsule t tensor - - t = - now we re sharing memory between tensors t tensor - - t tensor - - t tensor - - hasattr ext_tensor __dlpack__ Only populate kwargs any optional arguments fact None Otherwise leave them out since we might end up falling back no-extra-kwargs __dlpack__ call kwargs dict str Any = kwargs max_version = copy None kwargs copy = copy Parse device parameter At moment can either torch device str representing torch device e g cpu cuda etc device None isinstance device str device = torch device device isinstance device torch device raise AssertionError f from_dlpack unsupported device type type device kwargs dl_device = torch _C _torchDeviceToDLDevice device ext_device = ext_tensor __dlpack_device__ ext_device either CUDA ROCm we need pass current stream ext_device DLDeviceType kDLCUDA DLDeviceType kDLROCM stream = torch cuda current_stream f cuda ext_device cuda_stream pointer stream public attribute documented The array API specify default legacy stream must passed value CUDA https data-apis org array-api latest API_specification array_object html dlpack-self-stream-none#dlpack-self-stream-none is_cuda = ext_device == DLDeviceType kDLCUDA Since pytorch using PTDS default lets directly pass legacy stream stream_ptr = is_cuda stream cuda_stream == stream cuda_stream kwargs stream = stream_ptr try Try running __dlpack__ while specifying ` max_version ` argument dlpack = ext_tensor __dlpack__ kwargs except TypeError If doesn t work try removing ` max_version ` argument kwargs pop max_version dlpack = ext_tensor __dlpack__ kwargs device None copy None raise AssertionError device copy kwargs supported when ext_tensor already DLPack capsule Old versions just call converter dlpack = ext_tensor torch _C _from_dlpack dlpack