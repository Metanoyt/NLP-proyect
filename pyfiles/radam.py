mypy allow-untyped-defs r Implementation RAdam algorithm typing cast Optional Union torch torch Tensor optimizer _capturable_doc _default_to_fused_or_foreach _differentiable_doc _disable_dynamo_if_unsupported _foreach_doc _get_capturable_supported_devices _get_scalar_dtype _get_value _maximize_doc _params_doc _to_scalar _use_grad_for_differentiable _view_as_real Optimizer ParamsT __all__ = RAdam radam RAdam Optimizer noqa D __init__ params ParamsT lr Union float Tensor = e- betas tuple float float = eps float = e- weight_decay float = decoupled_weight_decay bool = False foreach Optional bool = None maximize bool = False capturable bool = False differentiable bool = False noqa D isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Invalid learning rate lr = eps raise ValueError f Invalid epsilon value eps = betas raise ValueError f Invalid beta parameter index betas = betas raise ValueError f Invalid beta parameter index betas = weight_decay raise ValueError f Invalid weight_decay value weight_decay defaults = lr lr betas betas eps eps weight_decay weight_decay maximize maximize foreach foreach capturable capturable decoupled_weight_decay decoupled_weight_decay differentiable differentiable super __init__ params defaults __setstate__ state noqa D super __setstate__ state group param_groups group setdefault foreach None group setdefault maximize False group setdefault differentiable False group setdefault decoupled_weight_decay False group setdefault capturable False p group params p_state = state get p len p_state = torch is_tensor p_state step step_val = float p_state step p_state step = torch tensor step_val dtype=_get_scalar_dtype device=p device group capturable torch tensor step_val dtype=_get_scalar_dtype _init_group group params_with_grad grads exp_avgs exp_avg_sqs state_steps has_complex = False p group params p grad None has_complex &#124; = torch is_complex p params_with_grad append p p grad is_sparse raise RuntimeError RAdam does support sparse gradients grads append p grad state = state p Lazy state initialization len state == state step = torch zeros dtype=_get_scalar_dtype device=p device group capturable torch tensor dtype=_get_scalar_dtype Exponential moving average gradient values state exp_avg = torch zeros_like p memory_format=torch preserve_format Exponential moving average squared gradient values state exp_avg_sq = torch zeros_like p memory_format=torch preserve_format exp_avgs append state exp_avg exp_avg_sqs append state exp_avg_sq state_steps append state step has_complex _use_grad_for_differentiable step closure=None Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss _cuda_graph_capture_health_check loss = None closure None torch enable_grad loss = closure group param_groups params_with_grad list Tensor = grads list Tensor = exp_avgs list Tensor = exp_avg_sqs list Tensor = state_steps list Tensor = beta beta = cast tuple float float group betas has_complex = _init_group group params_with_grad grads exp_avgs exp_avg_sqs state_steps radam params_with_grad grads exp_avgs exp_avg_sqs state_steps beta =beta beta =beta lr=group lr weight_decay=group weight_decay eps=group eps maximize=group maximize foreach=group foreach capturable=group capturable differentiable=group differentiable decoupled_weight_decay=group decoupled_weight_decay has_complex=has_complex loss RAdam __doc__ = r Implements RAdam algorithm math \begin aligned \rule mm pt \\ \textbf input \gamma \text lr \ \beta_ \beta_ \text betas \ \theta_ \text params \ f \theta \text objective \ \lambda \text weightdecay \ \textit maximize \\ \hspace mm \epsilon \text epsilon \textit decoupled\_weight\_decay \\ \textbf initialize m_ \leftarrow \text first moment v_ \leftarrow \text second moment \\ \hspace mm \rho_ \infty \leftarrow -\beta_ - \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm \textbf \ \textit maximize \\ \hspace mm g_t \leftarrow -\nabla_ \theta f_t \theta_ t- \\ \hspace mm \textbf \\ \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \theta_t \leftarrow \theta_ t- \\ \hspace mm \textbf \ \lambda \neq \\ \hspace mm \textbf \ \textit decoupled\_weight\_decay \\ \hspace mm \theta_t \leftarrow \theta_ t - \gamma \lambda \theta_ t \\ \hspace mm \textbf \\ \hspace mm g_t \leftarrow g_t + \lambda \theta_ t \\ \hspace mm m_t \leftarrow \beta_ m_ t- + - \beta_ g_t \\ \hspace mm v_t \leftarrow \beta_ v_ t- + -\beta_ g^ _t \\ \hspace mm \widehat m_t \leftarrow m_t \big -\beta_ ^t \big \\ \hspace mm \rho_t \leftarrow \rho_ \infty - t \beta^t_ \big -\beta_ ^t \big \\ ex \hspace mm \textbf \ \rho_t \\ \hspace mm l_t \leftarrow \frac \sqrt -\beta^t_ \sqrt v_t +\epsilon \\ \hspace mm r_t \leftarrow \sqrt \frac \rho_t- \rho_t- \rho_ \infty \rho_ \infty - \rho_ \infty - \rho_t \\ \hspace mm \theta_t \leftarrow \theta_t - \gamma \widehat m_t r_t l_t \\ \hspace mm \textbf \\ \hspace mm \theta_t \leftarrow \theta_t - \gamma \widehat m_t \\ \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned For further details regarding algorithm we refer ` On variance adaptive learning rate beyond ` _ This implementation provides option use either original weight_decay implementation Adam where weight_decay applied gradient one AdamW where weight_decay applied weight through decoupled_weight_decay option When decoupled_weight_decay set False default uses original Adam style weight decay otherwise uses AdamW style which corresponds more closely ` author s implementation ` _ RAdam paper Further information about decoupled weight decay can found ` Decoupled Weight Decay Regularization ` _ + rf Args _params_doc lr float Tensor optional learning rate default e- betas Tuple float float optional coefficients used computing running averages gradient its square default eps float optional term added denominator improve numerical stability default e- weight_decay float optional weight decay L penalty default decoupled_weight_decay bool optional whether decouple weight decay AdamW obtain RAdamW If True algorithm does accumulate weight decay momentum nor variance default False _foreach_doc _maximize_doc _capturable_doc _differentiable_doc _On variance adaptive learning rate beyond https arxiv org abs _author s implementation https github com LiyuanLucasLiu RAdam _Decoupled Weight Decay Regularization https arxiv org abs _single_tensor_radam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor state_steps list Tensor beta float beta float lr float weight_decay float eps float decoupled_weight_decay bool differentiable bool maximize bool capturable bool has_complex bool torch jit is_scripting lr = _to_scalar lr i param enumerate params grad = grads i maximize -grads i exp_avg = exp_avgs i exp_avg_sq = exp_avg_sqs i step_t = state_steps i If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices param device type == step_t device type param device type capturable_supported_devices raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices torch is_complex param param = torch view_as_real param grad = torch view_as_real grad exp_avg = torch view_as_real exp_avg exp_avg_sq = torch view_as_real exp_avg_sq update step step_t += step = step_t capturable _get_value step_t weight_decay = decoupled_weight_decay param mul_ - lr weight_decay grad = grad add param alpha=weight_decay Decay first second moment running average coefficient exp_avg lerp_ grad - beta exp_avg_sq mul_ beta addcmul_ grad grad value= - beta bias_correction = - beta step bias_correction = - beta step correcting bias first moving moment bias_corrected_exp_avg = exp_avg bias_correction maximum length approximated SMA rho_inf = - beta - compute length approximated SMA rho_t = rho_inf - step beta step bias_correction _compute_rect pyrefly ignore unsupported-operation rho_t - rho_t - rho_inf rho_inf - rho_inf - rho_t _compute_adaptive_lr exp_avg_sq_sqrt = exp_avg_sq sqrt differentiable exp_avg_sq_sqrt = exp_avg_sq_sqrt add eps exp_avg_sq_sqrt = exp_avg_sq_sqrt add_ eps pyrefly ignore unsupported-operation bias_correction exp_avg_sq_sqrt Compute variance rectification term update parameters accordingly capturable update = torch where rho_t _compute_rect _compute_adaptive_lr param add_ bias_corrected_exp_avg lr update alpha=- rho_t param add_ bias_corrected_exp_avg lr _compute_adaptive_lr _compute_rect alpha=- param add_ bias_corrected_exp_avg lr alpha=- _multi_tensor_radam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor state_steps list Tensor beta float beta float lr float weight_decay float eps float decoupled_weight_decay bool differentiable bool maximize bool capturable bool has_complex bool len params == differentiable raise AssertionError _foreach ops don t support autograd If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices supports_xla=False all p device type == step device type p device type capturable_supported_devices p step zip params state_steps strict=True raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices lr = _to_scalar lr grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads exp_avgs exp_avg_sqs state_steps type ignore list-item grouped_params_ grouped_grads_ grouped_exp_avgs_ grouped_exp_avg_sqs_ grouped_state_steps_ _ grouped_tensors values grouped_params = cast list Tensor grouped_params_ grouped_grads = cast list Tensor grouped_grads_ grouped_exp_avgs = cast list Tensor grouped_exp_avgs_ grouped_exp_avg_sqs = cast list Tensor grouped_exp_avg_sqs_ grouped_state_steps = cast list Tensor grouped_state_steps_ Update steps If steps CPU foreach will fall back slow path which for-loop calling t add over over will then wrapped into Tensor over over again which slower than we just wrapped once now The alpha required assure we go right overload torch compiler is_compiling grouped_state_steps is_cpu torch _foreach_add_ grouped_state_steps torch tensor device= cpu alpha= torch _foreach_add_ grouped_state_steps has_complex _view_as_real grouped_params grouped_grads grouped_exp_avgs grouped_exp_avg_sqs maximize grouped_grads = torch _foreach_neg grouped_grads type ignore assignment maximum length approximated SMA rho_inf = - beta - compute length approximated SMA bias_correction Union tuple Tensor list Tensor bias_correction Union tuple Tensor list Tensor rho_t_list Union tuple Tensor list Tensor capturable bias_correction = torch _foreach_pow beta grouped_state_steps torch _foreach_neg_ bias_correction torch _foreach_add_ bias_correction bias_correction = torch _foreach_pow beta grouped_state_steps torch _foreach_mul_ bias_correction grouped_state_steps torch _foreach_mul_ bias_correction torch _foreach_div_ bias_correction bias_correction torch _foreach_neg_ bias_correction torch _foreach_add_ bias_correction rho_inf rho_t_list = bias_correction rho_t_list = rho_inf - _get_value step beta _get_value step - beta _get_value step step grouped_state_steps weight_decay = decoupled_weight_decay torch _foreach_mul_ grouped_params - lr weight_decay Reuse intermediate memory grouped_grads already allocated maximize maximize torch _foreach_add_ grouped_grads grouped_params alpha=weight_decay grouped_grads = torch _foreach_add type ignore assignment grouped_grads grouped_params alpha=weight_decay Decay first second moment running average coefficient torch _foreach_lerp_ grouped_exp_avgs grouped_grads - beta torch _foreach_mul_ grouped_exp_avg_sqs beta torch _foreach_addcmul_ grouped_exp_avg_sqs grouped_grads grouped_grads - beta Delete local intermediate since won t used anymore save peak memory del grouped_grads capturable num = torch _foreach_sub rho_t_list sub = torch _foreach_sub rho_t_list torch _foreach_mul_ num sub del sub torch _foreach_mul_ num rho_inf rho_inf = rho_inf - rho_inf - denom = torch _foreach_mul rho_t_list rho_inf torch _foreach_div_ num denom del denom torch _foreach_sqrt_ num TODO mlazos we should try get foreach_where op https github com pytorch pytorch issues rect = torch where rho_t n n rho_t zip num rho_t_list strict=True del num del rho_t_list unrect_step_size = torch where rect rect rect torch _foreach_mul_ unrect_step_size lr bias_correction = torch _foreach_pow beta grouped_state_steps torch _foreach_neg_ bias_correction torch _foreach_add_ bias_correction torch _foreach_div_ unrect_step_size bias_correction torch _foreach_neg_ unrect_step_size bias_correction = torch _foreach_pow beta grouped_state_steps torch _foreach_neg_ bias_correction torch _foreach_add_ bias_correction torch _foreach_sqrt_ bias_correction torch _foreach_mul_ bias_correction lr torch _foreach_mul_ bias_correction rect del rect torch _foreach_neg_ bias_correction torch _foreach_div_ bias_correction bias_correction del bias_correction rect = type ignore misc rho_t - type ignore arg-type rho_t - rho_inf rho_inf - rho_inf - rho_t rho_t rho_t rho_t_list unrectified = rect rect rect bias_correction = - beta _get_value step step grouped_state_steps unrect_step_size = lr rect bc - rect bc zip unrectified bias_correction strict=True bias_correction = - beta _get_value step lr rect bc - step rect bc zip grouped_state_steps rect bias_correction strict=True buffer = torch _foreach_sqrt grouped_exp_avg_sqs torch _foreach_add_ buffer eps torch _foreach_div_ buffer bias_correction torch _foreach_reciprocal_ buffer torch _foreach_add_ buffer unrect_step_size Here buffer = sqrt - beta ^t rect_step_size sqrt v + eps + unrect_step_size torch _foreach_addcmul_ grouped_params grouped_exp_avgs buffer _disable_dynamo_if_unsupported single_tensor_fn=_single_tensor_radam radam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor state_steps list Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim decoupled_weight_decay bool = False foreach Optional bool = None differentiable bool = False capturable bool = False has_complex bool = False maximize bool = False beta float beta float lr float weight_decay float eps float r Functional API performs RAdam algorithm computation See ` ~torch optim RAdam ` details all isinstance t torch Tensor t state_steps raise RuntimeError API has changed ` state_steps ` argument must contain list singleton tensors foreach None _ foreach = _default_to_fused_or_foreach params differentiable use_fused=False foreach torch jit is_scripting raise RuntimeError torch jit script supported foreach optimizers foreach torch jit is_scripting func = _multi_tensor_radam func = _single_tensor_radam func params grads exp_avgs exp_avg_sqs state_steps beta =beta beta =beta lr=lr weight_decay=weight_decay eps=eps maximize=maximize decoupled_weight_decay=decoupled_weight_decay differentiable=differentiable capturable=capturable has_complex=has_complex