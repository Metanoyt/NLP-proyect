mypy ignore-errors collections torch torch testing _internal common_utils TEST_WITH_ROCM torch testing _internal common_utils TestCase AutocastTestLists _rnn_cell_args n num_chunks is_lstm dev dtype input = torch randn n n device=dev dtype=torch float hx = torch randn n n device=dev dtype=torch float torch randn n n device=dev dtype=torch float is_lstm torch randn n n device=dev dtype=torch float weights = torch randn num_chunks n n device=dev dtype=torch float weight_ih torch randn num_chunks n n device=dev dtype=torch float weight_hh torch randn num_chunks n device=dev dtype=torch float bias_ih torch randn num_chunks n device=dev dtype=torch float bias_hh returns args tuple input + hx + weights Supplies ops arguments test_autocast_ test test_cuda py __init__ dev super __init__ n = Utility arguments created one-element tuples pointwise _fp = torch randn n dtype=torch float device=dev pointwise _fp = torch randn n dtype=torch float device=dev pointwise _fp = torch randn n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev dimsets = n n n n n n n n n n n n conv_args_fp = torch randn dimset dtype=torch float device=dev torch randn dimset dtype=torch float device=dev dimset dimsets bias_fp = torch randn n dtype=torch float device=dev element _fp = torch randn dtype=torch float device=dev pointwise _fp = torch randn n dtype=torch float device=dev pointwise _fp = torch randn n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev The lists below organize ops autocast needs test list_name corresponds test_autocast_list_name test test_cuda py Each op associated tuple valid arguments In addition cudnn conv ops supported ROCm hence will skipped passing TEST_WITH_ROCM flag those ops torch_fp list Some ops implement built-in type promotion These don t need autocasting autocasting relies their promotion so we include tests double-check torch_expect_builtin_promote = eq pointwise _fp + pointwise _fp torch bool ge pointwise _fp + pointwise _fp torch bool gt pointwise _fp + pointwise _fp torch bool le pointwise _fp + pointwise _fp torch bool lt pointwise _fp + pointwise _fp torch bool ne pointwise _fp + pointwise _fp torch bool add pointwise _fp + pointwise _fp torch float div pointwise _fp + pointwise _fp torch float mul pointwise _fp + pointwise _fp torch float cat pointwise _fp + pointwise _fp torch float equal pointwise _fp + pointwise _fp torch float stack pointwise _fp + pointwise _fp torch float methods_expect_builtin_promote = __eq__ pointwise _fp + pointwise _fp torch bool __ge__ pointwise _fp + pointwise _fp torch bool __gt__ pointwise _fp + pointwise _fp torch bool __le__ pointwise _fp + pointwise _fp torch bool __lt__ pointwise _fp + pointwise _fp torch bool __ne__ pointwise _fp + pointwise _fp torch bool __add__ pointwise _fp + pointwise _fp torch float __div__ pointwise _fp + pointwise _fp torch float __mul__ pointwise _fp + pointwise _fp torch float The remaining lists organize ops autocast treats explicitly torch_fp = deprecated _convolution _convolution conv_args_fp + bias_fp + False False True True current _convolution _convolution conv_args_fp + bias_fp + False False True True True conv d conv_args_fp conv d conv_args_fp conv d conv_args_fp conv_tbc conv_args_fp + bias_fp conv_transpose d conv_args_fp conv_transpose d conv_args_fp conv_transpose d conv_args_fp convolution conv_args_fp + bias_fp + False cudnn_convolution conv_args_fp + False True True TEST_WITH_ROCM cudnn_convolution_transpose conv_args_fp + False True True TEST_WITH_ROCM prelu pointwise _fp + element _fp addmm mat _fp + mat _fp + mat _fp addmv pointwise _fp + mat _fp + pointwise _fp addr mat _fp + pointwise _fp + pointwise _fp matmul mat _fp + mat _fp einsum bkhd bqhd- bqkh mat _fp + mat _fp mm mat _fp + mat _fp mv mat _fp + pointwise _fp chain_matmul mat _fp + mat _fp + mat _fp addbmm mat _fp + torch randn n n n device=dev dtype=torch float torch randn n n n device=dev dtype=torch float baddbmm torch randn n n n device=dev dtype=torch float torch randn n n n device=dev dtype=torch float torch randn n n n device=dev dtype=torch float bmm torch randn n n n device=dev dtype=torch float torch randn n n n device=dev dtype=torch float _thnn_fused_lstm_cell _thnn_fused_gru_cell Python-exposed far I can tell _thnn_fused_lstm_cell mat _fp + mat _fp + mat _fp + pointwise _fp + pointwise _fp _thnn_fused_gru_cell mat _fp + mat _fp + mat _fp + pointwise _fp + pointwise _fp lstm_cell _rnn_cell_args n num_chunks= is_lstm=True dev=dev dtype=torch float gru_cell _rnn_cell_args n num_chunks= is_lstm=False dev=dev dtype=torch float rnn_tanh_cell _rnn_cell_args n num_chunks= is_lstm=False dev=dev dtype=torch float rnn_relu_cell _rnn_cell_args n num_chunks= is_lstm=False dev=dev dtype=torch float torch_fp = acos pointwise _fp clamp - asin pointwise _fp clamp - cosh pointwise _fp erfinv pointwise _fp clamp - exp pointwise _fp expm pointwise _fp log pointwise _fp clamp log pointwise _fp clamp log pointwise _fp clamp log p pointwise _fp clamp - reciprocal pointwise _fp rsqrt pointwise _fp clamp sinh pointwise _fp tan pointwise _fp clamp - pow pointwise _fp + clamp + pointwise _fp pow pointwise _fp + clamp + pow + pointwise _fp This variant has backend documented API softmax pointwise _fp + log_softmax pointwise _fp + layer_norm pointwise _fp + pointwise _fp numel group_norm mat _fp + norm pointwise _fp norm pointwise _fp dim these need magma norm mat _fp p nuc norm mat _fp p nuc dim norm pointwise _fp p norm pointwise _fp p dim cosine_similarity mat _fp + mat _fp poisson_nll_loss mat _fp + mat _fp + True False e- torch nn _reduction get_enum mean cosine_embedding_loss torch tensor device=dev dtype=torch float torch tensor device=dev dtype=torch float torch tensor device=dev dtype=torch int hinge_embedding_loss mat _fp + torch ones n device=dev dtype=torch int kl_div mat _fp + torch rand n n device=dev dtype=torch float margin_ranking_loss mat _fp + mat _fp + torch ones n device=dev dtype=torch float triplet_margin_loss mat _fp + mat _fp + mat _fp binary_cross_entropy_with_logits mat _fp + torch rand n n device=dev dtype=torch float cumprod pointwise _fp + cumsum pointwise _fp + dist pointwise _fp + pointwise _fp pdist mat _fp cdist mat _fp + mat _fp prod pointwise _fp prod pointwise _fp + renorm mat _fp + sum pointwise _fp sum mat _fp + logsumexp mat _fp + torch_need_autocast_promote = addcdiv pointwise _fp + pointwise _fp + pointwise _fp clamp addcmul pointwise _fp + pointwise _fp + pointwise _fp atan pointwise _fp + pointwise _fp clamp bilinear torch randn dtype=torch float device=dev torch randn dtype=torch float device=dev torch randn dtype=torch float device=dev torch randn dtype=torch float device=dev cross torch randn dtype=torch float device=dev torch randn dtype=torch float device=dev dot pointwise _fp + pointwise _fp vdot pointwise _fp + pointwise _fp grid_sampler torch randn dtype=torch float device=dev torch randn dtype=torch float device=dev False index_put pointwise _fp + torch tensor device=dev dtype=torch long torch randn device=dev dtype=torch float index_put pointwise _fp + torch tensor device=dev dtype=torch long torch randn device=dev dtype=torch float tensordot torch randn dtype=torch float device=dev torch randn dtype=torch float device=dev scatter_add torch zeros dtype=torch float device=dev torch randint device=dev torch randn dtype=torch float device=dev scatter_add torch zeros dtype=torch float device=dev torch randint device=dev torch randn dtype=torch float device=dev nn_fp = linear mat _fp + mat _fp + mat _fp nn_fp = softplus pointwise _fp nll_loss torch rand n n device=dev dtype=torch float torch zeros n device=dev dtype=torch long nll_loss d torch rand n n n n device=dev dtype=torch half torch zeros n n n device=dev dtype=torch long l _loss mat _fp + mat _fp smooth_l _loss mat _fp + mat _fp mse_loss mat _fp + mat _fp multilabel_margin_loss mat _fp + torch ones n n device=dev dtype=torch long soft_margin_loss mat _fp + torch ones n n device=dev dtype=torch long multi_margin_loss mat _fp + torch ones n device=dev dtype=torch long linalg_fp = linalg_vecdot mat _fp + mat _fp linalg_multi_dot mat _fp + mat _fp + mat _fp methods_fp = __matmul__ mat _fp + mat _fp methods_fp = __pow__ torch rand n device=dev dtype=torch float banned = binary_cross_entropy torch rand n n device=dev dtype=torch float torch rand n n device=dev dtype=torch float torch _C _nn AutocastCPUTestLists Supplies ops arguments test_autocast_ test test_cpu py __init__ dev super __init__ n = Utility arguments created one-element tuples pointwise _bf = torch randn n dtype=torch bfloat device=dev pointwise _bf = torch randn n dtype=torch bfloat device=dev mat _bf = torch randn n n dtype=torch bfloat device=dev mat _bf = torch randn n n dtype=torch bfloat device=dev mat _bf = torch randn n n dtype=torch bfloat device=dev pointwise _fp = torch randn n dtype=torch float device=dev pointwise _fp = torch randn n dtype=torch float device=dev dummy_dimsets = n n n n n n n n n n n n n n n dummy_bf = torch randn dimset dtype=torch bfloat device=dev dimset dummy_dimsets dimsets = n n n n n n n n n n n n conv_args_fp = torch randn dimset dtype=torch float device=dev torch randn dimset dtype=torch float device=dev dimset dimsets element _fp = torch randn dtype=torch float device=dev pointwise _fp = torch randn n dtype=torch float device=dev pointwise _fp = torch randn n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev mat _fp = torch randn n n dtype=torch float device=dev dummy_fp = noqa F torch randn dimset dtype=torch float device=dev dimset dummy_dimsets The lists below organize ops autocast needs test list_name corresponds test_autocast_list_name test test_cpu py Each op associated tuple valid arguments Some ops implement built-in type promotion These don t need autocasting autocasting relies their promotion so we include tests double-check torch_expect_builtin_promote = eq pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool ge pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool gt pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool le pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool lt pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool ne pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool add pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch float div pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch float mul pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch float methods_expect_builtin_promote = __eq__ pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool __ge__ pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool __gt__ pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool __le__ pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool __lt__ pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool __ne__ pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch bool __add__ pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch float __div__ pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch float __mul__ pointwise _fp + pointwise _bf pointwise _fp + pointwise _fp torch float The remaining lists organize ops autocast treats explicitly torch_ = conv d conv_args_fp conv d conv_args_fp conv d conv_args_fp bmm torch randn n n n device=dev dtype=torch float torch randn n n n device=dev dtype=torch float mm mat _fp + mat _fp matmul mat _fp + mat _fp baddbmm torch randn n n n device=dev dtype=torch float torch randn n n n device=dev dtype=torch float torch randn n n n device=dev dtype=torch float addmm mat _fp + mat _fp + mat _fp _addmm_activation mat _fp + mat _fp + mat _fp beta alpha use_gelu True addbmm mat _fp + torch randn n n n device=dev dtype=torch float torch randn n n n device=dev dtype=torch float conv_tbc torch randn device=dev dtype=torch float torch randn device=dev dtype=torch float torch randn device=dev dtype=torch float conv_transpose d conv_args_fp conv_transpose d conv_args_fp conv_transpose d conv_args_fp prelu pointwise _fp + element _fp _native_multi_head_attention torch randn n n n device=dev dtype=torch float torch randn n n n device=dev dtype=torch float torch randn n n n device=dev dtype=torch float n torch randn n n device=dev dtype=torch float torch randn n device=dev dtype=torch float torch randn n n device=dev dtype=torch float torch randn n device=dev dtype=torch float torch_fp = poisson_nll_loss mat _bf + mat _bf + True False e- torch nn _reduction get_enum mean cosine_embedding_loss torch tensor device=dev dtype=torch bfloat torch tensor device=dev dtype=torch bfloat torch tensor device=dev dtype=torch int hinge_embedding_loss mat _bf + torch ones n device=dev dtype=torch int margin_ranking_loss mat _bf + mat _bf + torch ones n device=dev dtype=torch bfloat triplet_margin_loss mat _bf + mat _bf + mat _bf binary_cross_entropy_with_logits mat _bf + torch rand n n device=dev dtype=torch bfloat nn_ = linear mat _fp + mat _fp nn_fp = avg_pool d dummy_bf kernel_size stride binary_cross_entropy torch rand n n device=dev dtype=torch bfloat + torch rand n n device=dev dtype=torch bfloat reflection_pad d dummy_bf padding nll_loss torch rand n n device=dev dtype=torch bfloat torch zeros n device=dev dtype=torch long nll_loss d torch rand n n n n device=dev dtype=torch bfloat torch zeros n n n device=dev dtype=torch long l _loss mat _bf + mat _bf smooth_l _loss mat _bf + mat _bf mse_loss mat _bf + mat _bf multilabel_margin_loss mat _bf + torch ones n n device=dev dtype=torch long soft_margin_loss mat _bf + torch ones n n device=dev dtype=torch long multi_margin_loss mat _bf + torch ones n device=dev dtype=torch long huber_loss mat _bf + mat _bf torch_need_autocast_promote = cat pointwise _bf + pointwise _fp pointwise _fp + pointwise _fp stack pointwise _bf + pointwise _fp pointwise _fp + pointwise _fp TestAutocast TestCase args_maybe_kwargs op_with_args len op_with_args == op_with_args op_with_args op_with_args op_with_args op_with_args _run_autocast_outofplace op args run_as_type device out_type=None module=torch add_kwargs=None amp_dtype=torch bfloat helper cast args cast val to_type isinstance val torch Tensor val to_type val is_floating_point val isinstance val collections abc Iterable type val cast v to_type v val val add_kwargs None add_kwargs = assertFalse torch is_autocast_enabled device_type=device torch amp autocast device_type=device dtype=amp_dtype assertTrue torch is_autocast_enabled device_type=device out_type = out_type out_type None run_as_type output = output_method = None Try module variant requested module None hasattr module op output = getattr module op args add_kwargs isinstance output torch Tensor assertTrue out_type == output dtype f autocast torch op produced output dtype should produce out_type Try Tensor variant hasattr torch Tensor op output_method = getattr args op args add_kwargs isinstance output_method torch Tensor assertTrue out_type == output_method dtype f autocast torch op produced output_method dtype should produce torch out_type assertTrue output None output_method None f op found attribute either Tensor requested module module Accounts ops Tensors iterables other non-Tensors For example lstm_cell returns tuple equal returns bool compare first second isinstance first torch Tensor torch equal first second isinstance first collections abc Iterable all compare f s f s zip first second strict=False first == second If both torch Tensor variants found check outputs identical output None output_method None assertTrue type output type output_method comparison = compare output output_method assertTrue comparison f torch op result did match Tensor op result Compare numerics Python-side autocasting we expect does same thing C++-side autocasting should bitwise accurate output_to_compare = output output None output_method torch amp autocast device_type=device enabled=False assertFalse torch is_autocast_enabled device_type=device module None hasattr module op control = getattr module op cast args run_as_type add_kwargs control = getattr args run_as_type op cast args run_as_type add_kwargs assertTrue type output_to_compare type control comparison = compare output_to_compare control assertTrue comparison f torch op result did match control assertTrue torch is_autocast_enabled device_type=device assertFalse torch is_autocast_enabled device_type=device