Owner s oncall quantization ruff noqa F copy itertools operator random unittest typing NamedTuple TYPE_CHECKING numpy np torch torch jit torch nn functional F torch testing _internal hypothesis_utils hu hypothesis assume given HealthCheck note settings strategies st packaging version Version torch _VF TYPE_CHECKING torch _ops OpOverloadPacket torch nn modules utils _pair _single hu assert_deadline_disabled typing Optional torch backends xnnpack torch ao quantization PerChannelMinMaxObserver torch testing _internal common_cuda SM OrLater TEST_CUDA TEST_CUDNN TEST_CUDNN_VERSION torch testing _internal common_quantization skipIfNoFBGEMM skipIfNoONEDNN skipIfNoQNNPACK torch testing _internal common_quantized _calculate_dynamic_qparams _dequantize _quantize _snr override_qengines override_quantized_engine qengine_is_onednn qengine_is_qnnpack supported_qengines torch testing _internal common_utils IS_ARM IS_FBCODE IS_MACOS IS_PPC IS_SANDCASTLE raise_on_run_directly TestCase torch testing _internal optests opcheck torch utils cpp_extension ROCM_HOME np_dtype = torch quint np uint torch qint np int torch qint np int TEST_ROCM = TEST_CUDA torch version hip None ROCM_HOME None PointwisePostOp NamedTuple binary_attr str = none alpha float = unary_attr str = none scalars list = algorithm str = Make sure we won t have overflows vpmaddubsw instruction used FBGEMM On current Intel x architecture we need utilize vpmaddubsw instruction -bit int multiplication This instruction vertically multiplies each unsigned -bit integer corresponding signed -bit integer b producing intermediate signed -bit integers This function modifies weights eliminate overflow signed -bit integers avoid_vpmaddubsw_overflow_linear batch_size input_channels output_channels X X_min X_max W W_min W_max Version np __version__ = Version raise unittest SkipTest numpy overflow error i j np ndindex batch_size output_channels k range input_channels x = X i k - X_min x = X i k + - X_min w = W j k - - W_min w = W j k + - - W_min x w + x w - w _adjusted = - - float x w x W j k + = int w _adjusted + + W_min x w + x w - w _adjusted = - - float x w x W j k + = int w _adjusted + + W_min Go through same loop again double check we don t have any overflow i j np ndindex batch_size output_channels k range input_channels x = X i k - X_min x = X i k + - X_min w = W j k - - W_min w = W j k + - - W_min assert - = x w + x w Reference quantized Linear operator qlinear_ref X_q X_scale X_zp W_q W_scale W_zp b_q Y_scale Y_zp dtype=np uint X_q = np reshape X_q - X_q shape X_q ndim - row_offsets_ref = X_q sum axis= astype np int reshape - col_offsets_ref = W_q sum axis= astype np int reshape - assert X_q ndim == batch_size input_channels = X_q shape Prod_XqWq_ref = np matmul X_q astype np int W_q astype np int T - W_zp row_offsets_ref - X_zp col_offsets_ref + input_channels X_zp W_zp b_q None Prod_XqWq_ref += b_q Y_q_ref = _quantize Prod_XqWq_ref Y_scale X_scale W_scale Y_zp dtype=dtype Y_q_ref Computes output shape given pooling parameters pool_output_shape input_size kernel_size padding stride dilation ceiling_mode=False stride None stride = kernel_size output_size = input_size + padding - dilation kernel_size - - + stride - ceiling_mode stride + ceiling_mode output_size - stride = input_size + padding output_size -= output_size Util creating random tensor quantization params when Hypothesis undesirable _get_random_tensor_and_q_params shapes rand_scale torch_type X = torch rand shapes dtype=torch float - rand_scale Calculate reasonable quantization params min_val = torch min X max_val = torch max X torch_type == torch qint X_zero_point = int torch randint - - num_bins = X_scale = float max_val - min_val num_bins torch_type == torch qint X_zero_point = int torch randint - num_bins = X_scale = float max_val - min_val num_bins torch quint X_zero_point = num_bins = X_scale = float max_val - min_val num_bins X_scale == X_scale = e- X X_scale X_zero_point _quantize_fp e m t torch Tensor channelwise bool scale Optional torch Tensor = None quant_max = torch finfo torch float _e m fn max eps = torch Tensor torch finfo torch float eps channelwise scale = scale t reshape t shape - abs max - quant_max scale = torch max scale eps scale_reshape = scale reshape - + t dim - qt = t scale_reshape scale = scale t abs max reshape quant_max scale = torch max scale eps isinstance scale torch Tensor max scale eps item qt = t scale Clamp avoid NaN Convert two steps align fp - fp - fp qt = qt clamp - half torch float _e m fn qt scale _dequantize_fp e m qt torch Tensor scale torch Tensor dqt = qt float scale numel == per tensor dqt = dqt scale per channel scale_reshape = scale reshape - + qt dim - dqt = dqt scale_reshape dqt TestQuantizedOps TestCase Helper function test quantized activation functions _test_activation_function X fn_name test_configs r When writing unit test activation function instead specifying test routines only applicable activation function itself you utilize _test_activation_function provides general testing To utilize helper function test config must provided A test config list contains metadata about quantized activation functions will tested how tests need set up allows simpler more concise unit tests written specifying configurations needed calling provided helper function _test_activation_function Inside list each config dictionary represents suite tests assert correctness various quantization functions You can check out test_qrelu test_qrelu test_qsigmoid test_qhardsigmoid how their test configs specified Here s list fields can included test config quantized_fn list quantized functions tested reference_fn original reference function called dequantized X extra_kwargs additional keyword arguments each test entry ops_under_test must have least fields quantized_fn reference_fn output_range output range operator will map By default no specified range will controlled depend Xmin Xmax change_zero_point boolean flag indicating zero point parameter should determined based torch_type during quantization see sigmoid hardsigmoid examples By default specified change_zero_point assumed False zero point will just take default value X ` output_is_observed ` specified True we ll append extra output_scale output_zero_point keyword argument when calling quantized op Retrieves default parameters X X scale zero_point torch_type = X isinstance X torch Tensor X = torch from_numpy X X device type == cuda torch backends quantized engine == qnnpack Quantizes reference account max error q_min q_max only depend initial torch_type q_min q_max = torch iinfo torch_type min torch iinfo torch_type max op_group test_configs ref_op = op_group reference_fn q_op op_group quantized_fn memory_format torch channels_last torch contiguous_format memory_format == torch channels_last len X shape = continue X = X memory_format=memory_format Retrieves inplace keyword arguments some functions require inplace=True test in-place copy copy needed because these modified place extra_kwargs = \ copy copy op_group get extra_kwargs output_is_observed = \ copy copy op_group get output_is_observed False Quantizes dequantizes account max error qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type dqX = qX dequantize dqY_hat = ref_op dqX clone extra_kwargs Adjusts output_scale needed The output_scale determines quantization scale functions have constrained output range e x sigmoid ranges output_scale = scale output_range op_group f_min f_max = op_group output_range output_scale = f_max - f_min q_max - q_min + Adjusts output_zero_point needed see explanation change_zero_point parameter above output_zero_point determines additional offset will added scaled value during quantization op_group get change_zero_point False output_zero_point = torch_type == torch qint q_min output_zero_point = zero_point Quantizes dequantized version Y_hat qY_hat = torch quantize_per_tensor dqY_hat scale=output_scale zero_point=output_zero_point dtype=torch_type output_is_observed extra_kwargs update output_scale output_scale output_zero_point output_zero_point Finds qY using in-place non-in-place quantized operators qY = q_op qX extra_kwargs assertEqual qY qY_hat msg=f fn_name - q_op failed qY vs qY_hat Tests correctness quantized relu op override_qengines test_qrelu relu_test_configs = quantized_fn torch relu torch relu_ torch nn functional relu torch nn functional relu reference_fn torch nn functional relu quantized_fn torch nn functional relu torch nn functional relu reference_fn torch nn functional relu extra_kwargs inplace True devices = cpu cuda TEST_CUDA cpu device devices shapes = dtypes = torch quint torch qint scales = zero_points = test_cases = itertools product shapes dtypes scales zero_points shape dtype scale zero_point test_cases X = torch randn shape device=device X = X scale zero_point dtype _test_activation_function X relu relu_test_configs Tests correctness quantized relu op test_qrelu relu _test_configs = quantized_fn torch ops quantized relu torch ao nn quantized ReLU inplace=False torch ao nn quantized ReLU inplace=True reference_fn torch nn functional relu shapes = dtypes = torch quint torch qint scales = zero_points = test_cases = itertools product shapes dtypes scales zero_points shape dtype scale zero_point test_cases X = torch randn shape X = X scale zero_point dtype _test_activation_function X relu relu _test_configs Tests correctness quantized sigmoid op override_qengines given X=hu tensor shapes=hu array_shapes qparams=hu qparams test_sigmoid_non_observed X sigmoid_test_configs = quantized_fn torch sigmoid reference_fn torch sigmoid output_range change_zero_point True _test_activation_function X sigmoid sigmoid_test_configs Tests correctness quantized sigmoid op TODO enable after observed output supported qnnpack override_qengines skipIfNoFBGEMM given X=hu tensor shapes=hu array_shapes qparams=hu qparams test_sigmoid X sigmoid_test_configs = quantized_fn torch ops quantized sigmoid reference_fn torch sigmoid output_range change_zero_point True output_is_observed True _test_activation_function X sigmoid sigmoid_test_configs skipIfNoFBGEMM test_sigmoid_dequantize_rounding_error issue sigmoid_test_configs = quantized_fn torch ops quantized sigmoid reference_fn torch sigmoid output_range change_zero_point True output_is_observed True X = np full dtype=np float torch quint _test_activation_function X sigmoid sigmoid_test_configs Tests correctness quantized hardsigmoid op override_qengines test_qhardsigmoid hardsigmoid_test_configs = quantized_fn torch ao nn quantized functional hardsigmoid reference_fn torch nn functional hardsigmoid output_range change_zero_point True quantized_fn torch ao nn quantized functional hardsigmoid reference_fn torch nn functional hardsigmoid output_range change_zero_point True extra_kwargs inplace True shapes = dtypes = torch quint torch qint test_cases = itertools product shapes dtypes shape dtype test_cases X = np random rand shape astype np float dtype _test_activation_function X hardsigmoid hardsigmoid_test_configs override_qengines given X=hu tensor shapes=hu array_shapes qparams=hu qparams test_leaky_relu_observed_output X leaky_relu_test_configs = quantized_fn torch ops quantized leaky_relu reference_fn torch nn functional leaky_relu extra_kwargs negative_slope inplace False output_is_observed True _test_activation_function X leaky_relu leaky_relu_test_configs Tests correctness quantized relu op test_leaky_relu shapes = dtypes = torch quint torch qint memory_formats = torch channels_last torch contiguous_format test_cases = itertools product shapes dtypes memory_formats shape dtype memory_format test_cases memory_format == torch channels_last len shape = continue X scale zero_point torch_type alpha = \ torch randn shape dtype X = X memory_format=memory_format qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type dqX = qX dequantize torch nn functional op = torch nn functional leaky_relu dqY = op dqX negative_slope=alpha qY = torch quantize_per_tensor dqY scale=scale zero_point=zero_point dtype=torch_type qY_hat = op qX negative_slope=alpha assertEqual qY dequantize qY_hat dequantize msg=f F leaky_relu failed qY vs qY_hat Tests correctness quantized elu op given X=hu tensor shapes=hu array_shapes elements=hu floats - e e allow_nan=False allow_infinity=False qparams=hu qparams alpha=st floats allow_nan=False allow_infinity=False test_qelu X alpha X scale zero_point torch_type = X output_scale = output_zero_point = X = torch from_numpy X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type calculate ELU dqX quantize dqX = qX dequantize dqY_hat = dqX clone dqY_hat = torch nn functional elu dqX alpha qY_hat = torch quantize_per_tensor dqY_hat scale=output_scale zero_point=output_zero_point dtype=torch_type qY = torch ao nn quantized functional elu qX output_scale output_zero_point alpha=alpha assertEqual qY qY_hat msg=f F elu failed qY vs qY_hat Tests correctness quantized celu op given X=hu tensor shapes=hu array_shapes elements=hu floats - e e allow_nan=False allow_infinity=False qparams=hu qparams scale_max= e- alpha=st floats allow_nan=False allow_infinity=False test_qcelu X alpha X scale zero_point torch_type = X output_scale = output_zero_point = X = torch from_numpy X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type calculate CELU dqX quantize dqX = qX dequantize dqY_hat = torch nn functional celu dqX alpha qY_hat = torch quantize_per_tensor dqY_hat scale=output_scale zero_point=output_zero_point dtype=torch_type test regular qY = torch ops quantized celu qX output_scale output_zero_point alpha=alpha assertEqual qY qY_hat msg=f F celu failed qY vs qY_hat Tests correctness quantized gelu op test_qgelu shapes = dtypes = torch quint torch qint memory_formats = torch channels_last torch contiguous_format approximation = none tanh test_cases = itertools product shapes dtypes memory_formats approximation devices = cpu cuda TEST_CUDA cpu shape dtype memory_format approximate test_cases memory_format == torch channels_last len shape = continue X scale zero_point torch_type = \ torch randn shape dtype X = X memory_format=memory_format device devices X = X device=device qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type dqX = qX dequantize op = torch nn functional gelu dqY = op dqX approximate=approximate qY = torch quantize_per_tensor dqY scale=scale zero_point=zero_point dtype=torch_type qY_hat = op qX assertEqual qY dequantize qY_hat dequantize msg=f F gelu failed qY vs qY_hat Tests correctness quantized prelu op test_qprelu shapes = num_params = num_parameter = num_channels dtypes = torch quint torch qint memory_formats = torch channels_last torch contiguous_format test_cases = itertools product shapes num_params dtypes memory_formats shape num_param dtype memory_format test_cases memory_format == torch channels_last len shape = continue X scale zero_point torch_type = \ torch randn shape dtype X = X memory_format=memory_format num_parameter = num_param == len shape == shape W = torch randn num_parameter W w_scale w_zero_point = \ torch randn num_parameter qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type dqX = qX dequantize qW = torch quantize_per_tensor W scale=w_scale zero_point=w_zero_point dtype=torch_type dqW = qW dequantize op = torch nn functional prelu qop = torch ops quantized prelu dqY = op dqX dqW qY = torch quantize_per_tensor dqY scale=scale zero_point=zero_point dtype=torch_type qY_hat = qop qX qW scale zero_point assertEqual qY dequantize qY_hat dequantize msg=f F prelu failed qY vs qY_hat Tests correctness quantized qlayer_norm op skipIfNoFBGEMM test_qlayer_norm hypothesis flaky test create test cases manually side_lens = torch_types = torch qint torch quint y_scales = y_zero_points = channels_last_list = True False affine_list = True False combined = side_lens torch_types y_scales y_zero_points channels_last_list affine_list test_cases = itertools product combined override_quantized_engine fbgemm test_case test_cases side_len torch_type Y_scale Y_zero_point channels_last \ affine = test_case shapes = side_len In FP kernel mean variance calculated floating point In quantized kernel they calculated integer arithmetic Because numerics do always match exactly which expected acceptable We do two things allow failure test do use Hypothesis generate input tensor Hypothesis favors homogeneous inputs its search strategies which isn t representative inputs we care about tends maximize particular numerics difference allow small off Y_scale errors Even when variance input high there can off one errors result input value happens fall exactly bin boundary output scale If we want numerics match we could switch calculating mean+var floating point future cost speed X X_scale X_zero_point = \ _get_random_tensor_and_q_params shapes torch_type qX = torch quantize_per_tensor X scale=X_scale zero_point=X_zero_point dtype=torch_type channels_last qX = qX contiguous memory_format=torch channels_last dqX = qX dequantize Enforce non-homogeneous inputs enough_unique_vals_in_each_layer = sum dqX i shape float torch unique dqX i shape dqX i shape i range dqX shape == dqX shape assume enough_unique_vals_in_each_layer Initialize weights non-randomly reproducibility avoid flaky tests affine weight = torch ones qX size dtype=torch float bias = torch ones qX size dtype=torch float weight = None bias = None epsilon = e- qY = torch ops quantized layer_norm qX qX size weight=weight bias=bias eps=epsilon output_scale=Y_scale output_zero_point=Y_zero_point Y_hat = F layer_norm dqX dqX size weight=weight bias=bias eps=epsilon qY_hat = torch quantize_per_tensor Y_hat scale=Y_scale zero_point=Y_zero_point dtype=torch_type Due numerics difference mentioned above between calculating variance float vs int results can still slightly different dqY = qY dequantize dqY_hat = qY_hat dequantize diff = dqY - dqY_hat off-by-one errors magnitude Y_scale num_diff = torch sum diff Y_scale pct_diff = float num_diff diff numel + e- num_diff_off_by_one = torch sum diff diff = Y_scale pct_diff_off_by_one = float num_diff_off_by_one diff numel + e- assertTrue pct_diff e- assertTrue pct_diff_off_by_one Tests correctness quantized qnnpack_tanh op given X=hu tensor shapes=hu array_shapes qparams=hu qparams unittest skip broken without changes any relevant code we need remove hypothesis testing CI test_qtanh X Note QNNPACK tested separately TestQNNPackOps X scale zero_point torch_type = X X = torch from_numpy X Y = torch tanh X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type Quantize reference account max error Note output scale has + because we use scale ^BITS implementations f_min f_max = - q_min q_max = torch iinfo torch_type min torch iinfo torch_type max output_scale = f_max - f_min q_max - q_min + output_zero_point = int round q_max + q_min qY = torch quantize_per_tensor Y scale=output_scale zero_point=output_zero_point dtype=torch_type qY_hat = torch tanh qX assertEqual qY qY_hat msg=f TanH failed qY vs qY_hat Tests correctness quantized threshold op given X=hu tensor shapes=hu array_shapes elements=hu floats - e e allow_nan=False allow_infinity=False qparams=hu qparams threshold=hu floats - e e allow_nan=False allow_infinity=False value=hu floats - e e allow_nan=False allow_infinity=False test_qthreshold X threshold value X scale zero_point torch_type = X X = torch from_numpy X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type calculate threshold dqX quantize dqX = qX dequantize dqY_hat = dqX clone dqY_hat = torch nn functional threshold dqY_hat threshold value qY_hat = torch quantize_per_tensor dqY_hat scale=scale zero_point=zero_point dtype=torch_type ops_under_test = native torch threshold nn functional torch nn functional threshold ao nn quantized functional torch ao nn quantized functional threshold name op ops_under_test items qY = op qX threshold value assertEqual qY qY_hat msg=f name qthreshold failed Tests correctness quantized clamp op given X=hu tensor shapes=hu array_shapes max_numel= elements=hu floats - e e allow_nan=False qparams=hu qparams min_val=hu floats - e e allow_nan=False max_val=hu floats - e e allow_nan=False test_qclamp X min_val max_val X scale zero_point torch_type = X assume min_val = max_val Y_clamp = torch clamp torch from_numpy X min=min_val max=max_val qY_clamp = torch quantize_per_tensor Y_clamp scale=scale zero_point=zero_point dtype=torch_type X = torch from_numpy X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type ops_under_test = ops quantized torch ops quantized clamp name op ops_under_test items qY_clamp_hat = op qX min=min_val max=max_val assertEqual qY_clamp qY_clamp_hat msg=f name qclamp failed torch backends quantized engine == fbgemm override_quantized_engine fbgemm Y_min_clamp = torch clamp X min=min_val Y_max_clamp = torch clamp X max=max_val qY_min_clamp = torch quantize_per_tensor Y_min_clamp scale=scale zero_point=zero_point dtype=torch_type qY_max_clamp = torch quantize_per_tensor Y_max_clamp scale=scale zero_point=zero_point dtype=torch_type name op ops_under_test items qY_min_clamp_hat = op qX min=min_val assertEqual qY_min_clamp qY_min_clamp_hat msg=f name qclamp failed qY_max_clamp_hat = op qX max=max_val assertEqual qY_max_clamp qY_max_clamp_hat msg=f name qclamp failed Tests correctness quantized hardtanh op skipIfNoFBGEMM given X=hu tensor shapes=hu array_shapes max_numel= elements=hu floats - e e allow_nan=False allow_infinity=False qparams=hu qparams min_val=hu floats - e e allow_nan=False allow_infinity=False max_val=hu floats - e e allow_nan=False allow_infinity=False test_hardtanh X min_val max_val override_quantized_engine fbgemm X scale zero_point torch_type = X assume min_val = max_val Y = X copy Y Y min_val = min_val Y Y max_val = max_val qY = torch quantize_per_tensor torch from_numpy Y scale=scale zero_point=zero_point dtype=torch_type X = torch from_numpy X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type ops_under_test = ao nn quantized functional hardtanh torch ao nn quantized functional hardtanh name op ops_under_test items qY_hat = op qX min_val max_val assertEqual qY qY_hat msg=f name hardtanh failed ops_under_test_inplace = inplace ao nn quantized functional hardtanh torch ao nn quantized functional hardtanh name op_ ops_under_test_inplace items qY_hat = qX clone op_ qY_hat min_val max_val inplace=True assertEqual qY qY_hat msg=f name hardtanh failed Tests correctness quantized hardswish op override_qengines test_hardswish max_sides = side_lens = torch_types = torch quint torch qint y_scales = y_zero_points = combined = max_sides side_lens torch_types y_scales y_zero_points test_cases = itertools product combined test_case test_cases max_side side_len torch_type Y_scale Y_zero_point = test_case torch backends quantized engine == qnnpack torch_type = torch quint continue shapes = side_len max_side X X_scale X_zero_point = \ _get_random_tensor_and_q_params shapes torch_type memory_format torch channels_last torch contiguous_format memory_format == torch channels_last len shapes == X = X memory_format=memory_format qX = torch quantize_per_tensor X scale=X_scale zero_point=X_zero_point dtype=torch_type dqX = qX dequantize dqY_hat = F hardswish dqX qY_hat = torch quantize_per_tensor dqY_hat scale=Y_scale zero_point=Y_zero_point dtype=torch_type qY = torch ao nn quantized functional hardswish qX scale=Y_scale zero_point=Y_zero_point assertEqual qY qY_hat msg=f Hardswish failed qY vs qY_hat torch backends quantized engine Tests correctness binary op + scalar _test_binary_op_scalar_relu A b binary_op_name binary_op quantized_op quantized_op_relu copy op_scalar = quantized_op op_scalar_relu = quantized_op_relu A scale zero_point dtype = A A = A astype np float qA = torch quantize_per_tensor torch from_numpy A scale zero_point dtype binary_op_name == add C = binary_op qA dequantize round b scale scale C = binary_op qA dequantize b C_relu = copy deepcopy C C_relu C_relu = C_hat = op_scalar qA b C_ref = torch quantize_per_tensor C C_hat q_scale C_hat q_zero_point dtype C_relu_hat = op_scalar_relu qA b C_relu_ref = torch quantize_per_tensor C_relu C_relu_hat q_scale C_relu_hat q_zero_point dtype assertEqual C_ref dequantize C_hat dequantize msg=f binary_op_name _scalar results don t match f C_ref dequantize vs C_hat dequantize assertEqual C_relu_ref dequantize C_relu_hat dequantize msg=f binary_op_name _scalar_relu results don t match f C_relu_ref dequantize vs C_relu_hat dequantize unittest skipIf IS_MACOS skipping macos test given A=hu tensor shapes=hu array_shapes elements=hu floats - e e allow_nan=False qparams=hu qparams b=hu floats - e e allow_nan=False allow_infinity=False test_add_scalar_relu A b _test_binary_op_scalar_relu A b add operator add torch ops quantized add torch ops quantized add_relu unittest skipIf IS_MACOS skipping macos test given A=hu tensor shapes=hu array_shapes elements=hu floats - e e allow_nan=False qparams=hu qparams b=hu floats - e e allow_nan=False allow_infinity=False test_mul_scalar_relu A b _test_binary_op_scalar_relu A b mul operator mul torch ops quantized mul torch ops quantized mul_relu Tests correctness add add_relu op test_qadd_relu_same_qparams dtype torch quint torch qint torch qint add_relu = torch ops quantized add_relu add = torch ops quantized add add_out = torch ops quantized add add_relu_out = torch ops quantized add_relu NB This strange size so we exercise both vectorized implementation -element chunks time well scalar implementation A = torch arange - dtype=torch float B = torch arange - dtype=torch float scale = zero_point = qA = torch quantize_per_tensor A scale=scale zero_point=zero_point dtype=dtype qB = torch quantize_per_tensor B scale=scale zero_point=zero_point dtype=dtype Add ReLU ground truth C = qA dequantize + qB dequantize numpy qC = _quantize C scale zero_point dtype=np_dtype dtype qC_hat = add qA qB scale=scale zero_point=zero_point np testing assert_equal qC qC_hat int_repr Quantized addition failed qC_out_hat = torch _empty_affine_quantized qC shape scale=scale zero_point=zero_point dtype=dtype add_out qA qB out=qC_out_hat assertEqual qC_hat qC_out_hat msg= Add out failed Add + ReLU ground truth Crelu = C copy Crelu C = qCrelu = _quantize Crelu scale zero_point dtype=np_dtype dtype qCrelu_hat = add_relu qA qB scale=scale zero_point=zero_point np testing assert_equal qCrelu qCrelu_hat int_repr Quantized addition ReLU failed qCrelu_out_hat = torch _empty_affine_quantized qCrelu shape scale=scale zero_point=zero_point dtype=dtype add_relu_out qA qB out=qCrelu_out_hat assertEqual qCrelu_hat qCrelu_out_hat msg= AddReLU out failed Tests correctness cudnn add add_relu op Similar test_qadd_relu_different_qparams will probably merge future unittest skipIf TEST_CUDNN cudnn enabled unittest skipIf SM OrLater requires sm later unittest skipIf TEST_ROCM supported rocm unittest skip currently working feature isn t used test_qadd_relu_cudnn dtype = torch qint add_relu = torch ops quantized add_relu add = torch ops quantized add A = torch arange - dtype=torch float torch device cuda B = torch arange - dtype=torch float torch device cuda scale_A = scale_B = scale_C = zero_point = qA = torch quantize_per_tensor A scale=scale_A zero_point=zero_point dtype=dtype qB = torch quantize_per_tensor B scale=scale_B zero_point=zero_point dtype=dtype Add ground truth C = qA dequantize + qB dequantize device= cpu numpy qC = _quantize C scale_C zero_point dtype=np_dtype dtype qC_hat = add qA qB scale=scale_C zero_point=zero_point device= cpu np testing assert_equal qC qC_hat int_repr Quantized addition failed Add + ReLU ground truth Crelu = C copy Crelu C = qCrelu = _quantize Crelu scale_C zero_point dtype=np_dtype dtype qCrelu_hat = add_relu qA qB scale=scale_C zero_point=zero_point device= cpu np testing assert_equal qCrelu qCrelu_hat int_repr Quantized addition ReLU failed Tests correctness cudnn add add_relu op nhwc format unittest skipIf TEST_CUDNN cudnn enabled unittest skipIf SM OrLater requires sm later unittest skipIf TEST_ROCM supported rocm unittest skip currently working feature isn t used test_qadd_relu_cudnn_nhwc dtype = torch qint add_relu = torch ops quantized add_relu add = torch ops quantized add A = torch rand device= cuda B = torch rand device= cuda scale_A = scale_B = scale_C = zero_point = qA = torch quantize_per_tensor A scale=scale_A zero_point=zero_point dtype=dtype qB = torch quantize_per_tensor B scale=scale_B zero_point=zero_point dtype=dtype Add ground truth C = qA dequantize + qB dequantize device= cpu numpy qC = _quantize C scale_C zero_point dtype=np_dtype dtype qC_hat = add qA qB scale=scale_C zero_point=zero_point device= cpu np testing assert_equal qC qC_hat int_repr Quantized addition failed Add + ReLU ground truth Crelu = C copy Crelu C = qCrelu = _quantize Crelu scale_C zero_point dtype=np_dtype dtype qCrelu_hat = add_relu qA qB scale=scale_C zero_point=zero_point device= cpu np testing assert_equal qCrelu qCrelu_hat int_repr Quantized addition ReLU failed Tests correctness add add_relu op test_qadd_relu_different_qparams dtype torch quint torch qint torch qint add_relu = torch ops quantized add_relu add = torch ops quantized add add_out = torch ops quantized add add_relu_out = torch ops quantized add_relu NB This strange size so we exercise both vectorized implementation -element chunks time well scalar implementation A = torch arange - dtype=torch float B = torch arange - dtype=torch float scale_A = zero_point_A = scale_B = zero_point_B = scale_C = zero_point_C = qA = torch quantize_per_tensor A scale=scale_A zero_point=zero_point_A dtype=dtype qB = torch quantize_per_tensor B scale=scale_B zero_point=zero_point_B dtype=dtype Add ground truth C = qA dequantize + qB dequantize numpy qC = _quantize C scale_C zero_point_C dtype=np_dtype dtype qC_hat = add qA qB scale=scale_C zero_point=zero_point_C np testing assert_equal qC qC_hat int_repr Quantized addition failed qC_out_hat = torch _empty_affine_quantized qC shape scale=scale_C zero_point=zero_point_C dtype=dtype add_out qA qB out=qC_out_hat assertEqual qC_hat qC_out_hat msg= Add out failed Add + ReLU ground truth Crelu = C copy Crelu C = qCrelu = _quantize Crelu scale_C zero_point_C dtype=np_dtype dtype qCrelu_hat = add_relu qA qB scale=scale_C zero_point=zero_point_C np testing assert_equal qCrelu qCrelu_hat int_repr Quantized addition ReLU failed qCrelu_out_hat = torch _empty_affine_quantized qCrelu shape scale=scale_C zero_point=zero_point_C dtype=dtype add_relu_out qA qB out=qCrelu_out_hat assertEqual qCrelu_hat qCrelu_out_hat msg= AddReLU out failed Tests correctness mul mul_relu op test_qmul_relu_same_qparams dtype torch quint torch qint torch qint mul_relu = torch ops quantized mul_relu mul = torch ops quantized mul mul_out = torch ops quantized mul mul_relu_out = torch ops quantized mul_relu A = torch arange - dtype=torch float B = torch arange - dtype=torch float scale = zero_point = qA = torch quantize_per_tensor A scale=scale zero_point=zero_point dtype=dtype qB = torch quantize_per_tensor B scale=scale zero_point=zero_point dtype=dtype mul ReLU ground truth C = qA dequantize qB dequantize numpy qC = _quantize C scale zero_point dtype=np_dtype dtype qC_hat = mul qA qB scale=scale zero_point=zero_point np testing assert_equal qC qC_hat int_repr Quantized mulition failed qC_out_hat = torch _empty_affine_quantized qC shape scale=scale zero_point=zero_point dtype=dtype mul_out qA qB out=qC_out_hat assertEqual qC_hat qC_out_hat msg= mul out failed mul + ReLU ground truth Crelu = C copy Crelu C = qCrelu = _quantize Crelu scale zero_point dtype=np_dtype dtype qCrelu_hat = mul_relu qA qB scale=scale zero_point=zero_point np testing assert_equal qCrelu qCrelu_hat int_repr Quantized mulition ReLU failed qCrelu_out_hat = torch _empty_affine_quantized qCrelu shape scale=scale zero_point=zero_point dtype=dtype mul_relu_out qA qB out=qCrelu_out_hat assertEqual qCrelu_hat qCrelu_out_hat msg= mulReLU out failed Scalar multiplication b B C_ref = qA dequantize numpy b item qC_hat = torch ops quantized mul qA b item assertEqual C_ref qC_hat dequantize Scalar multiplication + relu b B C_ref = qA dequantize numpy b item C_ref C_ref = qC_hat = torch ops quantized mul_relu qA b item assertEqual C_ref qC_hat dequantize Tests correctness mul mul_relu op test_qmul_relu_different_qparams dtype torch quint torch qint torch qint mul_relu = torch ops quantized mul_relu mul = torch ops quantized mul mul_out = torch ops quantized mul mul_relu_out = torch ops quantized mul_relu A = torch arange - dtype=torch float B = torch arange - dtype=torch float scale_A = zero_point_A = scale_B = zero_point_B = scale_C = zero_point_C = qA = torch quantize_per_tensor A scale=scale_A zero_point=zero_point_A dtype=dtype qB = torch quantize_per_tensor B scale=scale_B zero_point=zero_point_B dtype=dtype mul ground truth C = qA dequantize qB dequantize numpy qC = _quantize C scale_C zero_point_C dtype=np_dtype dtype qC_hat = mul qA qB scale=scale_C zero_point=zero_point_C np testing assert_equal qC qC_hat int_repr Quantized multiplication failed qC_out_hat = torch _empty_affine_quantized qC shape scale=scale_C zero_point=zero_point_C dtype=dtype mul_out qA qB out=qC_out_hat assertEqual qC_hat qC_out_hat msg= mul out failed mul + ReLU ground truth Crelu = C copy Crelu C = qCrelu = _quantize Crelu scale_C zero_point_C dtype=np_dtype dtype qCrelu_hat = mul_relu qA qB scale=scale_C zero_point=zero_point_C np testing assert_equal qCrelu qCrelu_hat int_repr Quantized multiplication ReLU failed qCrelu_out_hat = torch _empty_affine_quantized qCrelu shape scale=scale_C zero_point=zero_point_C dtype=dtype mul_relu_out qA qB out=qCrelu_out_hat assertEqual qCrelu_hat qCrelu_out_hat msg= mulReLU out failed Tests correctness matmul op given num_dims=st integers outer_dims=st lists st integers min_size= max_size= m=st integers k=st integers n=st integers dtypes=st sampled_from torch qint np int torch quint np uint test_qmatmul num_dims outer_dims m k n dtypes torch_dtype np_dtype = dtypes size_a = outer_dims num_dims - + m k size_b = outer_dims num_dims - + k n A = torch randn size=size_a dtype=torch float B = torch randn size=size_b dtype=torch float scale_A = zero_point_A = scale_B = zero_point_B = scale_C = zero_point_C = qA = torch quantize_per_tensor A scale=scale_A zero_point=zero_point_A dtype=torch_dtype qB = torch quantize_per_tensor B scale=scale_B zero_point=zero_point_B dtype=torch_dtype matmul ground truth C = torch matmul qA dequantize qB dequantize numpy qC = _quantize C scale_C zero_point_C dtype= np_dtype qC_hat = torch ops quantized matmul qA qB scale=scale_C zero_point=zero_point_C np testing assert_equal qC qC_hat int_repr Quantized multiplication failed Using per channel quantization fails axis = scales_A = torch rand size= A shape axis zero_points_A = torch randint low= high= size= A shape axis scales_B = torch rand size= B shape axis zero_points_B = torch randint low= high= size= B shape axis qA = torch quantize_per_channel A scales=scales_A zero_points=zero_points_A axis=axis dtype=torch qint qB = torch quantize_per_channel B scales=scales_B zero_points=zero_points_B axis=axis dtype=torch qint np testing assert_raises_regex RuntimeError per-tensor torch ops quantized matmul qA qB scale_C zero_point_C Tests correctness quantized softmax op given dims=st lists st integers min_size= max_size= test_qsoftmax dims num_dims dim memory_format torch contiguous_format d softmax over last dim torch contiguous_format dims softmax along last dim torch contiguous_format dims softmax along last dim requires permute torch channels_last dims softmax along last dim contiguous torch channels_last Channels Last doesn t require permute torch channels_last_ d Channels Last D doesn t require permute size = dims num_dims torch_dtype = torch quint np_dtype = np uint scale_X = zero_point_X = X = torch rand size=size dtype=torch float + zero_point_X X = X memory_format=memory_format scale_Y = zero_point_Y = qX = torch quantize_per_tensor X scale=scale_X zero_point=zero_point_X dtype=torch_dtype softmax ground truth Y = torch softmax qX dequantize dim=dim numpy qY = _quantize Y scale_Y zero_point_Y dtype=np_dtype qY_hat = torch ops quantized softmax qX dim=dim output_scale=scale_Y output_zero_point=zero_point_Y np testing assert_equal qY qY_hat int_repr Quantized softmax failed Tests correctness quantized softmax op using qnnpack skipIfNoQNNPACK test_qsoftmax_qnnpack override_quantized_engine qnnpack test_qsoftmax Tests correctness mul mul_relu op test_qmul_broadcast mul_relu = torch ops quantized mul_relu mul = torch ops quantized mul mul_out = torch ops quantized mul mul_relu_out = torch ops quantized mul_relu A = torch arange - dtype=torch float B = torch arange - dtype=torch float A = torch randn B = torch randn scale_A = zero_point_A = scale_B = zero_point_B = scale_C = zero_point_C = qA = torch quantize_per_tensor A scale=scale_A zero_point=zero_point_A dtype=torch quint qB = torch quantize_per_tensor B scale=scale_B zero_point=zero_point_B dtype=torch quint mul ground truth C = qA dequantize qB dequantize numpy qC = _quantize C scale_C zero_point_C qC_hat = mul qA qB scale=scale_C zero_point=zero_point_C np testing assert_equal qC qC_hat int_repr Quantized multiplication failed Tests quantized add works broadcasting test_qadd_broadcast A = torch randn B = torch randn qA = torch quantize_per_tensor A torch quint qB = torch quantize_per_tensor B torch quint output_scale = output_zp = ground truth C = qA dequantize + qB dequantize qC = torch quantize_per_tensor C output_scale output_zp torch quint quantized qC_hat_ = torch ops quantized add qA qB output_scale output_zp qC_hat_ = torch ops quantized add qB qA output_scale output_zp assertTrue torch allclose qC dequantize qC_hat_ dequantize assertTrue torch allclose qC dequantize qC_hat_ dequantize Tests channel shuffle operation quantized tensors given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= max_numel= qparams=hu qparams dtypes= torch quint groups=st integers test_channel_shuffle X groups X scale zero_point torch_type = X channels = X shape - iH iW = X shape - assume channels groups == = torch from_numpy X = torch rand shape a_out = torch nn functional channel_shuffle groups a_ref = torch quantize_per_tensor a_out scale=scale zero_point=zero_point dtype=torch_type a_ref = a_ref dequantize qa = torch quantize_per_tensor scale=scale zero_point=zero_point dtype=torch_type a_hat = torch nn functional channel_shuffle qa groups assertEqual a_ref a_hat dequantize msg= torch nn functional channel_shuffle results off Tests D max pool operation quantized tensors given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams kernel=st sampled_from stride=st sampled_from None dilation=st integers padding=st integers ceil_mode=st booleans test_max_pool d X kernel stride dilation padding ceil_mode X scale zero_point torch_type = X Check constraints assume kernel = padding Kernel cannot overhanging iW = X shape - oW = pool_output_shape iW kernel padding stride dilation ceil_mode assume oW = torch from_numpy X a_pool = torch nn functional max_pool d kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode a_ref = torch quantize_per_tensor a_pool scale=scale zero_point=zero_point dtype=torch_type a_ref = a_ref dequantize qa = torch quantize_per_tensor scale=scale zero_point=zero_point dtype=torch_type ops_under_test = torch torch max_pool d nn functional torch nn functional max_pool d ao nn quantized functional torch ao nn quantized functional max_pool d name op ops_under_test items a_hat = op qa kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode assertEqual a_ref a_hat dequantize msg=f name results off Test ops quantized separately because None treated a_hat = torch ops quantized max_pool d qa kernel_size=_single kernel stride=_single kernel stride None stride padding=_single padding dilation=_single dilation ceil_mode=ceil_mode assertEqual a_ref a_hat dequantize msg= ops quantized max_pool d results off TODO merge test test_max_pool d Tests D cudnn max pool operation quantized tensors given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= cudnn s support quantized pooling limited int currently qparams=hu qparams dtypes= torch qint kernel=st sampled_from stride=st sampled_from None currently there no support dilation cudnn pooling dilation=st integers padding=st integers ceil_mode=st booleans unittest skipIf TEST_CUDNN cudnn enabled unittest skipIf TEST_CUDNN_VERSION = cuDNN maxpool d mishandles - before v unittest skipIf TEST_ROCM supported rocm test_max_pool d_cudnn X kernel stride dilation padding ceil_mode X scale zero_point torch_type = X assume kernel = padding Kernel cannot overhanging iH iW = X shape - oH = pool_output_shape iH kernel padding stride dilation ceil_mode assume oH oW = pool_output_shape iW kernel padding stride dilation ceil_mode assume oW = torch from_numpy X device= cuda a_pool = torch nn functional max_pool d kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode a_ref = torch quantize_per_tensor a_pool scale=scale zero_point=zero_point dtype=torch_type a_ref = a_ref dequantize qa = torch quantize_per_tensor scale=scale zero_point=zero_point dtype=torch_type Test ops quantized separately because None treated a_hat = torch ops quantized max_pool d qa kernel_size=_pair kernel stride=_pair kernel stride None stride padding=_pair padding dilation=_pair dilation ceil_mode=ceil_mode assertEqual a_ref a_hat dequantize msg= ops quantized max_pool d results off Tests D max pool operation quantized tensors given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams kernel=st sampled_from stride=st sampled_from None dilation=st integers padding=st integers ceil_mode=st booleans test_max_pool d X kernel stride dilation padding ceil_mode X scale zero_point torch_type = X Check constraints assume kernel = padding Kernel cannot overhanging iH iW = X shape - oH = pool_output_shape iH kernel padding stride dilation ceil_mode assume oH oW = pool_output_shape iW kernel padding stride dilation ceil_mode assume oW = torch from_numpy X a_pool = torch nn functional max_pool d kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode a_ref = torch quantize_per_tensor a_pool scale=scale zero_point=zero_point dtype=torch_type a_ref = a_ref dequantize qa = torch quantize_per_tensor scale=scale zero_point=zero_point dtype=torch_type ops_under_test = torch torch max_pool d nn functional torch nn functional max_pool d ao nn quantized functional torch ao nn quantized functional max_pool d name op ops_under_test items a_hat = op qa kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode assertEqual a_ref a_hat dequantize msg=f name results off Test ops quantized separately because None treated a_hat = torch ops quantized max_pool d qa kernel_size=_pair kernel stride=_pair kernel stride None stride padding=_pair padding dilation=_pair dilation ceil_mode=ceil_mode assertEqual a_ref a_hat dequantize msg= ops quantized max_pool d results off unittest skipIf IS_FBCODE Skip pt e ops fbcode test_max_pool d_pt e kernel_list = stride_list = padding_list = dilation_list = ceil_mode_list = False True channels_last_input = False True options = itertools product kernel_list stride_list padding_list dilation_list ceil_mode_list channels_last_input kernel stride padding dilation ceil_mode channels_last options padding = kernel Continue invalid input continue input = torch randint dtype=torch uint channels_last input = input contiguous memory_format=torch channels_last a_pool = torch nn functional max_pool d input torch float kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode torch uint a_hat = torch ops quantized max_pool d input kernel_size=_pair kernel stride=_pair stride padding=_pair padding dilation=_pair dilation ceil_mode=ceil_mode assertEqual input is_contiguous a_hat is_contiguous msg= ops quantized max_pool d input output diff memory format assertEqual a_pool a_hat msg= ops quantized max_pool d results off Tests D max pool operation quantized tensors test_max_pool d torch_types = torch qint torch quint kernels = strides = dilations = paddings = ceil_modes = True False options = itertools product torch_types kernels strides dilations paddings ceil_modes torch_type kernel stride dilation padding ceil_mode options X = torch randint torch float scale = zero_point = Check constraints invalid input kernel = padding continue iT iH iW = X shape - oT = pool_output_shape iT kernel padding stride dilation ceil_mode oT continue oH = pool_output_shape iH kernel padding stride dilation ceil_mode oH continue oW = pool_output_shape iW kernel padding stride dilation ceil_mode oW continue a_pool = torch nn functional max_pool d X kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode a_ref = torch quantize_per_tensor a_pool scale=scale zero_point=zero_point dtype=torch_type a_ref = a_ref dequantize qa = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type ops_under_test = torch torch max_pool d nn functional torch nn functional max_pool d name op ops_under_test items a_hat = op qa kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode assertEqual a_ref a_hat dequantize msg=f name results off Tests max pool operation NHWC quantized tensors given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams kernel=st sampled_from stride=st sampled_from None dilation=st integers padding=st integers ceil_mode=st booleans test_max_pool d_nhwc X kernel stride dilation padding ceil_mode X scale zero_point torch_type = X Ensure we hit vectorized paths = + + hits interleaved path hits non-interleaved path hits scalar path X shape X = np repeat X X shape Check constraints assume kernel = padding Kernel cannot overhanging iH iW = X shape - oH = pool_output_shape iH kernel padding stride dilation ceil_mode assume oH oW = pool_output_shape iW kernel padding stride dilation ceil_mode assume oW X_nchw = np ascontiguousarray X transpose = torch from_numpy X_nchw permute a_pool = torch nn functional max_pool d kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode a_ref = torch quantize_per_tensor a_pool scale=scale zero_point=zero_point dtype=torch_type a_ref = a_ref dequantize qa = torch quantize_per_tensor torch from_numpy X_nchw scale=scale zero_point=zero_point dtype=torch_type permute assertTrue qa stride = sorted qa stride ops_under_test = torch torch max_pool d nn functional torch nn functional max_pool d ao nn quantized functional torch ao nn quantized functional max_pool d name op ops_under_test items a_hat = op qa kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode assertTrue a_hat stride = sorted a_hat stride assertEqual a_ref a_hat dequantize msg=f name results off Test ops quantized separately because None treated a_hat = torch ops quantized max_pool d qa kernel_size=_pair kernel stride=_pair kernel stride None stride padding=_pair padding dilation=_pair dilation ceil_mode=ceil_mode assertEqual a_ref a_hat dequantize msg= ops quantized max_pool d results off Tests D max pool operation quantized channel_last tensors test_max_pool d_nhwc torch_types = torch qint torch quint kernels = strides = dilations = paddings = ceil_modes = True False options = itertools product torch_types kernels strides dilations paddings ceil_modes torch_type kernel stride dilation padding ceil_mode options X = torch randint torch float X_copy = copy deepcopy X X = X contiguous memory_format=torch channels_last_ d scale = zero_point = Check constraints invalid input kernel = padding continue iT iH iW = X shape - oT = pool_output_shape iT kernel padding stride dilation ceil_mode oT continue oH = pool_output_shape iH kernel padding stride dilation ceil_mode oH continue oW = pool_output_shape iW kernel padding stride dilation ceil_mode oW continue a_pool = torch nn functional max_pool d X kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode a_ref = torch quantize_per_tensor a_pool scale=scale zero_point=zero_point dtype=torch_type a_ref = a_ref dequantize qa = torch quantize_per_tensor X_copy scale=scale zero_point=zero_point dtype=torch_type qa = qa contiguous memory_format=torch channels_last_ d ops_under_test = torch torch max_pool d nn functional torch nn functional max_pool d name op ops_under_test items a_hat = op qa kernel_size=kernel stride=stride padding=padding dilation=dilation ceil_mode=ceil_mode assertEqual a_ref a_hat dequantize msg=f name results off given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams dtypes=torch quint kernel=st sampled_from stride=st sampled_from None padding=st integers ceil_mode=st sampled_from True False count_include_pad=st sampled_from True False divisor_override=st sampled_from None None test_avg_pool d X kernel stride padding ceil_mode count_include_pad divisor_override Note we currently cannot test divisor_override because quantized op will clamp result within range However float op will X scale zero_point torch_type = X assume kernel = padding Kernel cannot overhanging iH iW = X shape - oH = pool_output_shape iH kernel padding stride dilation= assume oH oW = pool_output_shape iW kernel padding stride dilation= assume oW X = torch from_numpy X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type X = qX dequantize Run reference float tensor then quantize result comparison X_ref = torch nn functional avg_pool d X kernel_size=kernel stride=stride padding=padding ceil_mode=ceil_mode count_include_pad=count_include_pad divisor_override=divisor_override ops_under_test = nn functional torch nn functional avg_pool d ao nn quantized functional torch ao nn quantized functional avg_pool d error_message = r Results off \n\tExpected \n \n\tGot \n name op ops_under_test items qX_hat = op qX kernel_size=kernel stride=stride padding=padding ceil_mode=ceil_mode count_include_pad=count_include_pad divisor_override=divisor_override qX_ref = torch quantize_per_tensor X_ref scale=qX_hat q_scale zero_point=qX_hat q_zero_point dtype=torch_type assertEqual qX_ref int_repr torch double qX_hat int_repr torch double atol= rtol= msg=error_message format name qX_ref int_repr qX_hat int_repr assertEqual scale qX_hat q_scale msg=error_message format name + scale scale qX_hat q_scale assertEqual zero_point qX_hat q_zero_point msg=error_message format name + zero_point scale qX_hat q_zero_point given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams dtypes=torch qint kernel=st sampled_from stride=st sampled_from None padding=st integers ceil_mode=st sampled_from True False count_include_pad=st sampled_from True False divisor_override=st sampled_from None None test_avg_pool d_nhwc X kernel stride padding ceil_mode count_include_pad divisor_override Note we currently cannot test divisor_override because quantized op will clamp result within range However float op will we cannot test qint since float point precision much lower than int big number which will make test very flaky X scale zero_point torch_type = X H W = X shape - X shape X = np repeat X X shape assume kernel = padding Kernel cannot overhanging iH iW = X shape - oH = pool_output_shape iH kernel padding stride dilation= assume oH oW = pool_output_shape iW kernel padding stride dilation= assume oW X_nchw = np ascontiguousarray X transpose qX = torch quantize_per_tensor torch from_numpy X_nchw scale=scale zero_point=zero_point dtype=torch_type permute X = qX dequantize Run reference int_repr + round avoid double rounding error X_ref = torch nn functional avg_pool d X kernel_size=kernel stride=stride padding=padding ceil_mode=ceil_mode count_include_pad=count_include_pad divisor_override=divisor_override assertTrue qX stride = sorted qX stride ops_under_test = nn functional torch nn functional avg_pool d ao nn quantized functional torch ao nn quantized functional avg_pool d error_message = r Results off \n\tExpected \n \n\tGot \n name op ops_under_test items X_hat = op qX kernel_size=kernel stride=stride padding=padding ceil_mode=ceil_mode count_include_pad=count_include_pad divisor_override=divisor_override assertTrue X_hat stride = sorted X_hat stride qX_ref = torch quantize_per_tensor X_ref scale=X_hat q_scale zero_point=X_hat q_zero_point dtype=torch_type assertEqual qX_ref int_repr torch double X_hat int_repr torch double atol= rtol= msg=error_message format name qX_ref int_repr X_hat int_repr assertEqual scale X_hat q_scale msg=error_message format name + scale scale X_hat q_scale assertEqual zero_point X_hat q_zero_point msg=error_message format name + zero_point scale X_hat q_zero_point given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams dtypes=torch quint kernel=st sampled_from stride=st sampled_from None padding=st integers ceil_mode=st sampled_from True False count_include_pad=st sampled_from True False divisor_override=st sampled_from None None test_avg_pool d X kernel stride padding ceil_mode count_include_pad divisor_override Note we currently cannot test divisor_override because quantized op will clamp result within range However float op will X scale zero_point torch_type = X assume kernel = padding Kernel cannot overhanging iD iH iW = X shape - oD = pool_output_shape iD kernel padding stride dilation= assume oD oH = pool_output_shape iH kernel padding stride dilation= assume oH oW = pool_output_shape iW kernel padding stride dilation= assume oW X = torch from_numpy X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type X = qX dequantize Run reference float tensor then quantize result comparison X_ref = torch nn functional avg_pool d X kernel_size=kernel stride=stride padding=padding ceil_mode=ceil_mode count_include_pad=count_include_pad divisor_override=divisor_override ops_under_test = nn functional torch nn functional avg_pool d ao nn quantized functional torch ao nn quantized functional avg_pool d error_message = r Results off \n\tExpected \n \n\tGot \n name op ops_under_test items qX_hat = op qX kernel_size=kernel stride=stride padding=padding ceil_mode=ceil_mode count_include_pad=count_include_pad divisor_override=divisor_override qX_ref = torch quantize_per_tensor X_ref scale=qX_hat q_scale zero_point=qX_hat q_zero_point dtype=torch_type assertEqual qX_ref int_repr torch double qX_hat int_repr torch double atol= rtol= msg=error_message format name qX_ref int_repr qX_hat int_repr assertEqual scale qX_hat q_scale msg=error_message format name + scale scale qX_hat q_scale assertEqual zero_point qX_hat q_zero_point msg=error_message format name + zero_point scale qX_hat q_zero_point given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams dtypes=torch qint kernel=st sampled_from stride=st sampled_from None padding=st integers ceil_mode=st sampled_from True False count_include_pad=st sampled_from True False divisor_override=st sampled_from None None test_avg_pool d_nhwc X kernel stride padding ceil_mode count_include_pad divisor_override Note we currently cannot test divisor_override because quantized op will clamp result within range However float op will we cannot test qint since float point precision much lower than int big number which will make test very flaky X scale zero_point torch_type = X D H W = X shape - X shape X = np repeat X X shape assume kernel = padding Kernel cannot overhanging iD iH iW = X shape - oD = pool_output_shape iD kernel padding stride dilation= assume oD oH = pool_output_shape iH kernel padding stride dilation= assume oH oW = pool_output_shape iW kernel padding stride dilation= assume oW X_nchw = np ascontiguousarray X transpose qX = torch quantize_per_tensor torch from_numpy X_nchw scale=scale zero_point=zero_point dtype=torch_type permute X = qX dequantize Run reference int_repr + round avoid double rounding error X_ref = torch nn functional avg_pool d X kernel_size=kernel stride=stride padding=padding ceil_mode=ceil_mode count_include_pad=count_include_pad divisor_override=divisor_override assertTrue qX stride = sorted qX stride ops_under_test = nn functional torch nn functional avg_pool d ao nn quantized functional torch ao nn quantized functional avg_pool d error_message = r Results off \n\tExpected \n \n\tGot \n name op ops_under_test items X_hat = op qX kernel_size=kernel stride=stride padding=padding ceil_mode=ceil_mode count_include_pad=count_include_pad divisor_override=divisor_override assertTrue X_hat stride = sorted X_hat stride qX_ref = torch quantize_per_tensor X_ref scale=X_hat q_scale zero_point=X_hat q_zero_point dtype=torch_type assertEqual qX_ref int_repr torch double X_hat int_repr torch double atol= rtol= msg=error_message format name qX_ref int_repr X_hat int_repr assertEqual scale X_hat q_scale msg=error_message format name + scale scale X_hat q_scale assertEqual zero_point X_hat q_zero_point msg=error_message format name + zero_point scale X_hat q_zero_point Tests adaptive average pool operation NHWC quantized tensors test_adaptive_avg_pool d_nhwc side_lens = range dim_lens = range torch_type = torch qint zero_points = combined = side_lens dim_lens zero_points test_cases = itertools product combined test_case test_cases output_size_h = random randint output_size_w = random randint side_len dim_len zero_point = test_case shapes = side_len dim_len X X_scale X_zero_point = \ _get_random_tensor_and_q_params shapes zero_point X = np array X scale = H W = X shape - output_size_h = min output_size_h H output_size_w = min output_size_w W output_size_h == output_size_w output_size = output_size_h output_size = output_size_h output_size_w X shape X = np repeat X X shape X ndim == X_nchw = np ascontiguousarray X transpose X = torch from_numpy X_nchw permute qX = torch quantize_per_tensor torch from_numpy X_nchw scale=scale zero_point=zero_point dtype=torch_type permute ndim == X_nchw = np ascontiguousarray X transpose X = torch from_numpy X_nchw permute qX = torch quantize_per_tensor torch from_numpy X_nchw scale=scale zero_point=zero_point dtype=torch_type permute Run reference int_repr + round avoid double rounding error X_ref = torch nn functional adaptive_avg_pool d qX int_repr torch double output_size round assertTrue qX stride = sorted qX stride ops_under_test = nn functional torch nn functional adaptive_avg_pool d ao nn quantized functional torch ao nn quantized functional adaptive_avg_pool d error_message = r Results off \n\tExpected \n \n\tGot \n name op ops_under_test items X_hat = op qX output_size=output_size assertTrue X_hat stride = sorted X_hat stride assertEqual X_ref X_hat int_repr atol= rtol= msg=error_message format name X_ref X_hat int_repr exact_dtype=False assertEqual scale X_hat q_scale msg=error_message format name + scale scale X_hat q_scale assertEqual zero_point X_hat q_zero_point msg=error_message format name + zero_point scale X_hat q_zero_point unittest skip currently working feature isn t used test_adaptive_avg_pool side_lens = range dim_lens = range torch_type = torch qint zero_points = combined = side_lens dim_lens zero_points test_cases = itertools product combined test_case test_cases output_size_d = random randint output_size_h = random randint output_size_w = random randint side_len dim_len zero_point = test_case shapes = side_len dim_len X X_scale X_zero_point = \ _get_random_tensor_and_q_params shapes zero_point X = np array X scale = ndim = X ndim dim_to_check = ndim = dim_to_check append ndim = dim_to_check append D H W = X shape - output_size_d = min output_size_d D output_size_h = min output_size_h H output_size_w = min output_size_w W X = torch from_numpy X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type dim dim_to_check dim == output_size_h == output_size_w output_size = output_size_h output_size = output_size_h output_size_w dim == output_size_d == output_size_h == output_size_w output_size = output_size_h output_size = output_size_d output_size_h output_size_w Run reference int_repr + round avoid double rounding error ref_op = getattr torch nn functional f adaptive_avg_pool dim d X_ref = ref_op qX int_repr torch float output_size round ops_under_test = nn functional getattr torch nn functional f adaptive_avg_pool dim d nn quantized functional getattr torch ao nn quantized functional f adaptive_avg_pool dim d ao nn quantized functional getattr torch ao nn quantized functional f adaptive_avg_pool dim d error_message = r Results off \n\tExpected \n \n\tGot \n name op ops_under_test items TODO torch cuda is_available should swapped flag checks cudnn enabled build when cudnn supports adaptive average pooling devices = cpu cuda dim == torch cuda is_available cpu device devices qX_hat = op qX device=device output_size=output_size assertEqual X_ref qX_hat int_repr atol= rtol= msg=error_message format name X_ref qX_hat exact_dtype=False assertEqual scale qX_hat q_scale msg=error_message format name + scale scale qX_hat q_scale assertEqual zero_point qX_hat q_zero_point msg=error_message format name + zero_point scale qX_hat q_zero_point Tests adaptive average pool operation NHWC quantized tensors test_adaptive_avg_pool d_ndhwc side_lens = range dim_lens = range torch_type = torch qint zero_point = combined = side_lens dim_lens test_cases = itertools product combined test_case test_cases output_size_d = random randint output_size_h = random randint output_size_w = random randint side_len dim_len = test_case shapes = side_len dim_len X X_scale X_zero_point = \ _get_random_tensor_and_q_params shapes zero_point X = np array X scale = D H W = X shape - output_size_d = min output_size_d D output_size_h = min output_size_h H output_size_w = min output_size_w W output_size_d == output_size_h == output_size_w output_size = output_size_h output_size = output_size_d output_size_h output_size_w X shape X = np repeat X X shape X ndim == X_ncdhw = np ascontiguousarray X transpose X = torch from_numpy X_ncdhw permute qX = torch quantize_per_tensor torch from_numpy X_ncdhw scale=scale zero_point=zero_point dtype=torch_type permute ndim == X_ncdhw = np ascontiguousarray X transpose X = torch from_numpy X_ncdhw permute qX = torch quantize_per_tensor torch from_numpy X_ncdhw scale=scale zero_point=zero_point dtype=torch_type permute Run reference int_repr + round avoid double rounding error X_ref = torch nn functional adaptive_avg_pool d qX int_repr torch double output_size round assertTrue qX stride = sorted qX stride ops_under_test = nn functional torch nn functional adaptive_avg_pool d ao nn quantized functional torch ao nn quantized functional adaptive_avg_pool d error_message = r Results off \n\tExpected \n \n\tGot \n name op ops_under_test items X_hat = op qX output_size=output_size assertTrue X_hat stride = sorted X_hat stride assertEqual X_ref X_hat int_repr atol= rtol= msg=error_message format name X_ref X_hat int_repr exact_dtype=False assertEqual scale X_hat q_scale msg=error_message format name + scale scale X_hat q_scale assertEqual zero_point X_hat q_zero_point msg=error_message format name + zero_point scale X_hat q_zero_point test_qtopk x_dims = Num elements shape sides = Side tensor generated dims = dimension over which perform topk largest = False True Return largest smallest element sorted = False True Return sorted dtypes = torch qint torch quint is_nhwc = False True Is input NHWC format test_cases = itertools product x_dims sides dims largest sorted dtypes is_nhwc k = x_dim side dim larg sort dtype nhwc test_cases nhwc x_dim = NHWC requires dimensions continue dim = x_dim Dimension find top-k should exist continue shape = side x_dim X scale zp = _get_random_tensor_and_q_params shape dtype qX = torch quantize_per_tensor X scale zp dtype nhwc qX = qX permute X = np transpose X unquantized_out = torch topk qX dequantize k dim=dim largest=larg sorted=sort values = torch quantize_per_tensor X scale zp dtype indices = torch tensor X long quantized_out = torch topk qX k dim=dim largest=larg sorted=sort assert len unquantized_out == len quantized_out torch testing assert_close quantized_out dequantize unquantized_out torch testing assert_close quantized_out unquantized_out Tests quantize concatenation both fused given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams num=st integers dim=st integers relu=st booleans test_cat X num dim relu tensors_q = tensors_ref = X scale zero_point torch_type = X assume dim X ndim X = torch from_numpy X new_shape = np array X shape new_shape dim = _ range num tensors_q append torch quantize_per_tensor X scale zero_point torch_type tensors_ref append X new_shape dim += tensors_ref - shape dim cat_ref = torch cat tensors_ref dim=dim cat_ref = torch quantize_per_tensor cat_ref scale zero_point torch_type cat_ref = cat_ref dequantize relu cat_ref = F relu cat_ref q_cat_op = torch ops quantized cat_relu q_cat_out_op = torch ops quantized cat_relu_out q_cat_op = torch ops quantized cat q_cat_out_op = torch ops quantized cat_out cat_q = q_cat_op tensors_q dim=dim scale=scale zero_point=zero_point cat_q = cat_q dequantize np testing assert_equal cat_ref numpy cat_q numpy cat_q_out = torch _empty_affine_quantized list new_shape scale=scale zero_point=zero_point dtype=torch_type q_cat_out_op tensors_q dim=dim out=cat_q_out cat_q_out = cat_q_out dequantize np testing assert_equal cat_ref numpy cat_q_out numpy Test cat per-channel quantized tensor ch_axis = scales = torch from_numpy np array X shape ch_axis scales = scales torch float zero_points = torch from_numpy np array X shape ch_axis zero_points = zero_points torch long tensors_q = torch quantize_per_channel X scales zero_points axis=ch_axis dtype=torch_type assertRaisesRegex RuntimeError supported cat cat_q = q_cat_op tensors_q dim=ch_axis scale=scale zero_point=zero_point given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams size=st sampled_from mode=st sampled_from bilinear nearest nearest-exact scale_factor=st sampled_from None align_corners=st sampled_from True False nhwc_layout=st sampled_from True False test_interpolate X size mode scale_factor align_corners nhwc_layout This test cover upsample_nearest d upsample_bilinear d X scale zero_point torch_type = X scale_factor None size = None mode nearest nearest-exact align_corners = None nhwc_layout X shape X = np repeat X X shape X_nchw = np ascontiguousarray X transpose X = torch from_numpy X_nchw permute qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type permute X = torch from_numpy X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type X_ref = torch nn functional interpolate qX int_repr torch float size=size scale_factor=scale_factor mode=mode align_corners=align_corners ops_under_test = nn functional torch nn functional interpolate ao nn quantized functional torch ao nn quantized functional interpolate error_message = r Results off \n\tExpected \n \n\tGot \n name op ops_under_test items qX_hat = op qX size=size scale_factor=scale_factor mode=mode align_corners=align_corners assertEqual X_ref qX_hat int_repr atol= rtol= msg=f name results off qX_hat= qX_hat int_repr X_ref= X_ref exact_dtype=False assertEqual scale qX_hat q_scale msg=error_message format name + scale scale qX_hat q_scale assertEqual zero_point qX_hat q_zero_point msg=error_message format name + zero_point scale qX_hat q_zero_point given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams size=st sampled_from mode=st sampled_from nearest nearest-exact scale_factor=st sampled_from None align_corners=st sampled_from True False nhwc_layout=st sampled_from True False test_interpolate d X size mode scale_factor align_corners nhwc_layout This test cover upsample_nearest d X scale zero_point torch_type = X scale_factor None size = None align_corners = None nhwc_layout X shape X = np repeat X X shape X_nchw = np ascontiguousarray X transpose X = torch from_numpy X_nchw permute qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type permute X = torch from_numpy X qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type X_ref = torch nn functional interpolate qX int_repr torch float size=size scale_factor=scale_factor mode=mode align_corners=align_corners ops_under_test = nn functional torch nn functional interpolate ao nn quantized functional torch ao nn quantized functional interpolate error_message = r Results off \n\tExpected \n \n\tGot \n name op ops_under_test items qX_hat = op qX size=size scale_factor=scale_factor mode=mode align_corners=align_corners assertEqual X_ref qX_hat int_repr atol= rtol= msg=f name results off qX_hat= qX_hat int_repr X_ref= X_ref exact_dtype=False assertEqual scale qX_hat q_scale msg=error_message format name + scale scale qX_hat q_scale assertEqual zero_point qX_hat q_zero_point msg=error_message format name + zero_point scale qX_hat q_zero_point Tests quantize concatenation both fused given X=hu tensor shapes=hu array_shapes min_dims= max_dims= min_side= max_side= qparams=hu qparams relu=st booleans test_cat_nhwc X relu X NHWC X scale zero_point torch_type = X Tile out X so channels X = np repeat X X shape X = torch from_numpy np ascontiguousarray X Y = X clone Y = torch from_numpy np ascontiguousarray Y We add fast path qcat when inputs share same scale zero_point will go direct memcpy instead dequant-cat-quant scaleX scaleY scale scale scale scale Here we quantize get quantized tensors NHWC both dims strides The permute switches so tensor looks like NCHW laid out memory NHWC qX = torch quantize_per_tensor X scaleX zero_point torch_type permute qY = torch quantize_per_tensor Y scaleY zero_point torch_type permute ref = torch cat qX dequantize qY dequantize dim= relu ref ref = ref = torch quantize_per_tensor ref scale=scale zero_point=zero_point dtype=torch_type relu out = torch ops quantized cat_relu qX qY dim= scale=scale zero_point=zero_point out = torch ops quantized cat qX qY dim= scale=scale zero_point=zero_point torch testing assert_close out dequantize ref dequantize assertNotEqual out stride sorted out stride override_qengines test_mean scale_list = zero_point_list = shapes = dtypes = torch quint torch qint dims = - test_cases = itertools product scale_list zero_point_list shapes dtypes dims op = torch mean scale zp shape dtype dim test_cases all d len shape d dim continue X = torch randn shape qX = torch quantize_per_tensor X scale zp dtype Y = op qX dequantize dim Y = torch quantize_per_tensor Y scale zp dtype dequantize qY = op qX dim assertEqual Y qY dequantize skipIfNoQNNPACK given keep=st booleans test_quantized_mean_qnnpack keep override_quantized_engine qnnpack using multiple sizes satisfy pytorch_q gavgpool_ukernel_up xm__sse -byte alignment demand under ASAN in_dim = keep out_dim = out_dim = X = torch ones in_dim Y = torch ones out_dim XQ = torch quantize_per_tensor X scale= zero_point= dtype=torch quint YQ = torch quantize_per_tensor Y scale= zero_point= dtype=torch quint MQ = XQ mean keepdim=keep assertTrue torch equal MQ YQ override_qengines test_std scale_list = zero_point_list = shapes = dtypes = torch quint torch qint dims = - unbiased_list = True False keep_dim_list = True False test_cases = itertools product scale_list zero_point_list shapes dtypes dims unbiased_list keep_dim_list op = torch std scale zp shape dtype dim unbiased keep_dim test_cases all d len shape d dim continue X = torch randn shape qX = torch quantize_per_tensor X scale zp dtype Y = op qX dequantize dim unbiased keep_dim Y = torch quantize_per_tensor Y scale zp dtype dequantize qY = op qX dim unbiased keep_dim assertEqual Y qY dequantize Tests correctness quantized equal op given X=hu tensor shapes=hu array_shapes qparams=hu qparams X =hu tensor shapes=hu array_shapes qparams=hu qparams X_per_channel=st booleans X _per_channel=st booleans test_equal X X X_per_channel X _per_channel X X_params = X scale zero_point torch_type = X_params X X _params = X scale zero_point torch_type = X _params X = torch from_numpy X X_per_channel X_scheme = per_channel channels = X shape - qX = torch quantize_per_channel X scales=torch tensor scale channels zero_points=torch tensor zero_point channels dtype=torch_type axis=X ndim - X_scheme = per_tensor qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type X = torch from_numpy X X _per_channel X _scheme = per_channel channels = X shape - qX = torch quantize_per_channel X scales=torch tensor scale channels zero_points=torch tensor zero_point channels dtype=torch_type axis=X ndim - X _scheme = per_tensor qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type equal_ref qX qX qX qscheme = qX qscheme False qX shape = qX shape False qX dtype = qX dtype False qX qscheme == torch per_tensor_affine qX q_scale = qX q_scale False qX q_zero_point = qX q_zero_point False qX qscheme == torch per_channel_affine qX q_per_channel_scales = qX q_per_channel_scales any False qX q_per_channel_zero_points = qX q_per_channel_zero_points any False raise NotImplementedError Don t know what do qX qscheme qX int_repr float = qX int_repr float any False True assertEqual qX equal qX equal_ref qX qX assertEqual qX equal qX equal_ref qX qX Tests quantized equal op input non-quantized tensor test_quantized_equal x = torch rand y = torch quantize_per_tensor x scale= zero_point= dtype=torch qint assertTrue torch equal x y assertTrue torch equal y x skipIfNoFBGEMM test_group_norm hypothesis flaky test create test cases manually batches_list = num_groups_list = channels_per_groups = elements_per_channels = torch_types = torch qint torch quint y_scales = y_zero_points = channels_last_list = True False affine_list = True False combined = batches_list num_groups_list channels_per_groups elements_per_channels torch_types y_scales y_zero_points channels_last_list affine_list test_cases = itertools product combined override_quantized_engine fbgemm test_case test_cases batches num_groups channels_per_group elements_per_channel \ torch_type Y_scale Y_zero_point channels_last \ affine = test_case num_channels = num_groups channels_per_group minimum rank channels_last shapes = batches num_channels elements_per_channel In FP kernel sums sums squares calculated floating point In int uint versions quantized kernel they calculated integer arithmetic which exact Because numerics do always match exactly which expected acceptable We do following allow failure test do use Hypothesis generate input tensor Hypothesis favors homogeneous inputs its search strategies which isn t representative inputs we care about tends maximize particular numerics difference allow small off Y_scale errors Even when variance input high there can off one errors result input value happens fall exactly bin boundary output scale If we want numerics match we could switch calculating mean+var floating point future cost speed X X_scale X_zero_point = \ _get_random_tensor_and_q_params shapes torch_type Initialize weights non-randomly reproducibility affine weight = torch ones num_channels float bias = torch ones num_channels float i range num_channels weight i = i bias i = i weight = None bias = None eps = qX = torch quantize_per_tensor X X_scale X_zero_point torch_type channels_last qX = qX contiguous memory_format=torch channels_last dqX = qX dequantize Enforce non-homogeneous inputs batch_idx range batches group_idx range num_groups ch_start = group_idx channels_per_group ch_end = ch_start + channels_per_group group_vals = dqX batch_idx ch_start ch_end assume float torch unique group_vals shape group_vals numel group_vals numel qY = torch ops quantized group_norm qX num_groups weight bias eps Y_scale Y_zero_point dqY_hat = F group_norm dqX num_groups=num_groups weight=weight bias=bias eps=eps qY_hat = torch quantize_per_tensor dqY_hat Y_scale Y_zero_point torch_type Due numerics difference mentioned above between calculating variance float vs int results can still slightly different dqY = qY dequantize dqY_hat = qY_hat dequantize diff = dqY - dqY_hat off-by-one errors magnitude Y_scale num_diff = torch sum diff Y_scale pct_diff = float num_diff diff numel + e- num_diff_off_by_one = torch sum diff diff = Y_scale pct_diff_off_by_one = float num_diff_off_by_one diff numel + e- assertTrue pct_diff e- assertTrue pct_diff_off_by_one skipIfNoFBGEMM test_instance_norm max_sides = shape_list = torch_types = torch qint torch quint y_scales = y_zero_points = channels_last_list = True False affine_list = True False combined = shape_list torch_types y_scales y_zero_points channels_last_list affine_list test_cases_product = itertools product combined test_cases = list test_cases_product NB Add just one test case test overflow case too slow run internally fbcode mode dev long pole x calls torch sort inside torch unique current implementation IS_SANDCASTLE test_cases append shape torch qint torch_type scale zero_point False channels_last True affine override_quantized_engine fbgemm test_case test_cases shapes torch_type Y_scale Y_zero_point channels_last affine = test_case channels_last shapes __len__ = required rank tensor use channels_last format continue In FP kernel sums sums squares calculated floating point In int uint versions quantized kernel they calculated integer arithmetic which exact Because numerics do always match exactly which expected acceptable We do following allow failure test do use Hypothesis generate input tensor Hypothesis favors homogeneous inputs its search strategies which isn t representative inputs we care about tends maximize particular numerics difference allow small off Y_scale errors Even when variance input high there can off one errors result input value happens fall exactly bin boundary output scale If we want numerics match we could switch calculating mean+var floating point future cost speed X X_scale X_zero_point = \ _get_random_tensor_and_q_params shapes torch_type num_channels = shapes affine weight = torch rand num_channels float bias = torch rand num_channels float i range num_channels weight i = i bias i = i weight = None bias = None eps = qX = torch quantize_per_tensor X X_scale X_zero_point torch_type channels_last qX = qX contiguous memory_format=torch channels_last dqX = qX dequantize Enforce non-homogeneous inputs batches = shapes batch_idx range batches ch_idx range num_channels ch_vals = dqX batch_idx ch_idx assume float torch unique ch_vals shape ch_vals numel ch_vals numel ch_vals numel qY = torch ops quantized instance_norm qX weight bias eps Y_scale Y_zero_point dqY_hat = F instance_norm dqX weight=weight bias=bias eps=eps qY_hat = torch quantize_per_tensor dqY_hat Y_scale Y_zero_point torch_type Due numerics difference mentioned above between calculating variance float vs int results can still slightly different dqY = qY dequantize dqY_hat = qY_hat dequantize diff = dqY - dqY_hat off-by-one errors magnitude Y_scale num_diff = torch sum diff Y_scale pct_diff = float num_diff diff numel + e- num_diff_off_by_one = torch sum diff diff = Y_scale pct_diff_off_by_one = float num_diff_off_by_one diff numel + e- assertTrue pct_diff e- assertTrue pct_diff_off_by_one skipIfNoFBGEMM test_batch_norm_relu hypothesis too slow test create test cases manually max_sides = side_lens = torch_types = torch qint torch quint combined = max_sides side_lens torch_types test_cases = itertools product combined override_quantized_engine fbgemm test_case test_cases max_side side_len torch_type = test_case Y_zero_point = Y_scale = shapes = side_len max_side X scale_x zero_point_x = \ _get_random_tensor_and_q_params shapes torch_type dtype_x = torch_type c = X shape mean = torch rand c float var = torch rand c float weight = torch rand c float bias = torch rand c float eps = qx = torch quantize_per_tensor X scale_x zero_point_x dtype_x len X shape == len X shape == qy = torch ops quantized batch_norm d_relu qx weight bias mean var eps Y_scale Y_zero_point len X shape == qy = torch ops quantized batch_norm d_relu qx weight bias mean var eps Y_scale Y_zero_point qy = torch ops quantized batch_norm d_relu qx weight bias mean var eps Y_scale Y_zero_point float_ref = F batch_norm qx dequantize weight=weight bias=bias running_mean=mean running_var=var training=False momentum= eps=eps numpy float_ref_relu = float_ref copy float_ref_relu float_ref = quantize_ref = torch quantize_per_tensor torch from_numpy float_ref_relu Y_scale Y_zero_point dtype_x assertEqual qy int_repr numpy quantize_ref int_repr numpy msg=f qy vs quantize_ref skipIfNoFBGEMM test_batch_norm hypothesis too slow test create test cases manually max_sides = side_lens = torch_types = torch qint torch quint combined = max_sides side_lens torch_types test_cases = itertools product combined override_quantized_engine fbgemm test_case test_cases max_side side_len torch_type = test_case Y_zero_point = Y_scale = shapes = side_len max_side X scale_x zero_point_x = \ _get_random_tensor_and_q_params shapes torch_type dtype_x = torch_type c = X shape mean = torch rand c float var = torch rand c float weight = torch rand c float bias = torch rand c float eps = qx = torch quantize_per_tensor X scale_x zero_point_x dtype_x len X shape == len X shape == qy = torch ops quantized batch_norm d qx weight bias mean var eps Y_scale Y_zero_point len X shape == qy = torch ops quantized batch_norm d qx weight bias mean var eps Y_scale Y_zero_point len X shape == qy = torch ops quantized batch_norm d qx weight bias mean var eps Y_scale Y_zero_point float_ref = F batch_norm qx dequantize weight=weight bias=bias running_mean=mean running_var=var training=False momentum= eps=eps quantize_ref = torch quantize_per_tensor float_ref Y_scale Y_zero_point dtype_x assertEqual qy int_repr numpy quantize_ref int_repr numpy msg=f qy vs quantize_ref override_qengines test_empty_batch scale = zero_point = X = torch ones dtype=torch float qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch quint upsample_nearest d qY = torch nn functional upsample_nearest qX scale_factor= np testing assert_equal qY size Quantized upsample_nearsest d batch size failed relu qY = torch nn functional relu qX np testing assert_equal qY size qX size Quantized relu batch size failed tanh qY = torch tanh qX np testing assert_equal qY size qX size Quantized tanh batch size failed sigmoid qY = torch sigmoid qX np testing assert_equal qY size qX size Quantized sigmoid batch size failed interpolate op = torch ao nn quantized functional interpolate mode nearest bilinear nearest-exact qY = op qX scale_factor= mode=mode np testing assert_equal qY size Quantized interpolate batch size failed avg_pool kernel = stride = padding = op = torch ao nn quantized functional avg_pool d qY = op qX kernel stride padding np testing assert_equal qY size Quantized avg_pool d batch size failed adaptive_avg_pool op = torch ao nn quantized functional adaptive_avg_pool d qY = op qX np testing assert_equal qY size Quantized adaptive_avg_pool d batch size failed max_pool dilation = qY = torch ops quantized max_pool d qX kernel stride padding dilation ceil_mode=False oH = pool_output_shape oW = pool_output_shape np testing assert_equal qY size oH oW Quantized maxpool d batch size failed hardtanh qY = torch ao nn quantized functional hardtanh qX - np testing assert_equal qY size qX size Quantized hardtanh batch size failed mul qY = torch ops quantized mul qX qX np testing assert_equal qY size qX size Quantized mul batch size failed add qY = torch ops quantized add qX qX np testing assert_equal qY size qX size Quantized addition batch size failed conv w = torch randn dtype=torch float qw = torch quantize_per_tensor w scale= zero_point= dtype=torch qint bias_float = torch ones dtype=torch float strides = pads = dilations = w_packed = torch ops quantized conv d_prepack qw bias_float strides pads dilations result = torch ops quantized conv d qX w_packed assertEqual result shape linear X = torch ones dtype=torch float qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch quint w = torch randn dtype=torch float qw = torch quantize_per_tensor w scale= zero_point= dtype=torch qint w_packed = torch ops quantized linear_prepack qw bias_float result = torch ops quantized linear qX w_packed assertEqual result shape dynamic linear result = torch ops quantized linear_dynamic X w_packed assertEqual result shape override_qengines test_linear_bias_unpack Verifies correctness bias unpack API LinearPackedParamBase bias_float = torch ones dtype=torch float w = torch randn dtype=torch float qw = torch quantize_per_tensor w scale= zero_point= dtype=torch qint w_packed = torch ops quantized linear_prepack qw bias_float test bias assertEqual w_packed bias bias_float test unpack assertEqual w_packed unpack qw test_advanced_indexing Verifies x syntax works quantized tensors dtype torch qint torch quint torch qint scale = zp = x_q = torch quantize_per_tensor torch randn scale zp dtype reference x_fp = x_q dequantize single dim single index x_q_s = x_q x_fp _s = x_fp x_fp _s _ref = \ torch quantize_per_tensor x_fp _s scale zp dtype assertEqual x_q_s x_fp _s _ref multiple dim single index x_q_s = x_q x_fp _s = x_fp x_fp _s _ref = \ torch quantize_per_tensor x_fp _s scale zp dtype assertEqual x_q_s x_fp _s _ref single dim multiple indices x_q_s = x_q x_fp _s = x_fp x_fp _s _ref = \ torch quantize_per_tensor x_fp _s scale zp dtype assertEqual x_q_s x_fp _s _ref multiple dim multiple indices x_q_s = x_q x_fp _s = x_fp x_fp _s _ref = \ torch quantize_per_tensor x_fp _s scale zp dtype assertEqual x_q_s x_fp _s _ref override_qengines test_custom_module_lstm QuantizableLSTMSplitGates torch ao nn quantizable LSTM classmethod from_float cls other qconfig=None super from_float other qconfig split_gates=True qengine = torch backends quantized engine batch_size = seq_len = input_size = hidden_size = num_layers = dropout = This supported Bias = False True Batch_first = False True Bidirectional = False True Split_gates = False True dtype = np uint qtype = torch quint x = np random randn seq_len batch_size input_size scale zero_point = _calculate_dynamic_qparams x dtype=dtype x = torch from_numpy x torch float qx = torch quantize_per_tensor x scale=scale zero_point=zero_point dtype=qtype x = qx dequantize torch no_grad bias batch_first bidirectional split_gates itertools product Bias Batch_first Bidirectional Split_gates Assume dB sufficient functional equivalence Without bias linear performs poorly min_power = bias max_mse = e- bias e- batch_first x = x reshape batch_size seq_len input_size qx = qx reshape batch_size seq_len input_size x = x reshape seq_len batch_size input_size qx = qx reshape seq_len batch_size input_size lstm = torch nn Sequential torch nn LSTM input_size hidden_size num_layers=num_layers bias=bias batch_first=batch_first dropout=dropout bidirectional=bidirectional lstm eval y_ref = lstm x Prepare lstm qconfig = torch ao quantization get_default_qconfig qengine custom_config_dict = None split_gates switch split_gates True via from_float float_to_observed_custom_module_class torch nn LSTM QuantizableLSTMSplitGates observed_to_quantized_custom_module_class QuantizableLSTMSplitGates torch ao nn quantized LSTM lstm_prepared = torch ao quantization prepare lstm prepare_custom_config_dict=custom_config_dict assertTrue hasattr lstm_prepared layers assertEqual num_layers len lstm_prepared layers assertEqual lstm_prepared layers layer_fw cell split_gates split_gates assert isinstance lstm_prepared torch ao nn quantizable LSTM Calibrate y = lstm_prepared x assertEqual y_ref y Quantize lstm_quantized = torch ao quantization convert lstm_prepared convert_custom_config_dict=custom_config_dict assert type lstm_quantized torch ao nn quantized LSTM qy = lstm_quantized qx snr = _snr y qy snr = snr + snr signal mse power snr assertTrue power min_power mse max_mse msg= f Error too high SNR dB power f Signal signal MSE mse Trace jit_qmodule = torch jit trace lstm_quantized qx Script jit_qmodule = torch jit script lstm_quantized override_qengines test_custom_module_multi_head_attention MultiheadAttentionModel torch nn Module __init__ args kwargs super __init__ layer = torch nn MultiheadAttention args kwargs forward query key value key_padding_mask Optional torch Tensor = None need_weights bool = True attn_mask Optional torch Tensor = None layer query key value key_padding_mask need_weights attn_mask qengine = torch backends quantized engine min_power = max_mse = num_heads = batch_size = target_seq_length = source_seq_length = qembed_dim = Must divisible number heads kembed_dim = vembed_dim = dropout = This supported Bias = False True Add_bias_kv = False True Add_zero_attn = False True dtype = np uint qtype = torch quint kdim vdim kembed_dim vembed_dim None None fp_data = torch randn target_seq_length batch_size qembed_dim Q torch randn source_seq_length batch_size qembed_dim kdim None kembed_dim K torch randn source_seq_length batch_size qembed_dim vdim None vembed_dim V q_data = reduce_range = qengine x fbgemm onednn idx x enumerate fp_data scale zero_point = _calculate_dynamic_qparams x dtype=dtype reduce_range=reduce_range x = x torch float qx = torch quantize_per_tensor x scale=scale zero_point=zero_point dtype=qtype q_data append qx Dequantize data back reference fp_data idx = qx dequantize torch no_grad bias add_bias_kv add_zero_attn itertools product Bias Add_bias_kv Add_zero_attn mha = MultiheadAttentionModel qembed_dim num_heads dropout bias add_bias_kv add_zero_attn kdim=kdim vdim=vdim mha eval Prepare qengine_is_onednn ` reduce_range ` False default ONEDNN backend test fails earlier CPUs without VNNI So we use default qconfig ` reduce_range=True ` here mha qconfig = torch ao quantization get_default_qconfig mha qconfig = torch ao quantization get_default_qconfig qengine mha_prepared = torch ao quantization prepare mha Calibrate y = mha_prepared fp_data y_ref = mha fp_data Check result prepare assertEqual y_ref y Attention assertEqual y_ref y Weight Quantize mha_quantized = torch ao quantization convert mha_prepared name _param mha_quantized named_parameters assertTrue in_proj_weight name qy = mha_quantized q_data Reference result mha layer = mha_quantized layer dequantize y_ref = mha fp_data snr = _snr y qy signal mse power snr assertTrue power min_power mse max_mse msg= f Error too high SNR dB power f Signal signal MSE mse f Run bias= bias f add_bias_kv= add_bias_kv f add_zero_attn= add_zero_attn Verify result scriptable mha_quantized_scripted = torch jit script mha_quantized skipIfNoONEDNN test_int _mul_onednn output_dtype_list = torch uint torch float torch bfloat torch half shape_list = cases = itertools product shape_list output_dtype_list shape output_dtype cases = torch randn shape b = torch randn shape s_a z_a = s_b z_b = output_dtype == torch uint s_c z_c = s_c z_c = qa = torch quantize_per_tensor s_a z_a torch quint qb = torch quantize_per_tensor b s_b z_b torch quint dqa = qa dequantize dqb = qb dequantize c_ref = dqa dqb output_dtype == torch uint c_ref = torch ops quantized_decomposed quantize_per_tensor default c_ref s_c z_c torch uint c_ref = c_ref output_dtype a_int = qa int_repr b_int = qb int_repr c = torch ops onednn qmul tensor a_int s_a z_a b_int s_b z_b s_c z_c output_dtype assertEqual c c_ref skipIfNoONEDNN given relu_fused=st booleans test_int _add_onednn relu_fused output_dtype_list = torch uint torch float torch bfloat torch half shape_list = cases = itertools product shape_list output_dtype_list shape output_dtype cases = torch randn shape b = torch randn shape s_a z_a = s_b z_b = output_dtype == torch uint s_c z_c = s_c z_c = qa = torch quantize_per_tensor s_a z_a torch quint qb = torch quantize_per_tensor b s_b z_b torch quint dqa = qa dequantize dqb = qb dequantize c_ref = dqa + dqb relu_fused c_ref = torch nn functional relu c_ref output_dtype == torch uint c_ref = torch ops quantized_decomposed quantize_per_tensor default c_ref s_c z_c torch uint c_ref = c_ref output_dtype a_int = qa int_repr b_int = qb int_repr relu_fused c = torch ops onednn qadd_relu tensor a_int s_a z_a b_int s_b z_b s_c z_c output_dtype c = torch ops onednn qadd tensor a_int s_a z_a b_int s_b z_b s_c z_c output_dtype assertEqual c c_ref skipIfNoONEDNN test_int _batch_norm_onednn hypothesis too slow test create test cases manually channel_len_list = output_dtype_list = torch uint torch float torch bfloat torch half x_scale x_zero_point = cases = itertools product channel_len_list output_dtype_list channels out_dtype cases shapes = channels y_scale y_zero_point = out_dtype == torch uint x = torch randn shapes dtype=torch float mean = torch rand channels float var = torch rand channels float weight = torch rand channels float bias = torch rand channels float eps = qx = torch ops quantized_decomposed quantize_per_tensor default x x_scale x_zero_point torch uint y = torch ops onednn qbatch_norm d qx x_scale x_zero_point weight bias mean var eps y_scale y_zero_point out_dtype dqx = torch ops quantized_decomposed dequantize_per_tensor default qx x_scale x_zero_point torch uint y_ref = F batch_norm dqx weight=weight bias=bias running_mean=mean running_var=var training=False momentum= eps=eps out_dtype == torch uint y_ref = torch ops quantized_decomposed quantize_per_tensor default y_ref y_scale y_zero_point torch uint y_ref = y_ref out_dtype assertEqual y y_ref msg=f y vs y_ref TestDynamicQuantizedOps TestCase Tests correctness dynamic quantized linear linear_relu op override_qengines given batch_size=st integers input_channels=st integers output_channels=st integers use_bias=st booleans use_relu=st booleans use_multi_dim_input=st booleans use_channelwise=st booleans reduce_range=st booleans test_qlinear batch_size input_channels output_channels use_bias use_relu use_multi_dim_input use_channelwise reduce_range torch backends quantized engine == qnnpack reduce_range = False qlinear_prepack = torch ops quantized linear_prepack use_relu qlinear_dynamic = torch ops quantized linear_relu_dynamic qlinear_dynamic = torch ops quantized linear_dynamic use_multi_dim_input batch_size = Test multi-dim input tensor X_scale = X_zp = X_value_min = X_value_max = reduce_range X_value_max = X_q = np round np random rand batch_size input_channels X_value_max - X_value_min + X_value_min astype np uint X_q = X_value_min X_q = X_value_max W_scale = W_zp = W_scales = np ones output_channels W_zps = np zeros output_channels astype int W_value_min = - W_value_max = W_q = np round np random rand output_channels input_channels W_value_max - W_value_min + W_value_min astype np int W_q = W_value_min W_q = W_value_max b_value_min = - b_value_max = b_q = np round np random rand output_channels b_value_max - b_value_min + b_value_min astype np int use_bias None torch backends quantized engine x fbgemm onednn avoid_vpmaddubsw_overflow_linear batch_size input_channels output_channels X_q X_value_min X_value_max W_q W_value_min W_value_max X_fp = torch from_numpy _dequantize X_q X_scale X_zp dtype=torch float use_multi_dim_input X_fp = X_fp view int batch_size input_channels W_scale W_zp = _calculate_dynamic_qparams W_fp torch qint We currently only check case where W_scale = W_zp = use_channelwise W_fp = torch from_numpy _dequantize W_q W_scales reshape - W_zps reshape - dtype=torch float W_q = torch quantize_per_channel W_fp scales=torch from_numpy W_scales zero_points=torch from_numpy W_zps axis= dtype=torch qint b_fp = torch from_numpy _dequantize b_q X_scale W_scales dtype=torch float use_bias None W_fp = torch from_numpy _dequantize W_q W_scales W_zps dtype=torch float W_q = torch quantize_per_tensor W_fp scale=W_scales zero_point= W_zps astype int item dtype=torch qint b_fp = torch from_numpy _dequantize b_q X_scale int W_scales item dtype=torch float use_bias None Observe X_fp determine X_scale X_zero_point should match internals dynamic linear X_scale X_zp = _calculate_dynamic_qparams X_fp torch quint reduce_range X_q = torch quantize_per_tensor X_fp scale=X_scale zero_point=X_zp dtype=torch quint Weight prepacking operator dynamic quantized Linear W_prepack = qlinear_prepack W_q b_fp Dynamic quantized Linear operator prepacked weight Y_fp = qlinear_dynamic X_q dequantize W_prepack reduce_range Y_fp = qlinear_dynamic X_fp W_prepack b_fp Y_fp _ref = F linear X_q dequantize W_q dequantize b_fp Y_fp _ref = F linear X_fp W_fp b_fp use_multi_dim_input Y_fp _ref = Y_fp _ref view int batch_size output_channels use_relu Y_fp _ref Y_fp _ref = assertEqual Y_fp Y_fp _ref msg= torch ops quantized linear_dynamic results off skipIfNoFBGEMM given batch_size=st integers input_channels=st integers output_channels=st integers test_qlinear_legacy batch_size input_channels output_channels X_scale = X_zp = X_value_min = X_value_max = X_q = np round np random rand batch_size input_channels X_value_max - X_value_min + X_value_min astype np uint X_q = X_value_min X_q = X_value_max W_scale = W_zp = W_value_min = - W_value_max = W_q = np round np random rand output_channels input_channels W_value_max - W_value_min + W_value_min astype np int W_q = W_value_min W_q = W_value_max b_value_min = - b_value_max = b_q = np round np random rand output_channels b_value_max - b_value_min + b_value_min astype np int avoid_vpmaddubsw_overflow_linear batch_size input_channels output_channels X_q X_value_min X_value_max W_q W_value_min W_value_max X_fp = torch from_numpy _dequantize X_q X_scale X_zp dtype=torch float W_fp = torch from_numpy _dequantize W_q W_scale W_zp dtype=torch float b_fp = torch from_numpy _dequantize b_q X_scale W_scale dtype=torch float W_scale W_zp = _calculate_dynamic_qparams W_fp torch qint W_q = torch quantize_per_tensor W_fp scale=W_scale zero_point=W_zp dtype=torch qint Observe X_fp determine X_scale X_zero_point should match internals dynamic linear X_scale X_zp = _calculate_dynamic_qparams X_fp torch quint X_q = torch quantize_per_tensor X_fp scale=X_scale zero_point=X_zp dtype=torch quint W_int col_offsets W_scale W_zp = torch fbgemm_linear_quantize_weight W_q dequantize W_prepack = torch fbgemm_pack_quantized_matrix W_int clone W_int size W_int size Quantized Linear operator prepacked weight Y_fp = torch fbgemm_linear_int _weight X_q dequantize W_q dequantize W_prepack col_offsets W_scale W_zp b_fp Y_fp _ref = F linear X_q dequantize W_q dequantize b_fp Y_fp _ref = F linear X_fp W_fp b_fp assertEqual Y_fp Y_fp _ref msg= torch ops quantized fbgemm_linear_dynamic results off skipIfNoFBGEMM given input_channels=st integers output_channels=st integers exponent=st integers test_linear_prepack_fp _numerics input_channels output_channels exponent w = torch randn output_channels input_channels exponent bias = None w_packed_fp = torch ops quantized linear_prepack_fp w bias w_unpacked_fp = torch ops quantized linear_unpack_fp w_packed_fp w_fp = w torch float torch float assertTrue torch equal w_fp w_unpacked_fp skipIfNoFBGEMM test_qlinear_dynamic_fp options = itertools product batch_size input_channels output_channels True False use_bias True False use_relu batch_size input_channels output_channels use_bias use_relu options qlinear_prepack = torch ops quantized linear_prepack_fp use_relu qlinear_dynamic = torch ops quantized linear_relu_dynamic_fp qlinear_dynamic = torch ops quantized linear_dynamic_fp x = torch randn batch_size input_channels w = torch randn output_channels input_channels bias = torch randn output_channels use_bias None w_packed = qlinear_prepack w bias out = qlinear_dynamic x w_packed qlinear_dynamic_fp uses FP activation tensors FP weight tensors output FP w_fp = w torch float torch float ref = F linear x w_fp bias use_relu ref relu_ assertEqual out ref skipIfNoFBGEMM test_unpacked_qlinear_dynamic_fp options = itertools product batch_size input_channels output_channels batch_size input_channels output_channels options qlinear_dynamic = torch ops quantized linear_dynamic_fp _unpacked_weight x = torch randn batch_size input_channels w = torch randn output_channels input_channels bias = torch randn output_channels out = qlinear_dynamic x w bias qlinear_dynamic_fp uses FP activation tensors FP weight tensors output FP w_fp = w torch float torch float ref = F linear x w_fp bias assertEqual out ref skipIfNoFBGEMM test_unpacked_qlinear_dynamic_fp _opcheck qlinear_dynamic = torch ops quantized linear_dynamic_fp _unpacked_weight default x = torch randn device= cpu w = torch randn device= cpu bias = torch randn device= cpu opcheck qlinear_dynamic x w bias skipIfNoFBGEMM test_wrapped_fbgemm_linear_fp options = itertools product batch_size input_channels output_channels True False bias None batch_size input_channels output_channels bias_is_none options pack_op = torch ops _quantized wrapped_fbgemm_pack_gemm_matrix_fp linear_op = torch ops _quantized wrapped_fbgemm_linear_fp _weight x = torch randn batch_size input_channels w = torch randn output_channels input_channels bias = torch randn output_channels bias_is_none None w_packed = pack_op w out = linear_op x w_packed bias output_channels w_fp = w torch float torch float ref = F linear x w_fp bias assertEqual out ref skipIfNoFBGEMM test_wrapped_fbgemm_pack_gemm_matrix_fp _pt _compliant We using opcheck over here because output op we re testing _quantized wrapped_fbgemm_pack_gemm_matrix_fp deterministic due C-struct s producing This would fail check when we re trying match result between compiled eager version This only temporary solution long term we should able support PT torchbind natively func X W B packed_W = torch ops _quantized wrapped_fbgemm_pack_gemm_matrix_fp W torch ops _quantized wrapped_fbgemm_linear_fp _weight X packed_W B W size x = torch randn device= cpu w = torch randn device= cpu b = torch zeros device= cpu ref_out = func x w b compiled = torch compile func compiled_out = compiled x w b assertEqual ref_out compiled_out func X W packed_W = torch ops _quantized wrapped_fbgemm_pack_gemm_matrix_fp W torch ops _quantized wrapped_fbgemm_linear_fp _weight X packed_W None W size ref_out = func x w compiled = torch compile func compiled_out = compiled x w assertEqual ref_out compiled_out Tests correctness dynamic quantized lstm gru _get_rnn_inputs seq_len num_batches input_size hidden_size num_directions reduce_range For Input seq_len batch input_size X = torch randn seq_len num_batches input_size s z = _calculate_dynamic_qparams X torch quint reduce_range Xq = torch quantize_per_tensor X s z torch quint For H C num_layers num_directions batch hidden_size num_directions == H = torch randn num_directions num_batches hidden_size C = torch randn num_directions num_batches hidden_size H = torch zeros num_directions num_batches hidden_size C = torch zeros num_directions num_batches hidden_size s z = _calculate_dynamic_qparams H torch quint reduce_range Hq = torch quantize_per_tensor H s z torch quint s z = _calculate_dynamic_qparams C torch quint reduce_range Cq = torch quantize_per_tensor C s z torch quint Xq Hq Cq _get_rnn_weights_and_bias input_size hidden_size num_directions per_channel_quant rnn_type hidden_mult_map = LSTM LSTMCell GRU GRUCell RNNTanh RNNReLU hidden_mult = hidden_mult_map rnn_type weights = torch randn hidden_mult hidden_size input_size weights = torch randn hidden_mult hidden_size hidden_size scale = torch ones weights size scale = torch ones weights size zero_point = torch zeros scale size int zero_point = torch zeros scale size int b = torch zeros hidden_mult hidden_size per_channel_quant Wq = torch quantize_per_channel weights scale zero_point torch qint Wq = torch quantize_per_channel weights scale zero_point torch qint Wq = torch quantize_per_tensor weights float scale int zero_point torch qint Wq = torch quantize_per_tensor weights float scale int zero_point torch qint Wq Wq b b given num_batches=st integers input_size=st integers hidden_size=st integers num_directions=st integers per_channel_quant=st booleans override_qengines test_qlstmGRU num_batches input_size hidden_size num_directions per_channel_quant We test only seq length num layers dynamic quantization occurs multiple times within LSTM op we do model quantization between multiple calls linear op within lstm op seq_len = rnn_type LSTM GRU dtype torch qint torch float Fp quantization supported qnnpack onednn torch backends quantized engine qnnpack onednn dtype == torch float continue torch backends quantized engine == qnnpack reduce_range = False reduce_range = True Xq Hq Cq = _get_rnn_inputs seq_len num_batches input_size hidden_size num_directions reduce_range Wq Wq b b = _get_rnn_weights_and_bias input_size hidden_size num_directions per_channel_quant rnn_type dtype == torch qint packed_ih = torch ops quantized linear_prepack Wq b packed_hh = torch ops quantized linear_prepack Wq b cell_params = torch ops quantized make_quantized_cell_params_dynamic packed_ih packed_hh b b reduce_range W_ref = Wq dequantize W_ref = Wq dequantize packed_ih = torch ops quantized linear_prepack_fp Wq dequantize b packed_hh = torch ops quantized linear_prepack_fp Wq dequantize b cell_params = torch ops quantized make_quantized_cell_params_fp packed_ih packed_hh W_ref = Wq dequantize torch float torch float W_ref = Wq dequantize torch float torch float rnn_type == LSTM num_directions result_ref = _VF lstm Xq dequantize Hq dequantize Cq dequantize W_ref W_ref b b W_ref W_ref b b True False num_directions False result_dynamic = torch quantized_lstm Xq dequantize Hq dequantize Cq dequantize cell_params cell_params True False True False dtype=torch qint use_dynamic=True result_ref = _VF lstm Xq dequantize Hq dequantize Cq dequantize W_ref W_ref b b True False num_directions False result_dynamic = torch quantized_lstm Xq dequantize Hq dequantize Cq dequantize cell_params True False num_directions False dtype=torch qint use_dynamic=True rnn_type == GRU num_directions result_ref = _VF gru Xq dequantize Hq dequantize W_ref W_ref b b W_ref W_ref b b True False True False result_dynamic = torch quantized_gru Xq dequantize Hq dequantize cell_params cell_params True False True False result_ref = _VF gru Xq dequantize Hq dequantize W_ref W_ref b b True False False False result_dynamic = torch quantized_gru Xq dequantize Hq dequantize cell_params True False False False assertEqual result_ref result_dynamic msg= torch quantized_lstm results off given num_batches=st integers input_size=st integers hidden_size=st integers per_channel_quant=st booleans override_qengines test_qrnncell num_batches input_size hidden_size per_channel_quant We test only seq length num layers dynamic quantization occurs multiple times within LSTM op we do model quantization between multiple calls linear op within lstm op seq_len = rnn_type LSTMCell GRUCell RNNTanh RNNReLU dtype torch qint torch float Fp quantization supported qnnpack onednn torch backends quantized engine qnnpack onednn dtype == torch float continue torch backends quantized engine == qnnpack reduce_range = False reduce_range = True Xq Hq Cq = _get_rnn_inputs seq_len num_batches input_size hidden_size reduce_range Wq Wq b b = _get_rnn_weights_and_bias input_size hidden_size per_channel_quant rnn_type dtype == torch qint packed_ih = torch ops quantized linear_prepack Wq b packed_hh = torch ops quantized linear_prepack Wq b W_ref = Wq dequantize W_ref = Wq dequantize packed_ih = torch ops quantized linear_prepack_fp Wq dequantize b packed_hh = torch ops quantized linear_prepack_fp Wq dequantize b W_ref = Wq dequantize torch float torch float W_ref = Wq dequantize torch float torch float state = LSTMCell Hq dequantize Cq dequantize GRUCell Hq dequantize RNNTanh Hq dequantize RNNReLU Hq dequantize fn_dict = LSTMCell torch _VF lstm_cell GRUCell torch _VF gru_cell RNNTanh torch _VF rnn_tanh_cell RNNReLU torch _VF rnn_relu_cell qfn_dict = LSTMCell torch ops quantized quantized_lstm_cell_dynamic GRUCell torch ops quantized quantized_gru_cell_dynamic RNNTanh torch ops quantized quantized_rnn_tanh_cell_dynamic RNNReLU torch ops quantized quantized_rnn_relu_cell_dynamic W_ref_dict = torch float Wq dequantize torch float torch float Wq dequantize torch float torch float torch qint Wq dequantize Wq dequantize result_ref = fn_dict rnn_type Xq dequantize state rnn_type W_ref W_ref b b result_dynamic = qfn_dict rnn_type Xq dequantize state rnn_type packed_ih packed_hh b b assertEqual result_ref result_dynamic msg= torch quantized_rnncell results off _test_qconv_op_impl q_mod dq_op dim dtype The goal here show dynamic op same calc params- quantize_input- quantized op- dequantize output qengine_is_qnnpack IS_PPC supported QNNPACK qengine_is_qnnpack reduce_range = False reduce_range = True X_fp = torch randn dim s z = _calculate_dynamic_qparams X_fp dtype reduce_range quantized_module = q_mod packed_params = quantized_module _packed_params quantized_module scale quantized_module zero_point = s z X_q = torch quantize_per_tensor X_fp s z dtype Y_q_ref = quantized_module X_q Y_ref = torch dequantize Y_q_ref X_dq = torch dequantize X_q Y = dq_op X_dq packed_params reduce_range assertEqual Y Y_ref override_qengines test_dynamic_conv d q_mod = torch ao nn quantized Conv d dq_op = torch ops quantized conv d_dynamic dim = dtype = torch quint _test_qconv_op_impl q_mod dq_op dim dtype override_qengines test_dynamic_conv d q_mod = torch ao nn quantized Conv d dq_op = torch ops quantized conv d_dynamic dim = dtype = torch quint _test_qconv_op_impl q_mod dq_op dim dtype override_qengines test_dynamic_conv d q_mod = torch ao nn quantized Conv d dq_op = torch ops quantized conv d_dynamic dim = dtype = torch quint _test_qconv_op_impl q_mod dq_op dim dtype override_qengines test_dynamic_convtranspose d q_mod = torch ao nn quantized ConvTranspose d dq_op = torch ops quantized conv_transpose d_dynamic dim = dtype = torch quint _test_qconv_op_impl q_mod dq_op dim dtype override_qengines test_dynamic_convtranspose d q_mod = torch ao nn quantized ConvTranspose d dq_op = torch ops quantized conv_transpose d_dynamic dim = dtype = torch quint _test_qconv_op_impl q_mod dq_op dim dtype override_qengines test_dynamic_convtranspose d q_mod = torch ao nn quantized ConvTranspose d dq_op = torch ops quantized conv_transpose d_dynamic dim = dtype = torch quint qengine_is_qnnpack TODO fix MakeDeConvOutputShape overflowing convT d qnnpack _test_qconv_op_impl q_mod dq_op dim dtype skipIfNoONEDNN test_linear_dynamic_fp _onednn options = itertools product batch_size input_channels output_channels True False use_bias True False use_relu batch_size input_channels output_channels use_bias use_relu options qlinear_prepack = torch ops onednn linear_prepack_fp use_relu qlinear_dynamic = torch ops onednn linear_relu_dynamic_fp qlinear_dynamic = torch ops onednn linear_dynamic_fp x = torch randn batch_size input_channels w = torch randn output_channels input_channels bias = torch randn output_channels use_bias None w_packed = qlinear_prepack w x shape out = qlinear_dynamic x w_packed bias qlinear_dynamic_fp uses FP activation tensors FP weight tensors output FP w_fp = w torch float torch float ref = F linear x w_fp bias use_relu ref relu_ assertEqual out ref TestQuantizedLinear TestCase _test_qlinear_impl batch_size input_channels output_channels use_bias post_op use_multi_dim_input use_channelwise post_op_kwargs decimal_val = dtypes = torch quint torch backends quantized engine == qnnpack QNNPACK supports uint kernels In op we shift int weight values uint par fbgemm However causes some rounding issues rare cases So we relax check allow off one results decimal_val = only qnnpack qengine supports qint when xnnpack available torch backends xnnpack enabled dtypes append torch qint qengine_is_onednn IS_ARM dtypes append torch qint dtype dtypes No support channelwise xnnpack int dtype == torch qint use_channelwise nptype = np_dtype dtype qlinear_prepack = torch ops quantized linear_prepack post_op == relu qlinear = torch ops quantized linear_relu post_op == leaky_relu qlinear = torch ops quantized linear_leaky_relu qlinear = torch ops quantized linear use_multi_dim_input batch_size = Test multi-dim input tensor X_scale = X_zp = X_value_min = - dtype == torch qint X_value_max = dtype == torch qint X_q = np round np random rand batch_size input_channels X_value_max - X_value_min + X_value_min astype nptype W_scales = np random rand output_channels xnnpack forces W_zp when using symmetric quantization ONEDNN only supports symmetric quantization weight dtype == torch qint qengine_is_onednn W_zps = np zeros output_channels astype int W_zps = np round np random rand output_channels - astype int when using symmetric quantization special restriction xnnpack fully connected op weight - instead - W_value_min = - dtype == torch qint - W_value_max = W_q = np round np random rand output_channels input_channels W_value_max - W_value_min + W_value_min astype np int weight always int _t b_value_min = - b_value_max = b_q = np round np random rand output_channels b_value_max - b_value_min + b_value_min astype np int use_bias None torch backends quantized engine x fbgemm onednn IS_ARM avoid_vpmaddubsw_overflow_linear batch_size input_channels output_channels X_q X_value_min X_value_max W_q W_value_min W_value_max X = torch from_numpy _dequantize X_q X_scale X_zp dtype=torch float X_q = torch quantize_per_tensor X scale=X_scale zero_point=X_zp dtype=dtype use_channelwise W = torch from_numpy _dequantize W_q W_scales reshape - W_zps reshape - dtype=torch float W_q = torch quantize_per_channel W scales=torch from_numpy W_scales zero_points=torch from_numpy W_zps axis= dtype=torch qint b = torch from_numpy _dequantize b_q X_scale W_scales dtype=torch float use_bias None b_q = torch quantize_per_channel b scales=torch from_numpy X_scale W_scales zero_points=torch zeros output_channels dtype=torch long axis= dtype=torch qint use_bias None W = torch from_numpy _dequantize W_q W_scales W_zps dtype=torch float W_q = torch quantize_per_tensor W scale=W_scales zero_point= W_zps astype int item dtype=torch qint b = torch from_numpy _dequantize b_q X_scale W_scales item dtype=torch float use_bias None b_q = torch quantize_per_tensor b scale=X_scale W_scales item zero_point= dtype=torch qint use_bias None Compare X_scale W_scale input_channels X_value_max W_value_max Y_scale max uint Y_scale = Y_zp = Weight prepacking operator quantized Linear float_bias = b use_bias None W_prepack = qlinear_prepack W_q float_bias use_multi_dim_input X_q = X_q view int batch_size input_channels Quantized Linear operator prepacked weight Y_q = qlinear X_q W_prepack Y_scale Y_zp post_op_kwargs use_channelwise post_op none relu Test per-tensor quantization only Reference quantized Linear operator Y_q_ref = qlinear_ref X_q X_scale X_zp W_q W_scales W_zps b_q Y_scale Y_zp dtype=nptype post_op == relu Y_q_ref Y_q_ref Y_zp = Y_zp use_multi_dim_input Y_q_ref = np reshape Y_q_ref int batch_size output_channels Assert equal np testing assert_array_almost_equal Y_q_ref Y_q int_repr numpy decimal=decimal_val Test both per-tensor per-channel quantization Reference quantized result PyTorch Linear operator W_fp = W_q dequantize dtype=torch float X_fp = X_q dequantize dtype=torch float b_fp = b_q dequantize dtype=torch float use_bias None Y_fp _ref = F linear X_fp W_fp b_fp post_op == relu Y_fp _ref Y_fp _ref = post_op == leaky_relu Y_fp _ref = F leaky_relu Y_fp _ref post_op_kwargs Y_q_ref = torch quantize_per_tensor Y_fp _ref Y_scale Y_zp dtype Assert equal np testing assert_array_almost_equal Y_q_ref int_repr numpy Y_q int_repr numpy decimal=decimal_val Tests correctness quantized linear op override_qengines test_qlinear batch_size_list = input_channels_list = output_channels_list = use_bias_list = True False use_multi_dim_input_list = True False use_channelwise_list = True False post_op = none cases = itertools product batch_size_list input_channels_list output_channels_list use_bias_list use_multi_dim_input_list use_channelwise_list batch_size input_channels output_channels use_bias \ use_multi_dim_input use_channelwise cases _test_qlinear_impl batch_size input_channels output_channels use_bias post_op use_multi_dim_input use_channelwise Tests correctness quantized linear_relu op override_qengines test_qlinear_relu batch_size_list = input_channels_list = output_channels_list = use_bias_list = True False use_multi_dim_input_list = True False use_channelwise_list = True False post_op = relu cases = itertools product batch_size_list input_channels_list output_channels_list use_bias_list use_multi_dim_input_list use_channelwise_list batch_size input_channels output_channels use_bias \ use_multi_dim_input use_channelwise cases _test_qlinear_impl batch_size input_channels output_channels use_bias post_op use_multi_dim_input use_channelwise given batch_size=st integers input_channels=st integers output_channels=st integers use_bias=st booleans use_relu=st booleans use_multi_dim_input=st booleans use_channelwise=st booleans skipIfNoFBGEMM test_qlinear_with_input_q_dq_qweight_dq_output_fp batch_size input_channels output_channels use_bias use_relu use_multi_dim_input use_channelwise decimal_val = dtypes = torch quint dtype dtypes No support channelwise xnnpack int ONEDNN does support qint dtype == torch qint use_channelwise qengine_is_onednn nptype = np_dtype dtype qlinear_prepack = torch ops quantized linear_prepack use_relu qlinear = torch ops quantized linear_with_input_q_dq_qweight_dq_relu_output_fp qlinear = torch ops quantized linear_with_input_q_dq_qweight_dq_output_fp use_multi_dim_input batch_size = Test multi-dim input tensor X_scale = X_zp = X_value_min = - dtype == torch qint X_value_max = dtype == torch qint X_q = np round np random rand batch_size input_channels X_value_max - X_value_min + X_value_min astype nptype W_scales = np random rand output_channels xnnpack forces W_zp when using symmetric quantization ONEDNN only supports symmetric quantization weight dtype == torch qint qengine_is_onednn W_zps = np zeros output_channels astype int W_zps = np round np random rand output_channels - astype int when using symmetric quantization special restriction xnnpack fully connected op weight - instead - W_value_min = - dtype == torch qint - W_value_max = W_q = np round np random rand output_channels input_channels W_value_max - W_value_min + W_value_min astype np int weight always int _t b_value_min = - b_value_max = b_q = np round np random rand output_channels b_value_max - b_value_min + b_value_min astype np int use_bias None torch backends quantized engine x fbgemm onednn avoid_vpmaddubsw_overflow_linear batch_size input_channels output_channels X_q X_value_min X_value_max W_q W_value_min W_value_max X = torch from_numpy _dequantize X_q X_scale X_zp dtype=torch float X_q = torch quantize_per_tensor X scale=X_scale zero_point=X_zp dtype=dtype use_channelwise W = torch from_numpy _dequantize W_q W_scales reshape - W_zps reshape - dtype=torch float W_q = torch quantize_per_channel W scales=torch from_numpy W_scales zero_points=torch from_numpy W_zps axis= dtype=torch qint b = torch from_numpy _dequantize b_q X_scale W_scales dtype=torch float use_bias None b_q = torch quantize_per_channel b scales=torch from_numpy X_scale W_scales zero_points=torch zeros output_channels dtype=torch long axis= dtype=torch qint use_bias None W = torch from_numpy _dequantize W_q W_scales W_zps dtype=torch float W_q = torch quantize_per_tensor W scale=W_scales zero_point= W_zps astype int item dtype=torch qint b = torch from_numpy _dequantize b_q X_scale W_scales item dtype=torch float use_bias None b_q = torch quantize_per_tensor b scale=X_scale W_scales item zero_point= dtype=torch qint use_bias None Compare X_scale W_scale input_channels X_value_max W_value_max Y_scale max uint Y_scale = Y_zp = Weight prepacking operator quantized Linear float_bias = b use_bias None W_prepack = qlinear_prepack W_q float_bias use_multi_dim_input X = X view int batch_size input_channels X_q = X_q view int batch_size input_channels Quantized Linear operator prepacked weight Y_q_dq = qlinear X X_scale X_zp W_prepack Test both per-tensor per-channel quantization Reference quantized result PyTorch Linear operator W_fp = W_q dequantize dtype=torch float X_fp = X_q dequantize dtype=torch float b_fp = b_q dequantize dtype=torch float use_bias None Y_fp _ref = F linear X_fp W_fp b_fp use_relu Y_fp _ref Y_fp _ref = decimal_val = np testing assert_array_almost_equal Y_fp _ref numpy Y_q_dq numpy decimal=decimal_val given batch_size=st integers cudnn v there limitation input channels should multiple int tensors cudnn v should multiple input_channels=st sampled_from constraints output channels appear relax seems we can use any positive integer here except It clear why will work TODO check Yang output_channels=st integers use_bias=st booleans use_relu=st booleans use_multi_dim_input=st booleans use_channelwise=st sampled_from False channelwise currently supported qlinear cudnn skipIfNoFBGEMM unittest skipIf TEST_CUDNN cudnn enabled unittest skipIf TEST_CUDNN torch backends cudnn version == expected failure cuDNN unittest skipIf SM OrLater requires sm later unittest skipIf TEST_ROCM supported rocm TODO check yang regarding CUDNN flags unittest skip currently working feature isn t used test_qlinear_cudnn batch_size input_channels output_channels use_bias use_relu use_multi_dim_input use_channelwise qlinear_prepack = torch ops quantized linear_prepack use_relu qlinear_op = torch ops quantized linear_relu qlinear_op = torch ops quantized linear X_scale = X_zp = X_value_min = - X_value_max = X_q = np round np random rand batch_size input_channels X_value_max - X_value_min + X_value_min astype np int W_scale = W_zp = W_value_min = - W_value_max = W_q = np round np random rand output_channels input_channels W_value_max - W_value_min + W_value_min astype np int b_value_min = - b_value_max = b_q = np round np random rand output_channels b_value_max - b_value_min + b_value_min astype np int use_bias None use_bias b_value_min = - b_value_max = b_q = np round np random rand output_channels b_value_max - b_value_min + b_value_min astype np int bias = None avoid_vpmaddubsw_overflow_linear batch_size input_channels output_channels X_q X_value_min X_value_max W_q W_value_min W_value_max quant_dtype = torch qint X = torch from_numpy _dequantize X_q X_scale X_zp dtype=torch float device= cuda X_q = torch quantize_per_tensor X scale=X_scale zero_point=X_zp dtype=quant_dtype W = torch from_numpy _dequantize W_q W_scale W_zp dtype=torch float device= cuda W_q = torch quantize_per_tensor W scale=W_scale zero_point=W_zp dtype=quant_dtype b = torch from_numpy _dequantize b_q X_scale W_zp dtype=torch float device= cuda use_bias None b_q = torch quantize_per_tensor b scale=X_scale W_scale zero_point= dtype=quant_dtype use_bias None Y_scale = Y_zp = Weight prepacking operator quantized Linear float_bias = b use_bias None W_prepack = qlinear_prepack W_q float_bias use_bias None Quantized Linear operator prepacked weight Y_q = qlinear_op X_q W_prepack Y_scale Y_zp device= cpu Y_q_ref = qlinear_ref X_q X_scale X_zp W_q W_scale W_zp b_q Y_scale Y_zp dtype=np int use_relu Y_q_ref Y_q_ref Y_zp = Y_zp decimal_val = np testing assert_array_almost_equal Y_q_ref Y_q int_repr numpy decimal=decimal_val Tests correctness quantized linear_unpack op given W=hu tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch qint use_channelwise=st booleans override_qengines test_qlinear_unpack W use_channelwise W W_scale W_zp torch_type = W use_channelwise output_channels = W shape W_scales = torch rand output_channels torch double W_zps = torch round torch rand output_channels - torch int qlinear_prepack = torch ops quantized linear_prepack qlinear_unpack = torch ops quantized linear_unpack ONEDNN only supports symmetric quantization weight qengine_is_onednn use_channelwise W_zps = torch zeros output_channels torch int W_zp = W = torch from_numpy W use_channelwise W_q = torch quantize_per_channel W W_scales W_zps dtype=torch_type W_q = torch quantize_per_tensor W scale=W_scale zero_point=W_zp dtype=torch_type Weight prepacking operator quantized Linear W_prepack = qlinear_prepack W_q Weight unpack operator quantized Linear Used serialization W_q_origin = qlinear_unpack W_prepack Assert equal np testing assert_equal W_q int_repr W_q_origin int_repr numpy use_channelwise np testing assert_array_almost_equal np float W_q q_per_channel_scales numpy np float W_q_origin q_per_channel_scales numpy decimal= np testing assert_equal W_q q_per_channel_zero_points numpy W_q_origin q_per_channel_zero_points numpy np testing assert_equal np float W_q q_scale np float W_q_origin q_scale np testing assert_equal W_q q_zero_point W_q_origin q_zero_point Tests correctness _quantized wrapped_quantized_linear op skipIfNoFBGEMM given m=st integers k=st integers n=st integers test_wrapped_quantized_linear m n k input = torch randn m k dtype=torch float input_scale = torch tensor input_zero_point = torch tensor weight = torch randn n k dtype=torch float weight_scale = torch tensor weight_zero_point = torch tensor bias = torch randn n dtype=torch float output_scale = torch tensor output_zero_point = torch tensor out_channel = n ret = torch ops _quantized wrapped_quantized_linear input input_scale input_zero_point weight weight_scale weight_zero_point bias output_scale output_zero_point out_channel qinput = torch quantize_per_tensor input input_scale input_zero_point torch quint qweight = torch quantize_per_tensor weight weight_scale weight_zero_point torch qint qlinear_prepack = torch ops quantized linear_prepack qweight bias qlinear = torch ops quantized linear qinput qlinear_prepack output_scale output_zero_point ret_ref = qlinear dequantize assertEqual ret ret_ref Tests correctness _quantized _wrapped_linear_prepack _quantized _wrapped_quantized_linear_prepacked ops skipIfNoFBGEMM given m=st integers k=st integers n=st integers test_wrapped_quantized_linear_prepacked m n k input = torch randn m k dtype=torch float input_scale = torch tensor input_zero_point = torch tensor weight = torch randn n k dtype=torch float weight_scale = torch tensor weight_zero_point = torch tensor bias = torch randn n dtype=torch float output_scale = torch tensor output_zero_point = torch tensor out_channel = n ret_ = torch ops _quantized _wrapped_linear_prepack weight weight_scale weight_zero_point bias ret_ = torch ops _quantized _wrapped_quantized_linear_prepacked input input_scale input_zero_point ret_ output_scale output_zero_point out_channel qinput = torch quantize_per_tensor input input_scale input_zero_point torch quint qweight = torch quantize_per_tensor weight weight_scale weight_zero_point torch qint qlinear_prepack = torch ops quantized linear_prepack qweight bias qlinear = torch ops quantized linear qinput qlinear_prepack output_scale output_zero_point ret_ref = qlinear dequantize assertEqual ret_ ret_ref Tests correctness quantized linear_unpack after freeing original tensor op skipIfNoQNNPACK given W=hu tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch qint override_qengines test_qlinear_qnnpack_free_memory_and_unpack W assert qengine_is_qnnpack W W_scale W_zp torch_type = W qlinear_prepack = torch ops quantized linear_prepack qlinear_unpack = torch ops quantized linear_unpack W = torch from_numpy W ONEDNN only supports symmetric quantization weight qengine_is_onednn W_zp = W_q = torch quantize_per_tensor W scale=W_scale zero_point=W_zp dtype=torch_type Weight prepacking operator quantized Linear W_prepack = qlinear_prepack W_q dummy_input = torch randn W shape Make sure we free original tensor running matrix multiplication backend torch ops quantized linear_dynamic dummy_input W_prepack torch ops quantized linear_dynamic dummy_input W_prepack At step original tensor should recovered data_ptr W_q_origin = qlinear_unpack W_prepack Assert equal np testing assert_equal W_q int_repr W_q_origin int_repr numpy np testing assert_equal np float W_q q_scale np float W_q_origin q_scale np testing assert_equal W_q q_zero_point W_q_origin q_zero_point skipIfNoONEDNN test_qlinear_leaky_relu override_quantized_engine onednn batch_size_list = input_channels_list = output_channels_list = use_bias_list = True False use_multi_dim_input_list = True False use_channelwise_list = True False negative_slopes_list = post_op = leaky_relu cases = itertools product batch_size_list input_channels_list output_channels_list use_bias_list use_multi_dim_input_list use_channelwise_list negative_slopes_list batch_size input_channels output_channels use_bias \ use_multi_dim_input use_channelwise neg_slope cases _test_qlinear_impl batch_size input_channels output_channels use_bias post_op use_multi_dim_input use_channelwise negative_slope=neg_slope skipIfNoONEDNN test_qlinear_tanh override_quantized_engine onednn batch_size_list = input_channels_list = output_channels_list = use_bias_list = True False use_multi_dim_input_list = True False use_channelwise_list = True False post_op = tanh cases = itertools product batch_size_list input_channels_list output_channels_list use_bias_list use_multi_dim_input_list use_channelwise_list batch_size input_channels output_channels use_bias \ use_multi_dim_input use_channelwise cases _test_qlinear_impl batch_size input_channels output_channels use_bias post_op use_multi_dim_input use_channelwise _test_qlinear_pt e_helper qlinear_op post_op= none unary_post_op_args= post_op_algorithms= none qlinear_prepack = torch ops onednn qlinear_prepack linear_op = F linear in_channels_list = out_channels_list = batch_size = use_bias_list = True False weight_quant_per_channel_list = True False output_dtype_list = None torch float torch bfloat x_scale x_zp = w_scale w_zp = y_scale y_zp = input_dim_list = cases = itertools product in_channels_list out_channels_list use_bias_list weight_quant_per_channel_list output_dtype_list post_op_algorithms input_dim_list override_quantized_engine onednn ic oc use_bias weight_quant_per_channel output_dtype post_op_algo input_dim cases used_y_scale = y_scale used_y_zp = y_zp fp _out = output_dtype == torch float bfloat _out = output_dtype == torch bfloat fp _out bfloat _out used_y_scale used_y_zp = x _scale x _zp = x _scale x _zp = x = torch rand batch_size ic + ic input_dim == torch rand batch_size ic w = torch rand oc ic qx = torch quantize_per_tensor x x_scale x_zp torch quint weight_quant_per_channel w_scales = torch Tensor w_scale oc w_zps = torch zeros oc dtype=torch int qw = torch quantize_per_channel w w_scales w_zps torch qint w_scales = torch Tensor w_scale w_zps = torch Tensor w_zp dtype=torch int qw = torch quantize_per_tensor w w_scale w_zp torch qint use_bias b = torch rand oc b = None x_ref = qx dequantize w_ref = qw dequantize y_ref = linear_op x_ref w_ref b compute CPU tensors qx_cpu = qx int_repr qw_cpu = qw int_repr qw_packed = qlinear_prepack qw_cpu x shape post_op none relu gelu qy_cpu = qlinear_op qx_cpu x_scale x_zp qw_packed w_scales w_zps b used_y_scale used_y_zp output_dtype post_op unary_post_op_args post_op_algo post_op == relu y_ref = F relu y_ref post_op == gelu y_ref = F gelu y_ref approximate=post_op_algo qy_ref = torch quantize_per_tensor y_ref used_y_scale used_y_zp torch quint post_op sum sum_relu x _int = torch randint y_ref size x = x _scale x _int - x _zp float qx = torch quantize_per_tensor x scale=x _scale zero_point=x _zp dtype=torch quint unary_post_op = relu post_op == sum_relu none binary_alpha = we only support alpha= now accum = qx int_repr output_dtype None qx dequantize bfloat _out accum = accum bfloat qy_cpu = qlinear_op qx_cpu x_scale x_zp qw_packed w_scales w_zps accum b used_y_scale used_y_zp output_dtype x _scale x _zp sum binary_alpha unary_post_op unary_post_op_args post_op_algo y_ref = y_ref + x binary_alpha unary_post_op == relu y_ref = F relu y_ref qy_ref = torch quantize_per_tensor y_ref used_y_scale used_y_zp torch quint post_op add add_relu used_y_scale used_y_zp = output_dtype None Only support int output continue x = torch randn y_ref size unary_post_op = relu post_op == add_relu none binary_alpha = we only support alpha= now qy_cpu = qlinear_op qx_cpu x_scale x_zp qw_packed w_scales w_zps x b used_y_scale used_y_zp output_dtype add binary_alpha unary_post_op unary_post_op_args post_op_algo y_ref = y_ref + x binary_alpha unary_post_op == relu y_ref = F relu y_ref qy_ref = torch quantize_per_tensor y_ref used_y_scale used_y_zp torch quint Compare results fp _out bfloat _out qy_cpu = torch quantize_per_tensor qy_cpu torch float used_y_scale used_y_zp dtype=torch quint int_repr assertEqual x dim qy_cpu dim np testing assert_array_almost_equal qy_ref int_repr cpu numpy qy_cpu cpu numpy decimal= err_msg=f X x W w b b x_s x_scale x_zp x_zp w_s w_scale w_zp w_zp y_s y_scale y_zp y_zp unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_pt e qlinear = torch ops onednn qlinear_pointwise _test_qlinear_pt e_helper qlinear none unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_relu_pt e qlinear = torch ops onednn qlinear_pointwise _test_qlinear_pt e_helper qlinear relu unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_gelu_pt e qlinear = torch ops onednn qlinear_pointwise post_op_algorithms = none tanh _test_qlinear_pt e_helper qlinear gelu post_op_algorithms=post_op_algorithms unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_sum_pt e qlinear = torch ops onednn qlinear_pointwise binary _test_qlinear_pt e_helper qlinear sum unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_sum_relu_pt e qlinear = torch ops onednn qlinear_pointwise binary _test_qlinear_pt e_helper qlinear sum_relu unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_add_pt e qlinear = torch ops onednn qlinear_pointwise binary _test_qlinear_pt e_helper qlinear add unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_add_relu_pt e qlinear = torch ops onednn qlinear_pointwise binary _test_qlinear_pt e_helper qlinear add_relu _test_qlinear_fp _helper qlinear_op post_op= none unary_post_op_args= post_op_algorithms= none qlinear_prepack = torch ops onednn qlinear_prepack linear_op = F linear in_channels_list = out_channels_list = batch_size = use_bias_list = True False weight_quant_per_channel_list = True False output_dtype_list = None torch float torch bfloat y_scale y_zp = input_dim_list = cases = itertools product in_channels_list out_channels_list use_bias_list weight_quant_per_channel_list output_dtype_list post_op_algorithms input_dim_list override_quantized_engine onednn ic oc use_bias weight_quant_per_channel output_dtype post_op_algo input_dim cases used_y_scale = y_scale used_y_zp = y_zp fp _out = output_dtype == torch float bfloat _out = output_dtype == torch bfloat fp _out bfloat _out used_y_scale = x _scale x _zp = x _scale x _zp = x = torch rand batch_size ic + ic input_dim == torch rand batch_size ic w = torch rand oc ic qx x_scale = _quantize_fp e m x channelwise=False qw w_scales = _quantize_fp e m w channelwise=weight_quant_per_channel use_bias b = torch rand oc bfloat _out b = b torch bfloat b = None compute reference result x_ref = _dequantize_fp e m qx x_scale w_ref = _dequantize_fp e m qw w_scales b None y_ref = linear_op x_ref w_ref b torch float y_ref = linear_op x_ref w_ref compute fp linear qw_packed = qlinear_prepack qw x shape x_zp = w_zps = torch zeros_like w_scales dtype=torch int post_op none relu gelu qy = qlinear_op qx x_scale x_zp qw_packed w_scales w_zps b used_y_scale used_y_zp output_dtype post_op unary_post_op_args post_op_algo post_op == relu y_ref = F relu y_ref post_op == gelu y_ref = F gelu y_ref approximate=post_op_algo post_op sum sum_relu x = torch rand_like y_ref x _q x _scale = _quantize_fp e m x channelwise=False x _dq = _dequantize_fp e m x _q x _scale unary_post_op = relu post_op == sum_relu none binary_alpha = we only support alpha= now output_dtype fp bf accumulate x output_dtype None fp accumulate x _dq accum = x _q output_dtype None x accum_ref = x _dq output_dtype None x clone x _scale = x _scale output_dtype None bfloat _out accum = accum bfloat accum_ref = accum_ref bfloat qy = qlinear_op qx x_scale x_zp qw_packed w_scales w_zps accum b used_y_scale used_y_zp output_dtype x _scale x _zp sum binary_alpha unary_post_op unary_post_op_args post_op_algo y_ref = y_ref + accum_ref binary_alpha unary_post_op == relu y_ref = F relu y_ref post_op add add_relu output_dtype None Only support fp output continue x = torch rand_like y_ref unary_post_op = relu post_op == add_relu none binary_alpha = we only support alpha= now qy = qlinear_op qx x_scale x_zp qw_packed w_scales w_zps x b used_y_scale used_y_zp output_dtype add binary_alpha unary_post_op unary_post_op_args post_op_algo y_ref = y_ref + x binary_alpha unary_post_op == relu y_ref = F relu y_ref Compare results output_dtype None y_ref = _quantize_fp e m y_ref False used_y_scale y_ref = y_ref output_dtype assertEqual x dim qy dim assertEqual y_ref float qy float assert torch isnan qy any unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_fp qlinear = torch ops onednn qlinear_pointwise _test_qlinear_fp _helper qlinear none unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_relu_fp qlinear = torch ops onednn qlinear_pointwise _test_qlinear_fp _helper qlinear relu unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_gelu_fp qlinear = torch ops onednn qlinear_pointwise post_op_algorithms = none tanh _test_qlinear_fp _helper qlinear gelu post_op_algorithms=post_op_algorithms unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_sum_fp qlinear = torch ops onednn qlinear_pointwise binary _test_qlinear_fp _helper qlinear sum unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_sum_relu_fp qlinear = torch ops onednn qlinear_pointwise binary _test_qlinear_fp _helper qlinear sum_relu unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_add_fp qlinear = torch ops onednn qlinear_pointwise binary _test_qlinear_fp _helper qlinear add unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qlinear_add_relu_fp qlinear = torch ops onednn qlinear_pointwise binary _test_qlinear_fp _helper qlinear add_relu unittest skipIf IS_MACOS Known test failure Mac TestQuantizedEmbeddingOps TestCase _test_embedding_bag_unpack_impl pack_fn unpack_fn bit_rate optimized_qparams weights data_type = weights dtype qtype = torch quint bit_rate == w_packed = pack_fn weights w_packed = pack_fn weights optimized_qparams=optimized_qparams w_unpacked = unpack_fn w_packed bit_rate == bit_rate == data_type = torch float torch quantize_per_channel does support float yet obs_weights = weights Combine D embeddings e g stacked combination embeddings dimension orthogonal channels len obs_weights shape stacked_shape = list weights size stacked_shape = stacked_shape obs_weights = weights reshape stacked_shape Check numerics prepack function accepts qtensor input We use min-max observer mimic quantization performed original function obs = PerChannelMinMaxObserver dtype=torch quint qscheme=torch per_channel_affine_float_qparams ch_axis= obs obs_weights Get scale zero point weight tensor qparams = obs calculate_qparams bit_rate == qtype = torch quint x Quantize weights bits qweight = torch quantize_per_channel obs_weights qparams qparams axis= dtype=qtype real_packed_weight = torch ops quantized embedding_bag_prepack qweight assertEqual isinstance real_packed_weight torch _C ScriptObject True unpacked_weight = torch ops quantized embedding_bag_unpack real_packed_weight assertEqual unpacked_weight int_repr numpy qweight int_repr numpy assertEqual unpacked_weight q_per_channel_scales qweight q_per_channel_scales assertEqual unpacked_weight q_per_channel_zero_points qweight q_per_channel_zero_points _test_embedding_bag_unpack_fn pack_fn unpack_fn num_embeddings embedding_dim bit_rate optimized_qparams num_batches data_type=np float when num_batches = will create D tensor unsplit_weight = torch from_numpy np random random_sample num_batches num_embeddings embedding_dim squeeze + astype np float test unsplit weight memory format ` contiguous ` _test_embedding_bag_unpack_impl pack_fn unpack_fn bit_rate optimized_qparams unsplit_weight test split weights memory format ` contiguous ` split_dim = len unsplit_weight shape - split_weights = torch split unsplit_weight dim=split_dim weight split_weights _test_embedding_bag_unpack_impl pack_fn unpack_fn bit_rate optimized_qparams weight embedding_bag_rowwise_offsets_run bit_rate num_embeddings embedding_dim num_offsets use_ bit_indices use_ bit_offsets enable_per_sample_weights include_last_offset fallback_to_no_sparse sparsity atol rtol pt_op = torch ops quantized embedding_bag_byte_rowwise_offsets pt_prepack_op = torch ops quantized embedding_bag_byte_prepack bit_rate == pt_op = torch ops quantized embedding_bag_ bit_rowwise_offsets pt_prepack_op = torch ops quantized embedding_bag_ bit_prepack bit_rate == pt_op = torch ops quantized embedding_bag_ bit_rowwise_offsets pt_prepack_op = torch ops quantized embedding_bag_ bit_prepack weights = torch from_numpy np random random_sample num_embeddings embedding_dim + astype np float max_segments = max_segment_length = num_lengths = np random randint max_segments + lengths = np random randint max_segment_length + size=num_lengths astype np int num_indices = np sum lengths lengths_to_offsets t offset_type=np int use_begin_offset=True Convert lengths offsets tt = np zeros t shape + dtype=offset_type tt = t tt = torch from_numpy np cumsum tt dtype=offset_type use_begin_offset tt - tt offsets = lengths_to_offsets lengths indices = torch from_numpy np random randint low= high=num_embeddings size=num_indices dtype=np int q_weights = pt_prepack_op weights per_sample_weights = torch from_numpy np random uniform low= high= size= len indices astype np float \ enable_per_sample_weights None include_last_offset offsets = torch cat offsets torch tensor indices size dtype=torch long Reference result will floating point torch nn EmbeddingBag get_reference_result num_embeddings embedding_dim include_last_offset weights per_sample_weights indices offsets embedding_bag = torch nn EmbeddingBag num_embeddings=num_embeddings embedding_dim=embedding_dim include_last_offset=include_last_offset _weight=weights scale_grad_by_freq=False mode= sum embedding_bag indices offsets per_sample_weights=per_sample_weights mapping_table = np zeros num_embeddings dtype=np int pruned_weights = weights prune_weights = sparsity prune_weights fallback_to_no_sparse Testing prune_weight mapping_table will fallback non sparse embedding look up kernel mapping_table = np zeros dtype=np int Prune generate mapping table num_compressed_rows = unpruned_ids = i range num_embeddings np random uniform sparsity mapping_table i = - q_weights i = weights i = mapping_table i = num_compressed_rows num_compressed_rows += unpruned_ids append i q_weights = q_weights unpruned_ids pruned_weights = weights unpruned_ids result = pt_op q_weights indices int use_ bit_indices indices offsets int use_ bit_offsets offsets mode= pruned_weights=prune_weights per_sample_weights=per_sample_weights compressed_indices_mapping=torch tensor mapping_table include_last_offset=include_last_offset reference_result = get_reference_result num_embeddings embedding_dim include_last_offset weights per_sample_weights indices offsets torch testing assert_close reference_result result atol=atol rtol=rtol bit_rate == bit_rate == Test operator accepts TorchBind packed weights bit_rate == qdtype = torch quint x op = torch ops quantized embedding_bag_ bit qdtype = torch quint op = torch ops quantized embedding_bag_byte obs = PerChannelMinMaxObserver dtype=qdtype qscheme=torch per_channel_affine_float_qparams ch_axis= obs pruned_weights Get scale zero point weight tensor qparams = obs calculate_qparams Quantize weights bits qweight = torch quantize_per_channel pruned_weights qparams qparams axis= dtype=qdtype packed_weight = torch ops quantized embedding_bag_prepack qweight result = op packed_weight indices offsets mode= pruned_weights=prune_weights per_sample_weights=per_sample_weights compressed_indices_mapping=torch tensor mapping_table include_last_offset=include_last_offset torch testing assert_close reference_result result atol=atol rtol=rtol Tests correctness embedding_bag_ bit quantized operator given num_embeddings=st integers embedding_dim=st integers filter lambda x x == num_offsets=st integers use_ bit_indices=st booleans use_ bit_offsets=st booleans enable_per_sample_weights=st booleans include_last_offset=st booleans fallback_to_no_sparse=st booleans sparsity=st sampled_from test_embedding_bag_byte num_embeddings embedding_dim num_offsets use_ bit_indices use_ bit_offsets enable_per_sample_weights include_last_offset fallback_to_no_sparse sparsity embedding_bag_rowwise_offsets_run num_embeddings embedding_dim num_offsets use_ bit_indices use_ bit_offsets enable_per_sample_weights include_last_offset fallback_to_no_sparse sparsity=sparsity atol= rtol= e- Tests correctness embedding_bag_ bit quantized operator given num_embeddings=st integers embedding_dim=st integers filter lambda x x == num_offsets=st integers use_ bit_indices=st booleans use_ bit_offsets=st booleans enable_per_sample_weights=st booleans include_last_offset=st booleans fallback_to_no_sparse=st booleans sparsity=st sampled_from test_embedding_bag_ bit num_embeddings embedding_dim num_offsets use_ bit_indices use_ bit_offsets enable_per_sample_weights include_last_offset fallback_to_no_sparse sparsity embedding_bag_rowwise_offsets_run num_embeddings embedding_dim num_offsets use_ bit_indices use_ bit_offsets enable_per_sample_weights include_last_offset fallback_to_no_sparse sparsity=sparsity atol= rtol= e- Tests correctness embedding_bag_ bit quantized operator given num_embeddings=st integers embedding_dim=st integers filter lambda x x == num_offsets=st integers use_ bit_indices=st booleans use_ bit_offsets=st booleans enable_per_sample_weights=st booleans include_last_offset=st booleans fallback_to_no_sparse=st booleans sparsity=st sampled_from test_embedding_bag_ bit num_embeddings embedding_dim num_offsets use_ bit_indices use_ bit_offsets enable_per_sample_weights include_last_offset fallback_to_no_sparse sparsity embedding_bag_rowwise_offsets_run num_embeddings embedding_dim num_offsets use_ bit_indices use_ bit_offsets enable_per_sample_weights include_last_offset fallback_to_no_sparse sparsity=sparsity atol= rtol= e- Tests correctness quantized bit embedding lookup operator given num_embeddings=st integers embedding_dim=st integers filter lambda x x == test_embedding num_embeddings embedding_dim dtypes = torch quint torch quint x quant_ops = torch ops quantized embedding_byte torch ops quantized embedding_ bit atols = rtols = e- e- prepack_op = torch ops quantized embedding_bag_prepack quant_op dtype atol rtol zip quant_ops dtypes atols rtols weights = torch from_numpy np random random_sample num_embeddings embedding_dim + astype np float obs = PerChannelMinMaxObserver dtype=dtype qscheme=torch per_channel_affine_float_qparams ch_axis= obs weights Get scale zero point weight tensor qparams = obs calculate_qparams Quantize weights bits qweight = torch quantize_per_channel weights qparams qparams axis= dtype=dtype max_segments = max_segment_length = num_lengths = np random randint max_segments + lengths = np random randint max_segment_length + size=num_lengths astype np int num_indices = np sum lengths indices = torch from_numpy np random randint low= high=num_embeddings size=num_indices dtype=np int packed_weight = prepack_op qweight qresult = quant_op packed_weight indices pruned_weights=False ref = torch embedding weights indices padding_idx=- scale_grad_by_freq=False sparse=False torch testing assert_close ref qresult atol=atol rtol=rtol test_embedding_ d_indices Tests case where D indices passed into operator In case operator computes correct offsets argument Output shape dependent indices dimension quant_op = torch ops quantized embedding_byte prepack_op = torch ops quantized embedding_bag_prepack indices = torch tensor weights = torch randn dtype=torch float ref = torch embedding weights indices padding_idx=- scale_grad_by_freq=False sparse=False obs = PerChannelMinMaxObserver dtype=torch quint qscheme=torch per_channel_affine_float_qparams ch_axis= obs weights qparams = obs calculate_qparams qweight = torch quantize_per_channel weights qparams qparams axis= dtype=torch quint packed_weight = prepack_op qweight qresult = quant_op packed_weight indices pruned_weights=False torch testing assert_close ref qresult atol= rtol= e- test_embedding_bag_ d_indices Tests case where D indices passed into operator In case operator computes correct offsets argument indices = torch tensor weights = torch randn dtype=torch float embedding_bag = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=False _weight=weights scale_grad_by_freq=False mode= sum result = embedding_bag indices pt_op = torch ops quantized embedding_bag_byte_rowwise_offsets pt_prepack_op = torch ops quantized embedding_bag_byte_prepack q_weights = pt_prepack_op weights qresult = pt_op q_weights indices mode= pruned_weights=False torch testing assert_close result qresult atol= rtol= e- Test TorchBind based embedding_bag operator obs = PerChannelMinMaxObserver dtype=torch quint qscheme=torch per_channel_affine_float_qparams ch_axis= obs weights Get scale zero point weight tensor qparams = obs calculate_qparams Quantize weights bits qweight = torch quantize_per_channel weights qparams qparams axis= dtype=torch quint packed_weight = torch ops quantized embedding_bag_prepack qweight qresult = torch ops quantized embedding_bag_byte packed_weight indices mode= torch testing assert_close result qresult atol= rtol= e- TestQuantizedConv TestCase _test_qconv_unpack_impl qconv_prepack_fn qconv_unpack_fn inputs strides i_pads o_pads channelwise X_data W_data bias_data groups transposed = inputs X X_scale X_zero_point X_qtype = X_data W W_scale W_zero_point W_qtype = W_data bias bias_scale bias_zero_point bias_qtype = bias_data W = torch from_numpy W float bias = torch from_numpy bias float channelwise transposed currently transposed conv per-channel per quantization does work ONEDNN only supports symmetric quantization weight zero output padding qengine_is_onednn W_zero_point = o_pads = len o_pads o_pads None None channelwise transposed output_channels = W shape IC OC G output_channels = W shape OC IC G W_scale = torch tensor W_scale output_channels W_zero_point = torch tensor W_zero_point output_channels W_q = torch quantize_per_channel W scales=W_scale zero_points=W_zero_point axis=int transposed dtype=W_qtype W_q = torch quantize_per_tensor W scale=W_scale zero_point=W_zero_point dtype=W_qtype isinstance strides int dilations = dilations = len strides transposed W_packed = qconv_prepack_fn W_q bias strides i_pads o_pads dilations groups W_packed = qconv_prepack_fn W_q bias strides i_pads dilations groups W_unpacked bias = qconv_unpack_fn W_packed Assert equal np testing assert_equal W_q int_repr numpy W_unpacked int_repr numpy channelwise np testing assert_array_almost_equal np float W_q q_per_channel_scales numpy np float W_unpacked q_per_channel_scales numpy decimal= np testing assert_equal W_q q_per_channel_zero_points numpy W_unpacked q_per_channel_zero_points numpy np testing assert_equal np float W_q q_scale np float W_unpacked q_scale np testing assert_equal W_q q_zero_point W_unpacked q_zero_point _make_qconv_tensors batch_size input_channels_per_group input_feature_map_shape output_channels_per_group groups kernels strides pads dilations X_scale X_zero_point W_scale W_zero_point use_bias use_channelwise use_transpose device=torch device cpu input_dtype=torch quint weight_dtype=torch qint assert use_channelwise use_transpose \ Cannot generate channelwise qconv_transpose_tensors input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups Padded input size should least big dilated kernel kernels = _single kernels strides = _single strides pads = _single pads dilations = _single dilations i range len kernels assume input_feature_map_shape i + pads i = dilations i kernels i - + W_scale = W_scale output_channels W_zero_point = W_zero_point output_channels Resize W_scale W_zero_points arrays equal output_channels W_scale = W_scale output_channels W_zero_point = W_zero_point output_channels For testing we use small values weights activations so no overflow occurs vpmaddubsw instruction If overflow occurs qconv implementation there no overflow In reference we can t exactly match results reference Please see comment qconv implementation file aten src ATen native quantized cpu qconv cpp more details W_value_min W_value_max = - operator expects them format output_channels input_channels groups kernel_d kernel_h kernel_w input_channels output_channels groups kernel_d kernel_h kernel_w use_transpose output_shape = input_channels output_channels_per_group output_shape = output_channels input_channels_per_group W_init = torch randint W_value_min W_value_max output_shape + kernels device=device b_init = torch randint output_channels device=device X_value_min X_value_max = X_init = torch randint X_value_min X_value_max batch_size input_channels + input_feature_map_shape device=device X = X_scale X_init - X_zero_point float use_channelwise W_shape = - + len kernels W_scales_tensor = torch tensor W_scale dtype=torch float device=device W_zero_points_tensor = torch tensor W_zero_point dtype=torch float device=device W = W_scales_tensor reshape W_shape W_init float - W_zero_points_tensor reshape W_shape float b = X_scale W_scales_tensor b_init float W = W_scale W_init - W_zero_point float b = X_scale W_scale b_init float X_q = torch quantize_per_tensor X scale=X_scale zero_point=X_zero_point dtype=input_dtype use_channelwise W_q = torch quantize_per_channel W W_scales_tensor W_zero_points_tensor long dtype=weight_dtype W_q = torch quantize_per_tensor W scale=W_scale zero_point=W_zero_point dtype=weight_dtype bias_float = b use_bias None X W X_q W_q bias_float _test_qconv_impl qconv_fn qconv_prepack_fn conv_op batch_size input_channels_per_group input_feature_map_shape output_channels_per_group groups kernels strides pads o_pads dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias post_op use_channelwise use_transpose device=torch device cpu input_dtype=torch quint weight_dtype=torch qint output_dtype=torch quint X _scale= X _zero_point= ONEDNN only supports symmetric quantization weight qengine_is_onednn W_zero_point None W_zero_point = len W_zero_point X W X_q W_q bias_float = _make_qconv_tensors batch_size input_channels_per_group input_feature_map_shape output_channels_per_group groups kernels strides pads dilations X_scale X_zero_point W_scale W_zero_point use_bias use_channelwise use_transpose device=device input_dtype=input_dtype weight_dtype=weight_dtype bias_float None bias_float = bias_float device Assign weights W = W_q dequantize X = X_q dequantize conv_op weight = torch nn Parameter W requires_grad=False conv_op bias = torch nn Parameter bias_float requires_grad=False use_bias None result_ref = conv_op X post_op == relu assert use_transpose Cannot fuse ReLU ConvTranspose relu = torch nn ReLU result_ref = relu result_ref post_op == add X_value_min X_value_max = X _init = torch randint X_value_min X_value_max result_ref size device=device X = X _scale X _init - X _zero_point float X _q = torch quantize_per_tensor X scale=X _scale zero_point=X _zero_point dtype=input_dtype result_ref = result_ref + X post_op == add_relu X_value_min X_value_max = X _init = torch randint X_value_min X_value_max result_ref size device=device X = X _scale X _init - X _zero_point float X _q = torch quantize_per_tensor X scale=X _scale zero_point=X _zero_point dtype=input_dtype result_ref = result_ref + X relu = torch nn ReLU result_ref = relu result_ref Quantize reference results comparison result_ref_q = torch quantize_per_tensor result_ref scale=Y_scale zero_point=Y_zero_point dtype=output_dtype qconv_prepack_fn None use_transpose W_prepack = qconv_prepack_fn W_q bias_float strides pads o_pads dilations groups W_prepack = qconv_prepack_fn W_q bias_float strides pads dilations groups post_op == add post_op == add_relu Y_q = qconv_fn X_q X _q W_prepack Y_scale Y_zero_point Y_q = qconv_fn X_q W_prepack Y_scale Y_zero_point quantized conv op without prepacking Y_q = qconv_fn X_q W_q bias_float strides pads dilations groups Y_scale Y_zero_point Make sure results match assert_array_almost_equal compares using following formula abs desired-actual -decimal https numpy org doc stable reference generated numpy testing assert_almost_equal html We use decimal = ignore off-by- differences between reference test Off-by- differences arise due order round zero_point addition operation i e addition followed round used reference round followed addition used test results may differ For example result round + while round + assuming rounding mode round-to-nearest ties-to-even np testing assert_array_almost_equal result_ref_q int_repr cpu numpy Y_q int_repr cpu numpy decimal= err_msg=f X X_q W W_q b bias_float strides strides pads pads o_pads o_pads dilations dilations groups groups y_s Y_scale y_zp Y_zero_point Return quantized data later reuse X_q W_q bias_float Tests correctness quantized convolution op given batch_size=st integers input_channels_per_group=st sampled_from height=st integers width=st integers output_channels_per_group=st sampled_from groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans use_channelwise=st booleans override_qengines test_qconv d batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w pads = pad_h pad_w dilations = dilation dilation qconv = torch ops quantized conv d qconv_prepack = torch ops quantized conv d_prepack conv_op = torch nn Conv d input_channels output_channels kernels strides pads dilations groups act_qdtypes = torch quint Only qnnpack qengine supports qint qengine_is_qnnpack torch backends xnnpack enabled act_qdtypes append torch qint X_qdtype act_qdtypes X_qdtype == torch qint W_zero_point = i range len W_zero_point _test_qconv_impl qconv qconv_prepack conv_op batch_size input_channels_per_group height width output_channels_per_group groups kernels strides pads None dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias none use_channelwise False input_dtype=X_qdtype output_dtype=X_qdtype given batch_size=st integers input_channels_per_group=st sampled_from height=st integers width=st integers output_channels_per_group=st sampled_from groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans use_channelwise=st booleans override_qengines test_qconv d_relu batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w pads = pad_h pad_w dilations = dilation dilation qconv = torch ops quantized conv d_relu qconv_prepack = torch ops quantized conv d_prepack conv_op = torch nn Conv d input_channels output_channels kernels strides pads dilations groups act_qdtypes = torch quint Only qnnpack qengine supports qint qengine_is_qnnpack torch backends xnnpack enabled act_qdtypes append torch qint X_qdtype act_qdtypes X_qdtype == torch qint W_zero_point = i range len W_zero_point _test_qconv_impl qconv qconv_prepack conv_op batch_size input_channels_per_group height width output_channels_per_group groups kernels strides pads None dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias relu use_channelwise False input_dtype=X_qdtype output_dtype=X_qdtype skipIfNoONEDNN test_qconv d_add batch_size = groups_list = input_channels_per_group = output_channels_per_group = height = width = kernel_h = kernel_w = stride_h = stride_w = pad_h = pad_w = dilation = X_scale = X_zero_point = W_scale = W_zero_point = - Y_scale = Y_zero_point = use_bias_list = False True use_channelwise_list = False True X _scale = X _zero_point_list = options = itertools product groups_list use_bias_list use_channelwise_list X _zero_point_list groups use_bias use_channelwise X _zero_point options override_quantized_engine onednn input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w pads = pad_h pad_w dilations = dilation dilation qconv = torch ops quantized conv d_add qconv_prepack = torch ops quantized conv d_prepack conv_op = torch nn Conv d input_channels output_channels kernels strides pads dilations groups X_qdtype = torch quint _test_qconv_impl qconv qconv_prepack conv_op batch_size input_channels_per_group height width output_channels_per_group groups kernels strides pads None dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias add use_channelwise False input_dtype=X_qdtype output_dtype=X_qdtype X _scale=X _scale X _zero_point=X _zero_point skipIfNoONEDNN test_qconv d_add_relu batch_size = height = width = groups_list = input_channels_per_group = output_channels_per_group = kernel_h = kernel_w = stride_h = stride_w = pad_h = pad_w = dilation = X_scale = X_zero_point = W_scale = W_zero_point = - Y_scale = Y_zero_point = use_bias_list = False True use_channelwise_list = False True X _scale = X _zero_point_list = options = itertools product groups_list use_bias_list use_channelwise_list X _zero_point_list groups use_bias use_channelwise X _zero_point options override_quantized_engine onednn input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w pads = pad_h pad_w dilations = dilation dilation qconv = torch ops quantized conv d_add_relu qconv_prepack = torch ops quantized conv d_prepack conv_op = torch nn Conv d input_channels output_channels kernels strides pads dilations groups X_qdtype = torch quint _test_qconv_impl qconv qconv_prepack conv_op batch_size input_channels_per_group height width output_channels_per_group groups kernels strides pads None dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias add_relu use_channelwise False input_dtype=X_qdtype output_dtype=X_qdtype X _scale=X _scale X _zero_point=X _zero_point TODO merge test test_qconv d when CUDNN runtime flags becomes available Tests correctness quantized D convolution cudnn op given batch_size=st integers cudnn only supports multiples we have explicitly added padding backend input_channels_per_group=st integers height=st integers width=st integers cudnn only supports multiples we have explicitly added padding backend output_channels_per_group=st integers groups=st integers currently padding only supports groups= kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers result dilation == correct dilation=st integers currently cudnn has only been verified work dilation = TODO check backend works dilation dilation=st integers X_scale=st floats X_zero_point=st sampled_from W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers min_size= max_size= Y_scale=st floats Y_zero_point=st sampled_from use_bias=st booleans TODO enable channelwise use_channelwise=st sampled_from False skipIfNoFBGEMM unittest skipIf TEST_CUDNN cudnn enabled unittest skipIf SM OrLater requires sm later unittest skipIf TEST_ROCM supported rocm unittest skip currently working feature isn t used test_qconv d_cudnn batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w pads = pad_h pad_w dilations = dilation dilation qconv = torch ops quantized conv d conv_op = torch nn Conv d input_channels output_channels kernels strides pads dilations groups torch device cuda _test_qconv_impl qconv torch ops quantized conv d_prepack conv_op batch_size input_channels_per_group height width output_channels_per_group groups kernels strides pads None dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias none use_channelwise False device=torch device cuda input_dtype=torch qint weight_dtype=torch qint output_dtype=torch qint given batch_size=st integers cudnn only supports multiples we have explicitly added padding backend input_channels_per_group=st integers height=st integers width=st integers cudnn only supports multiples we have explicitly added padding backend output_channels_per_group=st integers groups=st integers currently padding only supports groups= kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers result dilation == correct dilation=st integers currently cudnn has only been verified work dilation = TODO check backend works dilation dilation=st integers X_scale=st floats X_zero_point=st sampled_from W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers min_size= max_size= Y_scale=st floats Y_zero_point=st sampled_from use_bias=st booleans TODO enable channelwise use_channelwise=st sampled_from False skipIfNoFBGEMM unittest skipIf TEST_CUDNN cudnn enabled unittest skipIf SM OrLater requires sm later unittest skipIf TEST_ROCM supported rocm unittest skip currently working feature isn t used test_qconv d_relu_cudnn batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w pads = pad_h pad_w dilations = dilation dilation qconv = torch ops quantized conv d_relu conv_op = torch nn Conv d input_channels output_channels kernels strides pads dilations groups torch device cuda _test_qconv_impl qconv torch ops quantized conv d_prepack conv_op batch_size input_channels_per_group height width output_channels_per_group groups kernels strides pads None dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias relu use_channelwise False device=torch device cuda input_dtype=torch qint weight_dtype=torch qint output_dtype=torch qint unittest skip used local benchmarking comment when we want run test_benchmark batch_size = in_channel = out_channel = kernel_size = height = width = print parameters batch_size batch_size in_channel in_channel out_channel out_channel kernel_size kernel_size height height widht width conv = torch nn Conv d in_channel out_channel kernel_size cuda input = torch randn batch_size in_channel height width device= cuda weight = conv weight detach stride = padding = dilation = groups = conv_op = torch nn functional conv d profile torch profiler profile ProfilerActivity trace_handler p output = p key_averages table sort_by= self_cpu_time_total row_limit= p export_chrome_trace tmp trace_ + str p step_num + json my_schedule = torch profiler schedule wait= warmup= active= fp benchmark profile activities= ProfilerActivity CPU ProfilerActivity CUDA schedule=my_schedule on_trace_ready=trace_handler prof _ range conv_op input weight None stride padding dilation groups prof step print fp benchmark result print prof key_averages table sort_by= self_cpu_time_total row_limit= fp benchmark input_fp = input torch float weight_fp = input torch float profile activities= ProfilerActivity CPU ProfilerActivity CUDA schedule=my_schedule on_trace_ready=trace_handler prof _ range conv_op input_fp weight_fp None stride padding dilation groups prof step print fp benchmark result print prof key_averages table sort_by= self_cpu_time_total row_limit= input_int = torch quantize_per_tensor input torch qint contiguous memory_format=torch channels_last weight_int = torch quantize_per_tensor weight torch qint contiguous memory_format=torch channels_last scale = zero_point = conv_op = torch ops quantized conv d weight_prepacked = torch ops quantized conv d_prepack weight_int None stride padding dilation groups profile activities= ProfilerActivity CPU ProfilerActivity CUDA schedule=my_schedule on_trace_ready=trace_handler prof _ range conv_op input_int weight_prepacked scale zero_point prof step print int benchmark result print prof key_averages table sort_by= self_cpu_time_total row_limit= Tests correctness quantized convolution op override_qengines test_qconv_transpose d qengine_is_qnnpack Currently only QNNPACK supported qengine_is_qnnpack IS_PPC QNNPACK doesn t support these batch_size = input_channels_per_group_list = width = output_channels_per_group_list = groups_list = kernel_list = stride_list = pad = o_pad = dilation = X_scale = X_zero_point = W_scale = W_zero_point = Y_scale = Y_zero_point = use_bias_list = True False test_cases = itertools product input_channels_per_group_list output_channels_per_group_list groups_list kernel_list stride_list use_bias_list input_channels_per_group output_channels_per_group \ groups kernel stride use_bias test_cases input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel strides = stride pads = pad o_pads = o_pad dilations = dilation qconv = torch ops quantized conv_transpose d qconv_prepack = torch ops quantized conv_transpose d_prepack conv_op = torch nn ConvTranspose d in_channels=input_channels out_channels=output_channels kernel_size=kernels stride=strides padding=pads output_padding=o_pads groups=groups dilation=dilations bias=use_bias act_qdtypes = torch quint Only qnnpack qengine supports qint qengine_is_qnnpack torch backends xnnpack enabled act_qdtypes append torch qint X_qdtype act_qdtypes X_qdtype == torch qint W_zero_point = i range len W_zero_point X_q W_q bias_float = _test_qconv_impl qconv qconv_prepack conv_op batch_size input_channels_per_group width output_channels_per_group groups kernels strides pads o_pads dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias post_op= none use_channelwise=False use_transpose=True input_dtype=X_qdtype output_dtype=X_qdtype check doesn t error test_conv = torch ao nn quantized ConvTranspose d input_channels output_channels test_conv scale = Y_scale test_conv X_q Test module implementation qconv_op = torch ao nn quantized ConvTranspose d in_channels=input_channels out_channels=output_channels kernel_size=kernels stride=strides padding=pads output_padding=o_pads groups=groups dilation=dilations bias=use_bias qconv_op scale = Y_scale qconv_op zero_point = Y_zero_point qconv_op set_weight_bias W_q bias_float Y_dq_ref = conv_op X_q dequantize Y_q_ref = torch quantize_per_tensor Y_dq_ref scale=Y_scale zero_point=Y_zero_point dtype=X_qdtype Y_q = qconv_op X_q assertEqual Y_q_ref Y_q Tests correctness quantized convolution op given batch_size=st integers input_channels_per_group=st sampled_from height=st integers width=st integers output_channels_per_group=st sampled_from groups=st integers kernel_h=st integers kernel_w=st integers stride_h=st integers stride_w=st integers pad_h=st integers pad_w=st integers o_pad_h=st integers o_pad_w=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans override_qengines unittest skip broken without changes any relevant code we need remove hypothesis testing CI test_qconv_transpose d batch_size input_channels_per_group height width output_channels_per_group groups kernel_h kernel_w stride_h stride_w pad_h pad_w o_pad_h o_pad_w dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias qengine_is_qnnpack IS_PPC QNNPACK doesn t support these ONEDNN does support output paddings qengine_is_onednn o_pad_h o_pad_w = assume o_pad_h stride_h o_pad_h dilation assume o_pad_w stride_w o_pad_w dilation input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_h kernel_w strides = stride_h stride_w pads = pad_h pad_w o_pads = o_pad_h o_pad_w dilations = dilation dilation qconv = torch ops quantized conv_transpose d qconv_prepack = torch ops quantized conv_transpose d_prepack conv_op = torch nn ConvTranspose d in_channels=input_channels out_channels=output_channels kernel_size=kernels stride=strides padding=pads output_padding=o_pads groups=groups dilation=dilations bias=use_bias act_qdtypes = torch quint Only qnnpack qengine supports qint qengine_is_qnnpack torch backends xnnpack enabled act_qdtypes append torch qint X_qdtype act_qdtypes X_qdtype == torch qint W_zero_point = i range len W_zero_point X_q W_q bias_float = _test_qconv_impl qconv qconv_prepack conv_op batch_size input_channels_per_group height width output_channels_per_group groups kernels strides pads o_pads dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias post_op= none use_channelwise=False use_transpose=True input_dtype=X_qdtype output_dtype=X_qdtype check doesn t error test_conv = torch ao nn quantized ConvTranspose d input_channels output_channels test_conv scale = Y_scale test_conv X_q Test module implementation qconv_op = torch ao nn quantized ConvTranspose d in_channels=input_channels out_channels=output_channels kernel_size=kernels stride=strides padding=pads output_padding=o_pads groups=groups dilation=dilations bias=use_bias qconv_op scale = Y_scale qconv_op zero_point = Y_zero_point qconv_op set_weight_bias W_q bias_float Y_dq_ref = conv_op X_q dequantize Y_q_ref = torch quantize_per_tensor Y_dq_ref scale=Y_scale zero_point=Y_zero_point dtype=X_qdtype Y_q = qconv_op X_q assertEqual Y_q_ref Y_q Tests correctness quantized convolution op given batch_size=st integers input_channels_per_group=st sampled_from time=st integers height=st integers width=st integers output_channels_per_group=st sampled_from groups=st integers kernel_t=st integers kernel_h=st integers kernel_w=st integers stride_t=st integers stride_h=st integers stride_w=st integers pad_t=st integers pad_h=st integers pad_w=st integers o_pad_t=st integers o_pad_h=st integers o_pad_w=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans override_qengines unittest skip broken without changes any relevant code we need remove hypothesis testing CI test_qconv_transpose d batch_size input_channels_per_group time height width output_channels_per_group groups kernel_t kernel_h kernel_w stride_t stride_h stride_w pad_t pad_h pad_w o_pad_t o_pad_h o_pad_w dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias qengine_is_qnnpack QNNPACK doesn t support ONEDNN doesn t support output paddings qengine_is_onednn o_pad_t o_pad_h o_pad_w = assume o_pad_t stride_t o_pad_t dilation assume o_pad_h stride_h o_pad_h dilation assume o_pad_w stride_w o_pad_w dilation input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_t kernel_h kernel_w strides = stride_t stride_h stride_w pads = pad_t pad_h pad_w o_pads = o_pad_t o_pad_h o_pad_w dilations = dilation dilation dilation qconv = torch ops quantized conv_transpose d qconv_prepack = torch ops quantized conv_transpose d_prepack conv_op = torch nn ConvTranspose d in_channels=input_channels out_channels=output_channels kernel_size=kernels stride=strides padding=pads output_padding=o_pads groups=groups dilation=dilations bias=use_bias X_q W_q bias_float = _test_qconv_impl qconv qconv_prepack conv_op batch_size input_channels_per_group time height width output_channels_per_group groups kernels strides pads o_pads dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias post_op= none use_channelwise=False use_transpose=True check doesn t error test_conv = torch ao nn quantized ConvTranspose d input_channels output_channels test_conv scale = Y_scale test_conv X_q Test module implementation qconv_op = torch ao nn quantized ConvTranspose d in_channels=input_channels out_channels=output_channels kernel_size=kernels stride=strides padding=pads output_padding=o_pads groups=groups dilation=dilations bias=use_bias qconv_op scale = Y_scale qconv_op zero_point = Y_zero_point qconv_op set_weight_bias W_q bias_float Y_dq_ref = conv_op X_q dequantize Y_q_ref = torch quantize_per_tensor Y_dq_ref scale=Y_scale zero_point=Y_zero_point dtype=torch quint Y_q = qconv_op X_q assertEqual Y_q_ref Y_q given inputs=hu tensor_conv spatial_dim= batch_size_range= input_channels_per_group_range= output_channels_per_group_range= feature_map_range= kernel_range= max_groups= can_be_transposed=False qparams= hu qparams dtypes=torch quint zero_point_min= zero_point_max= hu qparams dtypes=torch qint zero_point_min= zero_point_max= hu qparams dtypes=torch qint zero_point_min= zero_point_max= stride=st integers pad=st integers o_pad=st integers channelwise=st booleans override_qengines test_qconv d_unpack inputs stride pad o_pad channelwise transposed = inputs - qengine = torch backends quantized engine qengine supported_qengines qengine == qnnpack assume channelwise QNNPACK doesn t support channelwise assume transposed Only QNNPACK supports transposed conv transposed qconv_prepack = torch ops quantized conv_transpose d_prepack qconv_unpack = torch ops quantized conv_transpose d_unpack qconv_prepack = torch ops quantized conv d_prepack qconv_unpack = torch ops quantized conv d_unpack _test_qconv_unpack_impl qconv_prepack qconv_unpack inputs stride pad o_pad channelwise given inputs=hu tensor_conv spatial_dim= batch_size_range= input_channels_per_group_range= output_channels_per_group_range= feature_map_range= kernel_range= max_groups= can_be_transposed=True qparams= hu qparams dtypes=torch quint zero_point_min= zero_point_max= hu qparams dtypes=torch qint zero_point_min= zero_point_max= hu qparams dtypes=torch qint zero_point_min= zero_point_max= stride=st integers pad=st integers o_pad=st integers channelwise=st booleans override_qengines test_qconv d_unpack inputs stride pad o_pad channelwise transposed = inputs - qengine = torch backends quantized engine qengine supported_qengines qengine == qnnpack assume channelwise QNNPACK doesn t support channelwise transposed qconv_prepack = torch ops quantized conv_transpose d_prepack qconv_unpack = torch ops quantized conv_transpose d_unpack qconv_prepack = torch ops quantized conv d_prepack qconv_unpack = torch ops quantized conv d_unpack _test_qconv_unpack_impl qconv_prepack qconv_unpack inputs stride stride pad pad o_pad o_pad channelwise Tests correctness quantized D convolution op given batch_size=st integers input_channels_per_group=st sampled_from output_channels_per_group=st sampled_from groups=st integers length=st integers kernel=st integers stride=st integers pad=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans use_channelwise=st booleans override_qengines test_qconv d batch_size input_channels_per_group output_channels_per_group groups length kernel stride pad dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups torch backends quantized engine == qnnpack use_channelwise = False conv d = torch nn Conv d input_channels output_channels kernel stride pad dilation groups qconv_prepack = torch ops quantized conv d_prepack qconv = torch ops quantized conv d act_qdtypes = torch quint Only qnnpack qengine supports qint qengine_is_qnnpack torch backends xnnpack enabled act_qdtypes append torch qint X_qdtype act_qdtypes X_qdtype == torch qint W_zero_point = i range len W_zero_point _test_qconv_impl qconv qconv_prepack conv d batch_size input_channels_per_group length output_channels_per_group groups kernel stride pad None dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias none use_channelwise False input_dtype=X_qdtype output_dtype=X_qdtype given batch_size=st integers input_channels_per_group=st sampled_from output_channels_per_group=st sampled_from groups=st integers length=st integers kernel=st integers stride=st integers pad=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans use_channelwise=st booleans override_qengines test_qconv d_relu batch_size input_channels_per_group output_channels_per_group groups length kernel stride pad dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups torch backends quantized engine == qnnpack use_channelwise = False conv d = torch nn Conv d input_channels output_channels kernel stride pad dilation groups qconv_prepack = torch ops quantized conv d_prepack qconv = torch ops quantized conv d_relu act_qdtypes = torch quint Only qnnpack qengine supports qint qengine_is_qnnpack torch backends xnnpack enabled act_qdtypes append torch qint X_qdtype act_qdtypes X_qdtype == torch qint W_zero_point = i range len W_zero_point _test_qconv_impl qconv qconv_prepack conv d batch_size input_channels_per_group length output_channels_per_group groups kernel stride pad None dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias relu use_channelwise False input_dtype=X_qdtype output_dtype=X_qdtype TODO merge test test_qconv d when CUDNN runtime flags becomes available Tests correctness quantized D convolution cudnn op given batch_size=st integers cudnn only supports multiples we have explicitly added padding backend input_channels_per_group=st integers cudnn only supports multiples we have explicitly added padding backend output_channels_per_group=st integers groups=st integers currently padding only supports groups= length=st integers kernel=st integers stride=st integers pad=st integers currently cudnn has only been verified work dilation = TODO check backend works dilation dilation=st integers X_scale=st floats currently conv cudnn backend only implemented int symmetric X_zero_point=st sampled_from W_scale=st lists st floats min_size= max_size= currently conv cudnn backend only implemented int symmetric W_zero_point=st lists st integers min_size= max_size= Y_scale=st floats currently conv cudnn backend only implemented int symmetric Y_zero_point=st sampled_from use_bias=st booleans TODO enable channelwise use_channelwise=st sampled_from False skipIfNoFBGEMM unittest skipIf TEST_CUDNN cudnn enabled unittest skipIf SM OrLater requires sm later unittest skipIf TEST_ROCM supported rocm unittest skip currently working feature isn t used test_qconv d_cudnn batch_size input_channels_per_group output_channels_per_group groups length kernel stride pad dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups conv d = torch nn Conv d input_channels output_channels kernel stride pad dilation groups torch device cuda qconv_prepack = torch ops quantized conv d_prepack qconv = torch ops quantized conv d _test_qconv_impl qconv qconv_prepack conv d batch_size input_channels_per_group length output_channels_per_group groups kernel stride pad None dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias none use_channelwise False device=torch device cuda input_dtype=torch qint weight_dtype=torch qint output_dtype=torch qint given batch_size=st integers cudnn only supports multiples we have explicitly added padding backend input_channels_per_group=st integers cudnn only supports multiples we have explicitly added padding backend output_channels_per_group=st integers groups=st integers currently padding only supports groups= length=st integers kernel=st integers stride=st integers pad=st integers currently cudnn has only been verified work dilation = TODO check backend works dilation dilation=st integers X_scale=st floats currently conv cudnn backend only implemented int symmetric X_zero_point=st sampled_from W_scale=st lists st floats min_size= max_size= currently conv cudnn backend only implemented int symmetric W_zero_point=st lists st integers min_size= max_size= Y_scale=st floats currently conv cudnn backend only implemented int symmetric Y_zero_point=st sampled_from use_bias=st booleans TODO enable channelwise use_channelwise=st sampled_from False skipIfNoFBGEMM unittest skipIf TEST_CUDNN cudnn enabled unittest skipIf SM OrLater requires sm later unittest skipIf TEST_ROCM supported rocm unittest skip currently working feature isn t used test_qconv d_relu_cudnn batch_size input_channels_per_group output_channels_per_group groups length kernel stride pad dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups conv d = torch nn Conv d input_channels output_channels kernel stride pad dilation groups torch device cuda qconv_prepack = torch ops quantized conv d_prepack qconv = torch ops quantized conv d_relu _test_qconv_impl qconv qconv_prepack conv d batch_size input_channels_per_group length output_channels_per_group groups kernel stride pad None dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias relu use_channelwise False device=torch device cuda input_dtype=torch qint weight_dtype=torch qint output_dtype=torch qint given batch_size=st integers input_channels_per_group=st sampled_from D=st integers H=st integers W=st integers output_channels_per_group=st sampled_from groups=st integers kernel_d=st integers kernel_h=st integers kernel_w=st integers stride_d=st integers stride_h=st integers stride_w=st integers pad_d=st integers pad_h=st integers pad_w=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans use_channelwise=st booleans qengine=st sampled_from qnnpack fbgemm test_qconv d batch_size input_channels_per_group D H W output_channels_per_group groups kernel_d kernel_h kernel_w stride_d stride_h stride_w pad_d pad_h pad_w dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise qengine qengine supported_qengines input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_d kernel_h kernel_w strides = stride_d stride_h stride_w pads = pad_d pad_h pad_w dilations = dilation dilation dilation override_quantized_engine qengine qconv = torch ops quantized conv d qconv_prepack = torch ops quantized conv d_prepack conv_op = torch nn Conv d input_channels output_channels kernels strides pads dilations groups _test_qconv_impl qconv qconv_prepack conv_op batch_size input_channels_per_group D H W output_channels_per_group groups kernels strides pads None dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias none use_channelwise use_transpose=False given batch_size=st integers input_channels_per_group=st sampled_from D=st integers H=st integers W=st integers output_channels_per_group=st sampled_from groups=st integers kernel_d=st integers kernel_h=st integers kernel_w=st integers stride_d=st integers stride_h=st integers stride_w=st integers pad_d=st integers pad_h=st integers pad_w=st integers dilation=st integers X_scale=st floats X_zero_point=st integers W_scale=st lists st floats min_size= max_size= W_zero_point=st lists st integers - min_size= max_size= Y_scale=st floats Y_zero_point=st integers use_bias=st booleans use_channelwise=st booleans qengine=st sampled_from qnnpack fbgemm test_qconv d_relu batch_size input_channels_per_group D H W output_channels_per_group groups kernel_d kernel_h kernel_w stride_d stride_h stride_w pad_d pad_h pad_w dilation X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias use_channelwise qengine qengine supported_qengines input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups kernels = kernel_d kernel_h kernel_w strides = stride_d stride_h stride_w pads = pad_d pad_h pad_w dilations = dilation dilation dilation override_quantized_engine qengine qconv = torch ops quantized conv d_relu qconv_prepack = torch ops quantized conv d_prepack conv_op = torch nn Conv d input_channels output_channels kernels strides pads dilations groups _test_qconv_impl qconv qconv_prepack conv_op batch_size input_channels_per_group D H W output_channels_per_group groups kernels strides pads None dilations X_scale X_zero_point W_scale W_zero_point Y_scale Y_zero_point use_bias relu use_channelwise use_transpose=False Tests correctness quantized qconv d_unpack op given inputs=hu tensor_conv spatial_dim= batch_size_range= input_channels_per_group_range= output_channels_per_group_range= feature_map_range= kernel_range= max_groups= qparams= hu qparams dtypes=torch quint zero_point_min= zero_point_max= hu qparams dtypes=torch qint zero_point_min= zero_point_max= hu qparams dtypes=torch qint zero_point_min= zero_point_max= stride_d=st integers stride_h=st integers stride_w=st integers pad_d=st integers pad_h=st integers pad_w=st integers o_pad=st integers channelwise=st booleans override_qengines test_qconv d_unpack inputs stride_d stride_h stride_w pad_d pad_h pad_w o_pad channelwise qengine_is_qnnpack QNNPACK doesn t support transposed = inputs - transposed qconv_prepack = torch ops quantized conv_transpose d_prepack qconv_unpack = torch ops quantized conv_transpose d_unpack qconv_prepack = torch ops quantized conv d_prepack qconv_unpack = torch ops quantized conv d_unpack _test_qconv_unpack_impl qconv_prepack qconv_unpack inputs stride_d stride_h stride_w pad_d pad_h pad_w o_pad o_pad o_pad channelwise test_conv_reorder_issue_onednn Ensure reorder failure issue conv fixed onednn backend Onednn backend used encounter reorder failure when running conv dynamic input shapes Solved https github com pytorch pytorch pull onednn supported_qengines override_quantized_engine onednn bs = ic oc = kh kw = bias = None strides paddings dilates = groups ih iw = w = torch randn oc groups ic kh kw qw = torch quantize_per_tensor w scale= zero_point= dtype=torch qint x = torch randn bs ic groups ih iw qx = torch quantize_per_tensor x scale= zero_point= dtype=torch quint w_packed = torch ops quantized conv d_prepack qw bias strides paddings dilates groups torch ops quantized conv d qx w_packed output_scale= output_zero_point= ih iw = x = torch randn bs ic groups ih iw qx = torch quantize_per_tensor x scale= zero_point= dtype=torch quint The following should pass when input shape changed torch ops quantized conv d qx w_packed output_scale= output_zero_point= skipIfNoONEDNN test_conv_transpose_reorder_issue_onednn override_quantized_engine onednn bs = ic oc = kh kw = ih iw = bias = None strides paddings output_paddings dilates groups = w = torch randn ic oc kh kw qw = torch quantize_per_tensor w scale= zero_point= dtype=torch qint x = torch randn bs ic ih iw qx = torch quantize_per_tensor x scale= zero_point= dtype=torch quint w_packed = torch ops quantized conv_transpose d_prepack qw bias strides paddings output_paddings dilates groups torch ops quantized conv_transpose d qx w_packed output_scale= output_zero_point= ih iw = x = torch randn bs ic ih iw qx = torch quantize_per_tensor x scale= zero_point= dtype=torch quint The following should pass when input shape changed torch ops quantized conv_transpose d qx w_packed output_scale= output_zero_point= _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group= input_feature_map_shape= output_channels_per_group= groups= kernels= strides= pads= dilations= X_scale= X_zero_point= W_scale= W_zero_point= Y_scale= Y_zero_point= use_bias=True post_op=PointwisePostOp use_channelwise=True X _scale= X _zero_point= qconv_output_dtype=None None torch float torch bfloat weight_in_channel_last_format=False qconv_x _dtype=None ONEDNN only supports symmetric quantization weight W_zero_point None W_zero_point = len W_zero_point fp _output = qconv_output_dtype torch float bfloat _output = qconv_output_dtype torch bfloat fp _output bfloat _output Y_scale = Y_zero_point = X _scale = X _zero_point = batch_size = o_pads = None device = torch device cpu input_dtype = torch quint weight_dtype = torch qint output_dtype = torch quint use_transpose = False X W X_q W_q bias_float = _make_qconv_tensors batch_size input_channels_per_group input_feature_map_shape output_channels_per_group groups kernels strides pads dilations X_scale X_zero_point W_scale W_zero_point use_bias use_channelwise use_transpose device=device input_dtype=input_dtype weight_dtype=weight_dtype bias_float None bias_float = bias_float device Assign weights W = W_q dequantize X = X_q dequantize conv_op weight = torch nn Parameter W requires_grad=False conv_op bias = torch nn Parameter bias_float requires_grad=False use_bias None result_ref = conv_op X X _q = None post_op binary_attr == sum X_value_min X_value_max = X _init = torch randint X_value_min X_value_max result_ref size device=device X = X _scale X _init - X _zero_point float X _q = torch quantize_per_tensor X scale=X _scale zero_point=X _zero_point dtype=input_dtype result_ref = result_ref + X post_op unary_attr == relu relu = torch nn ReLU result_ref = relu result_ref post_op unary_attr == relu assert use_transpose Cannot fuse ReLU ConvTranspose relu = torch nn ReLU result_ref = relu result_ref post_op unary_attr == hardtanh assert use_transpose Cannot fuse hardtanh ConvTranspose assert len post_op scalars == For post op hardtanh expect parameters passed hardtanh = torch nn Hardtanh min_val=post_op scalars max_val=post_op scalars result_ref = hardtanh result_ref post_op unary_attr == hardswish assert use_transpose Cannot fuse hardswish ConvTranspose hardswish = torch nn Hardswish result_ref = hardswish result_ref post_op unary_attr == swish assert use_transpose Cannot fuse silu ConvTranspose silu = torch nn SiLU result_ref = silu result_ref Quantize reference results comparison result_ref_q = torch quantize_per_tensor result_ref scale=Y_scale zero_point=Y_zero_point dtype=output_dtype Calculate result X path X_q_cpu_tensor = X_q int_repr W_q_cpu_tensor = W_q int_repr weight_scale = W_q q_per_channel_scales use_channelwise torch tensor W_q q_scale dtype=torch double device=device weight_zero_point = W_q q_per_channel_zero_points use_channelwise torch tensor W_q q_zero_point dtype=torch int device=device weight_in_channel_last_format W_q_cpu_tensor dim == W_q_cpu_tensor = W_q_cpu_tensor memory_format=torch channels_last_ d W_q_cpu_tensor dim == W_q_cpu_tensor = W_q_cpu_tensor memory_format=torch channels_last packed_weight = qconv_prepack W_q_cpu_tensor weight_scale X_scale X_zero_point strides pads dilations groups X_q_cpu_tensor size post_op binary_attr == sum X _cpu_tensor = X _q int_repr qconv_output_dtype None X _q dequantize qconv_x _dtype contiguous memory_format=torch channels_last Y_q_cpu_tensor = qconv X_q_cpu_tensor X_scale X_zero_point packed_weight weight_scale weight_zero_point X _cpu_tensor bias_float strides pads dilations groups Y_scale Y_zero_point qconv_output_dtype X _scale X _zero_point post_op binary_attr post_op alpha post_op unary_attr post_op scalars post_op algorithm Y_q_cpu_tensor = qconv X_q_cpu_tensor X_scale X_zero_point packed_weight weight_scale weight_zero_point bias_float strides pads dilations groups Y_scale Y_zero_point qconv_output_dtype post_op unary_attr post_op scalars post_op algorithm fp _output bfloat _output assertTrue Y_q_cpu_tensor dtype == qconv_output_dtype Y_q_cpu_tensor = torch quantize_per_tensor Y_q_cpu_tensor fp _output Y_q_cpu_tensor torch float scale=Y_scale zero_point=Y_zero_point dtype=output_dtype int_repr Make sure results match assert_array_almost_equal compares using following formula abs desired-actual -decimal https numpy org doc stable reference generated numpy testing assert_almost_equal html We use decimal = ignore off-by- differences between reference test Off-by- differences arise due order round zero_point addition operation i e addition followed round used reference round followed addition used test results may differ For example result round + while round + assuming rounding mode round-to-nearest ties-to-even np testing assert_array_almost_equal result_ref_q int_repr cpu numpy Y_q_cpu_tensor cpu numpy decimal= err_msg=f X X_q W W_q b bias_float strides strides pads pads o_pads o_pads dilations dilations groups groups y_s Y_scale y_zp Y_zero_point X X _q Return quantized data later reuse X_q W_q bias_float unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_pt e groups_list = input_channels_per_group = output_channels_per_group = length = kernel = stride = pad = dilation = W_scale = W_zero_point = use_bias_list = False True use_channelwise_list = False True output_dtype_list = None torch float torch bfloat options = itertools product groups_list use_bias_list use_channelwise_list output_dtype_list groups use_bias use_channelwise output_dtype options output_dtype None use_bias use_channelwise Remove some test combination reduce UT test time continue conv d = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernel stride pad dilation groups qconv = torch ops onednn qconv d_pointwise qconv_prepack = torch ops onednn qconv_prepack pointwise_post_op = PointwisePostOp _test_qconv_impl_cpu_tensor qconv qconv_prepack conv d input_channels_per_group=input_channels_per_group input_feature_map_shape= length output_channels_per_group=output_channels_per_group groups=groups kernels=kernel strides= stride pads= pad dilations= dilation W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise qconv_output_dtype=output_dtype unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_pt e groups_list = input_channels_per_group = output_channels_per_group = input_feature_map_shape = kernels = strides = pads = dilations = W_scale = W_zero_point = use_bias_list = False True use_channelwise_list = False True channel_last_weight_format_list = False True output_dtype_list = None torch float torch bfloat options = itertools product groups_list use_bias_list use_channelwise_list channel_last_weight_format_list output_dtype_list groups use_bias use_channelwise channel_last_weight_format output_dtype options output_dtype None channel_last_weight_format use_bias use_channelwise Remove some test combination reduce UT test time continue qconv = torch ops onednn qconv_pointwise qconv_prepack = torch ops onednn qconv_prepack conv_op = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernels strides pads dilations groups pointwise_post_op = PointwisePostOp _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group=input_channels_per_group input_feature_map_shape=input_feature_map_shape output_channels_per_group=output_channels_per_group groups=groups kernels=kernels strides=strides pads=pads dilations=dilations W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise qconv_output_dtype=output_dtype weight_in_channel_last_format=channel_last_weight_format unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_pt e input_channels_per_group = input_feature_map_shape = output_channels_per_group = groups_list = kernels = strides = pads = dilations = W_scale = W_zero_point = use_bias_list = False True use_channelwise_list = False True channel_last_weight_format_list = False True output_dtype_list = None torch float torch bfloat options = itertools product groups_list use_bias_list use_channelwise_list channel_last_weight_format_list output_dtype_list groups use_bias use_channelwise channel_last_weight_format output_dtype options output_dtype None channel_last_weight_format use_bias use_channelwise Remove some test combination reduce UT test time continue qconv = torch ops onednn qconv d_pointwise qconv_prepack = torch ops onednn qconv_prepack conv_op = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernels strides pads dilations groups pointwise_post_op = PointwisePostOp _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group=input_channels_per_group input_feature_map_shape=input_feature_map_shape output_channels_per_group=output_channels_per_group groups=groups kernels=kernels strides=strides pads=pads dilations=dilations W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise qconv_output_dtype=output_dtype weight_in_channel_last_format=channel_last_weight_format Test qconv post op relu unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_relu_pt e input_channels_per_group = output_channels_per_group = groups_list = input_feature_map_shape = kernels = strides = pads = dilations = W_scale = W_zero_point = use_bias_list = False True use_channelwise_list = False True output_dtype_list = None torch float torch bfloat options = itertools product groups_list use_bias_list use_channelwise_list output_dtype_list groups use_bias use_channelwise output_dtype options qconv = torch ops onednn qconv_pointwise qconv_prepack = torch ops onednn qconv_prepack conv_op = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernels strides pads dilations groups pointwise_post_op = PointwisePostOp unary_attr= relu _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group=input_channels_per_group input_feature_map_shape=input_feature_map_shape output_channels_per_group=output_channels_per_group groups=groups kernels=kernels strides=strides pads=pads dilations=dilations W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise qconv_output_dtype=output_dtype Test qconv post op hardtanh unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_hardtanh_pt e input_channels_per_group = output_channels_per_group = groups_list = input_feature_map_shape = kernels = strides = pads = dilations = W_scale = W_zero_point = use_bias_list = False True use_channelwise_list = False True output_dtype_list = None torch float torch bfloat options = itertools product groups_list use_bias_list use_channelwise_list output_dtype_list groups use_bias use_channelwise output_dtype options qconv = torch ops onednn qconv_pointwise qconv_prepack = torch ops onednn qconv_prepack conv_op = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernels strides pads dilations groups pointwise_post_op = PointwisePostOp unary_attr= hardtanh scalars= _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group=input_channels_per_group input_feature_map_shape=input_feature_map_shape output_channels_per_group=output_channels_per_group groups=groups kernels=kernels strides=strides pads=pads dilations=dilations W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise qconv_output_dtype=output_dtype Test qconv post op swish unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_swish_pt e input_channels_per_group = output_channels_per_group = groups_list = input_feature_map_shape = kernels = strides = pads = dilations = W_scale = W_zero_point = use_bias_list = False True use_channelwise_list = False True output_dtype_list = None torch float torch bfloat options = itertools product groups_list use_bias_list use_channelwise_list output_dtype_list groups use_bias use_channelwise output_dtype options qconv = torch ops onednn qconv_pointwise qconv_prepack = torch ops onednn qconv_prepack conv_op = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernels strides pads dilations groups pointwise_post_op = PointwisePostOp unary_attr= swish _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group=input_channels_per_group input_feature_map_shape=input_feature_map_shape output_channels_per_group=output_channels_per_group groups=groups kernels=kernels strides=strides pads=pads dilations=dilations W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise qconv_output_dtype=output_dtype Test qconv post op hardswish unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_hardswish_pt e input_channels_per_group = output_channels_per_group = groups_list = input_feature_map_shape = kernels = strides = pads = dilations = W_scale = W_zero_point = use_bias_list = False True use_channelwise_list = False True output_dtype_list = None torch float torch bfloat options = itertools product groups_list use_bias_list use_channelwise_list output_dtype_list groups use_bias use_channelwise output_dtype options qconv = torch ops onednn qconv_pointwise qconv_prepack = torch ops onednn qconv_prepack conv_op = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernels strides pads dilations groups pointwise_post_op = PointwisePostOp unary_attr= hardswish _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group=input_channels_per_group input_feature_map_shape=input_feature_map_shape output_channels_per_group=output_channels_per_group groups=groups kernels=kernels strides=strides pads=pads dilations=dilations W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise qconv_output_dtype=output_dtype Test qconv post op sum unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_sum_pt e groups_list = input_channels_per_group = output_channels_per_group = input_feature_map_shape = kernels = strides = pads = dilations = W_scale = W_zero_point = - use_bias_list = False True use_channelwise_list = False True output_dtype_list = None torch float torch bfloat X _zero_point_list = options = itertools product groups_list use_bias_list use_channelwise_list X _zero_point_list output_dtype_list groups use_bias use_channelwise X _zero_point output_dtype options qconv = torch ops onednn qconv d_pointwise binary qconv_prepack = torch ops onednn qconv_prepack conv_op = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernels strides pads dilations groups pointwise_post_op = PointwisePostOp binary_attr= sum _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group=input_channels_per_group input_feature_map_shape=input_feature_map_shape output_channels_per_group=output_channels_per_group groups=groups kernels=kernels strides=strides pads=pads dilations=dilations W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise X _zero_point=X _zero_point qconv_output_dtype=output_dtype qconv_x _dtype=output_dtype Test qconv post op sum relu unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_sum_relu_pt e groups_list = input_channels_per_group = output_channels_per_group = input_feature_map_shape = kernels = strides = pads = dilations = W_scale = W_zero_point = - use_bias_list = False True use_channelwise_list = False True X _zero_point_list = options = itertools product groups_list use_bias_list use_channelwise_list X _zero_point_list groups use_bias use_channelwise X _zero_point options qconv = torch ops onednn qconv d_pointwise binary qconv_prepack = torch ops onednn qconv_prepack conv_op = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernels strides pads dilations groups pointwise_post_op = PointwisePostOp binary_attr= sum unary_attr= relu _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group=input_channels_per_group input_feature_map_shape=input_feature_map_shape output_channels_per_group=output_channels_per_group groups=groups kernels=kernels strides=strides pads=pads dilations=dilations W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise X _zero_point=X _zero_point Test qconv post op sum unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_sum_relu_float_output_pt e groups = input_channels_per_group = output_channels_per_group = input_feature_map_shape = kernels = strides = pads = dilations = W_scale = W_zero_point = - use_bias_list = False True use_channelwise = True output_dtype_list = torch float torch bfloat X _zero_point = use_relu_list = True False options = itertools product use_bias_list output_dtype_list use_relu_list use_bias output_dtype use_relu options qconv_x _dtype = output_dtype qconv = torch ops onednn qconv d_pointwise binary qconv_prepack = torch ops onednn qconv_prepack conv_op = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernels strides pads dilations groups pointwise_post_op = PointwisePostOp binary_attr= sum unary_attr= relu use_relu PointwisePostOp binary_attr= sum _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group=input_channels_per_group input_feature_map_shape=input_feature_map_shape output_channels_per_group=output_channels_per_group groups=groups kernels=kernels strides=strides pads=pads dilations=dilations W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise X _zero_point=X _zero_point qconv_output_dtype=output_dtype qconv_x _dtype=qconv_x _dtype Test qconv d post op relu unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_relu_pt e input_channels_per_group = output_channels_per_group = groups_list = input_feature_map_shape = kernels = strides = pads = dilations = W_scale = W_zero_point = use_bias_list = False True use_channelwise_list = False True output_dtype_list = None torch float torch bfloat options = itertools product groups_list use_bias_list use_channelwise_list output_dtype_list groups use_bias use_channelwise output_dtype options qconv = torch ops onednn qconv_pointwise qconv_prepack = torch ops onednn qconv_prepack conv_op = torch nn Conv d input_channels_per_group groups output_channels_per_group groups kernels strides pads dilations groups pointwise_post_op = PointwisePostOp unary_attr= relu _test_qconv_impl_cpu_tensor qconv qconv_prepack conv_op input_channels_per_group=input_channels_per_group input_feature_map_shape=input_feature_map_shape output_channels_per_group=output_channels_per_group groups=groups kernels=kernels strides=strides pads=pads dilations=dilations W_scale=W_scale W_zero_point=W_zero_point use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise qconv_output_dtype=output_dtype _make_qconv_tensors_fp batch_size input_channels_per_group input_feature_map_shape output_channels_per_group groups kernels strides pads dilations use_bias use_channelwise use_transpose device=torch device cpu assert use_channelwise use_transpose \ Cannot generate channelwise qconv_transpose_tensors input_channels = input_channels_per_group groups output_channels = output_channels_per_group groups Padded input size should least big dilated kernel kernels = _single kernels strides = _single strides pads = _single pads dilations = _single dilations i range len kernels assume input_feature_map_shape i + pads i = dilations i kernels i - + operator expects them format output_channels input_channels groups kernel_d kernel_h kernel_w input_channels output_channels groups kernel_d kernel_h kernel_w use_transpose output_shape = input_channels output_channels_per_group output_shape = output_channels input_channels_per_group X = torch rand batch_size input_channels + input_feature_map_shape device=device X_q X_scale = _quantize_fp e m X channelwise=False W = torch randn output_shape + kernels device=device W_q W_scale = _quantize_fp e m W channelwise=use_channelwise bias_float = torch randn output_channels device=device use_bias None X W X_q W_q X_scale W_scale bias_float _test_qconv_impl_cpu_tensor_fp qconv qconv_prepack conv_op input_channels_per_group= input_feature_map_shape= output_channels_per_group= groups= kernels= strides= pads= dilations= Y_scale= use_bias=True post_op=PointwisePostOp use_channelwise=True X _scale= qconv_output_dtype=None None torch float torch bfloat weight_in_channel_last_format=False We assume FP quantization always symmetric fp _output = qconv_output_dtype torch float bfloat _output = qconv_output_dtype torch bfloat fp _output bfloat _output Y_scale = X _scale = batch_size = device = torch device cpu use_transpose = False X W X_q W_q X_scale W_scale bias_float = _make_qconv_tensors_fp batch_size input_channels_per_group input_feature_map_shape output_channels_per_group groups kernels strides pads dilations use_bias use_channelwise use_transpose device=device Assign weights dqW = _dequantize_fp e m W_q W_scale dqX = _dequantize_fp e m X_q X_scale conv_op weight = torch nn Parameter dqW requires_grad=False conv_op bias = torch nn Parameter bias_float requires_grad=False use_bias None result_ref = conv_op dqX X = None X _q = None X _scale = post_op binary_attr == sum X _dtype = qconv_output_dtype qconv_output_dtype torch float X = torch rand_like result_ref device=device dtype=X _dtype qconv_output_dtype None X _q X _scale = _quantize_fp e m X channelwise=False X _dq = _dequantize_fp e m X _q X _scale X _scale = X _scale item X _dq = X result_ref = result_ref + X _dq post_op unary_attr == relu relu = torch nn ReLU result_ref = relu result_ref post_op unary_attr == relu assert use_transpose Cannot fuse ReLU ConvTranspose relu = torch nn ReLU result_ref = relu result_ref post_op unary_attr == hardtanh assert use_transpose Cannot fuse hardtanh ConvTranspose assert len post_op scalars == For post op hardtanh expect parameters passed hardtanh = torch nn Hardtanh min_val=post_op scalars max_val=post_op scalars result_ref = hardtanh result_ref post_op unary_attr == hardswish assert use_transpose Cannot fuse hardswish ConvTranspose hardswish = torch nn Hardswish result_ref = hardswish result_ref post_op unary_attr == swish assert use_transpose Cannot fuse silu ConvTranspose silu = torch nn SiLU result_ref = silu result_ref Quantize reference results comparison qconv_output_dtype None result_ref = _quantize_fp e m result_ref False Y_scale result_ref = result_ref qconv_output_dtype Calculate result PT E path weight_in_channel_last_format W_q dim == W_q = W_q memory_format=torch channels_last_ d W_q dim == W_q = W_q memory_format=torch channels_last X_scale_scalar = X_scale item packed_weight = qconv_prepack W_q W_scale X_scale_scalar X_zero_point strides pads dilations groups X_q size post_op binary_attr == sum accum = X _q contiguous memory_format=torch channels_last X _q None X contiguous memory_format=torch channels_last result = qconv X_q X_scale_scalar X_zero_point packed_weight W_scale torch zeros dtype=torch int W_zero_point accum bias_float strides pads dilations groups Y_scale Y_zero_point qconv_output_dtype X _scale X _zero_point post_op binary_attr post_op alpha post_op unary_attr post_op scalars post_op algorithm result = qconv X_q X_scale_scalar X_zero_point packed_weight W_scale torch zeros dtype=torch int W_zero_point bias_float strides pads dilations groups Y_scale Y_zero_point qconv_output_dtype post_op unary_attr post_op scalars post_op algorithm fp _output bfloat _output assertTrue result dtype == qconv_output_dtype assertEqual result float result_ref float atol= e- rtol= e- assert torch isnan result any _test_qconv_fp _helper nd pointwise_post_op nd = - conv d d d pointwise_post_op binary_attr = none Only conv d supports binary post op assert nd == groups_list = input_channels_per_group = output_channels_per_group = length = kernel = stride = pad = dilation = use_bias_list = False True use_channelwise_list = False True output_dtype_list = None torch float torch bfloat options = itertools product groups_list use_bias_list use_channelwise_list output_dtype_list groups use_bias use_channelwise output_dtype options output_dtype None use_bias use_channelwise Remove some test combination reduce UT test time continue conv_mod = getattr torch nn f Conv nd d input_channels_per_group groups output_channels_per_group groups kernel stride pad dilation groups qconv = torch ops onednn qconv_pointwise pointwise_post_op binary_attr == none torch ops onednn qconv d_pointwise binary qconv_prepack = torch ops onednn qconv_prepack _test_qconv_impl_cpu_tensor_fp qconv qconv_prepack conv_mod input_channels_per_group=input_channels_per_group input_feature_map_shape= length nd output_channels_per_group=output_channels_per_group groups=groups kernels= kernel nd strides= stride nd pads= pad nd dilations= dilation nd use_bias=use_bias post_op=pointwise_post_op use_channelwise=use_channelwise qconv_output_dtype=output_dtype unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_fp pointwise_post_op = PointwisePostOp _test_qconv_fp _helper pointwise_post_op unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_relu_fp pointwise_post_op = PointwisePostOp unary_attr= relu _test_qconv_fp _helper pointwise_post_op unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_fp pointwise_post_op = PointwisePostOp _test_qconv_fp _helper pointwise_post_op unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_relu_fp pointwise_post_op = PointwisePostOp unary_attr= relu _test_qconv_fp _helper pointwise_post_op unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_hardtanh_fp pointwise_post_op = PointwisePostOp unary_attr= hardtanh scalars= _test_qconv_fp _helper pointwise_post_op unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_swish_fp pointwise_post_op = PointwisePostOp unary_attr= swish _test_qconv_fp _helper pointwise_post_op unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_hardswish_fp pointwise_post_op = PointwisePostOp unary_attr= hardswish _test_qconv_fp _helper pointwise_post_op unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_sum_fp pointwise_post_op = PointwisePostOp binary_attr= sum _test_qconv_fp _helper pointwise_post_op unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_sum_relu_fp pointwise_post_op = PointwisePostOp binary_attr= sum unary_attr= relu _test_qconv_fp _helper pointwise_post_op unittest skipIf IS_FBCODE Skip pt e ops fbcode skipIfNoONEDNN test_qconv d_fp pointwise_post_op = PointwisePostOp torch manual_seed For reproducibility D conv tests _test_qconv_fp _helper pointwise_post_op TestPadding TestCase given batch_size=st integers channels=st integers width=st integers qtype=st sampled_from hu _ALL_QINT_TYPES test_reflection_pad d batch_size channels width qtype padding = width x = torch arange batch_size channels width torch float x = x resize batch_size channels width Per-Tensor test scale zp = _calculate_dynamic_qparams x qtype qx = torch quantize_per_tensor x scale zp qtype padding_op = torch nn ReflectionPad d padding y_ref = padding_op x qy_ref = torch quantize_per_tensor y_ref scale zp qtype qy_hat = padding_op qx assertEqual qy_ref qy_hat Out variant qy_hat = torch _C _nn reflection_pad d qx padding out=qy_hat assertEqual qy_ref qy_hat given batch_size=st integers channels=st integers height=st integers width=st integers qtype=st sampled_from hu _ALL_QINT_TYPES test_reflection_pad d batch_size channels height width qtype padding = width width height height x = torch arange batch_size channels height width torch float x = x resize batch_size channels height width Per-Tensor test scale zp = _calculate_dynamic_qparams x qtype qx = torch quantize_per_tensor x scale zp qtype padding_op = torch nn ReflectionPad d padding y_ref = padding_op x qy_ref = torch quantize_per_tensor y_ref scale zp qtype qy_hat = padding_op qx assertEqual qy_ref qy_hat Out variant qy_hat = torch _C _nn reflection_pad d qx padding out=qy_hat assertEqual qy_ref qy_hat given batch_size=st integers channels=st integers hwd=st integers For D max input size would x x d=st sampled_from value=st floats - allow_nan=False allow_infinity=False qtype=st sampled_from hu _ALL_QINT_TYPES test_constant_padNd batch_size channels d hwd value qtype padding = hwd shape = batch_size channels hwd op = torch nn ConstantPad d d = shape append hwd op = torch nn ConstantPad d d == shape append hwd op = torch nn ConstantPad d numel = np prod shape x = torch arange numel torch float x = x resize shape Per-Tensor test scale zp = _calculate_dynamic_qparams x qtype qx = torch quantize_per_tensor x scale zp qtype padding_op = op padding value y_ref = padding_op x qy_ref = torch quantize_per_tensor y_ref scale zp qtype qy_hat = padding_op qx assertEqual qy_ref qy_hat unittest skipUnless qnnpack supported_qengines This Pytorch Build has been built does support QNNPACK TestQNNPackOps TestCase Tests correctness quantized qnnpack_relu op given X=hu tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch quint zero_point_min= zero_point_max= test_qnnpack_relu X override_quantized_engine qnnpack X scale zero_point torch_type = X relu = torch nn functional relu X = torch from_numpy X Y = X clone qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type qY_hat = relu qX Y Y = qY = torch quantize_per_tensor Y scale=scale zero_point=zero_point dtype=torch_type assertEqual qY qY_hat Tests correctness quantized qnnpack_tanh op skipIfNoFBGEMM test_qnnpack_tanh Note In QNNPACK output scale zero_point can only respectively uses LUT bins shapes = memory_formats = torch channels_last torch contiguous_format test_cases = itertools product shapes memory_formats shape memory_format test_cases X scale zero_point torch_type = torch randn shape torch quint memory_format == torch channels_last len shape = continue X = X memory_format=memory_format qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type Floating point reference Y = torch tanh qX dequantize qY = torch quantize_per_tensor Y scale= zero_point= dtype=torch quint override_quantized_engine fbgemm qYserver = torch tanh qX override_quantized_engine qnnpack qY_hat = torch tanh qX assertEqual qY qY_hat msg=f QNNPACK TanH failed FP ref memory_format memory_format assertEqual qYserver qY_hat msg=f QNNPACK TanH failed FBGEMM ref memory_format memory_format Tests correctness quantized qnnpack_sigmoid op skipIfNoFBGEMM test_qnnpack_sigmoid Note In QNNPACK output scale zero_point can only respectively uses LUT bins shapes = memory_formats = torch channels_last torch contiguous_format test_cases = itertools product shapes memory_formats shape memory_format test_cases X scale zero_point torch_type = torch randn shape torch quint memory_format == torch channels_last len shape = continue X = X memory_format=memory_format qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type Floating point reference Y = torch sigmoid qX dequantize qY = torch quantize_per_tensor Y scale= zero_point= dtype=torch quint override_quantized_engine fbgemm qYserver = torch sigmoid qX override_quantized_engine qnnpack qY_hat = torch sigmoid qX assertEqual qY qY_hat msg=f QNNPACK Sigmoid failed FP ref memory_format memory_format assertEqual qYserver qY_hat msg=f QNNPACK Sigmoid failed FBGEMM ref memory_format memory_format skipIfNoFBGEMM test_qnnpack_sigmoid_sweep Input parameters f_min = - f_max = scale = f_max - f_min zero_point = dtype = torch quint step = scale x = np arange f_min f_max + step step X = torch from_numpy x torch float qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=dtype dqX = qX dequantize Floating point reference Y = torch sigmoid dqX qY = torch quantize_per_tensor Y scale= zero_point= dtype=torch quint override_quantized_engine fbgemm qYserver = torch sigmoid qX override_quantized_engine qnnpack qY_hat = torch sigmoid qX assertEqual qY qY_hat msg= QNNPACK Sigmoid failed FP ref assertEqual qYserver qY_hat msg= QNNPACK Sigmoid failed FBGEMM ref Tests correctness quantized add qnnpack op settings suppress_health_check= HealthCheck filter_too_much given A=hu tensor shapes=hu array_shapes qparams=hu qparams dtypes= torch quint torch qint zero_point=st sampled_from scale_A=st sampled_from scale_B=st sampled_from scale_C=st sampled_from test_qnnpack_add A zero_point scale_A scale_B scale_C override_quantized_engine qnnpack A_temp = A channels_last True False channels_last len A_temp shape = continue A scale_a zero_point_A torch_type = A_temp B scale_b zero_point_B torch_type = A_temp A = torch from_numpy A B = torch from_numpy B torch_type == torch qint torch backends xnnpack enabled continue channels_last A = A memory_format=torch channels_last B = B memory_format=torch channels_last assume scale_A scale_C = - assume scale_A scale_C assume scale_B scale_C = - assume scale_B scale_C zero_point_C = np_dtype = np uint torch_type == torch qint zero_point_C = np_dtype = np int qA = torch quantize_per_tensor A scale=scale_A zero_point=zero_point dtype=torch_type qB = torch quantize_per_tensor B scale=scale_B zero_point=zero_point dtype=torch_type Add ground truth C = qA dequantize + qB dequantize numpy qC = _quantize C scale_C zero_point_C dtype=np_dtype qC_qnnp = torch ops quantized add qA qB scale_C zero_point_C np testing assert_equal qC qC_qnnp int_repr Quantized addition failed Crelu = C copy Crelu C = qCrelu = torch quantize_per_tensor torch from_numpy Crelu scale_C zero_point_C dtype=torch_type qCrelu_hat = torch ops quantized add_relu qA qB scale=scale_C zero_point=zero_point_C np testing assert_equal qCrelu int_repr numpy qCrelu_hat int_repr Quantized addition ReLU failed Tests correctness quantized add qnnpack mul settings suppress_health_check= HealthCheck filter_too_much given A=hu tensor shapes=hu array_shapes qparams=hu qparams dtypes= torch quint torch qint zero_point=st sampled_from scale_A=st sampled_from scale_B=st sampled_from scale_C=st sampled_from test_qnnpack_mul A zero_point scale_A scale_B scale_C override_quantized_engine qnnpack A_temp = A channels_last True False channels_last len A_temp shape = continue A scale_a zero_point_A torch_type = A_temp B scale_b zero_point_B torch_type = A_temp A = torch from_numpy A B = torch from_numpy B torch_type == torch qint torch backends xnnpack enabled continue channels_last A = A memory_format=torch channels_last B = B memory_format=torch channels_last assume scale_A scale_C = - assume scale_A scale_C assume scale_B scale_C = - assume scale_B scale_C zero_point_C = np_dtype = np uint torch_type == torch qint zero_point_C = np_dtype = np int qA = torch quantize_per_tensor A scale=scale_A zero_point=zero_point dtype=torch_type qB = torch quantize_per_tensor B scale=scale_B zero_point=zero_point dtype=torch_type Add ground truth C = qA dequantize qB dequantize numpy qC = _quantize C scale_C zero_point_C dtype=np_dtype qC_qnnp = torch ops quantized mul qA qB scale_C zero_point_C np testing assert_equal qC qC_qnnp int_repr Quantized addition failed Crelu = C copy Crelu C = qCrelu = torch quantize_per_tensor torch from_numpy Crelu scale_C zero_point_C dtype=torch_type qCrelu_hat = torch ops quantized mul_relu qA qB scale=scale_C zero_point=zero_point_C np testing assert_equal qCrelu int_repr numpy qCrelu_hat int_repr Quantized addition ReLU failed Tests quantized add works broadcasting test_qnnpack_add_broadcast _run_test A B qA = torch quantize_per_tensor A dtype qB = torch quantize_per_tensor B dtype output_scale = output_zp = ground truth C = qA dequantize + qB dequantize qC = torch quantize_per_tensor C output_scale output_zp dtype quantized qC_hat_ = torch ops quantized add qA qB output_scale output_zp qC_hat_ = torch ops quantized add qB qA output_scale output_zp assertTrue torch allclose qC dequantize qC_hat_ dequantize assertTrue torch allclose qC dequantize qC_hat_ dequantize override_quantized_engine qnnpack dtype torch qint torch quint dtype == torch qint torch backends xnnpack enabled continue channels_last True False d A = torch randn B = torch randn channels_last A = A memory_format=torch channels_last B = B memory_format=torch channels_last _run_test A B d C = torch randn D = torch randn channels_last C = C memory_format=torch channels_last_ d D = D memory_format=torch channels_last_ d _run_test C D Tests correctness quantized qnnpack_maxpool d op given A=hu tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch quint kernel=st sampled_from stride=st sampled_from padding=st sampled_from test_qnnpack_maxpool d A kernel stride padding torch nn functional F override_quantized_engine qnnpack A scale zero_point torch_type = A X = torch from_numpy A np_type = np uint dilation = Check constraints assume kernel = padding Kernel cannot overhanging iH iW = X shape - oH = pool_output_shape iH kernel padding stride dilation assume oH oW = pool_output_shape iW kernel padding stride dilation assume oW k = kernel kernel s = stride stride d = dilation dilation p = padding padding q_max_pool = torch ops quantized max_pool d = scale X - zero_point dtype=torch float qa = torch quantize_per_tensor scale=scale zero_point=zero_point dtype=torch_type a_ref = qa dequantize a_pool = F max_pool d a_ref kernel_size=k stride=s padding=p dilation=d a_pool_nhwc = a_pool permute qa_pool = q_max_pool qa k s p d ceil_mode=False qa_pool_int = qa_pool dequantize np testing assert_equal a_pool numpy qa_pool_int numpy given batch_size=st integers channels=st sampled_from height=st integers width=st integers kernel=st integers stride=st integers padding=st integers scale=st floats zero_point=st integers test_avg_pool d batch_size channels height width kernel stride padding scale zero_point override_quantized_engine qnnpack torch nn functional F X_init = torch from_numpy np random randint batch_size channels height width X = scale X_init - zero_point dtype=torch float Check constraints assume kernel = padding Kernel cannot overhanging iH iW = X shape - oH = pool_output_shape iH kernel padding stride assume oH oW = pool_output_shape iW kernel padding stride assume oW k = kernel kernel s = stride stride p = padding padding q_avg_pool = torch ao nn quantized functional avg_pool d x_q = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch quint a_pool = F avg_pool d x_q dequantize torch float kernel_size=k stride=s padding=p qa_pool = q_avg_pool x_q k s p Quantize Ref Output a_pool_q = torch quantize_per_tensor a_pool scale=scale zero_point=zero_point dtype=torch quint np testing assert_array_almost_equal a_pool_q int_repr numpy qa_pool int_repr numpy decimal= given batch_size=st integers channels=st sampled_from height=st integers width=st integers output_height=st integers output_width=st integers scale=st floats zero_point=st integers test_adaptive_avg_pool d batch_size channels height width output_height output_width scale zero_point override_quantized_engine qnnpack Check constraints assume height = output_height assume width = output_width torch nn functional F X_init = torch from_numpy np random randint batch_size channels height width X = scale X_init - zero_point dtype=torch float iH iW = X shape - q_avg_pool = torch ao nn quantized functional adaptive_avg_pool d x_q = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch quint a_pool = F adaptive_avg_pool d x_q dequantize torch float output_height output_width qa_pool = q_avg_pool x_q output_height output_width Quantize Ref Output a_pool_q = torch quantize_per_tensor a_pool scale=scale zero_point=zero_point dtype=torch quint np testing assert_array_almost_equal a_pool_q int_repr numpy qa_pool int_repr numpy decimal= given batch_size=st integers channels=st sampled_from height=st integers width=st integers scale=st floats zero_point=st integers test_mean batch_size channels height width scale zero_point override_quantized_engine qnnpack dim = X_init = torch from_numpy np random randint batch_size channels height width X = scale X_init - zero_point dtype=torch float qX = torch quantize_per_tensor X scale zero_point torch quint Y = torch mean qX dequantize dim Y = torch quantize_per_tensor Y scale zero_point torch quint qY = torch mean qX dim np testing assert_array_almost_equal Y int_repr numpy qY int_repr numpy decimal= Tests correctness quantized hardtanh op test_hardtanh qnnpack torch backends quantized supported_engines override_quantized_engine qnnpack shapes = memory_formats = torch channels_last torch contiguous_format min_vals = - - max_vals = - test_cases = itertools product shapes memory_formats min_vals max_vals shape memory_format min_val max_val test_cases X scale zero_point torch_type = torch randn shape torch quint memory_format == torch channels_last len shape = continue Y = X clone Y Y min_val = min_val Y Y max_val = max_val qY = torch quantize_per_tensor Y scale=scale zero_point=zero_point dtype=torch_type qX = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch_type qY_hat = torch ao nn quantized functional hardtanh qX min_val max_val assertEqual qY qY_hat msg=f hardtanh failed \nactual qY_hat \nexpected qY \nmemory_format memory_format Tests correctness tensor comparators TestComparatorOps TestCase Tests element-wise equality ops given A=hu tensor shapes= qparams=hu qparams B=hu tensor shapes= qparams=hu qparams test_compare_tensor_tensor A B A scale_a zero_point_a dtype_a = A B scale_b zero_point_b dtype_b = B tA = torch from_numpy A tB = torch from_numpy B qA = torch quantize_per_tensor tA scale=scale_a zero_point=zero_point_a dtype=dtype_a qB = torch quantize_per_tensor tB scale=scale_b zero_point=zero_point_b dtype=dtype_b dqA = qA dequantize dqB = qB dequantize ops_under_test = __eq__ __ne__ __ge__ __le__ __gt__ __lt__ eq ne ge le gt lt op ops_under_test result_ref = getattr dqA op dqB result = getattr qA op qB assertEqual result_ref result msg=f tensor op tensor failed Reversed broadcasting result_ref = getattr dqB op dqA result = getattr qB op qA assertEqual result_ref result msg=f tensor op tensor failed given A=hu tensor shapes= qparams=hu qparams b=hu floats allow_infinity=False allow_nan=False test_compare_tensor_scalar A b A scale_a zero_point_a dtype_a = A tA = torch from_numpy A qA = torch quantize_per_tensor tA scale=scale_a zero_point=zero_point_a dtype=dtype_a dqA = qA dequantize ops_under_test_reversible = __eq__ __ne__ __ge__ __le__ __gt__ __lt__ ops_under_test_nonreversible = eq ne ge le gt lt op ops_under_test_reversible result_ref = getattr dqA op b result = getattr qA op b note f result_ref result_ref note f result result assertEqual result_ref result msg=f tensor op scalar failed Reversed broadcasting result_ref = getattr b op dqA result = getattr b op qA note f result_ref result_ref note f result result assertEqual result_ref result msg=f scalar op tensor failed op ops_under_test_nonreversible result_ref = getattr dqA op b result = getattr qA op b note f result_ref result_ref note f result result assertEqual result_ref result msg=f tensor op scalar failed Tests correctness quantized embedding_bag_ byte &#124; bit &#124; bit _prepack_with_rowwise_min_max ops TestQuantizedWithMinMax TestCase Validates rowwsie_min_max quantization functions equivalent ones without test_quantize_tensor_with_min_max num_rows_list = num_cols_list = Map quantization bit rate tuple quantize function rowwise_min_max quantize function without rowwise_min_max bit_rate_to_quant_fn dict int tuple OpOverloadPacket OpOverloadPacket = torch ops quantized embedding_bag_byte_prepack_with_rowwise_min_max torch ops quantized embedding_bag_byte_prepack torch ops quantized embedding_bag_ bit_prepack_with_rowwise_min_max torch ops quantized embedding_bag_ bit_prepack torch ops quantized embedding_bag_ bit_prepack_with_rowwise_min_max torch ops quantized embedding_bag_ bit_prepack quant_fn_with_rowwise_min_max quant_fn bit_rate_to_quant_fn values torch_dtype torch float torch float num_rows num_cols itertools product num_rows_list num_cols_list weight = torch rand num_rows num_cols dtype=torch_dtype rowwise_min_max = torch stack weight min dim= values weight max dim= values dim= Perform quantization rowwise_min_max weight_quantized = quant_fn_with_rowwise_min_max weight rowwise_min_max assert weight_quantized dtype == torch uint Confirm quantization matching one without rowwise_min_max weight_quantized_no_rowwise_min_max = quant_fn weight assert torch equal weight_quantized weight_quantized_no_rowwise_min_max Confirtm incorrect rowwise_min_max will result different quantization output incorrect_rowwise_min_max = torch stack weight max dim= values weight max dim= values dim= weight_incorrectly_quantized = quant_fn_with_rowwise_min_max weight incorrect_rowwise_min_max assert weight_incorrectly_quantized dtype == torch uint assert torch equal weight_incorrectly_quantized weight_quantized_no_rowwise_min_max __name__ == __main__ raise_on_run_directly test test_quantization py