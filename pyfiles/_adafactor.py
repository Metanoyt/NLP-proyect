mypy allow-untyped-decorators mypy allow-untyped-defs typing cast Optional TYPE_CHECKING Union torch torch Tensor optimizer _disable_dynamo_if_unsupported _get_scalar_dtype _maximize_doc _params_doc _to_scalar Optimizer ParamsT TensorListList __all__ = Adafactor adafactor Adafactor Optimizer __init__ params ParamsT lr Union float Tensor = e- beta _decay float = - eps tuple Optional float float = None e- d float = weight_decay float = foreach Optional bool = None maximize bool = False isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Learning rate should = lr = beta _decay raise ValueError f beta _decay should = beta _decay eps None = eps raise ValueError f epsilon should = eps = eps raise ValueError f epsilon should = eps = d raise ValueError f Clipping threshold d should = d = weight_decay raise ValueError f weight_decay should = weight_decay defaults = lr lr beta _decay beta _decay eps eps d d weight_decay weight_decay foreach foreach maximize maximize super __init__ params defaults __setstate__ state super __setstate__ state group param_groups group setdefault foreach None p group params p_state = state get p len p_state = torch is_tensor p_state step step_val = float p_state step p_state step = torch tensor step_val dtype=_get_scalar_dtype _init_group group params_with_grad grads row_vars col_vars variances state_steps p group params p grad None continue torch is_complex p raise RuntimeError Adafactor does support complex parameters p grad is_sparse raise RuntimeError Adafactor does support sparse gradients params_with_grad append p grads append p grad state = state p State initialization len state == note crcrpar Deliberately host ` step ` CPU both capturable fused off This because kernel launches costly CUDA XLA state step = torch tensor dtype=_get_scalar_dtype p grad dim row_shape = list p grad shape row_shape - = Row factor variance NOT same shape grads will reduced along last dim state row_var = p grad new_zeros row_shape col_shape = list p grad shape col_shape - = Col factor variance NOT same shape grads will reduced along penultimate dim state col_var = p grad new_zeros col_shape state variance = torch zeros_like p grad memory_format=torch preserve_format row_vars append state get row_var None col_vars append state get col_var None variances append state get variance None state_steps append state step False has_complex torch no_grad step closure=None r Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss _cuda_graph_capture_health_check loss = None closure None torch enable_grad loss = closure group param_groups params_with_grad list Tensor = grads list Tensor = row_vars list Optional Tensor = col_vars list Optional Tensor = variances list Optional Tensor = state_steps list Tensor = eps eps = group eps has_complex = _init_group group params_with_grad grads row_vars col_vars variances state_steps adafactor params_with_grad grads row_vars col_vars variances state_steps d=group d lr=group lr beta _decay=group beta _decay weight_decay=group weight_decay eps =eps eps =eps foreach=group foreach maximize=group maximize grad_scale=getattr grad_scale None found_inf=getattr found_inf None has_complex=has_complex loss Adafactor __doc__ = r Implements Adafactor algorithm math \begin aligned \rule mm pt \\ \textbf input \gamma \text lr \ \tau \text \beta_ \text decay \ \theta_ \text params \ f \theta \text objective \\ \hspace mm \ \epsilon_ \epsilon_ \text epsilons \ d \text clipping threshold \\ \hspace mm \ \lambda \text weight decay \ \textit maximize \\ \textbf initialize \ R_ \leftarrow \text second moment row factor \\ \hspace mm \ C_ \leftarrow \text second moment col factor \\ \hspace mm \ \widehat V _ \leftarrow \text second moment vectors \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm \textbf \ \textit maximize \\ \hspace mm G_t \leftarrow -\nabla_ \theta f_t \theta_ t- \\ \hspace mm \textbf \\ \hspace mm G_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \widehat \beta _ _t \leftarrow - t^ \tau \\ \hspace mm \rho_t \leftarrow min lr \frac \sqrt t \\ \hspace mm \alpha_t \leftarrow max \epsilon_ \text RMS \theta_ t- \rho_t \\ \hspace mm \theta_t \leftarrow \theta_ t- - \gamma \lambda \theta_ t- \\ \hspace mm \textbf \ \text dim G_t \\ \hspace mm R_t \leftarrow \widehat \beta _ _t R_ t- + -\widehat \beta _ _t G_t \odot G_t \cdot _m \\ \hspace mm C_t \leftarrow \widehat \beta _ _t C_ t- + -\widehat \beta _ _t ^\top_n \cdot G_t \odot G_t \\ \hspace mm \widehat V _t \leftarrow \frac R_t \cdot C_t max ^\top_n \cdot R_t \epsilon_ \\ \hspace mm \textbf \\ \hspace mm \widehat V _t \leftarrow \widehat \beta _ _t \widehat V _ t- + -\widehat \beta _ _t \cdot G_t \odot G_t \\ \hspace mm U_t \leftarrow \frac G_t max \sqrt \widehat V _t \epsilon_ \\ \hspace mm \widehat U _t \leftarrow \frac U_t max \frac \text RMS U_t d \\ \hspace mm \theta_t \leftarrow \theta_ t- - \alpha_t \widehat U _t \\ \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned For further details regarding algorithm we refer ` Adafactor Adaptive Learning Rates Sublinear Memory Cost ` _ + rf Args _params_doc lr float Tensor optional unlike other optimizers Adafactor does require learning rate Noam Shazeer Mitchell Stern do use lr all Deviating paper implementation uses lr applying weight decay maximum value relative step size rho_t Note paper constant used maximum value relative step size so we set default value default e- beta _decay float optional decay rate beta beta standardly refers coefficient used computing running average gradient squared default - eps Tuple float float optional epsilon term added denominator update calculation improve numerical stability This use epsilon deviates algorithm written paper See note below more details epsilon term used avoid having too small weight update when applying parameter scaling default None e- d float optional clipping threshold used avoid larger-than-desired updates weight_decay float optional weight decay coefficient default e- foreach bool optional whether foreach implementation optimizer used Note foreach implementation uses ~ sizeof params more peak memory than for-loop version due intermediates being tensorlist vs just one tensor As Adafactor commonly used when memory prohibitive Adafactor will default slower single tensor for-loop implementation unless flag explicitly True This behavior contrary other optimizers which will attempt defaulting foreach CUDA faster runtime default None _maximize_doc + r Note The implementation Adafactor subtly differs Noam Shazeer Mitchell Stern implementations some other frameworks its use learning rate math ` \epsilon_ ` Regarding learning rate hyperparameter Noam Shazeer Mitchell Stern do use lr all stated algorithm uses math ` \rho_t ` update clipping affect step size This implementation allows ` lr ` influence maximum value math ` \rho_t ` math \begin aligned \hspace mm \rho_t \leftarrow min lr \frac \sqrt t \end aligned This differs Noam Shazeer Mitchell Stern who use constant maximum value math ` \rho_t ` math \begin aligned \hspace mm \rho_t \leftarrow min \frac \sqrt t \end aligned Noam Shazeer Mitchell Stern do enforce opinion how weight decay should computed so we use learning rate coefficient decoupled weight decay similar what suggested ` Decoupled Weight Decay Regularization ` _ Regarding use math ` \epsilon_ ` The implementation attempts replicate presumed intention Noam Shazeer Mitchell Stern use math ` \epsilon_ ` stabilizing term when squared gradient becomes small This stabilization can written math \begin aligned \hspace mm R_t \leftarrow \widehat \beta _ _t R_ t- + -\widehat \beta _ _t G_t \odot G_t + _n \cdot ^\top_m \cdot _m \\ \hspace mm C_t \leftarrow \widehat \beta _ _t C_ t- + -\widehat \beta _ _t ^\top_n \cdot G_t \odot G_t + _n \cdot ^\top_m \\ \hspace mm \widehat V _t \leftarrow \frac R_t \cdot C_t max ^\top_n \cdot R_t \epsilon_ \\ \hspace mm U_t \leftarrow \frac G_t max \sqrt \widehat V _t \epsilon_ \\ \end aligned where row column factors gradient squared math ` R_t ` math ` C_t ` left alone we apply math ` \epsilon_ ` final calculation variance estimate math ` \widehat V _t ` update math ` U_t ` This contrast Noam Shazeer Mitchell Stern other frameworks which apply math ` \epsilon_ ` both row column factors squared gradient calculations after math \begin aligned \hspace mm R_t \leftarrow \widehat \beta _ _t R_ t- + -\widehat \beta _ _t G_t \odot G_t + \epsilon_ _n \cdot ^\top_m \cdot _m \\ \hspace mm C_t \leftarrow \widehat \beta _ _t C_ t- + -\widehat \beta _ _t ^\top_n \cdot G_t \odot G_t + \epsilon_ _n \cdot ^\top_m \\ \hspace mm \widehat V _t \leftarrow \frac R_t \cdot C_t ^\top_n \cdot R_t \\ \hspace mm U_t \leftarrow \frac G_t \sqrt \widehat V _t \\ \end aligned You may note Noam Shazeer Mitchell Stern describe using sum squared gradients while implementation uses mean instead This choice mathematically equivalent allows greater numerical stability large sums _Adafactor\ Adaptive Learning Rates Sublinear Memory Cost https arxiv org pdf _Decoupled Weight Decay Regularization https arxiv org abs _single_tensor_adafactor params list Tensor grads list Tensor If grad -dimensional aka vector there no factorization necessary so row_var col_var will None while variance will filled Contrarily grad multiple dimensions we will factor along last dimensions so row_var col_var will filled variance will None row_vars list Optional Tensor col_vars list Optional Tensor variances list Optional Tensor state_steps list Tensor grad_scale Optional Tensor found_inf Optional Tensor d float lr Union Tensor float beta _decay float weight_decay float eps Optional float eps float maximize bool has_complex bool grad_scale None found_inf None raise AssertionError Grad scaling should occur outside optimizer step torch jit is_scripting assert due JIT being dumb realizing ops below have overloads handle both float Tensor lrs so we just assert s float since most people using JIT using floats isinstance lr float raise AssertionError f Expected lr float got type lr lr = _to_scalar lr i param enumerate params grad = grads i maximize -grads i step_t = state_steps i row_var = row_vars i col_var = col_vars i variance = variances i eps None eps = torch finfo param dtype eps update step step_t += step_float = step_t item one_minus_beta _t = step_float beta _decay rho_t = min lr step_float alpha = max eps param norm item param numel rho_t Perform stepweight decay weight_decay = param mul_ - lr weight_decay grad dim row_var None col_var None raise AssertionError row_var col_var should defined when grad multidimensional same g g mean dim=- w o materializing intermediate size g row_mean = torch norm grad dim=- keepdim=True square_ div_ grad size - row_var lerp_ row_mean one_minus_beta _t same g g mean dim=- w o materializing intermediate size g col_mean = torch norm grad dim=- keepdim=True square_ div_ grad size - col_var lerp_ col_mean one_minus_beta _t var_estimate = row_var col_var var_estimate div_ row_var mean dim=- keepdim=True clamp_ min=eps variance None raise AssertionError variance should defined when grad vector grad_squared = grad grad variance lerp_ grad_squared one_minus_beta _t avoid writing into variance during update var_estimate = variance clone square eps we sqrt after keep eps s magnitude update = var_estimate clamp_ min=eps eps rsqrt_ update mul_ grad denom = max update norm item update numel d param add_ update alpha=-alpha denom _group_tensors_by_device_dtype_and_is_multidim tensorlists TensorListList - dict tuple Optional torch device Optional torch dtype bool list list Optional Tensor Groups tensors device dtype AND multidimensionality -- whether tensor has multiple dims just one dim vector This allows foreach impl Adafactor assume every group params will either factored grouped_tensors = Optimizer _group_tensors_by_device_and_dtype tensorlists ultra_grouped_tensors dict tuple Optional torch device Optional torch dtype bool list list Optional Tensor = device dtype tensorlists _ grouped_tensors items matrix_key = device dtype True vector_key = device dtype False assumes grad second tensorlist j tensor enumerate tensorlists tensor None raise AssertionError grad should None tensor dim matrix_key ultra_grouped_tensors ultra_grouped_tensors matrix_key = _ tensorlists i range len tensorlists ultra_grouped_tensors matrix_key i append tensorlists i j vector_key ultra_grouped_tensors ultra_grouped_tensors vector_key = _ tensorlists i range len tensorlists ultra_grouped_tensors vector_key i append tensorlists i j ultra_grouped_tensors _multi_tensor_adafactor params list Tensor grads list Tensor If grad -dimensional aka vector there no factorization necessary so row_var col_var will None while variance will filled Contrarily grad multiple dimensions we will factor along last dimensions so row_var col_var will filled variance will None row_vars list Optional Tensor col_vars list Optional Tensor variances list Optional Tensor state_steps list Tensor grad_scale Optional Tensor found_inf Optional Tensor d float lr Union Tensor float beta _decay float weight_decay float eps Optional float eps float maximize bool has_complex bool len params == grad_scale None found_inf None raise AssertionError Grad scaling should occur outside optimizer step lr = _to_scalar lr grouped_tensors = _group_tensors_by_device_dtype_and_is_multidim params grads row_vars col_vars variances state_steps type ignore list-item _ dtype is_multidim device_params_ device_grads_ device_row_vars_ device_col_vars_ device_variances_ device_state_steps_ grouped_tensors items device_params = cast list Tensor device_params_ device_grads = cast list Tensor device_grads_ device_state_steps = cast list Tensor device_state_steps_ eps None dtype None raise AssertionError dtype needed compute eps when eps unset eps = torch finfo dtype eps TYPE_CHECKING assert device_state_steps None maximize device_grads = torch _foreach_neg device_grads type ignore assignment Update steps If steps CPU foreach will fall back slow path which for-loop calling t add over over will then wrapped into Tensor over over again which slower than we just wrapped once now The alpha required assure we go right overload torch compiler is_compiling device_state_steps is_cpu torch _foreach_add_ device_state_steps torch tensor device= cpu alpha= torch _foreach_add_ device_state_steps one_minus_beta _ts = beta _ts = rho_ts = s device_state_steps one_minus_beta _ts append s item beta _decay beta _ts append - s item beta _decay rho_ts append min lr s item alphas = max eps p norm item p numel r p r zip device_params rho_ts strict=True Perform stepweight decay weight_decay = torch _foreach_mul_ device_params - lr weight_decay is_multidim device_row_vars = cast list Tensor device_row_vars_ device_col_vars = cast list Tensor device_col_vars_ device_row_vars None device_col_vars None raise AssertionError row_var col_var should defined when grad multidimensional same g g mean dim=- w o materializing intermediate size g row_means = torch norm grad dim=- keepdim=True grad device_grads torch _foreach_mul_ row_means row_means torch _foreach_div_ row_means grad size - grad device_grads torch _foreach_lerp_ device_row_vars row_means one_minus_beta _ts del row_means same g g mean dim=- w o materializing intermediate size g col_means = torch norm grad dim=- keepdim=True grad device_grads torch _foreach_mul_ col_means col_means torch _foreach_div_ col_means grad size - grad device_grads torch _foreach_lerp_ device_col_vars col_means one_minus_beta _ts del col_means var_estimates = row_var col_var row_var col_var zip device_row_vars device_col_vars strict=True row_var_means = row_var mean dim=- keepdim=True row_var device_row_vars torch _foreach_clamp_min_ row_var_means eps torch _foreach_div_ var_estimates row_var_means del row_var_means device_variances = cast list Tensor device_variances_ device_variances None raise AssertionError variance should defined when grad vector grads_squared = torch _foreach_mul device_grads device_grads torch _foreach_lerp_ device_variances grads_squared one_minus_beta _ts del grads_squared avoid writing into variance during update var_estimates = v clone v device_variances square eps we sqrt after keep eps s magnitude torch _foreach_clamp_min_ var_estimates eps eps torch _foreach_rsqrt_ var_estimates torch _foreach_mul_ var_estimates device_grads updates = var_estimates alphas = -a max update norm item update numel d update zip alphas updates strict=True torch _foreach_mul_ updates alphas torch _foreach_add_ device_params updates _disable_dynamo_if_unsupported single_tensor_fn=_single_tensor_adafactor adafactor params list Tensor grads list Tensor row_vars list Optional Tensor col_vars list Optional Tensor variances list Optional Tensor state_steps list Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim foreach Optional bool = None grad_scale Optional Tensor = None found_inf Optional Tensor = None has_complex bool = False d float lr Union float Tensor beta _decay float weight_decay float eps float eps float maximize bool r Functional API performs Adafactor algorithm computation See ` ~torch optim Adafactor ` details torch compiler is_compiling all isinstance t torch Tensor t state_steps raise RuntimeError ` state_steps ` argument must contain list singleton tensors foreach func = _multi_tensor_adafactor func = _single_tensor_adafactor func params grads row_vars col_vars variances state_steps d=d lr=lr beta _decay=beta _decay weight_decay=weight_decay eps =eps eps =eps maximize=maximize grad_scale=grad_scale found_inf=found_inf has_complex=has_complex