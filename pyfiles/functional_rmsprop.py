mypy allow-untyped-defs typing Optional torch torch optim _functional F torch Tensor torch distributed optim _deprecation_warning _scripted_functional_optimizer_deprecation_warning __all__ list str = Define TorchScript compatible Functional RMSprop Optimizer where we use these optimizer functional way Instead using ` param grad ` when updating parameters we explicitly allow distributed optimizer pass gradients ` step ` function In way we could separate gradients parameters allow multithreaded trainer update parameters without data traces accumulating same grad NOTE This should only used distributed optimizer internals meant expose user torch jit script _FunctionalRMSprop __init__ params list Tensor lr float = e- alpha float = eps float = e- weight_decay float = momentum float = centered bool = False foreach bool = False maximize bool = False _allow_empty_param_list bool = False _scripted_functional_optimizer_deprecation_warning stacklevel= defaults = lr lr alpha alpha eps eps weight_decay weight_decay momentum momentum centered = centered foreach = foreach maximize = maximize len params == _allow_empty_param_list raise ValueError optimizer got empty parameter list NOTE we only have one param_group don t allow user add additional param group s common use case param_group = params params state = torch jit annotate dict torch Tensor dict str torch Tensor step gradients list Optional Tensor params = param_group params params_with_grad = grads = square_avgs = grad_avgs = momentum_buffer_list = state_steps = lr = defaults lr alpha = defaults alpha eps = defaults eps momentum = defaults momentum weight_decay = defaults weight_decay len params = len gradients raise ValueError gradients passed does equal size parameters + f Params length len params + f Gradients length len gradients has_complex = False param gradient zip params gradients gradient None has_complex &#124; = torch is_complex param params_with_grad append param grads append gradient Lazy state initialization param state state param = state = state param state step = torch tensor state square_avg = torch zeros_like param memory_format=torch preserve_format momentum state momentum_buffer = torch zeros_like param memory_format=torch preserve_format centered state grad_avg = torch zeros_like param memory_format=torch preserve_format state = state param square_avgs append state square_avg momentum momentum_buffer_list append state momentum_buffer centered grad_avgs append state grad_avg state_steps append state step torch no_grad F rmsprop params_with_grad grads square_avgs grad_avgs momentum_buffer_list state_steps lr=lr alpha=alpha eps=eps weight_decay=weight_decay momentum=momentum centered=self centered foreach=self foreach maximize=self maximize has_complex=has_complex