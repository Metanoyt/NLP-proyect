__future__ annotations argparse itertools os typing TYPE_CHECKING TypeVar Union libfb py log set_simple_logging type ignore torchgen gen torchgen context native_function_manager torchgen model DispatchKey NativeFunctionsGroup NativeFunctionsViewGroup torchgen static_runtime config generator TYPE_CHECKING collections abc Sequence Given list ` grouped_native_functions ` sorted their op names list lists each which groups ops share base name For example ` mean ` ` mean dim ` grouped together function NativeGroupT = TypeVar NativeGroupT bound=Union NativeFunctionsGroup NativeFunctionsViewGroup group_functions_by_op_name grouped_native_functions Sequence NativeGroupT - Sequence Sequence NativeGroupT grouped_native_functions groups = is_supported g NativeFunctionsGroup &#124; NativeFunctionsViewGroup - bool native_function_manager g generator is_supported g eligible_ops = g g grouped_native_functions is_supported g groups = list group k group itertools groupby eligible_ops key=config func_name_base_str groups clang_format cpp_file_path str - None subprocess subprocess check_call clang-format -i cpp_file_path write_cpp cpp_ops Sequence str file_path str - None code = \n join cpp_ops generated = f lint-ignore-every CLANGTIDY HOWTOEVEN AUTO-GENERATED FROM torchgen static_runtime gen_static_runtime_ops py #include torch csrc jit runtime static ops h #include ATen CPUFunctions h #include ATen InferSize h #include ATen NativeFunctions h #include ATen Parallel h #include ATen ScalarOps h #include ATen TensorUtils h #include ATen cpu vec functional h #include ATen cpu vec vec h #include ATen native EmbeddingBag h #include ATen native Fill h #include ATen native IndexingUtils h #include ATen native NonSymbolicBC h #include ATen native Resize h #include ATen native SharedReduceOps h #include ATen native TensorAdvancedIndexing h #include ATen native cpu SerialStackImpl h #include ATen native layer_norm h #include ATen native quantized cpu fbgemm_utils h #include ATen native quantized cpu qembeddingbag h #include ATen native quantized cpu qembeddingbag_prepack h #include ATen quantized QTensorImpl h #include ATen quantized Quantizer h #include c core ScalarType h #include c core WrapDimMinimal h #include c util irange h #include torch csrc jit ir ir h #include torch csrc jit runtime static impl h #include torch csrc jit runtime static te_wrapper h #include torch csrc jit runtime vararg_functions h #include torch csrc jit tensorexpr ir h #include torch csrc jit tensorexpr ir_simplifier h #include torch csrc jit tensorexpr llvm_codegen h #include torch csrc jit tensorexpr loopnest h namespace torch namespace jit code namespace jit namespace torch open file_path w f f write generated clang_format file_path write_test_cpp cpp_ops Sequence str file_path str - None code = \n join cpp_ops generated = f lint-ignore-every CLANGTIDY HOWTOEVEN AUTO-GENERATED FROM torchgen static_runtime gen_static_runtime_ops py #include gtest gtest h #include torch csrc jit runtime static impl h #include torch torch h #include test_utils h using namespace caffe using namespace torch using namespace torch jit using namespace torch jit test using c IValue code open file_path w f f write generated clang_format file_path main - None parser = argparse ArgumentParser description= Generate ATen source files parser add_argument -s -- source-path help= path source directory ATen default= caffe aten src ATen parser add_argument -p -- generated-ops-cpp-path help= path directory generate op dispatcher cpp file default= caffe torch csrc jit runtime static generated_ops cpp parser add_argument -t -- generated-ops-test-cpp-path help= path directory generate op dispatcher cpp file default= caffe benchmarks static_runtime test_generated_ops cc options = parser parse_args native_yaml_path = os path join options source_path native native_functions yaml tags_yaml_path = os path join options source_path native tags yaml parsed_yaml = gen parse_native_yaml native_yaml_path tags_yaml_path native_functions backend_indices = parsed_yaml native_functions parsed_yaml backend_indices op_generator = generator GenOpDispatcher test_case_generator = generator GenOpTestCase native_functions_groups = g g gen get_grouped_native_functions native_functions isinstance g NativeFunctionsGroup supported_functions_groups = group_functions_by_op_name native_functions_groups out_variant_op_result = op_generator out_variant groups backend_indices DispatchKey CPU groups supported_functions_groups out_variant_test_result = test_case_generator out_variant groups groups supported_functions_groups native_functions_view_groups = g g gen get_grouped_by_view_native_functions native_functions isinstance g NativeFunctionsViewGroup supported_functions_view_groups = group_functions_by_op_name native_functions_view_groups view_op_result = op_generator view groups backend_indices DispatchKey CPU groups supported_functions_view_groups view_test_result = test_case_generator view groups groups supported_functions_view_groups op_result = out_variant_op_result + \n\n + view_op_result test_result = out_variant_test_result + \n\n + view_test_result write_cpp op_result options generated_ops_cpp_path write_test_cpp test_result options generated_ops_test_cpp_path print f \ntotal grouped native ops len gen get_grouped_native_functions native_functions d print f grouped native ops out variant len native_functions_groups d supported_functions_num = sum len groups groups supported_functions_groups print f generated functions groups out variant supported_functions_num d print f \nview grouped native ops len native_functions_view_groups d supported_view_functions_num = sum len groups groups supported_functions_view_groups print f generated functions view groups supported_view_functions_num d print f \noverall generated supported_functions_num + supported_view_functions_num d __name__ == __main__ set_simple_logging escape_newlines=False main