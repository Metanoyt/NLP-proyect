Owner s module dynamo functools unittest weakref torch torch _dynamo test_case torch _dynamo testing torch testing _internal common_cuda TEST_MULTIGPU torch testing _internal common_utils requires_cuda requires_multigpu = functools partial unittest skipIf TEST_MULTIGPU requires multiple cuda devices TestStreams torch _dynamo test_case TestCase classmethod setUpClass cls super setUpClass classmethod tearDownClass cls super tearDownClass requires_cuda test_stream_weakref s = torch Stream weakref ref s requires_cuda test_event_weakref e = torch Event weakref ref e requires_cuda test_stream_enter_exit fn x y s = torch Stream s = torch Stream s z = torch add x y s z = torch add x y y = z + + z y inp = torch ones + torch ones expected = fn inp fn_opt = torch compile fn fullgraph=True actual = fn_opt inp assertEqual expected actual requires_cuda test_stream_context_graph_break fn x y s = torch Stream s = torch Stream s z = torch add x y s z = torch add x y y = z + + z torch _dynamo graph_break y = y + y inp = torch ones + torch ones expected = fn inp fn_opt = torch compile fn actual = fn_opt inp assertEqual expected actual requires_cuda test_stream_input fn x y s z = torch add x y y = z + y s inp = torch ones + torch ones torch Stream device= cuda expected = fn inp fn_opt = torch compile fn fullgraph=True actual = fn_opt inp assertEqual expected actual requires_cuda test_local_stream_return fn x y s = torch Stream z = torch add x y y = z + y s inp = torch ones + torch ones fn_opt = torch compile fn fullgraph=True _ s = fn_opt inp _ s = fn_opt inp Streams will different values each invocation so don t check equality assertIsInstance s torch Stream Stream should newly allocated each call assertNotEqual s s requires_cuda test_get_current_stream_return fn x s s s = torch accelerator current_stream x s s_inp = torch Stream device= cuda inp = torch ones + s_inp fn_opt = torch compile fn fullgraph=True _ s = fn_opt inp _ s = fn_opt inp assertEqual s_inp s assertEqual s s requires_cuda requires_multigpu test_get_current_stream_return_different_device fn x s s s s s = torch accelerator current_stream torch device cuda s s = torch Stream device= cuda s = torch Stream device= cuda inp = torch ones + s s fn_opt = torch compile fn fullgraph=True s_act = fn_opt inp s_exp = fn inp assertEqual s_act s_exp requires_cuda requires_multigpu test_get_current_stream_return_no_index fn x s s s s s = torch accelerator current_stream torch device cuda s s = torch Stream device= cuda s = torch Stream device= cuda inp = torch ones + s s fn_opt = torch compile fn fullgraph=True s_act = fn_opt inp s_exp = fn inp assertEqual s_act s_exp test_nested_stream_enter_exit pass test_stream_enter_exit_graph_break pass test_nested_stream_enter_exit_graph_break pass test_local_stream_enter_exit pass test_local_stream_nested_enter_exit pass test_stream_with_mutation pass requires_cuda test_run_opcheck torch _dynamo variables streams fork_stream join_stream torch library opcheck sample_inputs = torch device cuda torch device cuda torch device cuda torch device cuda args sample_inputs opcheck fork_stream args opcheck join_stream args __name__ == __main__ torch _dynamo test_case run_tests run_tests