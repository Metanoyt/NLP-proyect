math os re warnings collections abc Callable contextlib nullcontext copy deepcopy enum auto Enum functools partial wraps typing Any Optional TYPE_CHECKING Union typing_extensions Self torch torch distributed _tools fake_collectives torch nn optim torch _guards active_fake_mode torch distributed _tools common_utils get_untyped_storages torch distributed _tools mod_tracker ModTracker torch distributed tensor DTensor torch optim optimizer register_optimizer_step_post_hook register_optimizer_step_pre_hook torch utils _python_dispatch TorchDispatchMode torch utils _pytree tree_flatten tree_map_only torch utils weak WeakIdKeyDictionary weakref TYPE_CHECKING torch utils hooks RemovableHandle This value hard-coded here https github com pytorch pytorch blob fba d f ff ab e ad fd c cuda CUDACachingAllocator cpp#L _PYTORCH_MIN_ALLOCATE = int os environ get PYTORCH_NO_CUDA_MEMORY_CACHING == _TOTAL_KEY = Total __all__ = MemTracker _RefType str Enum Base Class defining memory reference types categorizing tensors based their usage within model _State str Enum Base Class defining module state capture snapshots _MemRefType _RefType An enum define memory reference types categorizing tensors based their usage within model - PARAM Tensors registered nn Parameter within modules - BUFFER Tensors registered nn Buffer within modules - GRAD Gradients associated parameters - ACT Tensors produced during forward pass recomputation activation checkpointing - TMP Temporary memory used during backward pass including gradients activations - OPT Tensors holding optimizer states - OTH Tensors registered via ` track_external ` do fit above categories PARAM = Parameter BUFFER = Buffer GRAD = Gradient ACT = Activation TEMP = Temp OPT = Optstate OTH = Other _ModState _State An enum define state module - PRE_FW The module about run forward pass - POST_FW The module has finished running forward pass - PEAK_FW The module has reached peak memory usage during forward pass - PRE_BW The module about run backward pass - PRE_FW_AC The module about run forward pass activation checkpointing - POST_FW_AC The module has finished running forward pass activation checkpointing - POST_BW The module has finished running backward pass - PEAK_BW The module has reached peak memory usage during backward pass PRE_FW = Pre-Forward POST_FW = Post-Forward PEAK_FW = Peak-Forward PRE_BW = Pre-Backward PRE_FW_AC = Pre-Forward-AC POST_FW_AC = Post-Forward-AC POST_BW = Post-Backward PEAK_BW = Peak-Backward _ModMemStats A store memory statistics module Args mod_fqn str The fully qualified name module Attributes mod_fqn str The fully qualified name module parameter_mem int The memory usage parameters module buffer_mem int The memory usage buffers module input_mem int The memory usage inputs module output_mem int The memory usage outputs module snapshots Dict _ModState Dict torch device Dict str int A dictionary memory snapshots module different states defined ` ` _ModState ` ` Note The memory snapshot stored dictionary - Dict torch device Dict str int where each key device each value another dictionary keys memory reference types defined ` _MemRefType ` values memory consumed bytes __init__ mod_fqn str mod_fqn = mod_fqn parameter_mem int buffer_mem int input_mem int output_mem int local_peak dict torch device int = snapshots dict _ModState list dict torch device dict str int = _WeakRefInfo Manages memory statistics device attributes tensor storages __init__ size int element_size int device torch device reftype _RefType - None Initializes ` ` _WeakRefInfo ` ` object tensor storage properties Args size int The number elements tensor storage element_size int The size each element tensor storage device torch device The device which tensor allocated reftype _RefType The reference type tensor size = size element_size = element_size reftype = reftype pyrefly ignore read-only device = device mem_consumed = _calculate_mem_consumed _calculate_mem_consumed - int Calculates memory consumed tensor storage considering device-specific allocation rules Returns int The memory consumed bytes mem = size element_size device type == cuda math ceil mem _PYTORCH_MIN_ALLOCATE _PYTORCH_MIN_ALLOCATE mem update_mem_consumed st torch UntypedStorage - int Updates returns memory consumed storage size has changed Args st torch UntypedStorage The tensor storage check size updates Returns int The updated memory consumed bytes st size = size size = st size mem_consumed = _calculate_mem_consumed mem_consumed classmethod create_winfo cls st torch UntypedStorage device torch device reftype _RefType callback Optional Callable Self weakref ref Any = None - tuple Self weakref ref Creates new ` ` _WeakRefInfo ` ` instance weak reference ` ` torch UntypedStorage ` ` object optionally attaching callback weak reference Args st torch UntypedStorage The storage object which create weak reference info device torch device The device associated storage object reftype _RefType The type reference used categorize storage callback Optional Callable Self weakref ref A callback function called when storage object about finalized garbage collected The callback function should accept two arguments ` ` _WeakRefInfo ` ` instance weak reference storage Returns Tuple Self weakref ref A tuple containing newly created ` ` _WeakRefInfo ` ` instance weak reference storage object The weak reference may have attached callback provided winfo = cls st size st element_size device reftype w_st = weakref ref st partial callback winfo callback None winfo w_st _get_mem_divisor units str - int unit_dict = B KiB MiB GiB units unit_dict unit_dict units raise ValueError f Unsupported unit units Supported units join unit_dict keys _rounding_fn value int divisor int precision int - Union float int value divisor == round value divisor precision _print_snapshot snapshot dict torch device dict str int units str - None len snapshot == print No memory tracked divisor = _get_mem_divisor units dev dev_snap snapshot items _rounding_fn dev_snap _TOTAL_KEY divisor = continue print f Device dev f \t k value _rounding_fn v divisor units isinstance k _RefType f \t k _rounding_fn v divisor units k v dev_snap items sep= \n _print_snapshot_tabular snapshot dict torch device dict str int units str - None len snapshot == print No memory tracked try tabulate tabulate except ImportError err raise ImportError Please install tabulate use tabulate option err divisor = _get_mem_divisor units table_data = key_list = list next iter snapshot values keys headers = Device + f key value isinstance key _RefType f key key key_list dev dev_snap snapshot items _rounding_fn dev_snap _TOTAL_KEY divisor = continue row = str dev row extend f _rounding_fn v divisor units v dev_snap values table_data append row print tabulate table_data headers=headers tablefmt= rst _print_state_snapshots snapshots dict _State list dict torch device dict str int units str - None state snapshot_list snapshots items print f state value i snapshot enumerate snapshot_list print f i + _print_snapshot snapshot units print _print_state_snapshots_tabular snapshots dict _State list dict torch device dict str int units str - None try tabulate tabulate except ImportError err raise ImportError Please install tabulate use tabulate option err table_data = last_state_call = None divisor = _get_mem_divisor units state snapshot_list snapshots items i snapshot enumerate snapshot_list state_call = f state value i + dev dev_snap snapshot items _rounding_fn dev_snap _TOTAL_KEY divisor = continue row = State Call state_call state_call = last_state_call Device str dev last_state_call = state_call k v dev_snap items row f k value isinstance k _RefType f k = f _rounding_fn v divisor units table_data append row print tabulate table_data headers= keys tablefmt= rst _UpdateType Enum These used tracking updates continuouly maintained memory snapshot ADD - When new tensor storage tracked DEL - When tensor storage about finalized garbage collected REF - When tensor reference updated instance gradients marked generic backward reference types until grad_hook categorizes them gradients SIZE - When tensor s storage resized ADD = auto DEL = auto REF = auto SIZE = auto MemTracker TorchDispatchMode A TorchDispatchMode track categorize attribute tensor memory created accessed within its context It categorizes tracked tensors parameters buffers activations gradients temporary memory optimizer states defined ` ` _MemRefType ` ` within its context It captures memory ` snapshots ` modules called within its context various states defined ` ` _ModState ` ` Attributes memory_tracking A weakref key dictionary store memory statistics each module Each key reference module each value ` ` _ModMemStats ` ` object stores memory statistics module Note The MemTracker should used context manager The modules optimizers any other tensors created within context MemTracker will tracked default Any tensors stateful objects such modules optimizers etc need tracked created outside MemTracker should registered using ` track_external ` method The ` track_external ` method should called before MemTracker used Any tensors created outside ` ` MemTracker ` ` supplied ` track_external ` method will tracked ` ` MemTracker ` ` Example usage code-block python module = optimizer = inp = mem_tracker = MemTracker mem_tracker track_external module optimizer inp mem_tracker mt loss = module inp print After Forward mt display_snapshot current loss backward optimizer step optimizer zero_grad mt display_snapshot peak mt display_modulewise_snapshots depth= units= MiB Known Limitations - The ` ` MemTracker ` ` does track memory tensors bypass ` ` TorchDispatchMode ` ` ex under ` ` no_dispatch ` ` - Resizing tensor storages directly using non-Tensor methods other than using ` ` torch Untyped_Storage resize_ ` ` tracked File Github issue you have use-cases - If tensors traceable wrappable subclasses ` ` torch Tensor ` ` then tracker does know how track their storages File Github issue you have use-cases - During AC backward pass there might misattribution between activation temp memory peak memory will tracked accurately This will fixed next update hooking intricately ` ` torch uitls checkpoint ` ` __init__ - None memory_tracking = WeakIdKeyDictionary _curr_mem_snap dict torch device dict str int = _peak_mem dict torch device int = _peak_mem_snap dict torch device dict str int = _param_to_grad_hook_handles = WeakIdKeyDictionary _optimizer_hook_handles Optional tuple RemovableHandle RemovableHandle = None Dictionary store ` ` _WeakRefInfo ` ` instances corresponding each tensor s storage _WINFO = WeakIdKeyDictionary _mod_tracker = ModTracker This general memory tracker which can used any ` ` _RefType ` ` subclass _ref_class type _RefType = _MemRefType Flags track we AC region optimizer step region _in_opt bool = False _in_ac bool = False Weak references topmost AC module currently active _ac_mod Optional weakref ref = None _orig_resize = torch UntypedStorage resize_ _orig_dtensor_dispatch = DTensor _op_dispatcher dispatch _depth = _update_snap u_type _UpdateType winfo _WeakRefInfo old_mem_consumed Optional int = None old_reftype Optional _RefType = None - None Initialize flag track total memory might drop zero after updates maybe_zero = False Ensure device entry exists current memory snapshot initializing necessary pyrefly ignore no-matching-overload dev_snap = _curr_mem_snap setdefault winfo device dict fromkeys _ref_class dev_snap setdefault _TOTAL_KEY Handle different types updates based update type ` u_type ` u_type == _UpdateType ADD Increase memory consumed specific reference type update total dev_snap winfo reftype += winfo mem_consumed dev_snap _TOTAL_KEY += winfo mem_consumed u_type == _UpdateType DEL Decrease memory consumed specific reference type reduce total dev_snap winfo reftype -= winfo mem_consumed dev_snap _TOTAL_KEY -= winfo mem_consumed maybe_zero = True u_type == _UpdateType REF assert old_reftype None Adjust memory consumption between two reference types within same device dev_snap old_reftype -= winfo mem_consumed dev_snap winfo reftype += winfo mem_consumed u_type == _UpdateType SIZE assert old_mem_consumed None Adjust memory consumed reference type due change size change = winfo mem_consumed - old_mem_consumed dev_snap winfo reftype += change dev_snap _TOTAL_KEY += change maybe_zero = True raise ValueError f Invalid update type u_type Check total memory device has dropped zero maybe_zero _curr_mem_snap winfo device _TOTAL_KEY == Remove device entry memory snapshot total memory zero del _curr_mem_snap winfo device _update_and_maybe_create_winfos t torch Tensor reftype _RefType update_existing bool = False - set _WeakRefInfo sts = get_untyped_storages t winfos = set st sts Attempt retrieve existing ` ` _WeakRefInfo ` ` its weak reference tracking dictionary winfo _ = _WINFO get st None None winfo None If ` ` _WeakRefInfo ` ` exists check reference type needs updated old_reftype = winfo reftype old_reftype = reftype Update reference type apply changes via ` ` _update_snap ` ` winfo reftype = reftype _update_snap _UpdateType REF winfo old_reftype=old_reftype winfos add winfo update_existing If no existing ` ` _WeakRefInfo ` ` found update_existing True raise error raise KeyError No existing winfo found If no existing _WeakRefInfo found update_existing False create new ` ` _WeakRefInfo ` ` winfo w_st = _WeakRefInfo create_winfo st t device reftype _delete_callback Store new ` ` _WeakRefInfo ` ` its weak reference tracking dictionary _WINFO st = winfo w_st Update snapshot newly added ` ` _WeakRefInfo ` ` winfo mem_consumed _update_snap _UpdateType ADD winfo winfos add winfo winfos _delete_callback winfo _WeakRefInfo w_st weakref ref - None Callback called when storage object corresponding ` ` _WeakRefInfo ` ` instance about finalized winfo mem_consumed _update_snap _UpdateType DEL winfo _track_resize - None Need monkey-patch because ` ` torch UntypedStorage resize_ ` ` captured ` ` TorchDispatchMode ` ` wraps _orig_resize resize_ st torch UntypedStorage size int - None _orig_resize st size winfo _ = _WINFO get st None None winfo None winfo size = st size old_mem_consumed = winfo mem_consumed winfo update_mem_consumed st _update_snap _UpdateType SIZE winfo old_mem_consumed=old_mem_consumed torch UntypedStorage resize_ = resize_ type ignore method-assign assignment _restore_resize - None torch UntypedStorage resize_ = _orig_resize type ignore method-assign _update_peak_stats peak_state _State - None We first capture current memory snapshot current tracker state then We step through each modules we have tracked so far ` ` memory_tracking ` ` check currently active querying ` ` _mod_tracker parents ` ` If active we update per device peak memory usage module corresponding ` ` _State ` ` which can ` ` PEAK_FW ` ` ` ` PEAK_BW ` ` curr_snap = _curr_mem_snap mod_stats memory_tracking values mod_stats mod_fqn _mod_tracker parents peak_state mod_stats snapshots dev dev_snap curr_snap items mod_stats local_peak get dev dev_snap _TOTAL_KEY mod_stats local_peak dev = dev_snap _TOTAL_KEY mod_stats snapshots peak_state - dev = deepcopy dev_snap dev dev_snap curr_snap items _peak_mem get dev dev_snap _TOTAL_KEY _peak_mem dev = dev_snap _TOTAL_KEY _peak_mem_snap dev = deepcopy dev_snap _track reftype _RefType t torch Tensor - None Get storages tensor check we have already tracked them If yes then check storage size has changed update current snapshot Else create new ` ` _WeakRefInfo ` ` instance add dictionary sts = get_untyped_storages t st sts winfo _ = _WINFO get st None None winfo None winfo size = st size old_mem_consumed = winfo mem_consumed winfo update_mem_consumed st _update_snap _UpdateType SIZE winfo old_mem_consumed=old_mem_consumed winfo w_st = _WeakRefInfo create_winfo st t device reftype _delete_callback _WINFO st = winfo w_st Update current snapshot newly added ` ` _WeakRefInfo ` ` winfo mem_consumed _update_snap _UpdateType ADD winfo get_tracker_snapshot type str = current - dict torch device dict str int Capture snapshot memory usage breakdown per device based specified type Args type str The type snapshot capture Can current current memory usage peak peak memory usage Defaults current Returns Dict torch device Dict str int A dictionary where each key torch device each value another dictionary This inner dictionary has keys representing memory reference types defined ` ` _MemRefType ` ` values representing amount memory consumed bytes Raises ValueError If invalid type specified type == current deepcopy _curr_mem_snap type == peak deepcopy _peak_mem_snap raise ValueError f Invalid type type _track_module_params_and_buffers module nn Module install_grad_hooks bool = True - tuple int int Track parameters buffers module already tracked If parameters have gradients track gradients well If install_grad_hooks True install gradient hook parameters track gradients has already been installed Return total memory consumed parameters buffers _grad_hook grad torch Tensor - None _update_and_maybe_create_winfos grad _MemRefType GRAD param_memory = param module parameters winfos = _update_and_maybe_create_winfos param _MemRefType PARAM param_memory += sum winfo mem_consumed winfo winfos param grad None _update_and_maybe_create_winfos param grad _MemRefType GRAD _param_to_grad_hook_handles get param None None install_grad_hooks grad_hook_handle = param register_hook _grad_hook post_acc_grad_hook_handle = param register_post_accumulate_grad_hook lambda p _grad_hook p grad _param_to_grad_hook_handles param = grad_hook_handle post_acc_grad_hook_handle buffer_memory = buffer module buffers winfos = _update_and_maybe_create_winfos buffer _MemRefType BUFFER buffer_memory += sum winfo mem_consumed winfo winfos param_memory buffer_memory _track_inputs_or_outputs args Any - int Calculate memory consumed inputs outputs module input_or_output_memory = add_inps_or_outs t torch Tensor - None nonlocal input_or_output_memory sts = get_untyped_storages t st sts winfo _ = _WINFO get st None None winfo None input_or_output_memory += winfo mem_consumed tree_map_only torch Tensor add_inps_or_outs args input_or_output_memory _pre_fw_hook module nn Module inputs Any - None This installed pre-fwd user hook ` ` ModTracker ` ` Based following cases we set state capture memory snapshot module Case If module ` ` memory_tracking ` ` dictionary we track parameters buffers input output memory module Create new ` ` _ModMemStats ` ` instance module add ` ` memory_tracking ` ` dictionary Case If module already ` ` memory_tracking ` ` dictionary we backward means we AC region We check top most module AC region If we store weak reference set flag ` ` _in_ac ` ` True Case If module already ` ` memory_tracking ` ` dictionary we forward means module called second time If root module means we next iteration we error out If root module means s submodule being used multiple times same iteration which we allow track For Case we also initialize ` ` local_peak ` ` ` ` PEAK_FW ` ` snapshot module mod_name = _mod_tracker get_known_fqn module assert mod_name None module memory_tracking mod_stats = _ModMemStats mod_name param_mem buffer_mem = _track_module_params_and_buffers module install_grad_hooks=True input_mem = _track_inputs_or_outputs inputs mod_stats parameter_mem = param_mem mod_stats buffer_mem = buffer_mem mod_stats input_mem = input_mem memory_tracking module = mod_stats state = _ModState PRE_FW _mod_tracker is_bw mod_stats = memory_tracking module state = _ModState PRE_FW_AC _ac_mod None _ac_mod = weakref ref module _in_ac = True parents = set _mod_tracker parents - mod_name len parents == Global parents raise NotImplementedError MemTracker does support memory tracking multiple iterative calls Either use ` ` reset_mod_stats ` ` clear module memory stats previous iteration file github issue you need feature mod_stats = memory_tracking module state = _ModState PRE_FW input_mem = _track_inputs_or_outputs inputs mod_stats mod_fqn = mod_name mod_stats input_mem = input_mem mem_snapshot = get_tracker_snapshot state == _ModState PRE_FW mod_stats local_peak = dev dev_snap _TOTAL_KEY dev dev_snap mem_snapshot items mod_stats snapshots setdefault _ModState PEAK_FW append mem_snapshot mod_stats snapshots setdefault state append deepcopy mem_snapshot _post_fw_hook module nn Module inputs Any outputs Any - None This installed post-fwd user hook ` ` ModTracker ` ` Based following cases we set state capture memory snapshot module Case This called backward which means we AC region If top most module AC region we set flag ` ` _in_ac ` ` False Case This called forward so we calculate output memory module update its mod_stats mod_stats = memory_tracking module _mod_tracker is_bw state = _ModState POST_FW_AC _ac_mod None _ac_mod module _ac_mod = None _in_ac = False state = _ModState POST_FW output_mem = _track_inputs_or_outputs outputs mod_stats output_mem = output_mem mod_stats snapshots setdefault state append get_tracker_snapshot _pre_bw_hook module nn Module args Any - None This installed pre-bwd user hook ` ` ModTracker ` ` We set state capture snapshot module We also initialize ` ` local_peak ` ` ` ` PEAK_BW ` ` snapshot If module None we skip hook This can happen since installed inside multi-grad hook module s output tensors module itself may alive during backward module None warnings warn Module None Skipping PRE_BW hook stacklevel= mod_stats = memory_tracking module mem_snapshot = get_tracker_snapshot mod_stats local_peak = dev dev_snap _TOTAL_KEY dev dev_snap mem_snapshot items mod_stats snapshots setdefault _ModState PEAK_BW append mem_snapshot mod_stats snapshots setdefault _ModState PRE_BW append deepcopy mem_snapshot _post_bw_hook module nn Module args Any - None This installed post-bwd user hook ` ` ModTracker ` ` We set state capture snapshot module None This can happen since installed inside multi-grad hook module s input tensors module itself may alive during backward module None warnings warn Module None Skipping POST_BW hook stacklevel= mod_stats = memory_tracking module mod_stats snapshots setdefault _ModState POST_BW append get_tracker_snapshot _track_optimizer_states reftype _RefType optimizer optim Optimizer - None states optimizer state values val states values isinstance val torch Tensor _update_and_maybe_create_winfos val reftype _register_global_optimizer_hook - None Register hook optimizer step track optimizer states The pre-hook set flag ` ` _in_opt ` ` True The post-hook unsets flag also tracks any optimizer states created during optimizer step _opt_step_pre_hook optimizer optim Optimizer args Any kwargs Any - None _in_opt = True _opt_step_post_hook optimizer optim Optimizer args Any kwargs Any - None _track_optimizer_states _MemRefType OPT optimizer _in_opt = False _optimizer_hook_handles = register_optimizer_step_pre_hook _opt_step_pre_hook register_optimizer_step_post_hook _opt_step_post_hook _deregister_param_and_optimizer_hooks - None grad_hook_handle post_acc_grad_hook_handle _param_to_grad_hook_handles values grad_hook_handle remove post_acc_grad_hook_handle remove _param_to_grad_hook_handles clear _optimizer_hook_handles None handle _optimizer_hook_handles handle remove _optimizer_hook_handles = None track_external external Union nn Module optim Optimizer torch Tensor - None Track tensors stateful objects like modules optimizers etc created outside MemTracker This method should called before ` ` MemTracker ` ` used Any tensors module parameters buffers gradients activations optimizer states will categorized ` ` Other ` ` If you want them categorized custom name please file GitHub issue Any tensors created outside MemTracker supplied method will tracked ` ` MemTracker ` ` Args external Union nn Module optim Optimizer torch Tensor The external modules optimizers tensors tracked flat_external _ = tree_flatten external obj flat_external isinstance obj torch Tensor _update_and_maybe_create_winfos obj _MemRefType OTH isinstance obj torch nn Module _track_module_params_and_buffers obj install_grad_hooks=False isinstance obj optim Optimizer _track_optimizer_states _MemRefType OPT obj obj None continue raise TypeError f Object type type obj supported tracking f Only stateful objects like modules optimizers tensors supported display_snapshot type str = current units str = B tabulate bool = False - None Display memory usage breakdown snapshot tracker based specified type units Keyword args type str The type snapshot display Can current current memory usage peak peak memory usage Defaults current units str The units use displaying memory usage Defaults B Supports B KiB MiB GiB tabulate bool Whether display snapshot tabular format Defaults False snapshot = get_tracker_snapshot type tabulate _print_snapshot_tabular snapshot units _print_snapshot snapshot units display_modulewise_snapshots depth int = units str = B tabulate bool = False - None Print per device memory breakdown snapshot each module called within MemTracker Snapshots displayed states defined ` ` _ModState ` ` The module hierarchy displayed up specified depth Keyword Args depth int optional The depth module hierarchy display Defaults units str optional The units use memory tracking Defaults B Supports B KiB MiB GiB tabulate bool optional Whether display snapshot tabular format Defaults False natural_sort_key s str - list Union int str int text text isdigit text lower text re split - + s mod_stats sorted memory_tracking values key=lambda m_stats natural_sort_key m_stats mod_fqn mod_fqn = mod_stats mod_fqn mod_depth = mod_fqn count + mod_depth depth continue print f Module mod_fqn tabulate _print_state_snapshots_tabular mod_stats snapshots units _print_state_snapshots mod_stats snapshots units reset_mod_stats - None Reset all module memory stats Clears ` ` memory_tracking ` ` dictionary memory_tracking clear _track_dtensor_dispatch - None track_dtensor_dispatch op_call torch _ops OpOverload args tuple object kwargs dict str object - object op_call DTensor _op_dispatcher _custom_op_handlers nullcontext _orig_dtensor_dispatch op_call args kwargs DTensor _op_dispatcher dispatch = track_dtensor_dispatch type ignore method-assign assignment _restore_dtensor_dispatch - None DTensor _op_dispatcher dispatch = _orig_dtensor_dispatch type ignore method-assign __enter__ - MemTracker _depth == _register_global_optimizer_hook _mod_tracker register_user_hooks _pre_fw_hook _post_fw_hook _pre_bw_hook _post_bw_hook _track_resize _track_dtensor_dispatch _peak_mem_snap = get_tracker_snapshot _peak_mem = dev dev_snap _TOTAL_KEY dev dev_snap _peak_mem_snap items _mod_tracker __enter__ super __enter__ _depth += pyrefly ignore bad-override __exit__ args Any - None _depth -= _depth == _deregister_param_and_optimizer_hooks _mod_tracker clear_user_hooks _restore_resize _restore_dtensor_dispatch _mod_tracker __exit__ args super __exit__ args __torch_dispatch__ func types args= kwargs=None type ignore no-untyped-def func torch ops _c d_functional wait_tensor default active_fake_mode N B This hacky way override Meta IMPL wait_tensor The original impl returns new tensor which does happen eager mode when wait_tensor called pyrefly ignore index-error res = args res = func args kwargs If we tracking optimizer state we use optimizer reference type If we backward region AC region we use backward reference type Else we use forward reference type _in_opt reftype = _MemRefType OPT _mod_tracker is_bw _in_ac reftype = _MemRefType TEMP reftype = _MemRefType ACT tree_map_only torch Tensor partial _track reftype res peak_state = _ModState PEAK_BW _mod_tracker is_bw _ModState PEAK_FW _update_peak_stats peak_state res