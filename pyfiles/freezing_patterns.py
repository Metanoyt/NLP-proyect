mypy allow-untyped-defs functools torch torch _inductor compile_fx fake_tensor_prop torch _inductor utils GPU_TYPES _dynamo utils counters config pattern_matcher _return_true CallFunction fwd_only Ignored init_once_fakemode KeywordArg Match PatternMatcherPass register_graph_pattern register_replacement stable_topological_sort aten = torch ops aten First pass_patterns applied then then pass_patterns = PatternMatcherPass PatternMatcherPass PatternMatcherPass binary_folding_pass = PatternMatcherPass freezing_passes gm torch fx GraphModule aot_example_inputs Passes applied graph freeze pass freezing constant_fold lazy_init We need few rounds binary folding get rid all unnecessary nodes may need good method chose rounds number works like conv+binary+binary binary_folding = counters inductor binary_folding fake_tensor_prop gm aot_example_inputs True torch _inductor fx_passes binary_folding mark_mixed_dtype_allowed_computation_ops gm _ range constant_fold gm Make sure meta val properly set all nodes fake_tensor_prop gm aot_example_inputs True binary_folding_pass apply gm graph type ignore arg-type If we don t have binary folding we don t need run pass again TODO remove need run fake_tensor_prop whole model counters inductor binary_folding == binary_folding break binary_folding = counters inductor binary_folding torch _inductor fx_passes binary_folding recover_original_precision_folded_computation_ops gm constant_fold gm fake_tensor_prop gm aot_example_inputs True pattern pass_patterns pattern apply gm graph type ignore arg-type The CPU weight packing always assume conv s weight channels last So make sure layout_optimization when doing torch _C _has_mkldnn config cpp weight_prepack config layout_optimization mkldnn_fusion _eliminate_duplicate_packed_nodes _eliminate_duplicate_packed_nodes gm stable_topological_sort gm graph gm recompile gm graph lint init_once_fakemode lazy_init torch _C _has_mkldnn config cpp weight_prepack mkldnn_fusion _mkldnn_weight_pack_init _mkldnn_weight_pack_init binary_folding binary_folding_init addmm_patterns_init binary_folding_init register_freezing_graph_pattern pattern extra_check=_return_true pass_number= while pass_number len pass_patterns - pass_patterns append PatternMatcherPass register_graph_pattern pattern extra_check=extra_check pyrefly ignore bad-argument-type pass_dict=pass_patterns pass_number register_binary_folding_pattern pattern extra_check=_return_true register_graph_pattern pattern extra_check=extra_check pyrefly ignore bad-argument-type pass_dict=binary_folding_pass functools cache addmm_patterns_init addmm related patterns To avoid duplication also includes int WoQ GEMM pattern without bias device = next gpu gpu GPU_TYPES getattr torch gpu is_available cpu val = functools partial torch empty device=device requires_grad=False scale = functools partial torch empty device=device requires_grad=False check_int _woq_concat_linear_weights match is_cpu = match kwargs inp meta val is_cpu is_cpu config cpp enable_concat_linear Currently pattern only supported CPU False weight_inputs = w w w match kwargs weight_inputs append w all match kwargs wgt target torch ops prims convert_element_type default wgt weight_inputs False all next iter match kwargs wgt _input_nodes keys meta val dtype torch int wgt weight_inputs False all match kwargs wgt meta val dtype torch bfloat wgt weight_inputs False True check_concat_weights match is_cpu = match kwargs inp meta val is_cpu is_cpu config cpp enable_concat_linear False weight_inputs = w w w match kwargs weight_inputs append w equal_shape_inputs = weight_inputs b match kwargs bias_inputs = b b b match kwargs bias_inputs append b equal_shape_inputs append bias_inputs equal_shape_group equal_shape_inputs inps = match kwargs name name equal_shape_group all inp op == get_attr inp meta val shape == inps meta val shape inp inps False True int _woq_fusion_pattern inp w w w s s s inp w s inp w s inp w s int _woq_fusion_replacement inp w w w s s s cat_w = torch cat w w w dim= cat_s = torch cat s s s dim= mm = inp cat_w mul cat_s n n = w size w size mm tensor_split n n + n dim=- register_replacement pyrefly ignore bad-argument-type int _woq_fusion_pattern pyrefly ignore bad-argument-type int _woq_fusion_replacement val val val val scale scale scale pyrefly ignore bad-argument-type fwd_only pyrefly ignore bad-argument-type pass_patterns extra_check=check_int _woq_concat_linear_weights exclusive_arg_names= w w w s s s matmul_fuse_pattern inp w w w inp w inp w inp w matmul_replacement inp w w w cat_t = torch cat w w w dim= mm = inp cat_t mm chunk dim= register_replacement pyrefly ignore bad-argument-type matmul_fuse_pattern pyrefly ignore bad-argument-type matmul_replacement val val val val pyrefly ignore bad-argument-type fwd_only pyrefly ignore bad-argument-type pass_patterns extra_check=check_concat_weights exclusive_arg_names= w w w matmul_fuse_pattern_two inp w w inp w inp w matmul_replacement_two inp w w cat_t = torch cat w w dim= mm = inp cat_t mm chunk dim= register_replacement pyrefly ignore bad-argument-type matmul_fuse_pattern_two pyrefly ignore bad-argument-type matmul_replacement_two val val val pyrefly ignore bad-argument-type fwd_only pyrefly ignore bad-argument-type pass_patterns extra_check=check_concat_weights exclusive_arg_names= w w addmm_fuse_pattern_second inp w w w b b b aten addmm b inp w aten addmm b inp w aten addmm b inp w addmm_fuse_replacement_second inp w w w b b b cat_w = torch cat w w w dim= cat_b = torch cat b b b aten addmm cat_b inp cat_w chunk dim= register_replacement pyrefly ignore bad-argument-type addmm_fuse_pattern_second pyrefly ignore bad-argument-type addmm_fuse_replacement_second val _ range pyrefly ignore bad-argument-type fwd_only pyrefly ignore bad-argument-type pass_patterns extra_check=check_concat_weights exclusive_arg_names= w w w b b b same_dtype match match output_node args meta val dtype == match kwargs dtype register_graph_pattern CallFunction torch ops prims convert_element_type default Ignored KeywordArg dtype pyrefly ignore bad-argument-type pass_dict=pass_patterns extra_check=same_dtype unnecessary_dtype_convert match Match kwargs Remove unnecessary dtype conversion op probably left result Conv-Bn folding graph = match graph node = match output_node node replace_all_uses_with node args type ignore arg-type graph erase_node node