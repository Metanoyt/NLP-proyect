Copyright c Meta Platforms Inc affiliates Owner s oncall distributed copy re unittest warnings torch torch distributed dist torch testing _internal common_methods_invocations common_ops torch distributed _local_tensor LocalTensorMode reconcile_args torch distributed tensor distribute_tensor DTensor init_device_mesh Replicate Shard torch overrides resolve_name torch testing _internal common_device_type instantiate_device_type_tests ops torch testing _internal common_methods_invocations DecorateInfo op_db torch testing _internal common_utils run_tests suppress_warnings TestCase torch testing _internal distributed _tensor common_dtensor DTensorConverter DTensorOpTestBase torch utils _pytree pytree torch utils _debug_mode DebugMode torch utils _pytree tree_map rewrite common size variables sth can sharded evenly we can enable uneven shards later need adjust more sample inputs i e view reshape need adjust shape size well common_ops L = common_ops M = common_ops S = common_ops XS = Copied functorch xfail op_name variant_name= device_type=None dtypes=None op_name variant_name device_type dtypes True skip op_name variant_name= device_type=None dtypes=None op_name variant_name device_type dtypes False skipOps op_db test_case_name base_test_name to_skip all_opinfos = op_db xfail to_skip op_name variant_name device_type dtypes expected_failure = xfail matching_opinfos = o o all_opinfos o name == op_name o variant_test_name == variant_name assert len matching_opinfos = f Couldn t find OpInfo xfail opinfo matching_opinfos decorators = list opinfo decorators expected_failure decorator = DecorateInfo unittest expectedFailure test_case_name base_test_name device_type=device_type dtypes=dtypes decorators append decorator decorator = DecorateInfo unittest skip Skipped test_case_name base_test_name device_type=device_type dtypes=dtypes decorators append decorator opinfo decorators = tuple decorators This decorator doesn t modify fn any way wrapped fn fn wrapped repurpose_ops op_db base_test_name derived_test_name Copies op info database decorators applied base test updates them apply derived test The update required because decorators applied only name matches doesn t consider base classes Specifically we use function create two test classes one multi-threaded one local tensor flavors share common test body different rules skip fail Args op_db List OpInfo objects repurposed base_test_name The original test name replaced derived_test_name The new test name set decorators Returns list A new list OpInfo objects updated target names decorator repurposed_ops = opinfo op_db opinfo_copy = copy deepcopy opinfo decorator list opinfo_copy decorators hasattr decorator cls_name decorator cls_name == base_test_name decorator cls_name = derived_test_name repurposed_ops append opinfo_copy repurposed_ops Re-generate failed list turn dry_run below func check_dtensor_func test op dry_run=True then run sth like python test distributed tensor test_dtensor_ops py failed expect dtensor_fails = these sometimes pass sometimes fail we need remove many them list once op get full support varying sharding specs xfail __getitem__ xfail __rsub__ xfail _chunk_cat xfail _native_batch_norm_legit xfail _upsample_bilinear d_aa xfail addbmm xfail addmv xfail addr xfail all xfail allclose xfail alias_copy xfail aminmax xfail any xfail arange xfail argmax xfail argmin xfail as_strided xfail as_strided partial_views xfail as_strided_copy xfail as_strided_scatter xfail bernoulli xfail _batch_norm_with_update xfail block_diag xfail broadcast_shapes xfail cartesian_prod xfail cauchy xfail cdist xfail cholesky xfail cholesky_inverse xfail cholesky_solve xfail combinations xfail complex xfail count_nonzero xfail cross xfail cummax xfail cummin xfail diagonal_scatter xfail dist xfail expand_copy xfail exponential xfail equal xfail eye xfail fft fft xfail fft fft xfail fft fftn xfail fft fftshift xfail fft ifft xfail fft ifft xfail fft ifftshift xfail fft ihfft xfail fft ihfft xfail fft ihfftn xfail fft irfft xfail fft irfftn xfail fft rfft xfail fft rfft xfail fft rfftn xfail fill xfail flatten xfail flip xfail fliplr xfail flipud xfail floor_divide xfail fmax xfail fmin xfail frexp xfail full xfail geometric xfail geqrf xfail grid_sampler_ d xfail heaviside xfail histogram xfail histogramdd xfail index_add xfail index_copy xfail index_fill xfail index_put xfail index_reduce prod xfail index_reduce mean xfail index_reduce amax xfail index_reduce amin xfail index_select xfail isin xfail kthvalue xfail kron xfail linalg cholesky xfail linalg cholesky_ex xfail linalg cross xfail linalg det xfail linalg eig xfail linalg eigvals xfail linalg householder_product xfail linalg inv xfail linalg inv_ex xfail linalg ldl_factor xfail linalg ldl_factor_ex xfail linalg ldl_solve xfail linalg lstsq xfail linalg lstsq grad_oriented xfail linalg lu xfail linalg lu_factor xfail linalg lu_factor_ex xfail linalg lu_solve xfail linalg matrix_power xfail linalg pinv xfail linalg pinv hermitian xfail linalg slogdet xfail linalg solve xfail linalg solve_ex xfail linalg solve_triangular xfail linalg tensorinv xfail linalg tensorsolve xfail linalg vander xfail linalg vecdot xfail linspace xfail linspace tensor_overload xfail log_normal xfail logcumsumexp xfail logdet xfail logspace xfail logspace tensor_overload xfail logsumexp xfail lu xfail lu_solve xfail lu_unpack xfail masked_fill xfail masked_scatter xfail masked_select xfail masked argmax xfail masked argmin xfail masked logsumexp xfail masked median xfail matrix_exp xfail max reduction_with_dim xfail median xfail min reduction_with_dim xfail mode xfail multinomial xfail mv xfail max_pool d_with_indices_backward xfail nanmean xfail nanmedian xfail nanquantile xfail nansum xfail native_batch_norm xfail narrow_copy xfail ne xfail transpose xfail nn functional adaptive_avg_pool d xfail nn functional adaptive_avg_pool d xfail nn functional adaptive_avg_pool d xfail nn functional adaptive_max_pool d xfail nn functional adaptive_max_pool d xfail nn functional adaptive_max_pool d xfail nn functional alpha_dropout xfail nn functional avg_pool d xfail nn functional avg_pool d xfail nn functional avg_pool d xfail nn functional batch_norm xfail nn functional batch_norm without_cudnn xfail nn functional bilinear xfail nn functional binary_cross_entropy xfail nn functional binary_cross_entropy_with_logits xfail nn functional celu xfail nn functional conv d xfail nn functional conv d xfail nn functional conv d xfail nn functional conv_transpose d xfail nn functional conv_transpose d xfail nn functional conv_transpose d xfail nn functional cosine_similarity xfail nn functional ctc_loss xfail nn functional dropout xfail nn functional elu xfail nn functional fractional_max_pool d xfail nn functional fractional_max_pool d xfail nn functional glu xfail nn functional grid_sample xfail nn functional group_norm xfail nn functional hardshrink xfail nn functional hardsigmoid xfail nn functional hardswish xfail nn functional hardtanh xfail nn functional huber_loss xfail nn functional instance_norm xfail nn functional interpolate area xfail nn functional interpolate nearest xfail nn functional interpolate nearest-exact xfail nn functional leaky_relu xfail nn functional local_response_norm xfail nn functional logsigmoid xfail nn functional margin_ranking_loss xfail nn functional max_pool d xfail nn functional max_pool d xfail nn functional max_pool d xfail nn functional max_unpool d xfail nn functional max_unpool d grad xfail nn functional max_unpool d xfail nn functional max_unpool d grad xfail nn functional max_unpool d xfail nn functional max_unpool d grad xfail nn functional mish xfail nn functional mse_loss xfail nn functional multi_margin_loss xfail nn functional multilabel_margin_loss xfail nn functional multilabel_soft_margin_loss xfail nn functional pad reflect xfail nn functional pad replicate xfail nn functional pad replicate_negative xfail nn functional pairwise_distance xfail nn functional pdist xfail nn functional pixel_shuffle xfail nn functional pixel_unshuffle xfail nn functional prelu xfail nn functional relu xfail nn functional rrelu xfail nn functional selu xfail nn functional smooth_l _loss xfail nn functional soft_margin_loss xfail nn functional softplus xfail nn functional softshrink xfail nn functional threshold xfail nn functional triplet_margin_loss xfail nn functional triplet_margin_with_distance_loss xfail nn functional unfold xfail nn functional upsample_nearest xfail nonzero xfail normal xfail normal number_mean xfail normal in_place xfail ormqr xfail ones xfail pca_lowrank xfail permute_copy xfail pinverse xfail polar xfail put xfail quantile xfail rand_like xfail randint_like xfail randint xfail randn xfail randn_like xfail ravel xfail renorm xfail repeat_interleave xfail resize_ xfail resize_as_ xfail reshape xfail reshape_as xfail roll xfail rot xfail rsub xfail scalar_tensor xfail scatter_reduce amax xfail scatter_reduce amin xfail scatter_reduce mean xfail scatter_reduce prod xfail scatter_reduce sum xfail searchsorted xfail select_scatter xfail sparse sampled_addmm xfail sparse mm reduce xfail special airy_ai xfail special bessel_j xfail special bessel_j xfail special bessel_y xfail special bessel_y xfail special chebyshev_polynomial_t xfail special chebyshev_polynomial_u xfail special chebyshev_polynomial_v xfail special chebyshev_polynomial_w xfail special entr xfail special erfcx xfail special hermite_polynomial_h xfail special hermite_polynomial_he xfail special i e xfail special i xfail special i e xfail special laguerre_polynomial_l xfail special legendre_polynomial_p xfail special log_ndtr xfail special modified_bessel_i xfail special modified_bessel_i xfail special modified_bessel_k xfail special modified_bessel_k xfail special ndtri xfail special scaled_modified_bessel_k xfail special scaled_modified_bessel_k xfail special shifted_chebyshev_polynomial_t xfail special shifted_chebyshev_polynomial_u xfail special shifted_chebyshev_polynomial_v xfail special shifted_chebyshev_polynomial_w xfail special spherical_bessel_j xfail special xlog py xfail special zeta xfail squeeze multiple xfail squeeze_copy xfail signal windows bartlett xfail signal windows blackman xfail signal windows cosine xfail signal windows exponential xfail signal windows gaussian xfail signal windows general_cosine xfail signal windows general_hamming xfail signal windows hamming xfail signal windows hann xfail signal windows nuttall xfail signal windows kaiser xfail stack xfail std xfail std unbiased xfail std_mean xfail std_mean unbiased xfail stft xfail svd_lowrank xfail t_copy xfail take xfail take_along_dim xfail tensor_split xfail to_sparse xfail trace xfail triangular_solve xfail unbind xfail unbind_copy xfail unfold xfail unfold_copy xfail uniform xfail unflatten xfail unique_consecutive xfail unique xfail unsafe_split xfail unsafe_chunk xfail _unsafe_masked_index xfail _unsafe_masked_index_put_accumulate xfail var_mean xfail var_mean unbiased xfail vdot xfail view xfail view_as xfail view_copy xfail zeros TODO whc debug triage ops inside might even fail without dtensor tests we rescale op db common test size factor i e L M S which triggered original function run failures input generation becomes wrong we skip them now should enable later TODO need clean list remove all cases skip argwhere skip cumprod skip __rmatmul__ skip meshgrid list_of_tensors skip meshgrid variadic_tensors skip nn functional scaled_dot_product_attention skip nn functional softmin skip nn functional embedding skip nn functional embedding_bag skip nn functional feature_alpha_dropout with_train skip nn functional feature_alpha_dropout without_train skip nn functional hinge_embedding_loss skip nn functional cosine_embedding_loss skip fft hfft skip fft hfft skip fft hfft skip fft hfftn skip fft ifftn skip fft irfft skip istft skip isclose skip isreal skip matmul skip masked mean skip masked var skip masked std skip masked normalize skip prod skip _segment_reduce lengths skip _segment_reduce offsets TODO fix following ops skip squeeze skip empty skip empty_strided skip empty_like skip empty_permuted skip new_empty skip new_empty_strided dtensor_multi_threaded_fails = xfail full_like xfail nn functional dropout d xfail nn functional dropout d xfail masked cumprod skip nn functional multi_head_attention_forward Add list ops currently failing BW pass skip_bw = None corresponds transpose ops H T torch bucketize torch conj_physical torch eq torch isfinite torch isnan OP_DB_WORLD_SIZE = DEVICE_TYPE = cuda torch cuda is_available torch cuda device_count = OP_DB_WORLD_SIZE cpu TODO debug cuda illegal memory access issue re-enable cuda tests DEVICE_TYPE = cpu TestDTensorOps TestCase __test__ = False __init_subclass__ cls kwargs super __init_subclass__ kwargs cls __test__ = True property world_size - int OP_DB_WORLD_SIZE run_opinfo_test dtype op requires_grad=True sample_inputs_filter=lambda s True mesh = init_device_mesh DEVICE_TYPE world_size test each op dist tensor inputs normal inputs test samples = op sample_inputs DEVICE_TYPE dtype requires_grad=requires_grad sample_input samples sample_inputs_filter sample_input continue args = sample_input input + list sample_input args kwargs = sample_input kwargs run_dtensor_crossref op op args kwargs we need figure out way test out variant out variant testing tricky we need pre allocate dtensor out some them rely sharding placements pre-known i e mm out isinstance expected torch Tensor op supports_out func args kwargs out=expected check_dtensor_func test op assert_ref_dtensor_equal dtensor_rs rs flat_dtensor_rs = pytree tree_leaves dtensor_rs flat_rs = pytree tree_leaves rs assertEqual len flat_dtensor_rs len flat_rs dtensor_r r zip flat_dtensor_rs flat_rs isinstance r torch Tensor continue assertIsInstance dtensor_r torch Tensor assertEqualOnRank dtensor_r shape r shape f Shape mismatch original shape r shape dtensor shape dtensor_r shape assertEqualOnRank dtensor_r requires_grad r requires_grad op result requires_grad mismatch f original requires_grad r requires_grad f dtensor requires_grad dtensor_r requires_grad assertEqualOnRank dtensor_r r assertEqualOnRank x y msg=None rank= - None raise NotImplementedError run_dtensor_crossref func args kwargs to_dtensor = DTensorConverter mesh args kwargs concat_res_if_necessary func res object - object concat result corresponding dim ops like split so we can call backward single tensor resolve_name func None split resolve_name func dim = args len args == torch cat res dim=dim res TODO also handle cases where func raise exception op_args op_kwargs = reconcile_args args kwargs rs = func op_args op_kwargs rs = concat_res_if_necessary func rs to_replicate e object - object e full_tensor isinstance e DTensor e Suppress warnings doesn t matter test_meta py does matter you want use decorator cross-ref testing some tests may looking errors warnings catch_warnings warnings simplefilter ignore every comb sharding choices we test works dtensor_args dtensor_kwargs to_dtensor Only attempt we managed convert all tensors DTensor any them failed we re mixed tensor situation allowed DTensor try to_dtensor successful Handle special cases first there s any Suppress warnings doesn t matter test_meta py does matter you want use decorator cross-ref testing some tests may looking errors dtensor_rs = func dtensor_args dtensor_kwargs we need skip tests containing tensors zero elements now see issue https github com pytorch PiPPy issues TODO remove once issue above fixed flat_args = pytree tree_leaves dtensor_rs any isinstance e torch Tensor e numel == e flat_args continue redistribute all_gather results compare normal output dtensor_rs = tree_map to_replicate dtensor_rs dtensor_rs = concat_res_if_necessary func dtensor_rs try resolve_name func skip_bw isinstance dtensor_rs DTensor dtensor_rs to_local sum backward isinstance dtensor_rs tuple dtensor_rs to_local sum backward except Exception e TODO anj Remove guard exception after gaining more confidence torch distributed get_rank == print f failed run BW resolve_name func func str e assert_ref_dtensor_equal dtensor_rs rs raise RuntimeError f Failed convert args DTensor f originally args kwargs except Exception e raise RuntimeError f str e \n\nFailed run resolve_name func dtensor_args dtensor_kwargs e rs check_dtensor_func test_func opinfo dry_run=False try test_func except Exception dry_run raise dist get_rank == opinfo variant_test_name print f xfail opinfo name opinfo variant_test_name print f xfail opinfo name run_one_hot ops = op op op_db op name == nn functional one_hot assert len ops == op = ops num_classes = - appears have bug dtensor max item run_opinfo_test torch int op requires_grad=False sample_inputs_filter=lambda s s kwargs num_classes = - run_mean mesh = init_device_mesh DEVICE_TYPE world_size shape = world_size + world_size tensor = torch arange shape shape dtype=torch float reshape shape DEVICE_TYPE is_evenly_shardable True False is_evenly_shardable placement = Shard reduce_dim = placement = Shard reduce_dim = dtensor = distribute_tensor tensor mesh placement DebugMode record_torchfunction=False debug_mode mean = dtensor mean dim=reduce_dim full_tensor = mean full_tensor assertEqual full_tensor tensor mean dim=reduce_dim is_evenly_shardable assertTrue P- R debug_mode debug_string assertTrue S - R debug_mode debug_string test_embedding_error_msg mesh_ d = init_device_mesh DEVICE_TYPE world_size mesh_dim_names= dp tp mesh_ d = mesh_ d tp weight_global = torch randn device=DEVICE_TYPE weight_dtensor = distribute_tensor weight_global mesh_ d Shard input_global = torch randint device=DEVICE_TYPE input_dtensor = distribute_tensor input_global mesh_ d Shard Replicate expected_error_msg = Sharding propagation failed aten embedding default Spec f S Spec i S R DeviceMesh dp= tp= assertRaisesRegex RuntimeError re escape expected_error_msg _ = torch ops aten embedding default weight_dtensor input_dtensor TestMultiThreadedDTensorOps DTensorOpTestBase TestDTensorOps _op_db = repurpose_ops op_db TestDTensorOps TestMultiThreadedDTensorOps suppress_warnings ops _op_db allowed_dtypes= torch float skipOps _op_db TestMultiThreadedDTensorOps test_dtensor_op_db dtensor_fails &#124; dtensor_multi_threaded_fails test_dtensor_op_db dtype op run_opinfo_test dtype op test_mean run_mean test_one_hot run_one_hot TestLocalDTensorOps TestDTensorOps _op_db = repurpose_ops op_db TestDTensorOps TestLocalDTensorOps setUp - None super setUp torch distributed init_process_group fake rank= world_size=self world_size fake_pg = torch distributed distributed_c d _get_default_group tearDown super tearDown try dist destroy_process_group except AssertionError pass suppress_warnings ops _op_db allowed_dtypes= torch float skipOps _op_db TestLocalDTensorOps test_dtensor_op_db dtensor_fails test_dtensor_op_db dtype op run_opinfo_test dtype op test_mean LocalTensorMode frozenset range world_size run_mean test_one_hot run_one_hot run_opinfo_test dtype op requires_grad=True sample_inputs_filter=lambda s True LocalTensorMode frozenset range world_size super run_opinfo_test dtype op requires_grad sample_inputs_filter assertEqualOnRank x y msg=None rank= assertEqual x y msg only instantiate tests DEVICE_TYPE alone i e either CPU GPU instantiate_device_type_tests TestMultiThreadedDTensorOps globals only_for= DEVICE_TYPE instantiate_device_type_tests TestLocalDTensorOps globals only_for= DEVICE_TYPE __name__ == __main__ run_tests