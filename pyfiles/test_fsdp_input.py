Owner s oncall distributed sys torch torch distributed dist torch distributed fsdp FullyShardedDataParallel FSDP torch nn Linear Module torch optim SGD torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest torch testing _internal common_utils parametrize run_tests subtest TEST_WITH_DEV_DBG_ASAN dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TestInput FSDPTest property world_size skip_if_lt_x_gpu parametrize input_cls subtest dict name= dict subtest list name= list test_input_type device input_cls Test FSDP input being list dict only single GPU Model Module __init__ super __init__ layer = Linear forward input isinstance input list input = input assert isinstance input dict input input = input layer input fsdp_kwargs = device_id device model = FSDP Model device fsdp_kwargs optim = SGD model parameters lr= _ range in_data = torch rand device in_data requires_grad = True input_cls list in_data = in_data assertTrue input_cls dict in_data = in_data out = model in_data out sum backward optim step optim zero_grad devices = cuda hpu xpu instantiate_device_type_tests TestInput globals only_for=devices allow_xpu=True __name__ == __main__ run_tests