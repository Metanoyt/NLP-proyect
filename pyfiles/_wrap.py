Python implementation function wrapping functionality functorch dim __future__ annotations functools typing Any Optional TYPE_CHECKING torch torch utils _pytree tree_map _dim_entry DimEntry _enable_all_layers EnableAllLayers _tensor_info TensorInfo TYPE_CHECKING collections abc Callable handle_from_tensor tensor torch Tensor - torch Tensor Handle tensor conversion torch function integration tensor WrappedOperator This wraps PyTorch operations support first-class dimensions __init__ orig Callable wrapper_implementation Callable dim_name str = dim orig = orig wrapper_implementation = wrapper_implementation name = getattr orig __name__ doc = getattr orig __doc__ None dim_name = dim_name is_pointwise = False dim_offset = keepdim_offset = single_dim = False reduce = True Update docstring we have dim_name doc dim_name doc = f doc \nArgument dim_name can either integer torchdim Dim object \n function - Callable Create wrapped function calls our wrapper implementation wrapped_func args Any kwargs Any - Any wrapper_implementation args kwargs Copy metadata using functools update_wrapper just __name__ __doc__ functools update_wrapper wrapped_func orig assigned= __name__ updated= wrapped_func __doc__ = doc wrapped_func _wrap_dim dim Any ndim int keepdim bool = False - DimEntry Convert single dimension specification DimEntry object Dim isinstance dim Dim keepdim raise ValueError cannot preserve first-class dimensions keepdim=True DimEntry dim isinstance dim int i = dim while i = i -= ndim DimEntry i DimEntry _wrap_dims dim Any ndim int keepdim bool = False - list DimEntry Convert dimension specification list DimEntry objects de = _wrap_dim dim ndim keepdim result = de is_none result append de d dim result append _wrap_dim d ndim keepdim result patched_dim_method wrapper WrappedOperator args Any kwargs Any - Any This core method handles dimension-aware operations args raise ValueError Expected least one argument Get dimension argument dim_arg = kwargs get wrapper dim_name dim_arg None wrapper dim_offset len args Try get dim positional args accounting index dim_idx = wrapper dim_offset + dim_idx len args dim_arg = args dim_idx If no dimension argument provided fall back standard functorch handling dim_arg None info = TensorInfo create args ensure_batched=True ensure_present=False info wrapper orig args kwargs EnableAllLayers info levels guard assert info batchedtensor None guard inplace_update_layers info batchedtensor info levels new_args = list args new_args = handle_from_tensor info batchedtensor result = wrapper orig new_args kwargs guard from_batched result info has_device Handle dimension-aware operation info = TensorInfo create args info wrapper orig args kwargs Check keepdim parameter keepdim = False wrapper reduce keepdim_arg = kwargs get keepdim keepdim_arg None wrapper keepdim_offset len args keepdim_idx = wrapper keepdim_offset + keepdim_idx len args keepdim_arg = args keepdim_idx keepdim_arg None keepdim = bool keepdim_arg Wrap dimensions ndim = info ndim dims = _wrap_dims dim_arg ndim keepdim Convert dimensions indices validate dim_indices list int = seen = False len info levels d dims midx = None i level enumerate info levels level == d midx = i break midx None Try match position name more flexibly i level enumerate info levels hasattr level matches level matches d midx = i break midx None level_strs = str level level info levels raise ValueError f Tensor dimensions level_strs does contain d seen midx = True dim_indices append midx Determine new levels after reduction new_levels = wrapper reduce keepdim i level enumerate info levels seen i new_levels append level new_levels = info levels Create dimension indices original function len dim_indices == py_indices Any = dim_indices py_indices = tuple dim_indices Update arguments new_args = list args new_kwargs = kwargs copy assert info tensor None new_args = handle_from_tensor info tensor Update dimension argument wrapper dim_name new_kwargs new_kwargs wrapper dim_name = py_indices dim_idx = wrapper dim_offset + dim_idx len new_args new_args = list new_args new_args dim_idx = py_indices Call original function result = wrapper orig new_args new_kwargs Wrap results wrap_result obj Any - Any isinstance obj torch Tensor Tensor Tensor from_positional obj new_levels info has_device obj tree_map wrap_result result _wrap orig Callable dim_offset Optional int = None keepdim_offset Optional int = None dim_name Optional str = None single_dim Optional bool = None reduce Optional bool = None - Callable Wrap PyTorch function support first-class dimensions Args orig Original function wrap dim_offset Offset dimension argument default keepdim_offset Offset keepdim argument default dim_name Name dimension parameter default dim single_dim Whether function takes single dimension default False reduce Whether function reduces dimensions default True dim_name = dim_name dim wrapper = WrappedOperator orig patched_dim_method dim_name dim_offset None wrapper dim_offset = dim_offset keepdim_offset None wrapper keepdim_offset = keepdim_offset single_dim None wrapper single_dim = single_dim reduce None wrapper reduce = reduce wrapper function call_torch_function wrapper WrappedOperator func Callable types tuple args tuple = kwargs Optional dict = None - Any Handle __torch_function__ calls wrapped operators kwargs None kwargs = Import here avoid circular imports _Tensor Use torch function mechanism _Tensor _Tensor __torch_function__ func types args kwargs