Owner s module inductor importlib collections abc Callable typing Any Optional unittest skipIf torch torch utils _pytree pytree torch _inductor config torch _inductor test_case TestCase InductorTestCase torch _inductor utils run_and_get_code torch testing _internal common_utils instantiate_parametrized_tests parametrize torch testing _internal inductor_utils GPU_TYPE HAS_CPU HAS_GPU requires_gpu importlib import_module filelock instantiate_parametrized_tests CodegenInductorTest InductorTestCase run_and_compare func Callable Any args compile_kwargs Optional dict = None config_patches Optional dict = None atol float &#124; None = e- rtol float &#124; None = e- Runs module through Inductor comparing eager reference compile_kwargs None compile_kwargs = config_patches None config_patches = flatten_tensors tensors flat spec = pytree tree_flatten tensors flat config patch config_patches compiled = torch compile func backend= inductor compile_kwargs result code = run_and_get_code compiled args Check numerical accuracy ref_tensors = flatten_tensors func args actual_tensors = flatten_tensors result ref actual zip ref_tensors actual_tensors assertTrue torch allclose ref actual atol=atol rtol=rtol result code count_code substr str code list str expected Optional int count = sum prog count substr prog code expected None assertEqual count expected parametrize force_pointwise_cat False True test_force_pointwise_cat force_pointwise_cat bool func b torch cat + b + dim= = torch randn device=torch device cpu b = torch randn device=torch device cpu config_patches = force_pointwise_cat force_pointwise_cat _ code = run_and_compare func b config_patches=config_patches reinterpret_call = = reinterpret_tensor_wrapper config cpp_wrapper = reinterpret_tensor force_pointwise_cat count_code reinterpret_call code count_code reinterpret_call code requires_gpu skipIf GPU_TYPE == mps Triton available MPS test_cse_make_block_ptr_reduction func b tmp = b tmp = + b c = tmp + tmp c sum dim= config_patches = triton use_block_ptr True triton tile_reductions True triton prefer_nd_tiling True triton max_tiles split_reductions False = torch randn device=torch device GPU_TYPE b = torch randn device=torch device GPU_TYPE _ code = run_and_compare func b config_patches=config_patches atol= e- count_code = tl make_block_ptr in_ptr code count_code = tl load block_ptr code requires_gpu skipIf GPU_TYPE == mps Triton available MPS test_kernel_fusion_thresholds func b tmp = + tmp = tmp + tmp = tmp + tmp = tmp + b tmp tmp tmp = torch randn device=torch device GPU_TYPE b = torch randn device=torch device GPU_TYPE config_patches = max_fusion_size realize_reads_threshold realize_opcount_threshold inplace_buffers False _ code = run_and_compare func b config_patches=config_patches count_code triton jit code __name__ == __main__ torch _inductor test_case run_tests HAS_GPU HAS_CPU run_tests needs= filelock