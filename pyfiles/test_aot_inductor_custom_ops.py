Owner s module inductor This test requires libaoti_custom_ops so built which happens when BUILD_TEST = logging os sys unittest torch torch _export torch _inductor torch _inductor config torch _inductor config torch _inductor test_case TestCase torch export Dim export torch testing _internal common_utils torch testing _internal common_utils find_library_location IS_CI IS_FBCODE IS_MACOS IS_SANDCASTLE IS_WINDOWS skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_GPU_AND_TRITON torch testing _internal logging_utils LoggingTestCase make_logging_test torch utils _python_dispatch TorchDispatchMode IS_WINDOWS IS_CI sys stderr write Windows CI does have necessary dependencies test_torchinductor yet\n __name__ == __main__ sys exit raise unittest SkipTest requires sympy functorch filelock try try test_aot_inductor_utils check_model check_model_with_multiple_inputs code_check_count test_torchinductor copy_tests TestFailure except ImportError test_aot_inductor_utils manual=fbcode caffe test inductor aot_inductor_utils-library check_model check_model_with_multiple_inputs code_check_count test_torchinductor manual=fbcode caffe test inductor test_inductor-library copy_tests TestFailure except unittest SkipTest ImportError __name__ == __main__ sys exit raise torch library custom_op aoti_custom_ops fn_with_incorrect_optional_tensor mutates_args= fn_with_incorrect_optional_tensor x torch Tensor y torch Tensor z torch Tensor - torch Tensor z None x + y x + y + z fn_with_incorrect_optional_tensor register_fake fn_with_incorrect_optional_tensor_fake x torch Tensor y torch Tensor z torch Tensor - torch Tensor z None x + y x + y + z torch library custom_op aoti_custom_ops fn_ret_list_of_single_tensor mutates_args= fn_ret_list_of_single_tensor x torch Tensor - list torch Tensor s = x sum torch int torch randn s item fn_ret_list_of_single_tensor register_fake _ x ctx = torch _custom_op impl get_ctx i = ctx new_dynamic_size torch randn i torch library custom_op aoti_custom_ops fn_ret_single_tensor mutates_args= fn_ret_single_tensor x torch Tensor - torch Tensor s = x sum torch int torch randn s item fn_ret_single_tensor register_fake _ x ctx = torch _custom_op impl get_ctx i = ctx new_dynamic_size torch randn i AOTInductorTestsTemplate test_custom_op_add - None M torch nn Module __init__ device super __init__ device = device w = torch randn device=device forward x const = torch tensor device=self device x = torch ops aoti_custom_ops custom_add x const torch ops aoti_custom_ops custom_add x w m = M device device=self device args = torch randn device=self device check_model m args test_custom_op_add_output_path - None M torch nn Module forward x y torch ops aoti_custom_ops custom_add x y m = M device=self device args = torch randn device=self device torch randn device=self device config patch aot_inductor output_path model pt assertRaises Exception check_model m args test_fn_with_optional_tensor_output - None M torch nn Module forward x y torch ops aoti_custom_ops fn_with_optional_tensor_output x y m = M device=self device args = torch randn device=self device torch randn device=self device check_model m args test_fn_with_optional_tensor_output_ - None M torch nn Module forward x y torch ops aoti_custom_ops fn_with_optional_tensor_output_ x y m = M device=self device args = torch randn device=self device torch randn device=self device check_model m args test_fn_with_optional_tensor_nullopt_output - None M torch nn Module forward x y torch ops aoti_custom_ops fn_with_optional_tensor_nullopt_output x y m = M device=self device args = torch randn device=self device torch randn device=self device check_model m args test_fn_with_int_output - None M torch nn Module forward x y i = x shape z _ _ i i = torch ops aoti_custom_ops fn_with_int_output x y i z z i + i + i m = M device=self device args = torch randn device=self device torch randn device=self device check_model m args test_custom_op_all_inputs - None MyModel torch nn Module pyre-fixme Return type must annotated __init__ super __init__ pyre-fixme Return type must annotated pyre-fixme Parameter must annotated forward x y torch no_grad x_dim = x shape x_dim = x shape y_dim = y shape y_dim = y shape symint_ = x_dim + x_dim symint_ = y_dim y_dim z = torch concat x x _ = torch ops aoti_custom_ops fn_with_all_inputs tensor=x tensors= x y optional_tensors= None z b =False b s= True False i = i s= symint=symint_ symints= symint_ symint_ f = f s= scalar= scalars= string= hello strings= ab cde dtype=torch float memory_format=torch contiguous_format layout=torch strided device=torch device cpu optional o_tensor=None o_tensors= x y o_b =False o_b s= True False o_i =None o_i s= o_symint=symint_ o_symints= symint_ symint_ o_f = o_f s=None o_scalar=None o_scalars= o_string= hello o_strings= ab cde o_dtype=None o_memory_format=torch contiguous_format o_layout=torch strided o_device=None _ m = MyModel device=self device x = torch zeros device=self device y = torch ones device=self device args = x y m args check_model m args test_custom_op_with_multiple_outputs - None Model torch nn Module forward x y out = x + y tuple Tensor output out out = torch ops aoti_custom_ops fn_with_tuple_output out TensorList output out out = torch ops aoti_custom_ops fn_with_list_output out out tuple Tensor TensorList out out out = torch ops aoti_custom_ops fn_with_mix_outputs out out out out out out out out out out m = Model device=self device args = torch randn device=self device torch randn device=self device m args check_model m args test_custom_op_out_variant_without_return - None Model torch nn Module forward x y torch ops aoti_custom_ops fn_out_variant_without_return x y y m = Model device=self device args = torch randn device=self device torch randn device=self device m args check_model m args test_custom_op_with_reinterpret_view_inputs - None Model torch nn Module forward x out = x permute torch ops aoti_custom_ops fn_with_default_input out m = Model device=self device args = torch randn device=self device check_model m args test_custom_op_with_concat_inputs - None Model torch nn Module forward x y out = torch concat x y dim= torch ops aoti_custom_ops fn_with_default_input out m = Model device=self device args = torch randn device=self device torch randn device=self device check_model m args test_custom_op_missing_arg_with_default_value - None Model torch nn Module forward x missing second arg torch ops aoti_custom_ops fn_with_default_input x m = Model device=self device args = torch randn device=self device check_model m args test_custom_op_return_list_of_single_tensor - None Model torch nn Module forward x torch ops aoti_custom_ops fn_ret_list_of_single_tensor x + m = Model device=self device args = torch randn check_model m args test_custom_op_return_single_tensor - None Model torch nn Module forward x torch ops aoti_custom_ops fn_ret_single_tensor x + m = Model device=self device args = torch randn check_model m args unittest skipIf IS_FBCODE FbProxyExecutor doesn t have these error msgs test_incorrect_custom_op_schema M torch nn Module forward x y torch ops aoti_custom_ops fn_with_incorrect_optional_tensor x y None m = M device=self device args = torch randn device=self device torch randn device=self device assertRaisesRegex RuntimeError Expected extern kernel check_model m args test_boxed_run_inputs_clearing Borrowed test_torchinductor Model torch nn Module forward x y torch ops aoti_custom_ops custom_add x y inps = torch rand device=self device torch rand device=self device model = Model device=self device NOTE There additional references inps we use strict=True here which will cause inps deallocated time later test ep = torch export export model tuple inps strict=False package = torch _inductor aoti_compile_and_package ep fn_compiled = torch _inductor aoti_load_package package test_self = sentinel_seen = False TestRefMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None kwargs = kwargs kwargs nonlocal inps nonlocal test_self nonlocal sentinel_seen func torch ops aoti_custom_ops custom_add default inputs should deallocated point sentinel_seen = True test_self assertEqual len inps func args kwargs TestRefMode fn_compiled loader boxed_run inps assertEqual len inps assertTrue sentinel_seen skipIfXpu unittest skipIf IS_FBCODE unable find library -laoti_custom_ops test_custom_op_square - None Model torch nn Module forward x torch ops aoti_custom_ops fn_square x m = Model device=self device args = torch randn device=self device config patch aot_inductor custom_ops_to_c_shims torch ops aoti_custom_ops fn_square default AOTITorchError aoti_torch_cpu_fn_square AtenTensorHandle input AtenTensorHandle ret AOTITorchError aoti_torch_cuda_fn_square AtenTensorHandle input AtenTensorHandle ret config patch aot_inductor custom_op_libs aoti_custom_ops check_model m args AOTInductorLoggingTest LoggingTestCase make_logging_test dynamic=logging DEBUG test_shape_env_reuse records make sure ShapeEnv only created once reused afterwards Foo torch nn Module forward x x + inputs = torch randn dynamic_shapes = x Dim AUTO Dim AUTO ep = export Foo inputs dynamic_shapes=dynamic_shapes strict=False torch no_grad torch _inductor aot_compile ep module inputs assertEqual r msg == create_env r records count True common_utils instantiate_parametrized_tests AOTInductorTestsTemplate AOTICustomOpTestCase TestCase setUp IS_SANDCASTLE IS_FBCODE torch ops load_library caffe test inductor custom_ops IS_MACOS raise unittest SkipTest non-portable load_library call used test lib_file_path = find_library_location libaoti_custom_ops so IS_WINDOWS lib_file_path = find_library_location aoti_custom_ops dll os path exists lib_file_path raise unittest SkipTest libaoti_custom_ops built torch ops load_library str lib_file_path super setUp fail_cpu is_skip=False TestFailure cpu is_skip=is_skip fail_gpu suffixes tuple str is_skip=False TestFailure suffixes is_skip=is_skip test_failures xfail default set is_skip=True skip CPU_TEST_FAILURES = TODO failed internally test_multiple_output_alias fail_cpu is_skip=True test_failures xfail default set is_skip=True skip GPU_TEST_FAILURES = quantized unsupported GPU test_quantized_linear fail_gpu cuda xpu test_quanatized_int _linear fail_gpu cuda xpu test_quantized_linear_bias_none fail_gpu cuda xpu AOTInductorTestABICompatibleCpu AOTICustomOpTestCase device = cpu device_type = cpu check_model = check_model check_model_with_multiple_inputs = check_model_with_multiple_inputs code_check_count = code_check_count allow_stack_allocation = False use_minimal_arrayref_interface = False copy_tests AOTInductorTestsTemplate AOTInductorTestABICompatibleCpu cpu CPU_TEST_FAILURES unittest skipIf sys platform == darwin No CUDA MacOS AOTInductorTestABICompatibleGpu AOTICustomOpTestCase device = GPU_TYPE device_type = GPU_TYPE check_model = check_model check_model_with_multiple_inputs = check_model_with_multiple_inputs code_check_count = code_check_count allow_stack_allocation = False use_minimal_arrayref_interface = False copy_tests AOTInductorTestsTemplate AOTInductorTestABICompatibleGpu GPU_TYPE GPU_TEST_FAILURES __name__ == __main__ torch _inductor test_case run_tests cpp_extension N A fbcode HAS_GPU_AND_TRITON sys platform == darwin run_tests needs= filelock