mypy allow-untyped-defs contextlib functools hashlib importlib util itertools json logging os os path pathlib re sys tempfile time warnings collections defaultdict collections abc Callable dataclasses dataclass field typing Any Generic Optional Union typing_extensions ParamSpec weakref WeakSet torch _logging structured torch _guards CompileId torch _utils_internal log_trace_structured_event torch utils _traceback CapturedTraceback _P = ParamSpec _P log = logging getLogger __name__ This synthetic logger which doesn t correspond actual logger handles all our tracing logging which structured doesn t go stderr always goes dedicated log file We don t put these loggers classic module hierarchy because we don t want suppression logs also cause trace get suppressed traces typically collected unless we prod which case they always collected TODO Maybe we should allow some sub-hierarchy so you can control which traces you want collect performance reasons See https docs google com document d CX_hJ PNy f R y TJrfkSeLkvGjjjLU BSXgS AZ edit trace_log = logging getLogger torch __trace DEFAULT_LOG_LEVEL = logging WARNING LOG_ENV_VAR = TORCH_LOGS LOG_OUT_ENV_VAR = TORCH_LOGS_OUT LOG_FORMAT_ENV_VAR = TORCH_LOGS_FORMAT LOG_TRACE_ID_FILTER = TORCH_LOGS_TRACE_ID_FILTER TRACE_ENV_VAR = TORCH_TRACE DTRACE_ENV_VAR = TORCH_DTRACE LOG_TRACE_HANDLER Optional LazyTraceHandler = None GET_DTRACE_STRUCTURED = False dataclass LogRegistry shorthand name log qualified name Note only contains loggers registered register_log e g dynamo - torch _dynamo log_alias_to_log_qnames dict str list str = field default_factory=dict artifact logger qualified names populated lazily calls getArtifactLogger currently formatted module __ artifact_name e g torch _dynamo convert_frame __guards artifact_log_qnames set str = field default_factory=set child logs registered logs specified via open registration user ie placing torch _dynamo output_graph env var these need tracked so their levels can reset properly e g torch _dynamo output_graph child_log_qnames set str = field default_factory=set artifact names populated register_artifact e g guards artifact_names set str = field default_factory=set Artifacts should visible default error message visible_artifacts set str = field default_factory=set A short description each artifact artifact_descriptions dict str str = field default_factory=dict artifacts which displayed unless explicitly named settings Ex output_code NOT displayed even inductor log level set DEBUG It must explicitly named settings off_by_default_artifact_names set str = field default_factory=set logging format string artifacts artifact_log_formatters dict str logging Formatter = field default_factory=dict is_artifact name name artifact_names is_log alias alias log_alias_to_log_qnames register log alias register_log alias log_qnames Union str list str - None isinstance log_qnames str log_qnames = log_qnames log_alias_to_log_qnames alias = log_qnames register artifact name register_artifact_name name description visible off_by_default log_format - None artifact_names add name visible visible_artifacts add name artifact_descriptions name = description off default don t enable when log_name s log_level set DEBUG off_by_default off_by_default_artifact_names add name log_format None artifact_log_formatters name = logging Formatter log_format register qualified name artifact log needed know which logs need reset whenever log_state changed register_artifact_log artifact_log_qname - None artifact_log_qnames add artifact_log_qname register_child_log log_qname - None child_log_qnames add log_qname flattens all qnames together TODO consider memoizing get_log_qnames - set str set itertools chain from_iterable log_alias_to_log_qnames values get_artifact_log_qnames set artifact_log_qnames get_child_log_qnames set child_log_qnames is_off_by_default artifact_qname artifact_qname off_by_default_artifact_names dataclass LogState qualified log names - currently set log level log_qname_to_level dict str str = field default_factory=dict set currently enabled artifacts artifact_names set str = field default_factory=set enable_artifact artifact_name - None artifact_names add artifact_name is_artifact_enabled name name artifact_names enable_log log_qnames log_level - None isinstance log_qnames str log_qnames = log_qnames log_qname log_qnames log_qname_to_level log_qname = log_level get_log_level_pairs Returns all qualified module names which user requested explicit logging settings warning This function used all loggers regardless whether user specified them now only returns logs which explicitly mentioned user torch which always implicitly requested when we initialize our logging subsystem log_qname_to_level items clear - None log_qname_to_level clear artifact_names clear log_registry = LogRegistry log_state = LogState sample usage torch _logging set_logs torch _logging DEFAULT_LOGGING DEFAULT_LOGGING = dynamo logging INFO aot logging INFO inductor logging INFO fsdp logging INFO ddp_graphs True graph_breaks True guards True recompiles True dynamic logging INFO set_logs all Optional int = None dynamo Optional int = None aot Optional int = None autograd Optional int = None dynamic Optional int = None inductor Optional int = None distributed Optional int = None c d Optional int = None ddp Optional int = None fsdp Optional int = None dtensor Optional int = None onnx Optional int = None bytecode bool = False aot_graphs bool = False aot_joint_graph bool = False ddp_graphs bool = False graph bool = False graph_code bool = False graph_code_verbose bool = False graph_breaks bool = False graph_sizes bool = False guards bool = False recompiles bool = False recompiles_verbose bool = False trace_source bool = False trace_call bool = False trace_bytecode bool = False output_code bool = False kernel_code bool = False schedule bool = False perf_hints bool = False pre_grad_graphs bool = False post_grad_graphs bool = False ir_pre_fusion bool = False ir_post_fusion bool = False onnx_diagnostics bool = False fusion bool = False overlap bool = False export Optional int = None modules Optional dict str Union int bool = None cudagraphs bool = False sym_node bool = False compiled_autograd bool = False compiled_autograd_verbose bool = False cudagraph_static_inputs bool = False benchmarking bool = False autotuning bool = False graph_region_expansion bool = False inductor_metrics bool = False hierarchical_compile bool = False compute_dependencies bool = False - None Sets log level individual components toggles individual log artifact types warning This feature prototype may have compatibility breaking changes future note The ` ` TORCH_LOGS ` ` environment variable has complete precedence over function so set function does nothing A component set related features PyTorch All log messages emitted given component have their own log levels If log level particular message has priority greater than equal its component s log level setting emitted Otherwise suppressed This allows you instance silence large groups log messages relevant you increase verbosity logs components relevant The expected log level values ordered highest lowest priority ` ` logging CRITICAL ` ` ` ` logging ERROR ` ` ` ` logging WARNING ` ` ` ` logging INFO ` ` ` ` logging DEBUG ` ` ` ` logging NOTSET ` ` See documentation Python ` ` logging ` ` module more information log levels ` https docs python org library logging html#logging-levels ` _ An artifact particular type log message Each artifact assigned parent component A component can emit many different kinds artifacts In general artifact emitted either its corresponding setting argument list below turned its parent component set log level less than equal log level artifact Keyword args all ` Optional int ` The default log level all components Default ` ` logging WARN ` ` dynamo ` Optional int ` The log level TorchDynamo component Default ` ` logging WARN ` ` aot ` Optional int ` The log level AOTAutograd component Default ` ` logging WARN ` ` autograd ` Optional int ` The log level autograd Default ` ` logging WARN ` ` inductor ` Optional int ` The log level TorchInductor component Default ` ` logging WARN ` ` dynamic ` Optional int ` The log level dynamic shapes Default ` ` logging WARN ` ` distributed ` Optional int ` Whether log c d communication operations other debug info PyTorch Distributed components Default ` ` logging WARN ` ` c d ` Optional int ` Whether log c d communication operations related debug info PyTorch Distributed components Default ` ` logging WARN ` ` ddp ` Optional int ` Whether log debug info related ` ` DistributedDataParallel ` ` DDP PyTorch Distributed components Default ` ` logging WARN ` ` fsdp ` Optional int ` Whether log debug info related ` ` FullyShardedDataParallel ` ` FSDP PyTorch Distributed components Default ` ` logging WARN ` ` dtensor ` Optional int ` Whether log debug info related ` ` DTensor ` ` DTensor PyTorch Distributed components Default ` ` logging WARN ` ` onnx ` Optional int ` The log level ONNX exporter component Default ` ` logging WARN ` ` bytecode ` bool ` Whether emit original generated bytecode TorchDynamo Default ` ` False ` ` aot_graphs ` bool ` Whether emit graphs generated AOTAutograd Default ` ` False ` ` aot_joint_graph ` bool ` Whether emit joint forward-backward graph generated AOTAutograd Default ` ` False ` ` ddp_graphs ` bool ` Whether emit graphs generated DDPOptimizer Default ` ` False ` ` graph ` bool ` Whether emit graph captured TorchDynamo tabular format Default ` ` False ` ` graph_code ` bool ` Whether emit python source graph captured TorchDynamo Default ` ` False ` ` graph_code_verbose ` bool ` Whether emit verbose intermediate FX pass logs graph code Default ` ` False ` ` graph_breaks ` bool ` Whether emit graph breaks encountered TorchDynamo Default ` ` False ` ` graph_sizes ` bool ` Whether emit tensor sizes graph captured TorchDynamo Default ` ` False ` ` guards ` bool ` Whether emit guards generated TorchDynamo each compiled function Default ` ` False ` ` recompiles ` bool ` Whether emit guard failure reason message every time TorchDynamo recompiles function Default ` ` False ` ` recompiles_verbose ` bool ` Whether emit all guard failure reasons when TorchDynamo recompiles function even those actually run Default ` ` False ` ` trace_source ` bool ` Whether emit when TorchDynamo begins tracing new line Default ` ` False ` ` trace_call ` bool ` Whether emit detailed line location when TorchDynamo creates FX node corresponding function call Python + only Default ` ` False ` ` trace_bytecode ` bool ` Whether emit bytecode instructions traced stack state TorchDynamo traces bytecode Default ` ` False ` ` output_code ` bool ` Whether emit TorchInductor output code per-graph basis Default ` ` False ` ` kernel_code ` bool ` Whether emit TorchInductor output code per-kernel bases Default ` ` False ` ` schedule ` bool ` Whether emit TorchInductor schedule Default ` ` False ` ` perf_hints ` bool ` Whether emit TorchInductor perf hints Default ` ` False ` ` pre_grad_graphs ` bool ` Whether emit graphs before inductor grad passes Default ` ` False ` ` post_grad_graphs ` bool ` Whether emit graphs generated after post grad passes Default ` ` False ` ` ir_pre_fusion ` bool ` Whether emit graphs before inductor fusion passes Default ` ` False ` ` ir_post_fusion ` bool ` Whether emit graphs after inductor fusion passes Default ` ` False ` ` onnx_diagnostics ` bool ` Whether emit ONNX exporter diagnostics logging Default ` ` False ` ` fusion ` bool ` Whether emit detailed Inductor fusion decisions Default ` ` False ` ` overlap ` bool ` Whether emit detailed Inductor compute comm overlap decisions Default ` ` False ` ` sym_node ` bool ` Whether emit debug info various SymNode opterations Default ` ` False ` ` export ` Optional int ` The log level export Default ` ` logging WARN ` ` benchmarking ` bool ` Whether emit detailed Inductor benchmarking information Default ` ` False ` ` modules dict This argument provides alternate way specify above log component artifact settings format keyword args dictionary given single argument There two cases where useful new log component artifact has been registered keyword argument has been added function log level unregistered module needs set This can done providing fully-qualified module name key log level value Default ` ` None ` ` cudagraph_static_inputs ` bool ` Whether emit debug info cudagraph static input detection Default ` ` False ` ` autotuning ` bool ` Autotuning choice logs such kernel source perf tuning parameters Default ` ` False ` ` graph_region_expansion ` bool ` Whether emit detailed steps duplicate graph region tracker expansion algorithm Default ` ` False ` ` inductor_metrics ` bool ` Whether estimate runtimes nodes graph log them metrics table Default ` ` False ` ` hierarchical_compile ` bool ` Whether emit debug info hierarchical compilation Default ` ` False ` ` Example xdoctest +SKIP logging The following changes dynamo component emit DEBUG-level logs emit graph_code artifacts torch _logging set_logs dynamo=logging DEBUG graph_code=True The following enables logs different module torch _logging set_logs modules= unregistered module name logging DEBUG ignore env var set LOG_ENV_VAR os environ log warning Using TORCH_LOGS environment variable log settings ignoring call set_logs log_state clear modules = modules _set_logs kwargs - None alias val itertools chain kwargs items modules items type ignore union-attr val None continue log_registry is_artifact alias isinstance val bool raise ValueError f Expected bool enable artifact alias received val val log_state enable_artifact alias log_registry is_log alias alias log_registry child_log_qnames val logging _levelToName raise ValueError f Unrecognized log level log alias val valid level values f join str k k logging _levelToName keys log_state enable_log log_registry log_alias_to_log_qnames get alias alias val _is_valid_module alias _has_registered_parent alias log_registry register_log alias alias log_registry register_child_log alias log_state enable_log log_registry log_alias_to_log_qnames get alias alias val raise ValueError f Unrecognized log artifact name passed set_logs alias _init_logs _set_logs torch=all dynamo=dynamo aot=aot autograd=autograd inductor=inductor dynamic=dynamic bytecode=bytecode aot_graphs=aot_graphs aot_joint_graph=aot_joint_graph ddp_graphs=ddp_graphs distributed=distributed c d=c d ddp=ddp fsdp=fsdp dtensor=dtensor graph=graph graph_code=graph_code graph_code_verbose=graph_code_verbose graph_breaks=graph_breaks graph_sizes=graph_sizes guards=guards recompiles=recompiles recompiles_verbose=recompiles_verbose trace_source=trace_source trace_call=trace_call trace_bytecode=trace_bytecode output_code=output_code kernel_code=kernel_code schedule=schedule perf_hints=perf_hints pre_grad_graphs=pre_grad_graphs post_grad_graphs=post_grad_graphs ir_pre_fusion=ir_pre_fusion ir_post_fusion=ir_post_fusion onnx=onnx onnx_diagnostics=onnx_diagnostics fusion=fusion overlap=overlap sym_node=sym_node export=export cudagraphs=cudagraphs compiled_autograd=compiled_autograd compiled_autograd_verbose=compiled_autograd_verbose cudagraph_static_inputs=cudagraph_static_inputs benchmarking=benchmarking autotuning=autotuning graph_region_expansion=graph_region_expansion inductor_metrics=inductor_metrics hierarchical_compile=hierarchical_compile compute_dependencies=compute_dependencies get_loggers - list logging Logger Returns list all registered loggers logging getLogger qname qname log_registry get_log_qnames register_log setting_name log_name - None Enables log controlled env var user API setting_name Args setting_name shorthand name used env var user API log_name log name setting_name associated log_registry register_log setting_name log_name register_artifact setting_name description visible=False off_by_default=False log_format=None - None Enables artifact controlled env var user API name Args setting_name shorthand name used env var user API description A description what outputs visible Whether gets suggested users default off_by_default whether artifact should logged when ancestor loggers enabled level DEBUG log_registry register_artifact_name setting_name description visible off_by_default log_format getArtifactLogger module_qname artifact_name - logging Logger artifact_name log_registry artifact_names raise ValueError f Artifact name repr artifact_name registered f please call register_artifact repr artifact_name torch _logging registrations qname = module_qname + f __ artifact_name log = logging getLogger qname log artifact_name = artifact_name type ignore attr-defined log_registry register_artifact_log qname configure_artifact_log log log INCR_VERBOSITY_CHAR = + DECR_VERBOSITY_CHAR = - VERBOSITY_REGEX = + &#124; join re escape INCR_VERBOSITY_CHAR re escape DECR_VERBOSITY_CHAR + configure_artifact_log log - None If artifact off default then should only logged when explicitly enabled set propagate False so artifact propagated its ancestor logger log_registry is_off_by_default log artifact_name log propagate = False enable artifact logging when explicitly enabled log_state is_artifact_enabled log artifact_name log setLevel logging DEBUG log propagate = True match comma separated list loggable names whitespace allowed after commas _gen_settings_regex re compile r \+ &#124; - \w\ + \s \+ &#124; - \w\ + _validate_settings settings re fullmatch _gen_settings_regex settings None help_message verbose=False pad_to s length= assert len s = length s + length - len s verbose printed_artifacts = log_registry artifact_names printed_artifacts = log_registry visible_artifacts verbose heading = All registered names heading = Visible registered names use TORCH_LOGS= +help full list lines = all + sorted log_registry log_alias_to_log_qnames keys + sorted f pad_to name \t log_registry artifact_descriptions name name printed_artifacts setting_info = + \n join lines examples = Examples TORCH_LOGS= +dynamo aot will set log level TorchDynamo logging DEBUG AOT logging INFO TORCH_LOGS= -dynamo +inductor will set log level TorchDynamo logging ERROR TorchInductor logging DEBUG TORCH_LOGS= aot_graphs will enable aot_graphs artifact TORCH_LOGS= +dynamo schedule will enable set log level TorchDynamo logging DEBUG enable schedule artifact TORCH_LOGS= +some random module schedule will set log level some random module logging DEBUG enable schedule artifact TORCH_LOGS_FORMAT= levelname s message s any provided format string will set output format Valid keys levelname message pathname levelno lineno filename name TORCH_LOGS_OUT= tmp output txt will output logs tmp output txt well This useful when output long msg = f TORCH_LOGS Info examples heading setting_info msg _invalid_settings_err_msg settings verbose=False valid_settings = all + list log_registry log_alias_to_log_qnames keys + list log_registry artifact_names valid_settings = join sorted valid_settings msg = f Invalid log settings settings must comma separated list fully qualified module names registered log names registered artifact names For more info various settings try TORCH_LOGS= help Valid settings valid_settings msg process_env_var_string_for_windows env_var_str str - str When we setup logging config guide https docs pytorch org docs stable logging html Such TORCH_LOGS= +schedule +inductor +output_code On Linux shows declare -x SSH_TTY= dev pts declare -x TERM= xterm declare -x TORCH_LOGS= +schedule +inductor +output_code declare -x USER= xu On Windows shows TORCHINDUCTOR_WINDOWS_TESTS= TORCH_LOGS= +schedule +inductor +output_code UCRTVersion= For Linux shows quotes default And Windows shows quotes Besides Windows would auto assemble quotes when env var processing On Linux we will get variable +schedule +inductor +output_code On Windows we will get variable +schedule +inductor +output_code So we need remove outer quotes Windows _IS_WINDOWS = sys platform == win remove_outer_quotes s str - str len s = s == s - == s == s - == s - s _IS_WINDOWS env_var_str = remove_outer_quotes env_var_str env_var_str functools lru_cache _parse_log_settings settings settings = process_env_var_string_for_windows settings settings == settings == help raise ValueError help_message verbose=False settings == +help raise ValueError help_message verbose=True _validate_settings settings raise ValueError _invalid_settings_err_msg settings settings = re sub r \s+ settings log_names = settings split get_name_level_pair name clean_name = name replace INCR_VERBOSITY_CHAR clean_name = clean_name replace DECR_VERBOSITY_CHAR name == INCR_VERBOSITY_CHAR level = logging DEBUG name == DECR_VERBOSITY_CHAR level = logging ERROR level = logging INFO clean_name level log_state = LogState name log_names name level = get_name_level_pair name name == all name = torch log_registry is_log name assert level None log_qnames = log_registry log_alias_to_log_qnames name log_state enable_log log_qnames level log_registry is_artifact name log_state enable_artifact name _is_valid_module name _has_registered_parent name log_registry register_log name name log_registry register_child_log name log_state enable_log name level raise ValueError _invalid_settings_err_msg settings log_state _is_valid_module qname spec = importlib util find_spec qname spec None _update_log_state_from_env - None global log_state log_setting = os environ get LOG_ENV_VAR None log_setting None log_state = _parse_log_settings log_setting _has_registered_parent log_qname - bool cur_log = logging getLogger log_qname registered_log_qnames = log_registry get_log_qnames while cur_log parent cur_log name registered_log_qnames True cur_log = cur_log parent False make_module_path_relative abs_path Given absolute filepath corresponding Python module which loaded via normal mechanisms using sys path convert into relative path relative one Python search paths abs_path = pathlib Path abs_path resolve path sys path try rel_path = abs_path relative_to path except ValueError continue str rel_path str abs_path apply custom formats artifacts when necessary TorchLogsFormatter logging Formatter __init__ trace bool = False trace_id_filter Optional set str = None - None super __init__ _is_trace = trace _trace_id_filter = trace_id_filter format record artifact_name = getattr logging getLogger record name artifact_name None artifact_name None artifact_formatter = log_registry artifact_log_formatters get artifact_name None artifact_formatter None artifact_formatter format record record message = record getMessage record asctime = formatTime record m d H M S exception handling - copied logging Formatter format s = record message record exc_info Cache traceback text avoid converting multiple times s constant anyway record exc_text record exc_text = formatException record exc_info record exc_text s - = \n s = s + \n s = s + record exc_text record stack_info s - = \n s = s + \n s = s + formatStack record stack_info record rankprefix = _is_trace dist is_available dist is_initialized record rankprefix = f rank dist get_rank record traceid = _is_trace trace_id = torch _guards CompileContext current_trace_id None record traceid = f trace_id glog_level_to_abbr = DEBUG V V VERBOSE glog INFO I WARNING W ERROR E CRITICAL C shortlevel = glog_level_to_abbr get record levelname record levelname record artifactprefix = artifact_name None record artifactprefix = f __ artifact_name filepath = make_module_path_relative record pathname _trace_id_filter record traceid strip _trace_id_filter prefix = f record rankprefix shortlevel record asctime int record msecs d record process f filepath f record lineno record traceid record artifactprefix _is_trace assert s == try r = f prefix json dumps record metadata except TypeError log warning failing metadata r record metadata raise record payload None r += join f \n\t l l record payload split \n r lines = s split \n \n join f prefix l l lines _default_formatter fmt = os environ get LOG_FORMAT_ENV_VAR None trace_id_filter = item strip item os environ get LOG_TRACE_ID_FILTER split item strip fmt None TorchLogsFormatter trace_id_filter=trace_id_filter fmt short basic fmt = logging BASIC_FORMAT logging Formatter fmt DEFAULT_FORMATTER = _default_formatter _setup_handlers create_handler_fn log - None debug_handler = _track_handler create_handler_fn debug_handler setFormatter DEFAULT_FORMATTER debug_handler setLevel logging DEBUG log addHandler debug_handler handlers = WeakSet type ignore var-annotated mark handlers we ve created so we don t modify user handlers _track_handler handler handlers add handler handler _is_torch_handler handler handler handlers clears all torch handlers specified loggers _clear_handlers log - None to_remove = handler handler log handlers _is_torch_handler handler handler to_remove log removeHandler handler _reset_logs - None reset all registered logs log_qname log_registry get_log_qnames log = logging getLogger log_qname log setLevel logging WARNING log propagate = False _clear_handlers log reset all artifact child logs artifact_log_qname itertools chain log_registry get_artifact_log_qnames log_registry get_child_log_qnames log = logging getLogger artifact_log_qname log setLevel logging NOTSET log propagate = True trace_log propagate = False _clear_handlers trace_log _get_log_state log_state _set_log_state state - None global log_state log_state = state _init_logs log_file_name=None - None global GET_DTRACE_STRUCTURED _reset_logs _update_log_state_from_env out = os environ get LOG_OUT_ENV_VAR None out None log_file_name = out First reset all known registered loggers NOTSET so they respect their parent log level log_qname log_registry get_log_qnames But top level torch level defaults WARNING so our log messages don t leak lower levels log_qname == torch continue log = logging getLogger log_qname log setLevel logging NOTSET Now all loggers which user requested have non-standard logging behavior modify their log levels log_qname level log_state get_log_level_pairs log = logging getLogger log_qname log setLevel level Finally setup handlers all registered loggers log_qname log_registry get_log_qnames log = logging getLogger log_qname _setup_handlers logging StreamHandler log log_file_name None _setup_handlers lambda logging FileHandler log_file_name log configure artifact loggers note must happen last since levels ancestor loggers taken into account artifact_log_qname log_registry get_artifact_log_qnames log = logging getLogger artifact_log_qname configure_artifact_log log Setup handler special trace_log different default configuration trace_dir_name = os environ get TRACE_ENV_VAR None dtrace_dir_name = os environ get DTRACE_ENV_VAR None GET_DTRACE_STRUCTURED = True trace_dir_name = dtrace_dir_name This handler may remove itself trace_dir_name None we actually FB environment This allows us defer actually initializing until we actually need log anything This important because JK initializes C++ singleton which will pork our process we subsequently fork global LOG_TRACE_HANDLER LOG_TRACE_HANDLER None LOG_TRACE_HANDLER = LazyTraceHandler trace_dir_name This log ALWAYS debug level We will additionally test there any handlers before deciding actually call logging Do manually call trace_log setLevel logging DEBUG trace_log_handler = _track_handler LOG_TRACE_HANDLER trace_log_handler setFormatter TorchLogsFormatter trace=True trace_log addHandler trace_log_handler LazyTraceHandler logging StreamHandler Like FileHandler file allocated lazily only upon first log message __init__ root_dir Optional str - None This implemented same way delay implemented FileHandler root_dir = root_dir logging Handler __init__ stream = None _builtin_open = open cloned FileHandler cpython close - None acquire try try stream try flush finally stream = stream stream = None hasattr stream close stream close finally Issue call unconditionally prevent handler leak when delay set Also see Issue we also rely _closed being set True there logging StreamHandler close finally release emit record - None stream None root_dir None TRACE_LOG_DIR = logs torch version torch_version hasattr torch_version git_version os getenv MAST_HPC_JOB_NAME None log info LazyTraceHandler disabled because fbcode conda mast torch _utils_internal justknobs_check pytorch trace enable log info LazyTraceHandler disabled because justknobs_check pytorch trace enable returned False os path exists TRACE_LOG_DIR log info LazyTraceHandler disabled because s does exist TRACE_LOG_DIR os access TRACE_LOG_DIR os W_OK log info LazyTraceHandler disabled because s writeable TRACE_LOG_DIR root_dir = TRACE_LOG_DIR root_dir None os makedirs root_dir exist_ok=True ranksuffix = dist is_available dist is_initialized ranksuffix = f rank_ dist get_rank _ stream = tempfile NamedTemporaryFile mode= w+ suffix= log prefix=f dedicated_log_torch_trace_ ranksuffix dir=self root_dir delete=False log info LazyTraceHandler logging s stream name We go poof remove no-op trace_log removeHandler stream super emit record functools cache warning_once logger_obj args kwargs - None This function similar ` logger warning ` will emit warning same message only once Note The cache function arguments so different callers using same arguments will hit cache The assumption here all warning messages unique across code If they aren t then need switch another type cache includes caller frame information hashing function logger_obj warning args kwargs safe_grad_filter message category filename lineno file=None line=None - bool The grad attribute Tensor str message user_warning_filter message category filename lineno file=None line=None - bool category UserWarning contextlib contextmanager hide_warnings filter_fn=lambda args kwargs True A context manager temporarily suppresses warnings using public API https docs python org library warnings html#warnings showwarning Useful hide warnings without mutating warnings module state see https github com pytorch pytorch issues #issuecomment- NOTE Warnings issued under context will still cached __warningregistry__ count towards once default rule So you should NEVER use user-land function Filter must implement showwarning API filter_fn message category filename lineno file=None line=None - bool True show warning entry prior = warnings showwarning _showwarning args kwargs filter_fn args kwargs prior args kwargs try warnings showwarning = _showwarning yield finally warnings showwarning = prior LazyString Generic _P __init__ func Callable _P str args _P args kwargs _P kwargs - None func = func args = args kwargs = kwargs __str__ - str func args kwargs Logs time takes do structured logging frame compile id key always frame_id _ frame_compile_id structured_logging_overhead dict str float = defaultdict float add_structured_logging_overhead time_spent float - None global structured_logging_overhead key = None trace_id = torch _guards CompileContext current_trace_id None frame_id = trace_id compile_id frame_id frame_compile_id = trace_id compile_id frame_compile_id Why trace_id attempt like structured logging We aggregate across all attempts because compilation metric logged per successful attempt key = f frame_id _ frame_compile_id TODO deal structured logging occurs outside specific compile ids It s hard figure out where we would log we want compilation metrics itself key None key = str key structured_logging_overhead key += time_spent get_structured_logging_overhead - Optional float key = None trace_id = torch _guards CompileContext current_trace_id None frame_id = trace_id compile_id frame_id frame_compile_id = trace_id compile_id frame_compile_id key = f frame_id _ frame_compile_id key None structured_logging_overhead get key None trace_structured_artifact name str will go metadata encoding str payload_fn Callable Optional Union str object = lambda None compile_id Optional CompileId = None - None trace_structured artifact metadata_fn=lambda name name encoding encoding payload_fn=payload_fn compile_id=compile_id trace_structured name str NB metadata expected dict so adding more info forward compatible Tuple str int special case string interning metadata_fn Callable Union dict str Any tuple str int = dict payload_fn Callable Optional Union str object = lambda None suppress_context bool = False expect_trace_id bool = True Whether we expect have current trace id record_logging_overhead bool = True Whether record time spent structured logging compile_id Optional CompileId = None Optional unavailable trace - None metadata arbitrary JSON compatible struct s expected too long e g less than MB payload arbitrary string which can arbitrarily long expected have newlines so no lines too long assert name rank compiled_autograd_id frame_id frame_compile_id attempt severity timestamp pathname thread assert callable metadata_fn f metadata_fn should callable got type metadata_fn assert callable payload_fn f payload_fn should callable got type payload_fn trace_log never propagates ALWAYS DEBUG so also check there handlers instead checking log level trace_log handlers start_time = time time_ns record dict str object = record name = metadata_fn suppress_context TODO Actually rank probably should just emitted once top repeatedly spammed all logs since never changes we assume no interleaving dist is_available dist is_initialized record rank = dist get_rank trace_id = torch _guards CompileContext current_trace_id expect_trace_id trace_id None compile_id None Record stack log call better diagnose why we don t have frame id record stack = torch _logging structured from_traceback CapturedTraceback extract skip= summary cid = trace_id compile_id trace_id compile_id cid None cid compiled_autograd_id None record compiled_autograd_id = cid compiled_autograd_id cid frame_id None record frame_id = cid frame_id cid frame_compile_id None record frame_compile_id = cid frame_compile_id trace_id record attempt = trace_id attempt payload = payload_fn payload None isinstance payload str isinstance payload list special case look better payload = \n + \n join json dumps i i payload + \n json_default obj Sets aren t json serializable isinstance obj set list obj raise TypeError f Object type type obj JSON serializable force newlines so we unlikely overflow line limit payload = json dumps payload default=json_default indent= h = hashlib md usedforsecurity=False h update payload encode utf- record has_payload = h hexdigest trace_log debug extra= metadata record payload payload stacklevel= log_trace_structured_event name record record_logging_overhead Convert seconds nanoseconds add frame compile total structured_logging_overhead_s = time time_ns - start_time e add_structured_logging_overhead structured_logging_overhead_s dtrace_structured name str NB metadata expected dict so adding more info forward compatible Tuple str int special case string interning metadata_fn Callable Union dict str Any tuple str int = dict payload_fn Callable Optional Union str object = lambda None suppress_context bool = False expect_trace_id bool = False Whether we expect have current trace id record_logging_overhead bool = True Whether record time spent structured logging - None For logging more detailed information used debugging This may result program becoming slow GET_DTRACE_STRUCTURED trace_structured name metadata_fn payload_fn=payload_fn suppress_context=suppress_context expect_trace_id=expect_trace_id record_logging_overhead=record_logging_overhead torch _guards torch _utils_internal torch distributed dist