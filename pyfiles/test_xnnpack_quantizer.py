Owner s oncall mobile copy operator torch torch _dynamo torchdynamo torch ao ns fx utils compute_sqnr torch ao quantization default_dynamic_fake_quant default_dynamic_qconfig observer QConfig QConfigMapping torch ao quantization backend_config get_qnnpack_backend_config torch ao quantization qconfig default_per_channel_symmetric_qnnpack_qconfig default_symmetric_qnnpack_qconfig per_channel_weight_observer_range_neg_ _to_ weight_observer_range_neg_ _to_ torch ao quantization quantize_fx _convert_to_reference_decomposed_fx convert_to_reference_fx prepare_fx torch ao quantization quantize_pt e convert_pt e prepare_pt e torch ao quantization quantizer xnnpack_quantizer get_symmetric_quantization_config XNNPACKQuantizer torch export export torch testing _internal common_quantization NodeSpec ns PT EQuantizationTestCase skip_if_no_torchvision skipIfNoQNNPACK TestHelperModules torch testing _internal common_quantized override_quantized_engine torch testing _internal common_utils raise_on_run_directly skipIfNoQNNPACK TestXNNPACKQuantizer PT EQuantizationTestCase test_conv d quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config example_inputs = torch randn node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default _test_quantizer TestHelperModules ConvWithBNRelu dim= relu=False bn=False example_inputs quantizer node_occurrence node_list test_conv d quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config example_inputs = torch randn node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default _test_quantizer TestHelperModules ConvWithBNRelu relu=False bn=False example_inputs quantizer node_occurrence node_list test_conv d_with_conv d quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default m = TestHelperModules Conv dThenConv d _test_quantizer m m example_inputs quantizer node_occurrence node_list test_linear quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config m_eager = TestHelperModules TwoLinearModule eval Test d inputs example_inputs_ d = torch randn example_inputs_ d = torch randn example_inputs_ d = torch randn node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default qconfig = default_per_channel_symmetric_qnnpack_qconfig qconfig_mapping = QConfigMapping set_global qconfig example_inputs example_inputs_ d example_inputs_ d example_inputs_ d _test_quantizer m_eager example_inputs quantizer node_occurrence True qconfig_mapping test_linear_relu quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config m_eager = TestHelperModules LinearReluModel eval Test d inputs example_inputs_ d = torch randn example_inputs_ d = torch randn example_inputs_ d = torch randn node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel There should extra quantize_per_tensor dequantize_per_tensors relu torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default qconfig = default_per_channel_symmetric_qnnpack_qconfig qconfig_mapping = QConfigMapping set_global qconfig example_inputs example_inputs_ d example_inputs_ d example_inputs_ d _test_quantizer m_eager example_inputs quantizer node_occurrence node_list False executorch_backend_config does fuse linear-relu qconfig_mapping test_conv_linear_no_permute quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default qconfig = default_per_channel_symmetric_qnnpack_qconfig qconfig_mapping = QConfigMapping set_global qconfig Test d inputs example_inputs = torch randn _test_quantizer TestHelperModules Conv dWithTwoLinear example_inputs quantizer node_occurrence True qconfig_mapping test_conv_linear quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config Test d inputs example_inputs = torch randn node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default qconfig = default_per_channel_symmetric_qnnpack_qconfig qconfig_mapping = QConfigMapping set_global qconfig _test_quantizer TestHelperModules Conv dWithTwoLinearPermute example_inputs quantizer node_occurrence True qconfig_mapping test_linear_with_dynamic_shape quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config m_eager = TestHelperModules TwoLinearModule eval Test d inputs example_inputs_ d = torch randn node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default qconfig = default_per_channel_symmetric_qnnpack_qconfig qconfig_mapping = QConfigMapping set_global qconfig _test_quantizer m_eager example_inputs_ d quantizer node_occurrence True qconfig_mapping export_with_dynamic_shape=True test_obs_sharing_ops quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config m = TestHelperModules Conv dWithObsSharingOps eval example_inputs = torch randn node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten adaptive_avg_pool d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten hardtanh default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten mean default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _test_quantizer m example_inputs quantizer node_occurrence node_list test_set_module_name Sub torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x M torch nn Module __init__ - None super __init__ linear = torch nn Linear sub = Sub forward x x = linear x x = sub x x m = M eval example_inputs = torch randn quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_module_name sub quantization_config node_occurrence = torch ops aten linear default input output second linear torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = first linear quantized torch ops aten linear default second linear quantized torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _test_quantizer m example_inputs quantizer node_occurrence node_list test_set_module_name_with_underscores - None Test module name has underscore we can still quantize M torch nn Module __init__ - None super __init__ This module name has underscores which can part mangled name foo_bar = torch nn Linear baz = torch nn Linear forward x baz foo_bar x quantizer = XNNPACKQuantizer Set global no quantization then per-channel specific submodule quantizer set_module_name foo_bar get_symmetric_quantization_config is_per_channel=True example_inputs = torch randn m = M eval m = export m example_inputs strict=True module m = prepare_pt e m quantizer Use linear count instead names because names might change order should same count = n m graph nodes n op == call_function n target == torch ops aten linear default Get weight observer see per-channel vs per-tensor weight_observer_node = n args count == The weight tensor should per-tensor per-channel foo_bar assertEqual weight_observer_node op call_module observer_instance = getattr m weight_observer_node target assertEqual observer_instance qscheme torch per_channel_symmetric For baz should have no observer all assertNotEqual weight_observer_node op call_module count += test_set_module_type Sub torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x M torch nn Module __init__ - None super __init__ linear = torch nn Linear sub = Sub forward x x = linear x x = sub x x m = M eval example_inputs = torch randn quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_module_type Sub quantization_config node_occurrence = torch ops aten linear default input output second linear torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = first linear quantized torch ops aten linear default second linear quantized torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _test_quantizer m example_inputs quantizer node_occurrence node_list test_set_module_type_case_ M torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=True conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=True conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=True relu = torch nn ReLU avgpool = torch nn AdaptiveAvgPool d fc = torch nn Linear forward x x = conv x x = relu conv x + conv x x = avgpool x x = torch flatten x x = fc x x m = M eval example_inputs = torch randn quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True We only want annotate Linear type quantizer set_module_type torch nn Linear quantization_config node_occurrence = torch ops aten conv d default torch ops aten linear default input output linear torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = only linear quantized torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _test_quantizer m example_inputs quantizer node_occurrence node_list test_propagate_annotation quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config m = TestHelperModules Conv dPropAnnotaton eval example_inputs = torch randn program capture m = export m example_inputs strict=True module m = prepare_pt e m quantizer m example_inputs n m graph nodes n target torch ops aten view default torch ops aten hardtanh default input_act = getattr m n args target output_act = getattr m next iter n users target assertIs input_act output_act m = convert_pt e m node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated ns call_function torch ops quantized_decomposed quantize_per_channel default ns call_function torch ops quantized_decomposed dequantize_per_channel default checkGraphModuleNodes m expected_node_occurrence=node_occurrence test_dynamic_linear quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True is_dynamic=True quantizer set_global quantization_config m_eager = TestHelperModules TwoLinearModule eval node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default act_affine_quant_obs = observer PlaceholderObserver with_args dtype=torch qint qscheme=torch per_tensor_affine quant_min=- quant_max= eps= - is_dynamic=True qconfig = QConfig activation=act_affine_quant_obs weight=per_channel_weight_observer_range_neg_ _to_ qconfig_mapping = QConfigMapping set_global qconfig Test d inputs example_inputs_ d = torch randn example_inputs_ d = torch randn example_inputs example_inputs_ d example_inputs_ d _test_quantizer m_eager example_inputs quantizer node_occurrence True qconfig_mapping test_dynamic_linear_int _weight quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True is_dynamic=True weight_qmin= weight_qmax= quantizer set_global quantization_config m_eager = TestHelperModules TwoLinearModule eval node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default act_affine_quant_obs = observer PlaceholderObserver with_args dtype=torch qint qscheme=torch per_tensor_affine quant_min=- quant_max= eps= - is_dynamic=True qconfig = QConfig activation=act_affine_quant_obs weight=per_channel_weight_observer_range_neg_ _to_ with_args quant_min= quant_max= qconfig_mapping = QConfigMapping set_global qconfig Test d inputs example_inputs_ d = torch randn example_inputs_ d = torch randn example_inputs example_inputs_ d example_inputs_ d _test_quantizer m_eager example_inputs quantizer node_occurrence True qconfig_mapping test_qat_dynamic_linear quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True is_dynamic=True is_qat=True quantizer set_global quantization_config m_eager = TestHelperModules TwoLinearModule eval node_occurrence = torch ops quantized_decomposed choose_qparams tensor input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default act_affine_quant_obs = default_dynamic_fake_quant qconfig = QConfig activation=act_affine_quant_obs weight=per_channel_weight_observer_range_neg_ _to_ qconfig_mapping = QConfigMapping set_global qconfig Test d inputs example_inputs_ d = torch randn example_inputs_ d = torch randn example_inputs example_inputs_ d example_inputs_ d _test_quantizer m_eager example_inputs quantizer node_occurrence True qconfig_mapping is_qat=True test_dynamic_linear_with_conv quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=False is_dynamic=True quantizer set_global quantization_config m_eager = TestHelperModules ConvLinearWPermute eval node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor note quantize op weights const propagated torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default training_ir_node_occurrence = input output using quantize_per_tensor weight using quantize_per_channel In training IR decomposition different ` torch ops quantized_decomposed quantize_per_tensor default ` nodes becomes ` torch ops quantized_decomposed quantize_per_tensor tensor ` nodes torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor note quantize op weights const propagated torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default act_affine_quant_obs = observer PlaceholderObserver with_args dtype=torch qint qscheme=torch per_tensor_affine quant_min=- quant_max= eps= - is_dynamic=True qconfig = QConfig activation=act_affine_quant_obs weight=weight_observer_range_neg_ _to_ Test d inputs example_inputs = torch randn qconfig_mapping = QConfigMapping set_global qconfig _test_quantizer m_eager example_inputs quantizer node_occurrence True qconfig_mapping training_ir_node_occurrence=training_ir_node_occurrence test_gru test annotating fp GRU so produces q - dq - fp _gru - q - dq currently enough our use cases we may change annotation more precise future RNNDynamicModel torch nn Module __init__ mod_type super __init__ qconfig = default_dynamic_qconfig mod_type == GRU mod = torch nn GRU dtype=torch float mod_type == LSTM mod = torch nn LSTM dtype=torch float forward input_tensor hidden_tensor input_tensor = input_tensor hidden_tensor = hidden_tensor output_tensor hidden_out = mod input_tensor hidden_tensor output_tensor hidden_out override_quantized_engine qnnpack model_fx = RNNDynamicModel GRU niter = example_inputs = input_tensor torch tensor - - - dtype=torch float unsqueeze repeat niter hidden_tensor D num_layers N H_out torch tensor - dtype=torch float repeat model_graph = copy deepcopy model_fx qconfig_mapping = QConfigMapping set_object_type operator mul default_symmetric_qnnpack_qconfig model_fx = prepare_fx model_fx qconfig_mapping example_inputs backend_config=get_qnnpack_backend_config model_fx example_inputs model_fx = _convert_to_reference_decomposed_fx model_fx torchdynamo config patch allow_rnn=True model_graph = export model_graph example_inputs strict=True module quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=False is_dynamic=False quantizer set_global quantization_config model_graph = prepare_pt e model_graph quantizer model_graph example_inputs model_graph = convert_pt e model_graph assertEqual model_fx example_inputs model_graph example_inputs test_linear_gru test make sure GRU annotation does interfere linear annotation RNNDynamicModel torch nn Module __init__ mod_type super __init__ qconfig = default_dynamic_qconfig linear = torch nn Linear mod_type == GRU mod = torch nn GRU dtype=torch float mod_type == LSTM mod = torch nn LSTM dtype=torch float forward input_tensor hidden_tensor input_tensor = linear input_tensor input_tensor = input_tensor hidden_tensor = hidden_tensor output_tensor hidden_out = mod input_tensor hidden_tensor output_tensor hidden_out override_quantized_engine qnnpack model_fx = RNNDynamicModel GRU niter = example_inputs = input_tensor torch tensor - - - dtype=torch float unsqueeze repeat niter hidden_tensor D num_layers N H_out torch tensor - dtype=torch float repeat model_graph = copy deepcopy model_fx qconfig_mapping = QConfigMapping set_object_type operator mul default_symmetric_qnnpack_qconfig set_object_type torch nn Linear default_symmetric_qnnpack_qconfig model_fx = prepare_fx model_fx qconfig_mapping example_inputs backend_config=get_qnnpack_backend_config model_fx example_inputs model_fx = _convert_to_reference_decomposed_fx model_fx torchdynamo config patch allow_rnn=True model_graph = export model_graph example_inputs strict=True module quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=False is_dynamic=False quantizer set_global quantization_config model_graph = prepare_pt e model_graph quantizer model_graph example_inputs model_graph = convert_pt e model_graph assertEqual model_fx example_inputs model_graph example_inputs test_add_and_inplace_add quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config example_inputs = torch randn torch randn node_occurrence = two input one output first add output second add torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten add Tensor torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default TODO torch ops aten add Tensor torch ops quantized_decomposed quantize_per_tensor default _test_quantizer TestHelperModules AddInplaceAdd example_inputs quantizer node_occurrence node_list test_mul_and_inplace_mul quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config example_inputs = torch randn torch randn node_occurrence = two input one output first add output second add torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten mul Tensor torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default TODO torch ops aten mul Tensor torch ops quantized_decomposed quantize_per_tensor default _test_quantizer TestHelperModules MulInplaceMul example_inputs quantizer node_occurrence node_list test_add_mul_scalar quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config example_inputs = torch randn node_occurrence = two input one output first add output second add torch ops quantized_decomposed quantize_per_tensor default TODO torch ops quantized_decomposed dequantize_per_tensor default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten add Tensor torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten mul Tensor torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default TODO torch ops aten add Tensor torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default TODO torch ops aten mul Tensor torch ops quantized_decomposed quantize_per_tensor default _test_quantizer TestHelperModules AddMulScalar example_inputs quantizer node_occurrence node_list test_mul_float _max M torch nn Module forward x x e quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config example_inputs = torch randn quantized node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = torch ops aten mul Tensor _test_quantizer M example_inputs quantizer node_occurrence node_list test_add_mul_long M torch nn Module __init__ - None super __init__ t = torch tensor forward x x = x + t x = x t x quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config example_inputs = torch randn quantized node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = torch ops aten add Tensor torch ops aten mul Tensor _test_quantizer M example_inputs quantizer node_occurrence node_list test_cat_same_node Ensure concatenating same node does cause any unexpected behavior M torch nn Module __init__ super __init__ forward x x = torch cat x x x quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config example_inputs = torch randn node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten cat default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _test_quantizer M example_inputs quantizer node_occurrence node_list TODO express using _test_quantizer add test inception_v TestXNNPACKQuantizerModels PT EQuantizationTestCase skip_if_no_torchvision skipIfNoQNNPACK test_resnet torchvision override_quantized_engine qnnpack example_inputs = torch randn m = torchvision models resnet eval m_copy = copy deepcopy m program capture m = export m example_inputs strict=True module quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config m = prepare_pt e m quantizer checking we inserted observers correctly maxpool operator input output share observer instance assertEqual id m activation_post_process_ id m activation_post_process_ after_prepare_result = m example_inputs m = convert_pt e m after_quant_result = m example_inputs comparing existing fx graph mode quantization reference flow qconfig = default_per_channel_symmetric_qnnpack_qconfig qconfig_mapping = QConfigMapping set_global qconfig backend_config = get_qnnpack_backend_config m_fx = prepare_fx m_copy qconfig_mapping example_inputs backend_config=backend_config after_prepare_result_fx = m_fx example_inputs m_fx = convert_to_reference_fx m_fx backend_config=backend_config after_quant_result_fx = m_fx example_inputs result matches exactly after prepare Note currently will always true since we inserting observers check becomes useful when we add qat examples we can still manually inspect printed observers make sure matches assertEqual after_prepare_result after_prepare_result_fx assertEqual compute_sqnr after_prepare_result after_prepare_result_fx torch tensor float inf there slight differences after convert due different implementations quant dequant assertTrue torch max after_quant_result - after_quant_result_fx e- assertTrue compute_sqnr after_quant_result after_quant_result_fx __name__ == __main__ raise_on_run_directly test test_quantization py