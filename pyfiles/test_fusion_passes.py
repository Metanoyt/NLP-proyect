Owner s oncall quantization torch torch torch testing FileCheck torch testing _internal common_quantization QuantizationTestCase torch testing _internal common_utils raise_on_run_directly TestFusionPasses QuantizationTestCase test_quantized_add_relu_fusion MAdd torch nn Module forward x y = torch ops quantized add x y relu_out = torch relu relu_out A = torch arange - dtype=torch float B = torch arange - dtype=torch float scale = zero_point = qA = torch quantize_per_tensor A scale=scale zero_point=zero_point dtype=torch quint qB = torch quantize_per_tensor B scale=scale zero_point=zero_point dtype=torch quint Check quantized add + relu fusion m = MAdd scripted_m = torch jit script m ref_output = scripted_m qA qB Must inline graph In test case since we directly calling ops does matter however we calling nn modules we have inline graph torch _C _jit_pass_inline scripted_m graph torch _C _jit_pass_fuse_quantized_add_relu scripted_m graph FileCheck check_not aten relu check quantized add_relu run scripted_m graph output = scripted_m qA qB assertEqual ref_output output MAddOut torch nn Module forward x y z = torch ops quantized add_out x y z relu_out = torch relu relu_out qC = torch _empty_affine_quantized qA shape scale=scale zero_point=zero_point dtype=torch quint Check quantized add + relu fusion m = MAddOut scripted_m = torch jit script m ref_output = scripted_m qA qB qC Must inline graph In test case since we directly calling ops does matter however we calling nn modules we have inline graph torch _C _jit_pass_inline scripted_m graph torch _C _jit_pass_fuse_quantized_add_relu scripted_m graph FileCheck check_not aten relu check_not quantized add_out check quantized add_relu_out run scripted_m graph output = scripted_m qA qB qC assertEqual ref_output output MAddScalar torch nn Module forward x y float = torch ops quantized add_scalar x y relu_out = torch relu relu_out Check quantized add + relu fusion m = MAddScalar scripted_m = torch jit script m ref_output = scripted_m qA torch _C _jit_pass_inline scripted_m graph torch _C _jit_pass_fuse_quantized_add_relu scripted_m graph FileCheck check_not aten relu check_not quantized add_scalar check quantized add_scalar_relu run scripted_m graph output = scripted_m qA assertEqual ref_output output MAddScalarOut torch nn Module forward x y float z = torch ops quantized add_scalar_out x y z relu_out = torch relu relu_out qC = torch _empty_affine_quantized qA shape scale=scale zero_point=zero_point dtype=torch quint m = MAddScalarOut scripted_m = torch jit script m ref_output = scripted_m qA qC torch _C _jit_pass_inline scripted_m graph torch _C _jit_pass_fuse_quantized_add_relu scripted_m graph FileCheck check_not aten relu check_not quantized add_scalar_out check quantized add_scalar_relu_out run scripted_m graph output = scripted_m qA qC assertEqual ref_output output __name__ == __main__ raise_on_run_directly test test_quantization py