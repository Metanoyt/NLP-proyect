mypy allow-untyped-defs __future__ annotations collections contextlib dataclasses functools itertools logging math operator textwrap collections Counter typing Any Callable Generic Optional TYPE_CHECKING Union typing_extensions TypeVar sympy torch torch _logging torch _inductor metrics torch _inductor ir MultiTemplateBuffer torch _inductor tiling_utils analyze_memory_coalescing torch fx experimental symbolic_shapes free_unbacked_symbols torch fx immutable_collections immutable_dict torch utils _ordered_set OrderedSet torch utils _sympy functions FloorDiv Identity ModularIndexing torch utils _sympy symbol free_symbol_is_type prefix_str symbol_is_type SymT _dynamo utils counters config ir scheduler analyze_preserves_zero_mask prologue_preserves_zero_mask codecache code_hash PyCodeCache dependencies MemoryDep StarDep WeakDep TYPE_CHECKING ir IRNode optimize_indexing indexing_dtype_strength_reduction runtime coordinate_descent_tuner CoordescTuner runtime hints DeviceProperties runtime runtime_utils green_text next_power_of_ yellow_text scheduler BaseSchedulerNode BaseScheduling WhyNoFuse utils cache_property_on_self expr_fits_within_ bit get_dtype_size IndentedBuffer Placeholder prefix_is_reduction sympy_index_symbol sympy_product sympy_subs unique virtualized ops OpsWrapper V block_analysis BlockPatternMatcher common CSEVariable index_prevent_reordering Kernel PythonPrinter multi_kernel MultiKernel SizeHintMultiKernel simd_kernel_features DisableReduction EnableReduction NodeScheduleEntry NodeScheduleMarker SIMDKernelFeatures TYPE_CHECKING collections abc Iterable Iterator Sequence torch _inductor tiling_utils CoalesceVarAnalysis log = logging getLogger __name__ perf_hint_log = torch _logging getArtifactLogger __name__ perf_hints schedule_log = torch _logging getArtifactLogger __name__ schedule fusion_log = torch _logging getArtifactLogger __name__ fusion pexpr = PythonPrinter doprint all_prefixes = OrderedSet z y x r _ r _ get_max_tiles default int = - int max_tiles = torch _inductor config triton max_tiles max_tiles max_tiles None default dataclasses dataclass IterationRanges Each range tree represents multiple sets iteration indexing single tiled dimension output kernel If you have two loops ranges one another then range tree will i i i i Where i shared between both loops then split into different indexing vars All loop ranges must iterate over same number elements __init__ name str var_list list sympy Symbol var_ranges dict sympy Symbol sympy Expr numel sympy Expr prefix str kernel SIMDKernel divisor=sympy S One length=sympy S One root IterationRangesRoot - None super __init__ name = name var_list = var_list var_ranges = var_ranges numel = numel prefix = prefix divisor = divisor length = length kernel = kernel root = root property cache_property_on_self is_reduction - bool prefix_is_reduction prefix symbol - sympy Symbol sympy_index_symbol name property cache_property_on_self symt - SymT prefix_to_symt = prefix symt symt prefix prefix_str items prefix_to_symt prefix IterationRangesRoot IterationRanges Root iteration range tree represents single tiled dimension output kernel It contains multiple sets iteration represented IterationRangesEntry __init__ name str numel sympy Expr prefix str index int kernel SIMDKernel pid_cache Optional dict str str = None is_loop bool tensor_dim Optional int grid_dim Optional int has_zdim bool - None pid_cache None pid_cache = super __init__ name=name var_list= var_ranges= numel=numel prefix=prefix kernel=kernel root=self index = index Store all nodes one flat list nodes dict sympy Expr IterationRangesEntry = This re-ordering program ID triton mm template pid_cache tl program_id = pid_m pid_cache dict str str = pid_cache True dimension implemented single program looping over full dimension currently only used non-persistent reduction pyrefly ignore missing-argument assert is_loop is_reduction grid_dim None is_loop = is_loop Index corresponding dimension triton tensors tensor_dim = tensor_dim Index corresponding dimension triton grid grid_dim = grid_dim has_zdim = has_zdim __repr__ - str f IterationRangesRoot name r numel cache_clear - None node nodes values node cache_clear index_sym - sympy Symbol sympy_index_symbol f prefix index lookup divisor sympy Expr length sympy Expr - IterationRangesEntry Lookup given RangeTreeEntry creating needed V graph sizevars statically_known_equals divisor length numel expr = FloorDiv index_sym divisor expr = ModularIndexing index_sym divisor length expr nodes node = IterationRangesEntry f prefix next V kernel iter_vars_count divisor length expr V kernel range_tree_nodes node symbol = node var_list append node symbol var_ranges node symbol = length nodes expr = node nodes expr construct_entries lengths list sympy Expr - list IterationRangesEntry divisor = sympy S One itervars = length reversed lengths itervars append lookup divisor length divisor = divisor length reversed itervars construct lengths list sympy Expr - list sympy Symbol e symbol e construct_entries lengths vars_and_sizes index sympy Expr - tuple list sympy Symbol list sympy Expr Figure out vars tree used index get_sort_key x IterationRangesEntry - tuple int bool Gets key sorting nodes When two nodes have same divisor node length should handled first so current divisor changed after multiplied node length Returns ` length_is_one_hint ` ascending sort divisor_hint = V graph sizevars size_hint x divisor fallback=config unbacked_symint_fallback length_is_one_hint = V graph sizevars size_hint x length fallback=config unbacked_symint_fallback == divisor_hint length_is_one_hint nodes = V kernel range_tree_nodes get s s index free_symbols nodes = n n nodes n n prefix == prefix nodes sort key=lambda x get_sort_key x divisor = sympy S One index_vars = sizes = add node nonlocal divisor index_vars append node symbol sizes append node length divisor = divisor node length node nodes V graph sizevars statically_known_equals node divisor divisor fill unused index var add lookup divisor FloorDiv node divisor divisor divisor = node divisor add node V graph sizevars statically_known_equals numel divisor fill unused index var add lookup divisor FloorDiv numel divisor reversed index_vars reversed sizes IterationRangesEntry IterationRanges __init__ name str divisor sympy Expr length sympy Expr expr sympy Expr parent IterationRanges - None super __init__ name=name numel=parent numel length var_list=parent var_list var_ranges=parent var_ranges prefix=parent prefix divisor=divisor length=length kernel=parent kernel root=parent root parent = parent codegen = functools lru_cache None _codegen expr = expr __repr__ - str f IterationRangesEntry name divisor length expr var_ranges set_name name str - None codegen = lambda name type ignore assignment codegen cache_clear = lambda None type ignore method-assign name = name cache_clear - None codegen cache_clear _codegen - str V kernel codegen_iteration_ranges_entry name precomputed_args - list sympy Expr dynamic shapes find parts indexing expressions have precomputed precomputed_args list sympy Expr = isinstance expr sympy Symbol precomputed_args assert isinstance expr FloorDiv ModularIndexing type expr arg expr args isinstance arg sympy Integer sympy Symbol symbols = arg free_symbols len symbols all symbol_is_type s SymT SIZE s symbols precomputed_args append arg precomputed_args __hash__ - int hash name __eq__ other object - bool assert isinstance other IterationRangesEntry name == other name constant_repr value Union int float - str value == float inf float inf value == float -inf float -inf math isnan value float nan repr value CSEVariableType = TypeVar CSEVariableType bound=CSEVariable default=CSEVariable dataclasses dataclass PartialAccumulate buffer_name str reduction_type str value Any SIMDKernel Kernel CSEVariableType Generic CSEVariableType Common base Triton Halide codegen which both use flattened indexing rather than loop nests sexpr Callable sympy Expr str = pexpr kexpr Callable sympy Expr str allow_block_ptr bool = False pyrefly ignore bad-override kernel_name str __init__ tiling dict str sympy Expr features SIMDKernelFeatures pid_cache Optional dict str str = None override_persistent_reduction Optional bool = None override_cooperative_reduction Optional bool = None tiling_scores Optional dict str sympy Expr = None mix_order_reduction bool = False - None pid_cache None pid_cache = super __init__ features = features mutations = features get_mutations body = IndentedBuffer indexing_code = IndentedBuffer numels = prefix V graph sizevars simplify val prefix val tiling items range_trees list IterationRangesRoot = range_tree_nodes dict sympy Symbol IterationRangesEntry = iter_vars_count = itertools count inside_reduction = features is_reduction cooperative_reduction bool = override_cooperative_reduction override_cooperative_reduction None should_use_cooperative_reduction tiling_scores Optional dict str sympy Expr = tiling_scores tiling dict str sympy Expr = tiling persistent_reduction bool = override_persistent_reduction override_persistent_reduction None should_use_persistent_reduction mix_order_reduction bool = mix_order_reduction no_x_dim = want_no_x_dim code_hash Optional str = None Info enable multiple store_output calls epilogue subtiling store_output_ctr = itertools count is_native_matmul = False config triton native_matmul node features node_schedule isinstance node scheduler SchedulerNode isinstance node node ir ComputedBuffer node node get_reduction_type == dot is_native_matmul = True break define closure make cache local object functools cache simplify_indexing index sympy Expr index = V graph sizevars simplify_with_ranges index var_ranges tree range_trees index = combine_contiguous_dims index tree combine_modular_indexing_pairs index simplify_indexing = simplify_indexing initialize_range_tree pid_cache rsplit_size = saved_partial_accumulate list PartialAccumulate = _get_store_output_subgraph_name i int - str f STORE_OUTPUT_ i get_store_output_count total = next store_output_ctr store_output_ctr = itertools count start=total - step= total property cache_property_on_self num_reduction_dims - int sum prefix_is_reduction prefix prefix numels dtype_to_str dtype torch dtype - str raise NotImplementedError get_index_dtype_as_torch_dtype - torch dtype features select_index_dtype property index_dtype - str dtype_to_str get_index_dtype_as_torch_dtype want_no_x_dim - bool False construct_range_trees pid_cache Optional dict str str inside_reduction bool is_reduction bool numels dict str sympy Expr no_x_dim bool - list IterationRangesRoot active_prefixes = OrderedSet prefix prefix all_prefixes prefix numels no_r_dim = inside_reduction is_reduction filtered_index_map seq mask - dict Any int val idx idx val enumerate val val seq val mask grid_dims = x y z pointwise_tensor_dims = list reversed grid_dims reduction_dims = r _ r _ no_x_dim tensor_dims = reduction_dims no_r_dim tensor_dims = pointwise_tensor_dims tensor_dims = pointwise_tensor_dims + reduction_dims Filter out unused tensor dims Convert dicts O index lookup tensor_dim_map = filtered_index_map tensor_dims active_prefixes grid_dim_map = filtered_index_map grid_dims all_prefixes range_trees = i prefix enumerate active_prefixes is_reduction = prefix_is_reduction prefix tensor_dim = tensor_dim_map get prefix grid_dim = grid_dim_map get prefix index = i grid_dim None grid_dim range_trees append IterationRangesRoot f prefix index numels prefix prefix index type ignore arg-type pid_cache=pid_cache is_loop=is_reduction persistent_reduction tensor_dim=tensor_dim grid_dim=grid_dim has_zdim= z numels range_trees initialize_range_tree pid_cache dict str str - None range_trees = construct_range_trees pid_cache inside_reduction features is_reduction numels no_x_dim range_trees extend range_trees finalize_indexing indices Sequence sympy Expr - None Hook called right before codegen every index will used fused kernel store_reduction name str index sympy Expr value CSEVariable - None prior = inside_reduction inside_reduction = False try store name index value finally inside_reduction = prior should_use_cooperative_reduction - bool False defined subclass should_use_persistent_reduction - bool False defined subclass var_ranges - dict sympy Symbol sympy Expr dict itertools chain from_iterable tree var_ranges items tree range_trees triton_tensor_ndim - int sum int tree tensor_dim None tree range_trees indexing_size_str i int - str sizes = None triton_tensor_ndim sizes i = f join sizes dense_size_list - list str sizes = triton_tensor_ndim tree range_trees tree tensor_dim None continue pyrefly ignore missing-argument tree is_reduction inside_reduction sizes tree tensor_dim = f tree prefix upper BLOCK sizes dense_size_str - str sizes = dense_size_list f join sizes combine_modular_indexing_pairs index sympy Expr - sympy Expr isinstance index ModularIndexing index x = index args tree_node = range_tree_nodes get x None index new_index = sympy_subs index x tree_node expr new_index = V graph sizevars combine_modular_indexing_pairs new_index index now contains xindex etc which nonstandard fix up sympy_subs new_index tree_node root index_sym tree_node root lookup sympy S One tree_node root numel symbol combine_contiguous_dims index sympy Expr tree IterationRangesRoot - sympy Expr expand_res = V graph sizevars expand_floor_div index new_index denominator = expand_res type ignore misc FloorDiv _combine_contiguous_dims new_index tree denominator _combine_contiguous_dims index tree _combine_contiguous_dims index sympy Expr tree IterationRangesRoot - sympy Expr More aggressive simplification merge contiguous dims isinstance index sympy Integer sympy Symbol index index_vars sizes = tree vars_and_sizes index len sizes = index new_sizes reindex _prune = V graph sizevars _simplify_loops index_vars sizes index_prevent_reordering index index_vars sizes new_sizes == sizes index new_index_vars = tree construct new_sizes new_index = sympy_subs index dict zip index_vars reindex new_index_vars new_index disable_reduction - contextlib AbstractContextManager None should_flush = range_trees - is_loop cooperative_reduction contextlib contextmanager ctx features is_reduction assert inside_reduction yield should_flush calling codegen_body will flush all pending buffers write out reduction loop codegen_body inside_reduction = False try yield should_flush flush out any code before opening next loop codegen_body finally inside_reduction = True ctx set_ranges lengths sympy Expr - list sympy Symbol assert len lengths == len range_trees ranges construct length length ranges zip lengths range_trees staticmethod _split_iteration_ranges groups Iterable sympy Expr lengths Sequence Sequence sympy Expr - tuple list list sympy Expr list list Callable list sympy Expr sympy Expr Special case node s sizes there s nothing split all len length == length lengths group groups sv = V graph sizevars new_ranges list list sympy Expr = _ groups remaining = sv simplify g g groups var_count = itertools count add_range i int expr sympy Expr - int expr = sv simplify expr sv statically_known_multiple_of remaining i expr raise CantSplit guard last item out remaining i = FloorDiv remaining i expr new_ranges i append expr next var_count make_combined sizes list sympy Expr idxs list int - Callable list sympy Expr sympy Expr Builds nested expression s v i + v i s + v i sk + v i k+ assert len idxs == len sizes + getter flat_vars list sympy Expr - sympy Expr expr = flat_vars idxs s idx zip sizes idxs expr = s expr + flat_vars idx expr getter return_getters_groups = current_group = length_group lengths return_getters = size length_group sv statically_known_equals size type ignore arg-type return_getters append lambda _ sympy S Zero continue while current_group len remaining sv statically_known_equals remaining current_group type ignore arg-type scroll next group remaining elements current_group += During native matmul bmm we enforce tiling order z y x r When fusing bmm node loop z y x r pw node shape z y x we need split pw iteration range into three dimensions The group becomes z y x lengths z y x In case we decompose combined size z y x into three consecutive groups Previously _split_iteration_ranges supported splitting into most two dimensions we now extend do three splits when total size divisible all three group having z y x r= form is_bmm_then_pw = len remaining == remaining - == current_group + len remaining sv statically_known_gt size remaining current_group remaining current_group + is_bmm_then_pw need break size three sv statically_known_multiple_of size remaining current_group remaining current_group + raise CantSplit size = remaining current_group size = remaining current_group + size = FloorDiv size size size return_getters append make_combined size size add_range current_group size add_range current_group + size add_range current_group + size Two-dimensional tiling current_group + len remaining sv statically_known_gt size remaining current_group need break size two sv statically_known_multiple_of size remaining current_group raise CantSplit size = remaining current_group size = FloorDiv size remaining current_group return_getters append make_combined size add_range current_group size add_range current_group + size current_group len remaining return_getters append operator itemgetter add_range current_group size return_getters_groups append return_getters assert all V graph sizevars size_hint s == s remaining f failed set ranges remaining lengths new_ranges return_getters_groups classmethod prepare_split_iteration_lengths cls groups Iterable sympy Expr lengths Sequence Sequence sympy Expr reduction_numel sympy Expr = sympy S One - Sequence Sequence sympy Expr Fill reduction numel lengths missing sizevars = V graph sizevars len lengths == sizevars statically_known_equals reduction_numel sympy S One sizevars statically_known_equals sympy_product groups sympy_product lengths reduction_numel lengths reduction_numel lengths classmethod is_compatible cls groups Iterable sympy Expr lengths Sequence Sequence sympy Expr reduction_numel sympy Expr = sympy S One - bool lengths = cls prepare_split_iteration_lengths groups lengths reduction_numel try cls _split_iteration_ranges groups lengths True except CantSplit False split_and_set_ranges lengths Sequence Sequence sympy Expr - list list sympy Expr Split set iteration ranges kernel based provided lengths This method maps kernel s tiling structure node s iteration space handling both pointwise reduction dimensions appropriately Args lengths A sequence sequences symbolic expressions representing sizes different dimensions each node Returns A list lists symbolic expressions representing mapped iteration variables each dimension Create dictionary mapping each range tree prefix its total number elements tiling = rt prefix rt numel rt range_trees If we re inside reduction loop set all reduction dimensions This effectively disables reduction dimensions when needed inside_reduction prefix tiling prefix_is_reduction prefix tiling prefix = sympy S One Extract values tiling dictionary create groups groups = tiling values Map kernel s group structure node s sizes set ranges using set_ranges method returning resulting iteration variables map_kernel_groups_to_node_sizes groups lengths set_ranges classmethod map_kernel_groups_to_node_sizes cls groups Sequence sympy Expr lengths Sequence Sequence sympy Expr set_ranges - list list sympy Expr We may want fuse ` i s s ` into tiled kernel groups s s To do we need split up iteration space i into something like i s i s i = i s + i This function matches resplits lengths groups kernel enable tiled + non-tiled fusions len lengths == len groups all V graph sizevars simplify sympy_product x - g == x g zip lengths groups set_ranges lengths new_ranges return_getters_groups = cls _split_iteration_ranges groups lengths itervars = itertools chain from_iterable set_ranges new_ranges fn itervars fn fns fns return_getters_groups is_indirect_indexing index sympy Expr - bool tmpX means indirect indexing free_symbol_is_type index SymT TMP is_broadcasted index sympy Expr - bool Note This may correct when there indirect indexing is_indirect_indexing index False index_numels = len numels symbol index free_symbols symbol range_tree_nodes Non-iterated variables e g strides continue entry = range_tree_nodes symbol type ignore index assert isinstance entry parent IterationRangesRoot index_numels entry parent index = entry length If index variables only iterate over subset kernel numels then must broadcasted simplify = V graph sizevars simplify any simplify idx_range = simplify iter_range type ignore arg-type idx_range iter_range zip index_numels numels values index_to_str index sympy Expr - str Convert index expr string can used output code e g sympy expression s may actually appear ks generated kernel Index expressions often need passed arguments triton kernel Rename_indexing codegen_indexing keep track needed indices add new parameters function signature isinstance index list f join map index_to_str index kexpr rename_indexing index type ignore call-arg prepare_indexing index sympy Expr - sympy Expr index = simplify_indexing index index = sympy_subs index V graph sizevars precomputed_replacements simple replacements didn t get rid floor ceil try full subs len index atoms sympy floor len index atoms sympy ceiling index = index subs V graph sizevars precomputed_replacements last resort no range vars expr hoist TODO instead trying blindly find complicated exprs we should hoist inputs outputs sizes strides time indexing generated kernel inputs outputs set yet we d need deeper refactor do way len index atoms sympy ceiling index atoms sympy ceiling nested exprs atoms yields top level first so everything goes fine lower level replacements will come up empty symbols = free_symbols len symbols all symbol_is_type s SymT SIZE SymT PRECOMPUTED_SIZE s symbols replacements = V graph sizevars lookup_precomputed_size index = sympy_subs index replacements simp_index = simplify_indexing index Now we done simplifying we can unwrap Identity so downstream handling its contained expression will work previously tl full wrapping sympy Integer would occur simp_index = simp_index isinstance simp_index Identity simp_index args codegen_indexing simp_index active_range_trees - list IterationRangesRoot t t range_trees pyrefly ignore missing-argument t is_reduction inside_reduction codegen_indexing expr sympy Expr - sympy Expr expr = V graph sizevars simplify_with_ranges expr var_ranges sym sorted expr free_symbols key=str sym range_tree_nodes indexing expression complicated we precompute host side send result kernel argument replacements = ps range_tree_nodes sym precomputed_args type ignore index replacements ps = V graph sizevars lookup_precomputed_size ps len replacements range_tree_nodes sym expr = sympy_subs type ignore index range_tree_nodes sym expr replacements type ignore index range_tree_nodes sym codegen type ignore index expr codegen_nan_check - None raise NotImplementedError NYI codegen_nan_check deallocate_workspaces wrapper = V graph wrapper_code ws reversed args workspace_args wrapper generate_workspace_deallocation ws call_kernel name str node Optional IRNode = None deallocate_ws bool = True - None raise NotImplementedError NYI call_kernel contextlib contextmanager mask_loads mask Union str OpsWrapper value Union int float - Iterator str Context manager add additional mask tl load store prior = _load_mask prior_val = _load_other prior mask = ops logical_and mask prior mask = OpsWrapper _unwrap mask _load_mask = mask _load_other = value try TODO jansel do we need reshape here yield mask finally _load_mask = prior _load_other = prior_val get_strides_of_load index sympy Expr - dict sympy Symbol sympy Expr This gets stride index each tiling variables technically does index For example xindex = x + x + r x = xindex x = xindex r = rindex function would xindex rindex index_to_tile_indexes = k v expr k v range_tree_nodes items index_in_tile_vars = sympy_subs index index_to_tile_indexes type ignore arg-type strides = range_tree range_trees s = sympy_index_symbol range_tree name strides s = sympy_subs index_in_tile_vars s - sympy_subs index_in_tile_vars s strides staticmethod _map_tuple_or_scalar fn value isinstance value tuple tuple map fn value fn value estimate_flops - Optional int flops = node estimate_flops node NodeScheduleMarker only_nodes features node_schedule sum filter None flops estimate_kernel_num_bytes Try best estimate total size bytes kernel s inputs outputs which used estimating memory throughput kernel This information used checking how far we peak memory bandwidth It s important we want avoid overestimating sizes inputs outputs because can wrongfully give us very large memory traffic value which may even larger than theoretical bandwidth thus become very misleading This particularly problematic cases where we slice some inputs In those cases we should only count size slices instead original inputs because only slices contribute real memory traffic nbytes = ninplace_args = len unique args inplace_buffers values _ call_args _ _ = args python_argdefs buf_accesses = features buf_accesses For pointwise reduction kernels upper-bound numels output buffer FIXME This exactly right cases like below foo tensor tensor x = narrow tensor cat x tensor For example we will end up overestimate size slice s Potentially we could have precise inputs information we maintained original inputs Pointwise kernel created cat However I think might bit overwhelming we add such complexity only handling some particular cases benchmarking out_numel = V graph sizevars size_hint sympy_product numels values fallback=config unbacked_symint_fallback i arg enumerate call_args buf may narrowed In case number memory accesses should estimated based reinterpreted layout On other hand buf may broadcasted In case counting size underline storage would give us better estimation terms memory accesses arg buf_accesses nbytes append continue arg_numel = V graph get_numel arg buf_size = V graph sizevars size_hint arg_numel fallback=config unbacked_symint_fallback buf_size out_numel This arg points buf has been sliced We need count each individual slice have better estimation indices = OrderedSet Any no_index_dep_count = dep buf_accesses arg isinstance dep StarDep WeakDep indices add f no_index_dep_ no_index_dep_count no_index_dep_count += indices add dep index numel = len indices out_numel numel = buf_size dtype = V graph get_dtype arg dtype_size = get_dtype_size dtype pyrefly ignore bad-argument-type nbytes append numel dtype_size + int i ninplace_args sum nbytes warn_mix_layout kernel_name Print message kernel have mixed layout inputs Only care about D tensor now len args input_buffers == len args output_buffers == len args inplace_buffers == even input buffer output buffer have different layout can layout conversion kernel No need warn mix layouts argdefs call_args _signature _ = args python_argdefs uniform_stride_order = None pyrefly ignore bad-assignment arg_name call_args buf = V graph try_get_buffer arg_name buf continue layout = buf get_layout len layout size == ignore tensor only dimension non-zero len x x layout size x == == continue stride_order = ir get_stride_order layout stride uniform_stride_order None uniform_stride_order = stride_order uniform_stride_order = stride_order msg = yellow_text f Expected stride order uniform_stride_order found stride order + f stride_order kernel kernel_name log warning msg stride_order_list = ir get_stride_order V graph get_buffer name get_layout stride V graph try_get_buffer name None name call_args size_list = V graph get_buffer name get_layout size V graph try_get_buffer name None name call_args source_list = GraphInput name V graph graph_inputs IntermediateBuffer name V graph name_to_buffer None name call_args argdef_names = x name x argdefs msg = yellow_text f param names argdef_names \n buf names call_args \n strides stride_order_list + f \n sizes size_list \n sources source_list \n log warning msg msg = green_text f All inputs triton kernel kernel_name have uniform layout log warning msg welford_reduce_fallback dtype value sum_ = ops reduction dtype dtype sum value inside_reduction = False rnumel = ops index_expr features reduction_numel dtype mean = ops truediv sum_ rnumel inside_reduction = True dx = ops sub value mean dx = ops mul dx dx m = ops reduction dtype dtype sum dx OpsWrapper _unwrap mean m rnumel prepare_softmax_twopass_fallback dtype value vmax = ops reduction dtype dtype max value sub = ops sub value vmax exp = ops exp sub vsum = ops reduction dtype dtype sum exp OpsWrapper _unwrap vmax vsum codegen_kernel raise NotImplementedError codegen_body pass codegen_iteration_ranges_entry entry IterationRangesEntry pass SIMDScheduling BaseScheduling Single Instruction Multiple Data parent used fusion across multiple different backends kernel_type type Any = SIMDKernel override subclass group_fn sizes tuple V graph sizevars simplify sympy_product s s sizes can_fuse node node Hook called Scheduler determine Triton backend can fuse node node These nodes might already FusedSchedulerNodes isinstance node scheduler ForeachKernelSchedulerNode isinstance node scheduler ForeachKernelSchedulerNode scheduler ForeachKernelSchedulerNode can_fuse node node _ numel rnumel = node group _ numel rnumel = node group why = WhyNoFuse node node node is_split_scan node is_split_scan node is_reduction why Split scan cannot fuse reductions node is_split_scan node is_split_scan node is_reduction why Split scan cannot fuse reductions node is_reduction node is_reduction reduction_can_fuse = numel == numel rnumel == rnumel reduction_can_fuse torch _inductor scheduler MixOrderReduction reduction_can_fuse = MixOrderReduction can_fuse node node reduction_can_fuse why numel rnumel mismatch reduce s s s s numel numel rnumel rnumel reduction_can_fuse node is_native_matmul node is_native_matmul Ensure node always native matmul side node is_native_matmul node node = node node A native matmul node keeps its original loop order For example C z y x = torch bmm A z y r B z r x keeps z y x order see simplify_and_reorder ir py Triton kernels native matmul always tile loops z y x see get_tiling_and_scores file If candidate node node uses different loop order e g z x y r its tiling incompatible native matmul tiling z y x r This means _split_iteration_ranges will fail so these nodes should fused tiling = select_tiling node get_nodes numel rnumel all SIMDKernel is_compatible tiling values n get_ranges reduction_numel=rnumel n node get_nodes why invalid loop order tiling native matmul False reduction_can_fuse node is_reduction node is_reduction numel == numel rnumel == rnumel node is_template why numel rnumel mismatch non-reduce s s s s numel numel rnumel rnumel False prologue fusion input sizes differ output group fuse so long node matches group existing prologue nodes node node get_nodes dont need check epilogue nodes prologue fusion break after template node is_template break we would have already restricted prologue fusing had multiple uses so must fusing into node node used_buffer_names node get_buffer_names continue _ pro_numel pro_rnumel = node group numel == pro_numel rnumel == pro_rnumel why numel rnumel mismatch prologue mismatch s s s s numel pro_numel rnumel pro_rnumel False n node node n is_template True check bad combined tiling tiling = select_tiling node get_nodes numel rnumel tiling = select_tiling node get_nodes numel rnumel tiling = select_tiling node get_nodes + node get_nodes numel rnumel config triton tiling_prevents_pointwise_fusion cond = True len tiling len tiling cond = tiling == tiling == tiling cond = tiling == tiling len tiling cond = tiling == tiling cond why tiling mismatch s s s tiling tiling tiling False True node is_reduction node is_reduction assert rnumel == rnumel = numel == numel rnumel all SIMDKernel is_compatible numel rnumel n get_ranges n node get_nodes why nodes numel rnumel incompatibility False config triton tiling_prevents_reduction_fusion node is_template is_reduction_tiling_valid = tuple select_tiling node get_nodes numel values numel numel rnumel is_reduction_tiling_valid why invalid tiling reduction is_reduction_tiling_valid True numel = numel why nodes numel incompatibility numel == numel assert node is_reduction node is_reduction swap args hit case above can_fuse_horizontal node node can_fuse_vertical = can_fuse can_fuse_horizontal = can_fuse generate_node_schedule nodes numel rnumel node_schedule list Any = done = OrderedSet scheduler BaseSchedulerNode Writes reduced shape meaning they only present once reduction loop has ended not_ready_yet_nodes OrderedSet str = OrderedSet current_loop_buffer_usage OrderedSet str = OrderedSet maybe_split_index Optional int = None fits_in_main_body n _ node_numel node_rnumel = n group node_numel == numel node_rnumel == rnumel node_numel == numel rnumel node_rnumel == fits_outside_reduction n _ node_numel node_rnumel = n group node_numel == numel node_rnumel == rnumel = expect_improved_memory_usage n read n read_writes reads read name current_loop_buffer_usage True False schedule_node_in_loop n done add n node_schedule append n current_loop_buffer_usage update x name x n read_writes reads A scan modelled reduction scheduler has full sized output can used inside loop body n is_reduction isinstance n scheduler SchedulerNode isinstance n node ir ComputedBuffer isinstance n node data ir Scan not_ready_yet_nodes add n get_name node available within loop current_loop_buffer_usage update x name x n read_writes writes contextlib contextmanager end_current_reduction_loop nonlocal maybe_split_index node_schedule node_schedule - EnableReduction node_schedule pop node_schedule append DisableReduction maybe_split_index node_schedule insert maybe_split_index DisableReduction node_schedule insert maybe_split_index + EnableReduction maybe_split_index = None yield node_schedule append EnableReduction not_ready_yet_nodes clear current_loop_buffer_usage clear requires_closing_previous_reduction node node_schedule rnumel == False not_ready_yet_nodes node ancestors False assert node_schedule isinstance node_schedule - EnableReduction DisableReduction bool not_ready_yet_nodes node nodes node done continue done add node fits_in_main_body node requires_closing_previous_reduction node node_schedule end_current_reduction_loop pass need start new reduction loop current_loop_buffer_usage expect_improved_memory_usage node If we don t improve memory usage then better split into two loops maybe_split_index = maybe_split_index len node_schedule Memory usage got improved cancel loop split maybe_split_index = None schedule_node_in_loop node fits_outside_reduction node end_current_reduction_loop node_schedule append node raise NotImplementedError f unexpected group numel rnumel = node group node_schedule codegen_mix_order_reduction node node node = node node node node Make sure there no producer consumer relationship assert node ancestors node get_operation_names node ancestors node get_operation_names _codegen_mix_order_reduction node node _split_mix_order_reduction_epilogue node TODO do more validation here nodes = node get_nodes reductions = epilogues = node nodes node is_reduction reductions append node epilogues append node reductions epilogues _generate_kernel_code_for_mix_order_reduction kernel_features split_size for_benchmark for_benchmark True generated code benchmarking We need make sure benchmark harness code generated numel rnumel = kernel_features numel kernel_features reduction_numel node_schedule = kernel_features node_schedule kernel = create_kernel_choices kernel_features x numel r _ rnumel features kernel_features tiling_scores None mix_order_reduction True override_persistent_reduction True assert kernel persistent_reduction assert kernel mix_order_reduction kernel rsplit_size = split_size codegen_node_schedule_with_kernel node_schedule kernel allocate workspace kernel _ ws_name ws_off = kernel args workspace len kernel saved_partial_accumulate kernel numels r _ kernel numels x + kernel rsplit_size - kernel rsplit_size False dtype=torch float assert ws_off == f ws_off= kernel kernel codegen_body stack = contextlib ExitStack V set_kernel_handler kernel stack for_benchmark stack enter_context config patch benchmark_kernel=True src_code = kernel codegen_kernel for_benchmark only do we doing benchmarking When we generating final code kernel name should decided differently node type fx node name etc src_code = src_code replace str Placeholder KERNEL_NAME triton_ kernel ws_name src_code benchmark_codegened_module mod n_spills_threshold= node_names Optional OrderedSet str = None - tuple float str raise NotImplementedError _codegen_mix_order_reduction node node numel rnumel = scheduler MixOrderReduction get_numel_rnumel node V graph sizevars statically_known_gt numel rnumel _codegen_mix_order_reduction node node _pick_split_size overridden has highest priority config triton mix_order_reduction_split_size None config triton mix_order_reduction_split_size heuristics based number SMs device_prop = DeviceProperties create node get_device num_sm = device_prop multi_processor_count estimated_num_splits = num_sm split_size = max next_power_of_ numel estimated_num_splits split_size = min split_size split_size split_size = _pick_split_size metrics codegen_mix_order_reduction += assert V graph sizevars statically_known_gt numel rnumel split epilogue out node node _reductions node _epilogue = _split_mix_order_reduction_epilogue node converted_nodes = subnode node _reductions subnode cancel_reduction_split converted = subnode extract_pw_from_reduction converted swap_pw_red_dimension converted_nodes append converted node_schedule = generate_node_schedule node get_nodes + converted_nodes numel rnumel kernel_features = SIMDKernelFeatures node_schedule numel rnumel The autotuning skipped deterministic mode torch _inductor config deterministic config triton mix_order_reduction_split_size None config triton mix_order_reduction_autotune_split_size _bench candidate_split_size _ _ src_code = _generate_kernel_code_for_mix_order_reduction kernel_features split_size=candidate_split_size for_benchmark=True mod = PyCodeCache load src_code ms _ = benchmark_codegened_module mod ms split_size = CoordescTuner autotune_single_field _bench split_size print f Autotuning pick split size split_size kernel ws_name src_code = _generate_kernel_code_for_mix_order_reduction kernel_features split_size=split_size for_benchmark=False rename intermediate reduction output final reduction output is_split_reduction = bool node _reductions node _split_size rename = is_split_reduction subnode node _reductions bufname = subnode get_outputs node get_name username = subnode get_outputs users node get_outputs node get_name rename bufname = username assert scheduler scheduler removed_ops add subnode get_outputs users node get_name V graph removed_buffers add bufname partial_accum kernel saved_partial_accumulate partial_accum buffer_name = rename get partial_accum buffer_name partial_accum buffer_name kernel_name = define_kernel src_code node_schedule kernel kernel kernel_name = kernel_name kernel code_hash = code_hash src_code V set_kernel_handler kernel node kernel_features scheduler_nodes No need allocate buffer split reduction since we gonna allocate workspace store intermediate reduction reduction node get_outputs node get_name rename node mark_run workspace args still needed after call kernel call_kernel kernel kernel_name deallocate_ws=False V graph removed_buffers &#124; = kernel removed_buffers V graph inplaced_to_remove &#124; = kernel inplaced_to_remove extra round reduction assert len converted_nodes == len kernel saved_partial_accumulate nsplit = numel + split_size - split_size idx partial_accum enumerate kernel saved_partial_accumulate buffer_name = partial_accum buffer_name stride_str = f nsplit rnumel start = f idx stride_str end = f idx + stride_str reduction_type op = min amin max amax opname = reduction_type op get partial_accum reduction_type partial_accum reduction_type V graph wrapper_code writeline f buffer_name = ws_name start end view nsplit rnumel opname dim= mark buffer allocated so we don t try allocate again when s later used V graph wrapper_code allocated add buffer_name kernel deallocate_workspaces node _epilogue _codegen_nodes node _epilogue free_buffers_in_scheduler _codegen_nodes nodes Sequence scheduler SchedulerNode coalesce_analysis Optional CoalesceVarAnalysis = None assert scheduler nodes = node node nodes node get_name scheduler removed_ops nodes _ numel rnumel = max nodes key=lambda x int x is_reduction group node_schedule = generate_node_schedule nodes numel rnumel schedule_log debug Schedule \n s node_schedule codegen_node_schedule SIMDKernelFeatures node_schedule numel rnumel coalesce_analysis codegen_node node Union scheduler FusedSchedulerNode scheduler SchedulerNode Given set pre-fused nodes generate Triton kernel assert scheduler nodes = node node node get_nodes node get_name scheduler removed_ops len nodes == torch _inductor config triton coalesce_tiling_analysis len nodes = len node get_nodes assert scheduler node = scheduler FusedSchedulerNode scheduler nodes coalesce_analysis = analyze_memory_coalescing node coalesce_analysis = None _codegen_nodes nodes coalesce_analysis type ignore arg-type staticmethod can_use_ bit_indexing numel sympy Expr buffers Iterable Union ir Buffer ir TensorBox ir TorchBindObject ir IRNode - bool int_max = torch iinfo torch int max expr_fits_within_ bit numel False Any use MultiOutputLayout will create buffer Layout whose sizes accounted buf_sizes = buf get_layout storage_size buf buffers buf has_tensor_output buf buffers buf has_tensor_output isinstance buf ir MutationOutput mutated_bufs = buf get_mutation_buffers buf_sizes += buf get_layout storage_size buf mutated_bufs buf has_tensor_output all expr_fits_within_ bit size size buf_sizes False Only install guards -bit indexing there no correctness issue using -bit everything V graph sizevars check_leq numel int_max type ignore arg-type size buf_sizes V graph sizevars check_leq size int_max type ignore arg-type True codegen_node_schedule kernel_features SIMDKernelFeatures Generate code nodes kernel_features node_schedule = kernel_features node_schedule tiling tiling_score = get_tiling_and_scores node_schedule kernel_features numel kernel_features reduction_numel kernel_features coalesce_analysis kernels = create_kernel_choices kernel_features tiling features kernel_features tiling_scores tiling_score kernel kernels codegen_node_schedule_with_kernel node_schedule kernel MultiKernel merge_workspaces_inplace kernels kernel kernels V set_kernel_handler kernel src_code = kernel codegen_kernel kernel_name = define_kernel src_code node_schedule kernel log debug Generating kernel code kernel_name s kernel_name kernel kernel_name = kernel_name kernel code_hash = code_hash src_code del kernel final_kernel Union SIMDKernel MultiKernel len kernels final_kernel = MultiKernel kernels final_kernel = kernels V set_kernel_handler final_kernel node kernel_features scheduler_nodes node mark_run filter out NodeScheduleMarker base_scheduler_nodes = node node node_schedule isinstance node BaseSchedulerNode codegen_comment base_scheduler_nodes final_kernel kernel_name config cpp enable_kernel_profile V graph wrapper_code write_kernel_context_guard_begin V graph wrapper_code write_kernel_context_guard final_kernel kernel_name base_scheduler_nodes type ignore arg-type final_kernel call_kernel final_kernel kernel_name config cpp enable_kernel_profile V graph wrapper_code write_kernel_context_guard_end config nan_asserts final_kernel codegen_nan_check config warn_mix_layout final_kernel warn_mix_layout kernels kernel_name V graph removed_buffers &#124; = final_kernel removed_buffers V graph inplaced_to_remove &#124; = final_kernel inplaced_to_remove V graph wrapper_code supports_intermediate_hooks type ignore has-type config generate_intermediate_hooks Not every node schedule will actually live output we can t check dead buffers live_outs = kernels args live_output_buffers node kernel_features scheduler_nodes name = node get_name name live_outs continue assert node node None origin_node = node node get_origin_node origin_node None counters inductor intermediate_hooks += V graph wrapper_code writeline f run_intermediate_hooks origin_node name r name free_buffers_in_scheduler create_kernel_choices kernel_features SIMDKernelFeatures kernel_args kernel_kwargs - list SIMDKernel kernel_type kernel_args kernel_kwargs codegen_node_schedule_with_kernel node_schedule kernel kernel stack = contextlib ExitStack all_indexing = First pass collect indexing decide inplace updates node node_schedule node DisableReduction stack enter_context kernel disable_reduction node EnableReduction stack close node decide_inplace_update index_vars = kernel split_and_set_ranges node get_ranges all_indexing update dict fromkeys node _body indexing_from_args index_vars values kernel finalize_indexing all_indexing keys Second pass do codegen node node_schedule node DisableReduction stack enter_context kernel disable_reduction node EnableReduction stack close TODO - use split ranges indexing_dtype_strength_reduction node _body index_vars = kernel split_and_set_ranges node get_ranges node codegen index_vars _codegen_single_template kernel render template_node epilogue_nodes prologue_nodes only_gen_src_code=False Helper method codegen single template kernel variant buf_name_to_prologue_group = template_reads = template_node used_buffer_names prologue_group = prologue prologue_nodes names = prologue get_buffer_names prologue_group append prologue must end prologue group names template_reads assert len names == buf_name_to_prologue_group next iter names = prologue_group kernel prologue_fused_inputs add next iter names prologue_group = all prologue groups should have finalized use template assert len prologue_group == kernel only_gen_src_code prologue nodes can only fused their only use template so they necessarily allocated node template_node epilogue_nodes node mark_run partial_code = render num_store_subgraphs = kernel get_store_output_count i range num_store_subgraphs subgraph_name = kernel _get_store_output_subgraph_name i kernel set_subgraph_body subgraph_name node epilogue_nodes node codegen kernel split_and_set_ranges node get_ranges kernel cse invalidate OrderedSet input_name buffer kernel named_input_nodes items subgraph_name = f LOAD_INPUT_ input_name prologue_group = buf_name_to_prologue_group get buffer get_name can_codegen_without_upcast = all p_n can_codegen_without_upcasts p_n prologue_group TODO - doesn t work libdevice calls potentially other bugs upcasting fp downcasting gives large slowdown config patch triton codegen_upcast_to_fp can_codegen_without_upcast kernel set_subgraph_body subgraph_name prologue_node prologue_group len prologue_node get_buffer_names == len prologue_group == prologue_preserves_zero_mask prologue_node kernel prologue_fused_inputs_preserve_zero &#124; = prologue_node get_buffer_names prologue_node codegen kernel split_and_set_ranges prologue_node get_ranges kernel cse invalidate OrderedSet Template hooks must finalised after kernel remove_kernel_local_buffers called called when kernel context exited above when kernel handler set below This because hooks may add DeferredLine type lines which preclude lines involving buffers have been removed finalize must called after adding epilogue above V set_kernel_handler kernel isinstance partial_code str This used calculate flops TritonTemplateKernels ir IRNode current_origins template_node node origins partial_code finalize_hook DEF_KERNEL partial_code finalize_hook ARGDEFS strict=False TODO Maybe unify CUDATemplateKernel also use PartialRender flexible epilogue fusion input_name kernel named_input_nodes keys subgraph_name = f LOAD_INPUT_ input_name pyrefly ignore missing-attribute partial_code finalize_hook subgraph_name strict=False num_store_subgraphs = kernel get_store_output_count i range num_store_subgraphs subgraph_name = kernel _get_store_output_subgraph_name i pyrefly ignore missing-attribute partial_code finalize_hook subgraph_name isinstance partial_code str src_code = partial_code Ensure all hooks finalized before kernel defined Note some these hooks may have been registered kernel subclass src_code = partial_code finalize_remaining node_schedule = prologue_nodes template_node epilogue_nodes config benchmark_kernel num_gb = kernel estimate_kernel_num_bytes e src_code = f kernel imports_for_benchmark_kernel \n f src_code \n f kernel codegen_kernel_benchmark num_gb getvalue only_gen_src_code src_code kernel kernel_name = define_kernel src_code node_schedule kernel kernel _get_multikernel_shapes node MultiTemplateBuffer - tuple tuple int ir IRNode get_size arg isinstance arg IRNode None isinstance arg ir BaseView triton templates want base tensor arg = arg unwrap_view size = arg maybe_get_size None None tuple s s size out = arg list node inputs + node isinstance arg list tuple out append tuple get_size _arg _arg arg out append get_size arg tuple out _kernel_has_dynamic_shapes node MultiTemplateBuffer - bool shapes = _get_multikernel_shapes node any any isinstance s sympy Expr isinstance s sympy Integer s shape shape shapes _make_shape_cache_key node MultiTemplateBuffer hint int - tuple tuple int Returns cache key hint-based multi-graph key tuple shapes hint filled shapes = _get_multikernel_shapes node tuple tuple hint isinstance s sympy Expr isinstance s sympy Integer s s shape shape shapes codegen_template template_node epilogue_nodes prologue_nodes only_gen_src_code=False hint_override Optional int = None - Optional str Codegen triton template multi-kernel dispatch support If ` only_gen_src_code=True ` src code will returned instead being codegenned into wrapper _ _numel rnumel = template_node group assert rnumel == isinstance template_node node MultiTemplateBuffer template_node node _make_kernel_renders len template_node node _make_kernel_renders _kernel_has_dynamic_shapes template_node node kernels = src_codes = size_hint make_kernel_render template_node node _make_kernel_renders items kernel render = make_kernel_render template_node node hint_override=hint_override only_gen_src_code src_code = _codegen_single_template kernel render template_node epilogue_nodes prologue_nodes only_gen_src_code=True assert isinstance src_code str pyrefly ignore bad-argument-type src_codes append src_code size_hint None continue skip kernel generation based real runtime value only use hints kernel = _codegen_single_template kernel render template_node epilogue_nodes prologue_nodes only_gen_src_code=False shape_cache_key = None size_hint None _make_shape_cache_key template_node node size_hint kernels shape_cache_key = kernel only_gen_src_code \n\n join src_codes MultiKernel merge_workspaces_inplace list kernels values multi_kernel = SizeHintMultiKernel kernels node_schedule = prologue_nodes template_node epilogue_nodes codegen_comment node_schedule multi_kernel kernel_name multi_kernel call_kernel multi_kernel kernel_name V graph removed_buffers &#124; = multi_kernel removed_buffers V graph inplaced_to_remove &#124; = multi_kernel inplaced_to_remove free_buffers_in_scheduler None kernel render = template_node node make_kernel_render template_node node hint_override=hint_override only_gen_src_code _codegen_single_template kernel render template_node epilogue_nodes prologue_nodes only_gen_src_code=True kernel = _codegen_single_template kernel render template_node epilogue_nodes prologue_nodes only_gen_src_code=False node_schedule = prologue_nodes template_node epilogue_nodes codegen_comment node_schedule kernel kernel_name kernel call_kernel kernel kernel_name template_node node V graph removed_buffers &#124; = kernel removed_buffers V graph inplaced_to_remove &#124; = kernel inplaced_to_remove free_buffers_in_scheduler None codegen_sync V graph wrapper_code writeline V graph device_ops synchronize generate_combo_kernel_code subkernel_nodes list BaseSchedulerNode custom_part_algorithm bool enable_autotune bool mixed_sizes bool only_gen_src_code bool = False - list tuple str Any Any triton_combo_kernel ComboKernel fused_node_lists = node get_nodes node subkernel_nodes subkernel_map node_schedule_map = pn nodes zip subkernel_nodes fused_node_lists _ numel rnumel = max nodes key=lambda x int x is_reduction group node_schedule = generate_node_schedule nodes numel rnumel tiling = select_tiling node_schedule numel rnumel node_schedule_map pn = node_schedule tiling numel rnumel subkernel_map pn = ComboKernel create_triton_kernel tiling features=SIMDKernelFeatures node_schedule numel rnumel optimize_mask=not mixed_sizes partitions = ComboKernel horizontal_partition nodes=subkernel_nodes triton_scheduling=self custom_algorithm=custom_part_algorithm kernel_map=subkernel_map node_info_map=node_schedule_map log debug ComboKernels d nodes partitioned into s groups len subkernel_nodes len p p partitions kernel_code_list = node_group partitions len node_group == continue fused_node_lists = node get_nodes node node_group kernel = ComboKernel enable_autotune=enable_autotune mixed_sizes=mixed_sizes pn nodes zip node_group fused_node_lists codegen_node_schedule_with_kernel node_schedule_map pn kernel create_sub_kernel subkernel_map pn subkernel = subkernel_map pn node_schedule = node_schedule_map pn only_gen_src_code V set_kernel_handler subkernel type ignore call-arg node NodeScheduleMarker only_nodes node_schedule node mark_run V graph removed_buffers &#124; = subkernel removed_buffers V graph inplaced_to_remove &#124; = subkernel inplaced_to_remove src_code = kernel codegen_kernel kernel_code_list append src_code kernel node_group kernel_code_list codegen_combo_kernel combo_kernel_node subkernel_nodes = combo_kernel_node get_subkernel_nodes custom_part_algorithm = combo_kernel_node use_custom_partition_algo enable_autotune = combo_kernel_node enable_autotune mixed_sizes = config combo_kernel_allow_mixed_sizes config combo_kernel_allow_mixed_sizes == custom_part_algorithm kernel_code_list = generate_combo_kernel_code subkernel_nodes custom_part_algorithm enable_autotune mixed_sizes src_code kernel _ kernel_code_list kernel_name = define_kernel src_code combo_kernel_node kernel codegen_comment combo_kernel_node snodes kernel_name log debug ComboKernels generated kernel s kernel_name kernel call_kernel V graph wrapper_code kernel_name free_buffers_in_scheduler classmethod functools lru_cache candidate_tilings cls node numel reduction_numel - list CandidateTiling is_pointwise = reduction_numel == tile_ranges is_pointwise bool ranges rw - list CandidateTiling Compute tiling candidates dividing up iteration ranges assert len rw range_vars == len ranges f rw range_vars= ranges= isinstance dep MemoryDep filters out StarDeps StarDeps refer reads need access entire tensor they don t contribute read indexing information practically they don t have dep index so they can t used stride_hints below dep_sources = rw reads rw writes assert all isinstance dep MemoryDep StarDep dep itertools chain from_iterable dep_sources deps = dep dep itertools chain from_iterable dep_sources dep name V graph removed_buffers isinstance dep MemoryDep write_names = OrderedSet dep name dep rw writes collapse_ranges ranges Sequence sympy Expr - sympy Expr V graph sizevars simplify sympy_product ranges Default no tiling tilings = CandidateTiling tiling=cls create_partial_tiling collapse_ranges ranges is_pointwise name= none score= Find non-trivial tiling candidates dep deps strides = V graph sizevars stride_hints dep index rw range_vars assert len strides == len ranges try split = strides index + split == len ranges continue all s == s strides split broadcasted tensor all dimensions after split broadcast real split continue except ValueError continue tiled_groups = collapse_ranges ranges split collapse_ranges ranges split score number elements score = V graph sizevars size_hint sympy_product size size stride zip ranges strides stride = dep name write_names ngimel said contiguous writes more important than reads score = CandidateTiling is_good_size tiled_groups score = CandidateTiling is_good_size tiled_groups score = V graph sizevars size_hint score - sympy_product itertools chain ranges reduction_ranges = tilings append CandidateTiling tiling=cls create_partial_tiling collapse_ranges ranges split collapse_ranges ranges split reduction_numel score=score name=dep name tilings pointwise_ranges reduction_ranges = node get_ranges len pointwise_ranges = len reduction_ranges = free_unbacked_symbols pointwise_ranges + reduction_ranges Tile either pointwise reduction dims pointwise_ranges reduction_ranges = node get_ranges partial_tilings = tile_ranges is_pointwise pointwise_ranges is_pointwise reduction_ranges node pointwise_or_reduction_read_writes is_pointwise Fill missing ranges full_tilings = CandidateTiling tiling=cls complete_partial_tiling tiling tiling numel reduction_numel score=tiling score name=tiling name tiling partial_tilings full_tilings classmethod create_tiling cls pw_tiling Sequence sympy Expr reduction_tiling Sequence sympy Expr - immutable_dict str sympy Expr Create tiling dict pointwise reduction splits pw_prefixes = z y x -len pw_tiling reduction_prefixes = r _ r _ len reduction_tiling immutable_dict zip pw_prefixes pw_tiling zip reduction_prefixes reduction_tiling classmethod create_partial_tiling cls tiling Sequence sympy Expr is_pointwise bool - immutable_dict str sympy Expr cls create_tiling tiling is_pointwise tiling is_pointwise classmethod complete_partial_tiling cls tiling dict str sympy Expr numel sympy Expr reduction_numel sympy Expr - immutable_dict str sympy Expr Given tiling only pointwise reduction dimensions adds missing one splits = list tiling values is_pointwise = x tiling total_numel = numel reduction_numel missing_tiling = total_numel sympy_product splits tiling_args = splits missing_tiling is_pointwise missing_tiling splits cls create_tiling tiling_args classmethod get_nd_tilings cls node_schedule pointwise_numel reduction_numel - list immutable_dict str sympy Expr Creates N-dimensional tiling candidates attempting simplify loads stores tiling kernel into higher dimensions Returns list tilings ranked dimensionality is_pointwise = reduction_numel == tilings = OrderedSet immutable_dict str sympy Expr node EnableReduction filter node_schedule isinstance node scheduler SchedulerNode continue If reduction schedule skip nodes which missing their reduction ranges node_ranges = node get_ranges is_pointwise len node_ranges == continue Use node ranges default tiling candidate ranges_to_tile = node_ranges is_pointwise node_tilings = ranges_to_tile Search indexing expressions more candidates If we see modular indexing try subdivide ranges into their implied block shape memory_deps = dep dep node read_writes reads_and_writes isinstance dep MemoryDep len dep ranges dep memory_deps Attempt partition variable ranges into pointwise reduction groups To achieve merge leading ranges until we reach pointwise numel all_var_ranges = dep ranges items pointwise_vars_numel = sympy S One sizevars = V graph sizevars pointwise_end_idx var numel enumerate all_var_ranges pointwise_vars_numel = numel sizevars statically_known_geq pointwise_vars_numel pointwise_numel break Reject split does match total pointwise numel sizevars statically_known_equals pointwise_vars_numel pointwise_numel continue Partition var ranges into pointwise reduction splits reduction_start_idx = pointwise_end_idx + var_ranges = all_var_ranges reduction_start_idx is_pointwise all_var_ranges reduction_start_idx Pattern match subexpression pertaining each index variable index_tiling = var numel var_ranges index = BlockPatternMatcher get_subexpr_involving_symbol dep index var Heuristic bound maximum dimensionality block num_dims = max index count FloorDiv + index count ModularIndexing len ranges_to_tile Attempt pattern match index expr Failed matches default full range match_result = BlockPatternMatcher match_mod_div_block_expr index var numel num_dims dims = match_result match_result None numel index_tiling extend dims Prune dimensions size index_tiling = dim dim index_tiling V graph sizevars statically_known_equals dim sympy S One len index_tiling node_tilings append index_tiling Flatten leading dimensions assigning labels each dim node_tiling node_tilings num_leading_dims = max len node_tiling - get_max_tiles first_trailing_dim = num_leading_dims + collapsed_leading_dim = sympy_product node_tiling first_trailing_dim collapsed_splits = collapsed_leading_dim + tuple node_tiling first_trailing_dim tilings add cls complete_partial_tiling cls create_partial_tiling collapsed_splits is_pointwise pointwise_numel reduction_numel Rank tilings number dimensions E g prefer D D Since stable sort ties broken schedule order ranked_tilings = sorted tilings key=len reverse=True ranked_tilings classmethod compute_tiling_strategy cls node_schedule list NodeScheduleEntry pointwise_numel sympy Expr reduction_numel sympy Expr coalesce_analysis CoalesceVarAnalysis - tuple dict str sympy Expr Optional dict str sympy Expr Generates tiling score each tile according each tile s coalesced memory accesses tiling_var Optional sympy Expr = None coalesce_analysis suggested_split coalesce_analysis suggested_split var all_iter_vars = coalesce_analysis norm_read_writes index_vars all_red_vars = coalesce_analysis norm_read_writes reduce_vars ranges = coalesce_analysis norm_read_writes var_ranges pw_ranges = ranges v v all_iter_vars red_ranges = ranges v v all_red_vars torch _check sympy_product pw_ranges == pointwise_numel lambda f pw_ranges pointwise_numel node_schedule torch _check sympy_product red_ranges == reduction_numel lambda f red_ranges reduction_numel node_schedule score pointwise reduction split scored_sub_split dict Any tuple list int list int = score_split list tuple tuple list int list int tuple list int list int = process_node_vars vars_to_use tuple sympy Expr = use_split_var bool = False is_pointwise bool = False - tuple list int list int Generate tiling tiling score given vars use splits ranges = pw_ranges is_pointwise red_ranges target_numel = pointwise_numel is_pointwise reduction_numel Some kernels have no reduction ranges reduction numel ranges target_numel target_numel key = repr vars_to_use use_split_var is_pointwise out = scored_sub_split get key out splitting_vars = all_iter_vars is_pointwise all_red_vars splits = split_scores = prod = prev_var_coalesced_score = iterate non-dense dense v v_range zip splitting_vars ranges v vars_to_use prod = v_range prev_var_coalesced_score = coalesce_analysis coalesced_by_var get v continue use_split_var v == tiling_var var_tiling = coalesce_analysis suggested_split assert var_tiling None tile = var_tiling tiling_factor remainder = FloorDiv v_range var_tiling tiling_factor splits append prod remainder split_scores append var_tiling score splits append tile split_scores append coalesce_analysis coalesced_by_var get v prod = prev_var_coalesced_score = continue prod = v_range splits append prod split_scores append coalesce_analysis coalesced_by_var get v prod = prod = is_pointwise len splits == splits append prod split_scores append prev_var_coalesced_score penalize splits leave small blocks where we can t fully utilize full memory transaction TODO incorporate exact bitwidth read write coalesced write x more important i range len splits s = V graph sizevars size_hint splits i fallback= s = min s split_scores i = int split_scores i s scored_sub_split key = splits split_scores splits split_scores add default tiling score_split append process_node_vars is_pointwise=True process_node_vars is_pointwise=False tiling_var score_split append process_node_vars tiling_var use_split_var=True is_pointwise=True process_node_vars is_pointwise=False TODO add tests reduction splits config triton tile_reductions TODO we should ignore tiny increases score extra splits overlapping_iter_vars = all_iter_vars coalesce_analysis coalesced_by_var keys v overlapping_iter_vars score_split append process_node_vars v is_pointwise=True process_node_vars is_pointwise=False get_max_tiles default= == reduction_numel == vars_to_use itertools combinations overlapping_iter_vars score_split append process_node_vars vars_to_use is_pointwise=True process_node_vars is_pointwise=False tilings list tuple CandidateTiling immutable_dict str sympy Expr = pw_split pw_score red_split red_score score_split candidate = CandidateTiling cls create_tiling pw_split red_split score=sum pw_score + sum red_score tiling_score = cls create_tiling pw_score red_score tilings append candidate tiling_score default_tiling = cls create_tiling pointwise_numel reduction_numel add slight penalty longer tilings dont increase score much poor sizes bad_size_additional_tiling_penalty = good_size_tiling_penalty = score_mod t score_factor = tile_size t tiling values CandidateTiling is_good_size tile_size score_factor = score_factor bad_size_additional_tiling_penalty score_factor = score_factor good_size_tiling_penalty -t score score_factor apply penalty longer tilings dont increase score much cand tiling_score sorted tilings key=score_mod cls tiling_is_compatible node_schedule pointwise_numel reduction_numel cand tiling we always include default reduction numel == dont include tiling_len = len cand tiling - reduction_numel == tiling_len get_max_tiles default= perf_hint_log info Found optimal tiling s tiles torch _inductor config triton max_tiles set s Consider increasing tiling_len torch _inductor config triton max_tiles continue cand tiling tiling_score surprisingly default tiling always read compatible ` tiling_is_compatible ` TODO - look into occurs dynamic shapes often cand tiling == default_tiling cand tiling tiling_score default_tiling None classmethod tiling_is_compatible cls node_schedule list NodeScheduleEntry numel sympy Expr reduction_numel sympy Expr tiling dict str sympy Expr assert isinstance tiling dict all SIMDKernel is_compatible tiling values node get_ranges reduction_numel=reduction_numel node node_schedule isinstance node scheduler SchedulerNode classmethod get_first_compatible_tiling cls node_schedule list NodeScheduleEntry numel sympy Expr reduction_numel sympy Expr ranked_tilings list dict str sympy Expr tiling ranked_tilings cls tiling_is_compatible node_schedule numel reduction_numel tiling tiling None classmethod select_tiling cls node_schedule numel reduction_numel=sympy S One coalesce_analysis Optional CoalesceVarAnalysis = None - dict str sympy Expr cls get_tiling_and_scores node_schedule numel reduction_numel coalesce_analysis classmethod get_tiling_and_scores cls node_schedule numel reduction_numel=sympy S One coalesce_analysis Optional CoalesceVarAnalysis = None - tuple dict str sympy Expr Optional dict str sympy Expr Heuristics decide how tile kernels Currently we tile based stride- dimensions Returns ` tile tile reduction_numel ` s t ` tile tile == numel ` If reduction only tile reduction dims is_pointwise = reduction_numel == Tiled reductions gated config flag default_tiling = cls create_tiling numel reduction_numel Force tiling compatible matmul dimensions when natively generating matmul without template calls node EnableReduction filter node_schedule isinstance node node ir ComputedBuffer node node get_reduction_type == dot config triton native_matmul A M K B K N force tiling y M x N r _ K node_ranges = node get_ranges range_y_x = node_ranges M N range_r = node_ranges K tiling = cls create_tiling range_y_x range_r tiling None TODO enable default torch _inductor config triton coalesce_tiling_analysis coalesce_analysis config triton prefer_nd_tiling cls compute_tiling_strategy node_schedule numel reduction_numel coalesce_analysis is_pointwise config triton tile_reductions get_max_tiles default= = Emit perf hint case we miss opportunity tile reduction perf_hint_log level = logging WARNING node EnableReduction filter node_schedule config triton tile_reductions len cls candidate_tilings node numel reduction_numel perf_hint_log info textwrap dedent Reduction over non-contiguous dims Consider setting config triton tile_reductions True break default_tiling None seen_names OrderedSet str = OrderedSet candidate_tiles Counter CandidateTiling = collections Counter node EnableReduction filter node_schedule candidate_tiling cls candidate_tilings node numel reduction_numel candidate_tiling name seen_names continue candidate_tiling name None seen_names add candidate_tiling name candidate_tiles candidate_tiling += candidate_tiling score ranked_tilings list dict str sympy Expr = candidate_tiling tiling candidate_tiling score candidate_tiles most_common get_max_tiles default= = is_pointwise Consider adding third dimension tiling only when multiple b otherwise you have lot stragglers which annoying generate code NB More than three max tiles enabled default convert_tiling_to_ d tiling dict str sympy Expr tiling dict str sympy Expr - Optional dict str sympy Expr = tiling x tiling get y b b = tiling x tiling get y free_unbacked_symbols b V graph sizevars size_hint - b == None V graph sizevars size_hint - b swap so bigger b b = b b assert V graph sizevars size_hint - b V graph sizevars statically_known_multiple_of b None new_tiling = z y FloorDiv b x b r _ tiling r _ new_tiling i range len ranked_tilings new_ d_tiling = convert_tiling_to_ d ranked_tilings ranked_tilings i new_ d_tiling None ranked_tilings = new_ d_tiling + ranked_tilings break only choice now len ranked_tilings perf_hint_log info possibly bad tiling s ranked_tilings Optionally prefer tiling into many dimensions possible pyrefly ignore unbound-name config triton prefer_nd_tiling ranked_tilings = cls get_nd_tilings node_schedule numel reduction_numel + ranked_tilings tiling = cls get_first_compatible_tiling node_schedule numel reduction_numel ranked_tilings tiling None default_tiling None flush pass ready_to_flush - bool False generate_kernel_code_from_nodes nodes benchmark_kernel=False hint_override Optional int = None any n is_template n nodes _ numel rnumel = max nodes key=lambda x int x is_reduction group node_schedule = generate_node_schedule nodes numel rnumel tiling = select_tiling node_schedule numel rnumel kernel = kernel_type tiling features=SIMDKernelFeatures node_schedule numel rnumel codegen_node_schedule_with_kernel node_schedule kernel config patch benchmark_kernel benchmark_kernel V set_kernel_handler kernel src_code = kernel codegen_kernel prologue template epilogue = nodes get_prologue_template_epilogue nodes config patch benchmark_kernel benchmark_kernel src_code = codegen_template template epilogue prologue only_gen_src_code=True hint_override=hint_override pyrefly ignore missing-attribute src_code = src_code replace str Placeholder KERNEL_NAME triton_ src_code define_kernel src_code node_schedule kernel raise NotImplementedError dataclasses dataclass frozen=True CandidateTiling tiling dict str sympy Expr score int higher better name Optional str = None staticmethod is_good_size s Somewhat arbitrary heuristic used boost scores some sizes s = V graph sizevars size_hint s s = s == CantSplit Exception pass