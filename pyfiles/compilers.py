mypy ignore-errors copy logging os pickle random collections abc Callable contextlib contextmanager functools partial typing Union sympy torch torch fx fx torch nn nn torch utils _pytree pytree torch SymInt torch _decomp get_decompositions torch fx experimental symbolic_shapes bind_symbols aot_autograd aot_function aot_module make_boxed_compiler compile_utils strip_overloads partitioners default_partition draw_graph min_cut_rematerialization_partition log = logging getLogger __name__ These canonicalization needed here decompositions ops we re trying canonicalize CompositeImplicitAutograd _canonicalize fx_g node fx_g graph find_nodes op= call_function target=torch ops aten _to_copy node target = torch ops aten fx_g recompile fx_g contextmanager _disable_jit_autocast old_jit_autocast_flag = torch _C _jit_set_autocast_mode False try yield finally torch _C _jit_set_autocast_mode old_jit_autocast_flag make_boxed_compiler ts_compile fx_g fx GraphModule inps - Callable Compiles attr ` fx_g ` Torchscript compiler warning This API experimental likely change Args fx_g fx GraphModule The input Fx graph module compiled Returns Torch scripted model _disable_jit_autocast strip_overloads fx_g node fx_g graph find_nodes op= call_function target=torch ops aten _to_copy len node args == len node kwargs == dtype node kwargs node target = torch ops aten node fx_g graph nodes new_kwargs = k v node kwargs items isinstance v torch device v = v type new_kwargs k = v node kwargs = new_kwargs fx_g graph lint fx_g recompile f = torch jit script fx_g torch _C _jit_pass_remove_mutation f graph f = torch jit freeze f eval f = torch jit optimize_for_inference f any isinstance t torch _subclasses FakeTensor t inps f inps f _draw_graph_compile fx_g _ name clear_meta=True print fx_g code draw_graph fx_g name clear_meta=clear_meta fx_g draw_graph_compile name make_boxed_compiler partial _draw_graph_compile name=name make_boxed_compiler nop fx_g fx GraphModule _ - Callable Returns attr ` fx_g ` Fx graph module This no-op compiler can used check accuracy warning This API experimental likely change fx_g DebugInterpreter fx Interpreter run args symbol_mapping = bind_symbols module args super run args run_node n subst_symint ni isinstance ni SymInt ni r = sympy expand ni node expr xreplace symbol_mapping assert r is_number r int r subst_symint_tuple nis tuple subst_symint ni ni nis check_significant_strides b subst_symint numel idx range ndim subst_symint stride idx = b stride idx subst_symint size idx False True check nv rv desc assert callable desc assert nv dtype == rv dtype f desc nv dtype = rv dtype assert subst_symint_tuple nv size == rv size f desc nv size aka subst_symint_tuple nv size = rv size same_strides = check_significant_strides nv rv assert same_strides f desc nv stride aka subst_symint_tuple nv stride = rv stride r = super run_node n val n meta n_vals _n_spec = pytree tree_flatten n meta val r_vals _r_spec = pytree tree_flatten r TODO There some sort problem where we record operator returned tuple list then later turns out real version operator returned list tuple Need figure out what s actually going here error itself harmless enough we only getitem out outputs assert n_spec == r_spec f n_spec = r_spec assert len n_vals == len r_vals f len n_vals = len r_vals i nv rv zip range len n_vals n_vals r_vals isinstance rv torch Tensor continue check nv rv lambda f output i where symbol_mapping r make_boxed_compiler debug_nop fx_g fx GraphModule _ - Callable Returns slow interpreter over FX graph module also checks various debugging properties e g tracing strides matched real strides DebugInterpreter fx_g run make_boxed_compiler simple_ts_compile fx_g _ strip_overloads fx_g f = torch jit script fx_g f = torch jit freeze f eval f nnc_jit f aot_function f simple_ts_compile aten = torch ops aten default_decompositions = aten detach aten gelu_backward aten leaky_relu_backward aten sigmoid_backward aten threshold_backward aten hardtanh_backward aten hardsigmoid_backward aten hardswish_backward aten tanh_backward aten silu_backward aten elu_backward aten cudnn_batch_norm aten cudnn_batch_norm_backward aten masked_fill Scalar aten masked_fill Tensor aten elu aten leaky_relu aten hardtanh aten hardswish aten hardsigmoid aten conj_physical aten is_same_size default_decompositions = get_decompositions default_decompositions make_boxed_compiler print_compile fx_g _ print fx_g code fx_g memory_efficient_fusion fn Union Callable nn Module kwargs Wrapper function over func ` aot_function ` func ` aot_module ` perform memory efficient fusion It uses func ` min_cut_rematerialization_partition ` partitioner perform efficient recomputation It uses NVFuser compile generated forward backward graphs warning This API experimental likely change Args fn Union Callable nn Module A Python function ` ` nn Module ` ` takes one more arguments Must one more Tensors kwargs Any other overrides you want make settings Returns Returns ` ` Callable ` ` ` ` nn Module ` ` retains eager behavior original attr ` fn ` whose forward backward graphs have gone through recomputation optimizations graphs have been compiled nvfuser config = fw_compiler ts_compile bw_compiler ts_compile partition_fn min_cut_rematerialization_partition decompositions default_decompositions config update kwargs isinstance fn torch nn Module aot_module fn config aot_function fn config debug_compile fx_g inps fx_g to_folder foo print f ############################################################## To minimize FX graph copy paste below run ############################################################## torch torch fx fx functorch compile minifier check_nvfuser_subprocess check_nvfuser_correctness_subprocess inps = i shape i dtype i inps inps = torch ones shape dtype=dtype device= cuda shape dtype inps foo FxModule mod = FxModule cuda torch jit fuser fuser check_nvfuser_subprocess can replaced check_nvfuser_correctness_subprocess minifier fx symbolic_trace mod inps check_nvfuser_subprocess foo FxModule FxModule cuda inps ts_compile fx_g inps graph_index = get_inputs input_data_path Return random input given inputs meta generated _save_fx_default inputs = open input_data_path rb f inputs_meta = pickle load f inputs = meta inputs_meta len meta == type = meta input = type random rand type shape _stride dtype device = meta dtype torch int torch int torch int torch bool torch int torch uint int float input = torch randint shape dtype=dtype device=device input = torch rand shape dtype=dtype device=device inputs append input inputs _save_fx_default current_name folder_name dump_example_input gm example_inputs The forward backward joint computation graph will stored folder_name current_name current_name _forward_ graph_index folder_name current_name current_name _backward_ graph_index folder_name current_name current_name _joint_ graph_index respectively The input shape graphs will stored input files These files can loaded pickle list format type shape stride dtype device In case type = int float just type For joint graph input nested list where two inner lists have same format If dump_example_input True example_inputs will stored pt file Since each function might produce multiple graphs graph_index used distinguish difference graphs functorch compile aot_module_simplified get_input_meta args input_meta = len args isinstance args tuple joint input input_meta += get_input_meta args input_meta += get_input_meta args input_meta arg args type arg int type arg float input_meta append type arg input_meta append type arg arg shape arg stride arg dtype arg device input_meta graph_saver_helper gm_to_save args type_name global graph_index len gm_to_save graph nodes == log log logging WARNING No nodes graph s _ s _ s current_name type_name graph_index gm = copy deepcopy gm_to_save gm graph set_codegen torch fx graph CodeGen remove codegen gm recompile input_meta = get_input_meta args os makedirs f folder_name current_name exist_ok=True gm to_folder f folder_name current_name current_name _ type_name _ graph_index pickle dump input_meta open f folder_name current_name current_name _ type_name _ graph_index current_name _ type_name _ graph_index input noqa B wb noqa E dump_example_input torch save args f folder_name current_name current_name _ type_name _ graph_index current_name _ type_name _ graph_index pt noqa B noqa E graph_saver_forward gm fw_args graph_saver_helper gm fw_args forward gm graph_saver_backward gm bw_args graph_saver_helper gm bw_args backward global graph_index graph_index += gm graph_saver_joint gm joint_args graph_saver_helper gm joint_args joint default_partition gm joint_args aot_module_simplified gm example_inputs fw_compiler=graph_saver_forward bw_compiler=graph_saver_backward partition_fn=graph_saver_joint decompositions=default_decompositions WARNING This isn t tested anywhere graph_dumper_aot current_name folder_name dump_example_input=False Dump forward backward joint computation graph Example Usage save_fx_func = graph_dumper_aot current_name folder_name dump_example_input = False optimize_ctx = torchdynamo optimize save_fx_func torch enable_grad optimize_ctx result = forward_and_backward_pass model example_inputs global graph_index graph_index = partial _save_fx_default current_name folder_name dump_example_input