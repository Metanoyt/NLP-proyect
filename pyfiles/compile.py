mypy allow-untyped-defs typing Any cast Optional Union collections abc Callable torch torch _dynamo torch _dynamo testing CompileCounterWithBackend torch utils benchmark Timer __all__ = bench_all benchmark_compile _warned_tensor_cores = False _default_float_ _precision = torch get_float _matmul_precision try tabulate tabulate HAS_TABULATE = True except ModuleNotFoundError HAS_TABULATE = False tabulate = None type ignore assignment print tabulate installed please pip install tabulate use utility HAS_TABULATE _enable_tensor_cores global _warned_tensor_cores torch cuda is_available torch backends cuda matmul allow_tf False torch cuda get_device_capability = torch set_float _matmul_precision high _warned_tensor_cores print Your GPU supports tensor cores print we will enable automatically setting ` torch set_float _matmul_precision high ` _warned_tensor_cores = True _disable_tensor_cores torch set_float _matmul_precision _default_float_ _precision bench_loop model Union torch nn Module Callable sample_input Union torch Tensor Any num_iters int = optimizer Optional torch optim Optimizer = None loss_fn Optional Callable = None Define statement setup benchmark optimizer loss_fn Training mode stmt = output = model sample_input loss = loss_fn output loss_fn output sum loss backward optimizer step optimizer zero_grad Inference mode stmt = model sample_input Create Timer object timer = Timer stmt=stmt globals= model model sample_input sample_input optimizer optimizer loss_fn loss_fn result = timer timeit number=num_iters Get average time per iteration milliseconds avg_time = result mean round avg_time benchmark_compile model Union torch nn Module Callable sample_input Union torch Tensor Any num_iters int = backend Optional str = None mode Optional str = default optimizer Optional torch optim Optimizer = None loss_fn Union torch nn Module Callable None = None Use utility benchmark torch compile backend try torch _dynamo reset compile_counter_with_backend = CompileCounterWithBackend backend opt_model = torch compile model backend=compile_counter_with_backend mode=mode Compilation only happens after first inference compilation_time = bench_loop opt_model sample_input optimizer loss_fn running_time = bench_loop opt_model sample_input num_iters optimizer loss_fn compile_counter_with_backend frame_count == raise RuntimeError No compilation occurred during benchmarking compile_counter_with_backend frame_count raise RuntimeError Recompilation occurred during benchmarking except Exception e print e print f Failed compile backend mode mode None None opt_model = model compilation_time = None running_time = bench_loop opt_model sample_input num_iters optimizer loss_fn compilation_time = round compilation_time compilation_time None running_time = round running_time running_time None compilation_time running_time bench_all model Union torch nn Module Callable sample_input Union torch Tensor Any num_iters int = optimizer Optional torch optim Optimizer = None loss_fn Union torch nn Module Callable None = None This simple utility can used benchmark torch compile In particular ensures your GPU setup use tensor cores supports its It also tries out all main backends prints table results so you can easily compare them all Many backendds have their own optional dependencies so please pip install them separately You will get one table inference another training If you d like leverage utility training make sure pass torch optim Optimizer The important warnings Your GPU supports tensor cores we will enable automatically setting ` torch set_float _matmul_precision high ` If compilation fails any reason including dependency being included then we will print Failed compile backend mode mode field_names = Train Inference Backend Mode Compilation Time Average Running Time table = eager_time = None torch _dynamo reset _ eager_time = benchmark_compile model sample_input num_iters None None optimizer table append Training optimizer Inference Eager - - f eager_time ms backend torch _dynamo list_backends backend == inductor mode_options = cast list Optional str list torch _inductor list_mode_options keys + None mode mode_options mode == default continue torch _dynamo reset try torch cuda is_available _enable_tensor_cores compilation_time running_time = benchmark_compile model sample_input num_iters backend mode optimizer loss_fn finally torch cuda is_available _disable_tensor_cores table append Training optimizer Inference pyrefly ignore redundant-condition backend backend - mode mode None - f compilation_time ms compilation_time - f running_time ms running_time - torch _dynamo reset compilation_time running_time = benchmark_compile model sample_input num_iters backend None optimizer loss_fn running_time None table append Training optimizer Inference backend - f compilation_time ms - f running_time ms pyrefly ignore not-callable tabulate table headers=field_names tablefmt= github