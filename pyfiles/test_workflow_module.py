Owner s oncall quantization ruff noqa F Torch Standard library copy io itertools math unittest numpy np torch torch nn nn torch testing _internal hypothesis_utils hu Testing utils hypothesis given settings strategies st torch ao quantization convert default_debug_qconfig default_histogram_observer default_observer default_per_channel_weight_observer FakeQuantize FixedQParamsObserver FusedMovingAvgObsFakeQuantize get_embedding_qat_module_mappings get_embedding_static_quant_module_mappings HistogramObserver MinMaxObserver MovingAverageMinMaxObserver MovingAveragePerChannelMinMaxObserver NoopObserver PerChannelMinMaxObserver PlaceholderObserver prepare prepare_qat QConfig RecordingObserver torch ao quantization quantize _get_observer_dict hu assert_deadline_disabled torch testing _internal common_cuda TEST_CUDA TEST_MULTIGPU torch testing _internal common_quantization AnnotatedSingleLayerLinearModel DeFusedEmbeddingBagLinear QuantizationTestCase SingleLayerLinearModel test_only_eval_fn torch testing _internal common_quantized _fake_quantize_per_channel_affine_grad_reference _fake_quantize_per_channel_affine_reference override_qengines override_quantized_engine supported_qengines to_tensor torch testing _internal common_utils skipIfTorchDynamo TestCase NP_RANDOM_SEED = tolerance = e- copy modified torch ao quantization observer py _INT_DTYPES = torch qint torch quint torch quint x torch qint torch int torch uint torch int torch int torch uint TestObserver QuantizationTestCase given qdtype=st sampled_from _INT_DTYPES qscheme=st sampled_from torch per_tensor_affine torch per_tensor_symmetric reduce_range=st booleans test_per_tensor_observers qdtype qscheme reduce_range reduce_range cannot true symmetric quantization uint qdtype == torch quint qscheme == torch per_tensor_symmetric qdtype == torch qint reduce_range = False qdtype == torch quint x ObserverList = MinMaxObserver dtype=qdtype qscheme=qscheme reduce_range=reduce_range MovingAverageMinMaxObserver averaging_constant= dtype=qdtype qscheme=qscheme reduce_range=reduce_range _get_ref_params reduce_range qscheme dtype input_scale min_val max_val assert dtype _INT_DTYPES f Not supported dtype dtype supported dtypes _INT_DTYPES eps = torch tensor tolerance dtype torch qint torch int reduce_range quant_min quant_max = - quant_min quant_max = - dtype torch quint torch uint reduce_range quant_min quant_max = quant_min quant_max = dtype == torch int quant_min quant_max = - - dtype == torch uint quant_min quant_max = - dtype torch qint torch int quant_min quant_max = - - min_val_neg = torch tensor max_val_pos = torch tensor input_scale max_val qdtype torch qint torch tensor max_val scale zero_point = qscheme == torch per_tensor_symmetric qscheme == torch per_channel_symmetric scale = torch max -min_val_neg max_val_pos float quant_max - quant_min scale = torch max scale eps dtype torch quint torch uint zero_point = dtype torch uint zero_point = scale = torch max max_val_pos - min_val_neg float quant_max - quant_min eps zero_point = quant_min - torch round min_val_neg scale torch int zero_point = torch clamp zero_point quant_min quant_max scale zero_point myobs ObserverList Calculate Qparams should warning observers no data qparams = myobs calculate_qparams input_scale = qdtype torch qint type myobs MinMaxObserver x = torch tensor input_scale y = torch tensor input_scale Moving average min max x y matches extreme values x y used minmax observer x = torch tensor input_scale y = torch tensor input_scale result = myobs x result = myobs y assertEqual result y assertEqual myobs min_val input_scale assertEqual myobs max_val input_scale qparams = myobs calculate_qparams ref_scale ref_zero_point = _get_ref_params reduce_range qscheme qdtype input_scale assertEqual qparams item ref_zero_point assertEqual qparams item ref_scale atol= e- rtol= state_dict = myobs state_dict b = io BytesIO torch save state_dict b weights_only True False b seek loaded_dict = torch load b weights_only=weights_only key state_dict assertEqual state_dict key loaded_dict key loaded_obs = MinMaxObserver dtype=qdtype qscheme=qscheme reduce_range=reduce_range loaded_obs load_state_dict loaded_dict loaded_qparams = loaded_obs calculate_qparams assertEqual myobs min_val loaded_obs min_val assertEqual myobs max_val loaded_obs max_val assertEqual myobs calculate_qparams loaded_obs calculate_qparams given qdtype=st sampled_from torch qint torch quint qscheme=st sampled_from torch per_channel_affine torch per_channel_symmetric torch per_channel_affine_float_qparams ch_axis=st sampled_from reduce_range=st booleans test_per_channel_observers qdtype qscheme ch_axis reduce_range reduce_range cannot true symmetric quantization uint qscheme == torch per_channel_affine_float_qparams reduce_range = False qdtype == torch quint qscheme == torch per_channel_symmetric reduce_range = False ObserverList = PerChannelMinMaxObserver reduce_range=reduce_range ch_axis=ch_axis dtype=qdtype qscheme=qscheme MovingAveragePerChannelMinMaxObserver averaging_constant= reduce_range=reduce_range ch_axis=ch_axis dtype=qdtype qscheme=qscheme myobs ObserverList Calculate qparams should work empty observers qparams = myobs calculate_qparams x = torch tensor - - type myobs MovingAveragePerChannelMinMaxObserver Scaling input tensor model change min max values across batches result = myobs x result = myobs x assertEqual result x result = myobs x assertEqual result x qparams = myobs calculate_qparams ref_min_vals = - - - - - ref_max_vals = per_channel_symmetric_ref_scales = per_channel_affine_ref_scales = per_channel_affine_qint _zp = - - - - - - - - per_channel_affine_float_qparams_ref_scales = per_channel_affine_quint _zp = assertEqual myobs min_val ref_min_vals ch_axis assertEqual myobs max_val ref_max_vals ch_axis qscheme == torch per_channel_symmetric ref_scales = per_channel_symmetric_ref_scales ch_axis ref_zero_points = qdtype torch qint qscheme == torch per_channel_affine_float_qparams ref_scales = per_channel_affine_float_qparams_ref_scales ch_axis ref_zero_points = - ref_min_vals ch_axis i ref_scales i i range len ref_scales ref_scales = per_channel_affine_ref_scales ch_axis ref_zero_points = per_channel_affine_qint _zp ch_axis qdtype torch qint per_channel_affine_quint _zp ch_axis reduce_range ref_scales = s s ref_scales ref_zero_points = math floor z z ref_zero_points assertEqual qparams torch tensor ref_scales dtype=qparams dtype rtol= e- atol= qscheme == torch per_channel_affine_float_qparams assertEqual qparams torch tensor ref_zero_points dtype=qparams dtype rtol= e- atol= assertEqual qparams torch tensor ref_zero_points dtype=qparams dtype Test serializability state_dict = myobs state_dict b = io BytesIO torch save state_dict b b seek loaded_dict = torch load b key state_dict assertEqual state_dict key loaded_dict key loaded_obs = PerChannelMinMaxObserver reduce_range=reduce_range ch_axis=ch_axis dtype=qdtype qscheme=qscheme loaded_obs load_state_dict loaded_dict loaded_qparams = loaded_obs calculate_qparams assertEqual myobs min_val loaded_obs min_val assertEqual myobs max_val loaded_obs max_val assertEqual myobs calculate_qparams loaded_obs calculate_qparams test_observer_scriptable obs_list = MinMaxObserver MovingAverageMinMaxObserver obs obs_list scripted = torch jit script obs x = torch rand obs x scripted x assertEqual obs calculate_qparams scripted calculate_qparams buf = io BytesIO torch jit save scripted buf buf seek loaded = torch jit load buf assertEqual obs calculate_qparams loaded calculate_qparams unittest skipIf TEST_MULTIGPU multi-GPU supported unittest skipIf TEST_CUDA CUDA unavailable override_qengines test_state_dict_respects_device_affinity Tests loading state dict loads buffers correct device device_cpu = torch device cpu device_cuda = torch device cuda test_cases = itertools product device_cpu device_cuda device_cpu device_cuda MinMaxObserver MovingAverageMinMaxObserver PerChannelMinMaxObserver MovingAveragePerChannelMinMaxObserver TODO enable separate PR HistogramObserver PlaceholderObserver RecordingObserver NoopObserver FakeQuantize device_source device_target obs_cls test_cases calibrated source model model = obs_cls model device_source model torch randn device=device_source target model model = obs_cls model device_target model load_state_dict model state_dict verify buffers stayed model s device model_devices = p device p model parameters &#124; \ p device p model buffers some observers do have any buffers so lessEqual instead Equal assertLessEqual len model_devices len model_devices == model_device = next iter model_devices assertEqual model_device device_target test_histogram_observer_consistent_buffer_shape Ensures buffer shapes do change uninitialized initialized states HistogramObserver obs = HistogramObserver min_shape_before = obs min_val shape max_shape_before = obs max_val shape _ range obs torch randn assertEqual min_shape_before obs min_val shape assertEqual max_shape_before obs max_val shape test_histogram_observer_ignore_infinity Ensures HistogramObserver doesn t record values infinity obs = HistogramObserver obs = HistogramObserver x = torch randn obs x torch inf obs x obs x obs x torch inf assertTrue obs min_val = -torch inf obs max_val = torch inf assertEqual obs histogram obs histogram test_histogram_observer_handle_close_to_infinity sign - obser = HistogramObserver with_args reduce_range=False mask = torch tensor - sign obser mask obser mask - sign scale zp = obser calculate_qparams input = torch randn ref_result = torch softmax input + mask dim= quant_mask = torch quantize_per_tensor mask scale zp torch quint dequant_mask = quant_mask dequantize result = torch softmax input + dequant_mask dim= assertEqual result ref_result test_histogram_observer_handle_OOM_due_to_close_min_max_value obser = HistogramObserver with_args reduce_range=False close min max value st forward pass observer tends cause OOM following pass This due allocation histogram tensor during _combine_histograms With sanity check size histogram tensor we expect histogram observer can still work resetting histogram x = torch tensor e- obser x x = torch tensor obser x test_histogram_observer_save_load_state_dict Smoke test saving loading state_dict obs = HistogramObserver obs torch randn obs = HistogramObserver obs load_state_dict obs state_dict assertEqual obs min_val shape torch Size assertEqual obs max_val shape torch Size test_save_load_state_dict_script Tests we can save load state_dict observers scripted quantized model obs_list = MinMaxObserver MovingAverageMinMaxObserver HistogramObserver obs obs_list model = SingleLayerLinearModel eval qconfig = QConfig activation=default_observer weight=obs qconfig_dict = qconfig scripted = torch jit script model scripted = torch ao quantization prepare_jit scripted qconfig_dict x = torch rand scripted x obs_dict = torch ao quantization get_observer_state_dict scripted Load stats scripted_ = torch jit script model scripted_ = torch ao quantization prepare_jit scripted_ qconfig_dict torch ao quantization load_observer_state_dict scripted_ obs_dict Verify state_dict matches exactly original one assertEqual scripted state_dict scripted_ state_dict unittest skipIf TEST_MULTIGPU multi-GPU supported unittest skipIf TEST_CUDA CUDA unavailable test_observer_qparams_respects_device_affinity Ensure scale zero_point returned observer same device input tensor observerList = MinMaxObserver MovingAverageMinMaxObserver PerChannelMinMaxObserver MovingAveragePerChannelMinMaxObserver obs observerList device = torch device cuda x = torch randn device=device obs device result = obs x scale zero_point = obs calculate_qparams assertEqual x device scale device assertEqual x device zero_point device test_zero_numel obs_list = MinMaxObserver MovingAverageMinMaxObserver PerChannelMinMaxObserver MovingAveragePerChannelMinMaxObserver HistogramObserver FakeQuantize FixedQParamsObserver obs_cls obs_list obs_cls FixedQParamsObserver obs = obs_cls obs = obs_cls x = torch tensor verify no crash x = obs x test_dynamic_quant_observer obs = MovingAverageMinMaxObserver averaging_constant= is_dynamic=True x = torch randn obs x params = obs calculate_qparams _ range obs torch randn assertNotEqual params obs calculate_qparams obs x assertEqual params obs calculate_qparams test_dynamic_quant_observer_matching_choose_qparams obs = MovingAverageMinMaxObserver averaging_constant= is_dynamic=True x torch randn torch rand torch randn obs x params = obs calculate_qparams scale zero_point = torch _choose_qparams_per_tensor x assertEqual scale params assertEqual zero_point params test_per_channel_observers_load_state_dict observer_list = PerChannelMinMaxObserver MovingAveragePerChannelMinMaxObserver obs_cls observer_list obs = obs_cls obs torch randn new_obs = obs_cls make sure state_dict can loaded new_obs load_state_dict obs state_dict assertTrue torch equal obs min_val new_obs min_val assertTrue torch equal obs max_val new_obs max_val HistogramObserver works like does master _ReferenceHistogramObserver HistogramObserver __init__ args kwargs super __init__ args kwargs torch jit ignore _non_linear_param_search r Non-linear parameter search An approximation L error minimization selecting min max By selecting new min max we filter out outliers input distribution This follows implementation NormMinimization NonlinearQuantizationParamsSearch caffe quantization server norm_minimization cc _get_norm delta_begin delta_end density norm_type r Compute norm values uniformaly distributed between delta_begin delta_end norm = density integral_ begin end x^ = density end^ - begin^ assert norm_type == L Only L norms currently supported norm = norm_type == L norm = delta_end delta_end delta_end - delta_begin delta_begin delta_begin density norm _compute_quantization_error next_start_bin next_end_bin norm_type r Compute quantization error we use start_bin end_bin min max do quantization bin_width = max_val item - min_val item bins norm = dst_bin_width = bin_width next_end_bin - next_start_bin + dst_nbins dst_bin_width == src_bin range bins distances beginning first dst_bin beginning end src_bin src_bin_begin = src_bin - next_start_bin bin_width src_bin_end = src_bin_begin + bin_width which dst_bins beginning end src_bin belong dst_bin_of_begin = min dst_nbins - max math floor src_bin_begin dst_bin_width dst_bin_of_end = min dst_nbins - max math floor src_bin_end dst_bin_width dst_bin_of_begin_center = dst_bin_of_begin dst_bin_width + dst_bin_width density = histogram src_bin bin_width dst_bin_of_begin == dst_bin_of_end src_bin entirely within dst_bin delta_begin = src_bin_begin - dst_bin_of_begin_center delta_end = src_bin_end - dst_bin_of_begin_center norm = norm + _get_norm delta_begin delta_end density norm_type delta_begin = src_bin_begin - dst_bin_of_begin_center delta_end = dst_bin_width norm = norm + _get_norm delta_begin delta_end density norm_type norm = norm + dst_bin_of_end - dst_bin_of_begin - _get_norm -dst_bin_width dst_bin_width density norm_type dst_bin_of_end_center = dst_bin_of_end dst_bin_width + dst_bin_width delta_begin = -dst_bin_width delta_end = src_bin_end - dst_bin_of_end_center norm = norm + _get_norm delta_begin delta_end density norm_type norm assert histogram size == bins bins mistmatch bin_width = max_val - min_val bins cumulative sum total = torch sum histogram item cSum = torch cumsum histogram dim= stepsize = e- granularity alpha = lower bound beta = upper bound start_bin = end_bin = bins - norm_min = float inf while alpha beta Find next step next_alpha = alpha + stepsize next_beta = beta - stepsize find left right bins between quantile bounds l = start_bin r = end_bin while l end_bin cSum l next_alpha total l = l + while r start_bin cSum r next_beta total r = r - decide next move next_start_bin = start_bin next_end_bin = end_bin l - start_bin end_bin - r move start bin next_start_bin = l alpha = next_alpha move end bin next_end_bin = r beta = next_beta next_start_bin == start_bin next_end_bin == end_bin continue calculate quantization error using next_start_bin next_end_bin norm = _compute_quantization_error next_start_bin next_end_bin L norm norm_min break norm_min = norm start_bin = next_start_bin end_bin = next_end_bin new_min = min_val + bin_width start_bin new_max = min_val + bin_width end_bin + new_min new_max TestRecordHistogramObserver QuantizationTestCase TODO move quantize py test_record_observer qengine supported_qengines override_quantized_engine qengine model = AnnotatedSingleLayerLinearModel model qconfig = default_debug_qconfig model = prepare model run evaluation dump all tensors test_only_eval_fn model calib_data test_only_eval_fn model calib_data observer_dict = _get_observer_dict model observer_dict assertTrue fc module activation_post_process observer_dict keys observer recorded dict assertEqual len observer_dict fc module activation_post_process get_tensor_value len calib_data assertEqual observer_dict fc module activation_post_process get_tensor_value model calib_data given qdtype=st sampled_from torch qint torch quint test_observer_scriptable qdtype obs = RecordingObserver dtype=qdtype scripted = torch jit script obs x = torch rand obs x scripted x assertTrue torch equal obs get_tensor_value scripted get_tensor_value buf = io BytesIO torch jit save scripted buf buf seek loaded = torch jit load buf assertTrue torch equal obs get_tensor_value loaded get_tensor_value TestHistogramObserver QuantizationTestCase given qdtype=st sampled_from torch qint torch quint qscheme=st sampled_from torch per_tensor_affine torch per_tensor_symmetric test_observer_scriptable qdtype qscheme ob_list = HistogramObserver dtype=qdtype qscheme=qscheme default_histogram_observer obs ob_list scripted = torch jit script obs x = torch rand obs x scripted x assertTrue torch equal obs histogram scripted histogram buf = io BytesIO torch jit save scripted buf buf seek loaded = torch jit load buf assertTrue torch equal obs histogram scripted histogram given qdtype=st sampled_from torch qint torch quint qscheme=st sampled_from torch per_tensor_affine torch per_tensor_symmetric reduce_range=st booleans settings max_examples= test_histogram_observer qdtype qscheme reduce_range myobs = HistogramObserver bins= dtype=qdtype qscheme=qscheme reduce_range=reduce_range Calculate qparams should work empty observers qparams = myobs calculate_qparams x = torch tensor requires_grad=True y = torch tensor out_x = myobs x assertTrue out_x requires_grad myobs y assertEqual myobs min_val assertEqual myobs max_val assertEqual myobs histogram qparams = myobs calculate_qparams reduce_range qscheme == torch per_tensor_symmetric ref_scale = ref_zero_point = qdtype torch qint ref_scale = ref_zero_point = - qdtype torch qint qscheme == torch per_tensor_symmetric ref_scale = ref_zero_point = qdtype torch qint ref_scale = ref_zero_point = - qdtype torch qint assertEqual qparams item ref_zero_point assertEqual qparams item ref_scale atol= e- rtol= Test serializability state_dict = myobs state_dict b = io BytesIO torch save state_dict b b seek loaded_dict = torch load b key state_dict assertEqual state_dict key loaded_dict key loaded_obs = HistogramObserver bins= dtype=qdtype qscheme=qscheme reduce_range=reduce_range loaded_obs load_state_dict loaded_dict loaded_qparams = loaded_obs calculate_qparams assertEqual myobs min_val loaded_obs min_val assertEqual myobs max_val loaded_obs max_val assertEqual myobs histogram loaded_obs histogram assertEqual myobs bins loaded_obs bins assertEqual myobs calculate_qparams loaded_obs calculate_qparams test_histogram_observer_one_sided myobs = HistogramObserver bins= dtype=torch quint qscheme=torch per_tensor_affine reduce_range=True x = torch tensor y = torch tensor myobs x myobs y assertEqual myobs min_val qparams = myobs calculate_qparams assertEqual qparams item test_histogram_observer_same_inputs myobs = HistogramObserver bins= dtype=torch qint qscheme=torch per_tensor_symmetric reduce_range=False w = torch ones requires_grad=True x = torch zeros requires_grad=True y = torch tensor requires_grad=True z = torch tensor myobs w myobs x myobs x myobs y myobs z qparams = myobs calculate_qparams assertEqual myobs min_val assertEqual myobs max_val assertEqual myobs histogram skipIfTorchDynamo too slow given N=st sampled_from bins=st sampled_from dtype=st sampled_from torch qint torch quint qscheme=st sampled_from torch per_tensor_affine torch per_tensor_symmetric reduce_range=st booleans test_histogram_observer_against_reference N bins dtype qscheme reduce_range ref_obs = _ReferenceHistogramObserver bins=bins dtype=dtype qscheme=qscheme reduce_range=reduce_range my_obs = HistogramObserver bins=bins dtype=dtype qscheme=qscheme reduce_range=reduce_range _ range X = torch randn N my_obs X ref_obs X assertEqual my_obs histogram ref_obs histogram assertEqual my_obs min_val ref_obs min_val assertEqual my_obs max_val ref_obs max_val ref_qparams = ref_obs calculate_qparams my_qparams = my_obs calculate_qparams i range bins j range i + bins ref_qe = ref_obs _compute_quantization_error i j qe = my_obs _compute_quantization_error i j assertEqual ref_qe qe assertEqual ref_qparams my_qparams test_histogram_observer_extreme_inputs Ensures HistogramObserver able work correctly rare case extreme samll max values obs = HistogramObserver test_input = torch tensor e- e- Make sure runs two passes required based behavior forward func The first pass initializes min_val max_val second pass calls _adjust_min_max obs test_input obs test_input test_histogram_observer_correct_numel i range obs = HistogramObserver obs torch randn i i assertEqual obs histogram sum item i test_histogram_observer_single_inputs Make sure we pass single valued tensors observer code runs observer = HistogramObserver bins= = torch FloatTensor b = torch FloatTensor c = torch FloatTensor d = torch FloatTensor observer observer b observer c observer d assertEqual observer min_val assertEqual observer max_val assertEqual torch sum observer histogram test_histogram_observer_update_within_range_succeeds test update within existing range actually updates myobs = HistogramObserver bins= x = torch tensor y = torch tensor myobs x myobs y assertEqual myobs min_val assertEqual myobs max_val assertEqual myobs histogram TestFakeQuantize TestCase given device=st sampled_from cpu cuda torch cuda is_available cpu X=hu per_channel_tensor shapes=hu array_shapes qparams=hu qparams dtypes=torch qint test_fq_module_per_channel device X np random seed NP_RANDOM_SEED X scale zero_point axis torch_type = X quant_min = torch iinfo torch_type min quant_max = torch iinfo torch_type max X = to_tensor X device X requires_grad_ fq_module = FakeQuantize default_per_channel_weight_observer quant_min quant_max ch_axis=axis device Y_prime = fq_module X assert fq_module scale None assert fq_module zero_point None Y = _fake_quantize_per_channel_affine_reference X fq_module scale fq_module zero_point axis quant_min quant_max np testing assert_allclose Y cpu detach numpy Y_prime cpu detach numpy rtol=tolerance atol=tolerance Test backward dout = torch rand_like X dtype=torch float device=device Y_prime backward dout dX = _fake_quantize_per_channel_affine_grad_reference dout X fq_module scale fq_module zero_point axis quant_min quant_max np testing assert_allclose dX cpu numpy X grad cpu detach numpy rtol=tolerance atol=tolerance test_fq_serializable_per_channel observer = default_per_channel_weight_observer quant_min = - quant_max = fq_module = FakeQuantize observer quant_min quant_max X = torch tensor - - - dtype=torch float y_ref = fq_module X state_dict = fq_module state_dict assertEqual state_dict scale assertEqual state_dict zero_point b = io BytesIO torch save state_dict b b seek loaded_dict = torch load b key state_dict assertEqual state_dict key loaded_dict key test_quant_min_max_override observer = default_per_channel_weight_observer test no override fq_module = FakeQuantize observer assertEqual fq_module activation_post_process quant_min - assertEqual fq_module activation_post_process quant_max test quant_min quant_max override fq_module = FakeQuantize observer quant_min= quant_max= assertEqual fq_module activation_post_process quant_min assertEqual fq_module activation_post_process quant_max given device=st sampled_from cpu cuda torch cuda is_available cpu sampled_dtype=st sampled_from bf fp fp test_fused_moving_avg_obs_fake_quant device sampled_dtype try device == cpu sampled_dtype = fp dtype = bf torch bfloat fp torch half fp torch float sampled_dtype torch set_default_dtype dtype torch device device fake_quantize = FusedMovingAvgObsFakeQuantize fake_quantize forward torch rand finally torch set_default_dtype torch float _get_buffer_ids module Object addresses stay constant only all modifications in-place id v k v module _buffers items TestFusedModuleScriptable QuantizationTestCase test_fx_qat_convbn_fused_jit_scriptable Tests jit scriptability works fused ConvBN qengine fbgemm qnnpack override_quantized_engine qengine create conv-bn Model nn Module __init__ - None super __init__ conv = nn Conv d padding= bn = nn BatchNorm d forward x x = conv x x = bn x x model = Model model = torch fx symbolic_trace model fuse fused_model = torch ao quantization fuse_modules_qat model conv bn convert QAT qconfig_mapping = torch ao quantization get_default_qat_qconfig_mapping qengine quantizable_model = torch ao quantization quantize_fx prepare_qat_fx fused_model qconfig_mapping example_inputs=None assert isinstance quantizable_model conv torch ao nn intrinsic qat ConvBn d jit script scripted_model = torch jit script quantizable_model assertTrue isinstance scripted_model torch jit ScriptModule Expected prepared model scriptable test_qat_convbn_fused_jit_scriptable Tests jit scriptability works fused ConvBN qengine fbgemm qnnpack override_quantized_engine qengine create conv-bn Model nn Module __init__ - None super __init__ conv = nn Conv d padding= bn = nn BatchNorm d forward x x = conv x x = bn x x model = Model fuse fused_model = torch ao quantization fuse_modules_qat model conv bn convert QAT fused_model qconfig = torch ao quantization get_default_qconfig qengine torch ao quantization prepare_qat fused_model inplace=True assert isinstance fused_model conv torch ao nn intrinsic qat ConvBn d Test jit script fails Prepared eager module fails due observer hooks being scriptable assertRaises RuntimeError torch jit script fused_model TestDistributed QuantizationTestCase test_observers_preserve_buffers Tests observers only modify buffers place Note important because nn DataParallel depends assumption work correctly However DataParallel does expose IDs replicas so we test without DataParallel order easily access object IDs observer_types = torch ao quantization MinMaxObserver with_args dtype=torch qint torch ao quantization MovingAverageMinMaxObserver with_args dtype=torch qint torch ao quantization PerChannelMinMaxObserver with_args dtype=torch qint torch ao quantization MovingAveragePerChannelMinMaxObserver with_args dtype=torch qint torch ao quantization HistogramObserver with_args dtype=torch qint torch ao quantization RecordingObserver with_args dtype=torch qint torch ao quantization PlaceholderObserver with_args dtype=torch float observer_type observer_types observer = observer_type buffer_ids_before = _get_buffer_ids observer _ range inputs = torch rand observer inputs buffer_ids_after = _get_buffer_ids observer assertEqual buffer_ids_before buffer_ids_after msg=f str observer Buffers must modified place test_fake_quant_preserves_buffers Tests fake quant only modifies buffers place Note important because nn DataParallel depends assumption work correctly However DataParallel does expose IDs replicas so we test without DataParallel order easily access object IDs model = torch ao quantization FakeQuantize buffer_ids_before = _get_buffer_ids model _ range inputs = torch rand model inputs model apply torch ao quantization enable_fake_quant model apply torch ao quantization disable_fake_quant model apply torch ao quantization enable_observer model apply torch ao quantization disable_observer buffer_ids_after = _get_buffer_ids model assertEqual buffer_ids_before buffer_ids_after msg= FakeQuant Buffers must modified place unittest skipIf TEST_MULTIGPU multi-GPU supported unittest skipIf TEST_CUDA CUDA unavailable test_qat_data_parallel Tests doing QAT nn DataParallel does crash fbgemm torch backends quantized supported_engines override_quantized_engine fbgemm device = torch device cuda model = nn Sequential torch ao quantization QuantStub nn Conv d bias=False nn BatchNorm d nn ReLU nn Conv d stride= padding= bias=False nn BatchNorm d nn AvgPool d nn Sigmoid torch ao quantization DeQuantStub torch ao quantization fuse_modules_qat model inplace=True model qconfig = torch ao quantization get_default_qat_qconfig fbgemm torch ao quantization prepare_qat model inplace=True model = nn DataParallel model device_ids= model device model train epoch range inputs = torch rand device model inputs epoch = model apply torch ao quantization disable_observer epoch = model apply torch ao nn intrinsic qat freeze_bn_stats quant_model = copy deepcopy model module quant_model = torch ao quantization convert quant_model eval cpu inplace=False torch no_grad out = quant_model torch rand test_qat_convbn_fused_syncbn_replacement Tests SyncBatchNorm replacement works fused ConvBN fbgemm torch backends quantized supported_engines override_quantized_engine fbgemm create conv-bn Model nn Module __init__ - None super __init__ conv = nn Conv d padding= bn = nn BatchNorm d forward x x = conv x x = bn x x model = Model fuse fused_model = torch ao quantization fuse_modules_qat model conv bn convert QAT fused_model qconfig = torch ao quantization get_default_qconfig fbgemm torch ao quantization prepare_qat fused_model inplace=True replace DDP fused_model = nn SyncBatchNorm convert_sync_batchnorm fused_model assertTrue isinstance fused_model conv bn nn SyncBatchNorm Expected BN converted SyncBN test_syncbn_preserves_qconfig Makes sure BatchNorm fused qconfig exists convering module SyncBatchNorm preserves qconfig m = nn Sequential nn Conv d nn BatchNorm d m qconfig = torch ao quantization default_qconfig m = torch nn SyncBatchNorm convert_sync_batchnorm m assertTrue hasattr m qconfig missing qconfig after SyncBatchNorm conversion unittest skipIf TEST_MULTIGPU multi-GPU supported unittest skipIf TEST_CUDA CUDA unavailable override_qengines test_device_affinity Tests converting model QAT respects device affinity Model nn Module __init__ - None super __init__ conv = nn Conv d bn = nn BatchNorm d relu = nn ReLU forward x x = conv x x = bn x x = relu x x model = Model model qconfig = torch ao quantization get_default_qat_qconfig torch backends quantized engine device = torch device cuda model device torch ao quantization prepare_qat model inplace=True model_devices = p device p model parameters &#124; \ p device p model buffers assertEqual len model_devices model_device = next iter model_devices assertEqual model_device device ensure running input CUDA works without any needed changes input = torch randn device=device model input TestFusedObsFakeQuantModule TestCase given device=st sampled_from cpu cuda torch cuda is_available cpu settings deadline=None test_fused_obs_fq_module device Set up parameters x = torch randn device=device running_min_op = torch tensor float inf device=device running_max_op = torch tensor float -inf device=device avg_const = scale = torch tensor device=device zero_point = torch tensor dtype=torch int device=device Run forward Module mod = FusedMovingAvgObsFakeQuantize torch ao quantization enable_fake_quant mod torch ao quantization enable_observer mod mod device out = mod x Run operator directly pt_op = torch fused_moving_avg_obs_fake_quant out_ref = pt_op x mod observer_enabled mod fake_quant_enabled running_min_op running_max_op scale zero_point avg_const False Compare params reference torch testing assert_close out out_ref torch testing assert_close running_min_op mod activation_post_process min_val torch testing assert_close running_max_op mod activation_post_process max_val given device=st sampled_from cpu cuda torch cuda is_available cpu settings deadline=None test_fused_obs_fq_moving_avg_module device Set up parameters running_min_op = torch tensor float inf device=device running_max_op = torch tensor float -inf device=device avg_const = scale = torch tensor device=device zero_point = torch tensor dtype=torch int device=device mod = FusedMovingAvgObsFakeQuantize averaging_constant= mod device mod observer_enabled = mod fake_quant_enabled = i range x = torch randn device=device i mod observer_enabled = i mod fake_quant_enabled = Run forward Module out = mod x Run operator directly pt_op = torch fused_moving_avg_obs_fake_quant out_ref = pt_op x mod observer_enabled mod fake_quant_enabled running_min_op running_max_op scale zero_point avg_const False Compare params reference torch testing assert_close out out_ref torch testing assert_close running_min_op mod activation_post_process min_val torch testing assert_close running_max_op mod activation_post_process max_val given device=st sampled_from cpu cuda torch cuda is_available cpu settings deadline=None test_compare_fused_obs_fq_oss_module device mod = FusedMovingAvgObsFakeQuantize torch ao quantization enable_fake_quant mod torch ao quantization enable_observer mod mod device mod_ref = FakeQuantize torch ao quantization enable_fake_quant mod_ref torch ao quantization enable_observer mod_ref mod_ref device _ range x = torch randn device=device out = mod x out_ref = mod_ref x torch testing assert_close out out_ref torch testing assert_close mod_ref activation_post_process min_val mod activation_post_process min_val torch testing assert_close mod_ref activation_post_process max_val mod activation_post_process max_val test_fused_mod_per_channel devices = cpu cuda torch cuda is_available cpu m = n = device devices running_min_op = torch empty m device=device fill_ float inf running_max_op = torch empty m device=device fill_ float -inf avg_const = scale = torch empty m device=device fill_ zero_point = torch empty m dtype=torch int device=device fill_ obs = FusedMovingAvgObsFakeQuantize with_args averaging_constant=avg_const observer=MovingAveragePerChannelMinMaxObserver mod = obs mod = torch jit script mod mod device i range x = torch randn m n device=device i mod observer_enabled = i mod fake_quant_enabled = Run forward Module out = mod x Run operator directly pt_op = torch fused_moving_avg_obs_fake_quant out_ref = pt_op x mod observer_enabled mod fake_quant_enabled running_min_op running_max_op scale zero_point avg_const True False Compare params reference torch testing assert_close out out_ref mod observer_enabled torch testing assert_close running_min_op mod activation_post_process min_val torch testing assert_close running_max_op mod activation_post_process max_val mod fake_quant_enabled torch testing assert_close scale mod scale torch testing assert_close zero_point mod zero_point torch testing assert_close mod state_dict activation_post_process min_val running_min_op torch testing assert_close mod state_dict activation_post_process max_val running_max_op test_fused_mod_reduce_range obs = FusedMovingAvgObsFakeQuantize quant_min= quant_max= dtype=torch quint reduce_range=True assertEqual obs activation_post_process quant_min assertEqual obs activation_post_process quant_max test_embedding_bag_qat_config Model nn Module __init__ - None super __init__ emb = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True scale_grad_by_freq=False mode= sum emb = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True scale_grad_by_freq=False mode= sum forward indices torch cat emb indices emb indices qconfigs = torch ao quantization default_embedding_qat_qconfig torch ao quantization default_embedding_qat_qconfig_ bit qconfig qconfigs model = Model train indices = torch randint model qconfig = qconfig quant_model = prepare_qat model mapping=get_embedding_qat_module_mappings count_fake_quant = name mod quant_model named_modules name endswith weight_fake_quant count_fake_quant += assertEqual type mod FakeQuantize assertEqual count_fake_quant quant_model indices Ensure EmbeddingBags have float zero_point values assertEqual quant_model emb weight_fake_quant zero_point dtype torch float assertEqual quant_model emb weight_fake_quant zero_point dtype torch float inference_gm = convert quant_model eval cpu mapping=get_embedding_static_quant_module_mappings Ensure EmbeddingBags now quantized appropriate bitwidth assertEqual type inference_gm emb torch ao nn quantized EmbeddingBag assertEqual type inference_gm emb torch ao nn quantized EmbeddingBag assertEqual inference_gm emb dtype qconfig weight dtype assertEqual inference_gm emb dtype qconfig weight dtype test_embedding_qat_config qengine supported_qengines override_quantized_engine qengine model = DeFusedEmbeddingBagLinear indices = torch randint quant_model = prepare_qat model mapping=get_embedding_qat_module_mappings count_fake_quant = count_activation_postproc = name _mod quant_model named_modules name endswith weight_fake_quant count_fake_quant += name count activation_post_process == weight_fake_quant name count_activation_postproc += One embeddings one linear layer assertEqual count_fake_quant One embeddings NoOp One quantize one linear layer assertEqual count_activation_postproc assertEqual type quant_model emb weight_fake_quant FakeQuantize assertEqual quant_model emb weight_fake_quant zero_point dtype torch float assertEqual type quant_model emb activation_post_process NoopObserver assertEqual type quant_model linear weight_fake_quant FusedMovingAvgObsFakeQuantize assertEqual type quant_model linear activation_post_process FusedMovingAvgObsFakeQuantize quant_model indices inference_gm = convert quant_model mapping=get_embedding_static_quant_module_mappings Ensure Embedding now quantized assertEqual type inference_gm emb torch ao nn quantized Embedding Ensure Linear now quantized assertEqual type inference_gm linear torch ao nn quantized Linear test_default_fused_qat_config Model nn Module __init__ - None super __init__ linear = nn Linear relu = nn ReLU forward x x = linear x x = relu x x qengine fbgemm qnnpack model = Model model linear weight = torch nn Parameter torch randn sample_input = torch randn model qconfig = torch ao quantization get_default_qat_qconfig qengine version= ref_model = torch ao quantization QuantWrapper model ref_model = torch ao quantization prepare_qat ref_model ref_model sample_input count_fake_quant = name mod ref_model named_modules name endswith weight_fake_quant count_fake_quant += assertEqual type mod FusedMovingAvgObsFakeQuantize name count activation_post_process == weight_fake_quant name count_fake_quant += assertEqual type mod FusedMovingAvgObsFakeQuantize assertEqual count_fake_quant qengine == fbgemm lower_bnd = upper_bnd = obs match = MovingAveragePerChannelMinMaxObserver lower_bnd = upper_bnd = obs match = MovingAverageMinMaxObserver assertEqual ref_model quant activation_post_process activation_post_process quant_min lower_bnd assertEqual ref_model quant activation_post_process activation_post_process quant_max upper_bnd assertEqual type ref_model module linear weight_fake_quant activation_post_process obs match __name__ == __main__ raise RuntimeError This test file meant run directly use \n\n \tpython test test_quantization py TESTNAME\n\n instead