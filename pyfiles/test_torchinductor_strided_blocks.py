Owner s module inductor ruff noqa F contextlib dataclasses importlib math unittest collections abc Callable typing Any Optional Union torch torch utils _pytree pytree torch _dynamo debug_utils InputReader torch _inductor config torch _inductor choices InductorChoices torch _inductor codegen triton FixedTritonConfig torch _inductor runtime hints TRITON_MAX_BLOCK torch _inductor runtime runtime_utils get_max_y_grid is_power_of_ torch _inductor test_case TestCase InductorTestCase torch _inductor utils run_and_get_code torch _inductor virtualized V torch testing _internal common_utils decorateIf instantiate_parametrized_tests parametrize skipIfXpu subtest torch testing _internal inductor_utils GPU_TYPE HAS_CUDA_AND_TRITON HAS_GPU requires_gpu skip_windows_ci TRITON_HAS_CPU try test_torchinductor except ImportError test_torchinductor skip_windows_ci __name__ __file__ importlib import_module filelock max_block int = TRITON_MAX_BLOCK X Config shortcuts tiled_reduction_config = triton prefer_nd_tiling True triton tile_reductions True These xfails due current restrictions TMA descriptor API see Note TMA API Restrictions In some cases TMA descriptors cannot generated so tests assert expected number descriptors = equivalent block ptrs will fail xfail_if_use_tensor_descriptor fn fn _expected_failure_use_tensor_descriptor = True fn TMA_XFAIL = test_torchinductor TestFailure GPU_TYPE is_skip=False TMA_TEST_XFAIL = dict fromkeys test_pointwise_prefer_nd_tiling_False_full_size _view_size _stride _offset _require_block_ptr_True test_pointwise_prefer_nd_tiling_False_full_size _view_size _stride _offset _require_block_ptr_True test_pointwise_prefer_nd_tiling_False_full_size _view_size _stride _offset _require_block_ptr_True test_pointwise_prefer_nd_tiling_True_full_size _view_size _stride _offset _require_block_ptr_True test_pointwise_prefer_nd_tiling_True_full_size _view_size _stride _offset _require_block_ptr_True test_pointwise_prefer_nd_tiling_True_full_size _view_size _stride _offset _require_block_ptr_True test_reduction_prefer_nd_tiling_False_view_size _num_block_pointers_ _num_triton_kernels_ test_reduction_prefer_nd_tiling_False_view_size _num_block_pointers_ _num_triton_kernels_ test_reduction_prefer_nd_tiling_True_view_size _num_block_pointers_ _num_triton_kernels_ test_reduction_prefer_nd_tiling_True_view_size _num_block_pointers_ _num_triton_kernels_ test_ d_reduction_odd_shapes_view_size _num_block_pointers_ _num_triton_kernels_ _reduction_op test_broadcast_prefer_nd_tiling_False_x_size _y_size test_broadcast_prefer_nd_tiling_False_x_size _y_size test_broadcast_prefer_nd_tiling_True_x_size _y_size test_broadcast_prefer_nd_tiling_True_x_size _y_size test_broadcast_with_singleton_dims TMA_XFAIL BlockDescriptorTestBase InductorTestCase block_descriptor_constructor_str = tl make_block_ptr _discontiguous_tensor view_size tuple int device Union torch device str - torch Tensor Create padded tensor given size The strides correspond tensor twice large each dimension isinstance device str device = torch device device full_size = tuple dim dim view_size full = torch randn full_size device view = torch as_strided full view_size full stride view _assert_pointwise_ndims code num_dims int - None pointwise_blocks = XBLOCK YBLOCK ZBLOCK _assert_tiling_ndims code pointwise_blocks num_dims _assert_reduction_ndims code num_dims int - None reduction_blocks = R _BLOCK R _BLOCK _assert_tiling_ndims code reduction_blocks num_dims _assert_tiling_ndims code blocks list str num_dims int - None expected_block blocks num_dims assertIn expected_block code unexpected_block blocks num_dims assertNotIn unexpected_block code _get_lines_containing_substr code str substr str - str \n join line line code split \n substr line _run_and_compare InductorTestCase func Callable Any args compile_kwargs Optional dict = None expected_num_block_pointers Optional int = None expected_num_programs int = expected_num_triton_kernels int = config_patches Optional dict = None rtol Optional float = None atol Optional float = None Runs module through Inductor comparing eager reference compile_kwargs None compile_kwargs = config_patches None config_patches = flatten_tensors tensors flat spec = pytree tree_flatten tensors flat config patch config_patches compiled = torch compile func backend= inductor compile_kwargs result code = run_and_get_code compiled args Check numerical accuracy ref_tensors = flatten_tensors func args actual_tensors = flatten_tensors result ref actual zip ref_tensors actual_tensors Don t clobber default tolerance values tol = t v t v rtol rtol atol atol items v None assertTrue torch allclose ref actual tol count_code substr str expected Optional int count = sum prog count substr prog code expected None assertEqual count expected Check code assertEqual len code expected_num_programs count_code triton jit expected_num_triton_kernels count_code block_descriptor_constructor_str expected_num_block_pointers Verify D shapes aren t being transposed TMA store count_code tl trans result code instantiate_parametrized_tests CommonTemplate parametrize expected_num_block_pointers raises False This should pass True This should fail test_expected_num_block_pointers expected_num_block_pointers int raises bool Checks test harness verifies number block pointers correctly foo x y x + y device = torch device device inputs = torch randn device arg_idx range Expect failure bad inputs assertRaises AssertionError raises contextlib nullcontext Expect block pointers inputs output _run_and_compare foo inputs expected_num_block_pointers=expected_num_block_pointers parametrize prefer_nd_tiling False True parametrize full_size view_size stride offset require_block_ptr None None True None None True None None True None True Storage offset None True Non-default strides None True Transposed strides None None True Non-power-of- leading dim block ptr None None False Non-power-of- inner dims non-block ptr None None False Scalar non-block ptr subtest arg_values= max_block max_block None None True Inner dim multiple max_block decorators= test_torchinductor skip_if_triton_cpu Triton CPU slow test test_pointwise full_size tuple int view_size tuple int stride Optional tuple int offset Optional int require_block_ptr bool prefer_nd_tiling bool Test generating strided ND block pointers pointwise kernel If require_block_ptr True generated code must contain block pointers However ND block pointers supported all shapes So we also test some odd shapes require_block_ptr set False ensure block pointer analysis does break these cases get_input - torch Tensor device = torch device device full = torch randn full_size device Use original tensor s stride default view_stride = full stride stride None stride torch as_strided full view_size view_stride storage_offset=offset args = get_input arg_idx range Expect block pointers inputs output _run_and_compare torch add args expected_num_block_pointers= require_block_ptr None config_patches= triton prefer_nd_tiling prefer_nd_tiling parametrize prefer_nd_tiling False True parametrize x_size y_size Very important case index variables disjoint Unmatched dims first operand test_broadcast x_size tuple int y_size tuple int prefer_nd_tiling bool Test we can generate strided block pointers when inputs have different shapes they broadcast together foo x y = x + b = y + b x y = _discontiguous_tensor size device size x_size y_size Check input sizes same assertNotEqual x shape y shape Check least one dimension singleton all_dims = x shape + y shape assertIn all_dims Expect block pointers inputs one output _run_and_compare foo x y expected_num_block_pointers= config_patches= triton prefer_nd_tiling prefer_nd_tiling test_broadcast_with_singleton_dims This tests case when input output contains both zero strides singleton dimensions In case broadcasting dimensions generated descriptor need ignore dimensions have zero strides size This minified repro based HuggingFaceTB SmolLM - M original issue store index=x + y + y matched block params = BlockParameters shape= block_shape= YBLOCK + Min YBLOCK XBLOCK strides= offsets= yoffset ModularIndexing yoffset xoffset broadcasting_dims= False False True True False broadcast_shape= YBLOCK + Min YBLOCK XBLOCK error len broadcasting_dims = broadcast_shape forward expand_ permute_ mul_ clone = torch ops aten clone default expand_ memory_format=torch contiguous_format expand_ = None view_ = torch ops aten view default clone clone = None cos = torch ops aten cos default view_ view_ = None mul = torch ops aten mul Tensor cos cos = None unsqueeze_ = torch ops aten unsqueeze default mul mul = None mul_ = torch ops aten mul Tensor permute_ unsqueeze_ permute_ = unsqueeze_ = None add_ = torch ops aten add Tensor mul_ mul_ mul_ = mul_ = None unsqueeze_ = torch ops aten unsqueeze default add_ add_ = None unsqueeze_ load_args reader buf = reader storage storage_hash=None nbytes= device=self device reader tensor buf is_leaf=True expand_ buf = reader storage storage_hash=None nbytes= device=self device reader tensor buf is_leaf=True permute_ buf = reader storage storage_hash=None nbytes= device=self device reader tensor buf is_leaf=True mul_ load_args _version = input_reader = InputReader load_args input_reader args = input_reader args device == xpu atol = e- rtol = e- atol = None rtol = None _run_and_compare forward args expected_num_block_pointers= atol=atol rtol=rtol parametrize x_size y_size TODO T test_expand_broadcast x_size tuple int y_size tuple int When load store have different shapes we should use broadcast foo x y_size x expand y_size clone get_input size tuple int - torch Tensor device = torch device device full = torch randn size device view = torch as_strided full size full stride view x = get_input x_size y = y_size Check input sizes same assertNotEqual x_size y_size Check valid broadcast assertEqual len x_size len y_size i j zip x_size y_size i = assertEqual i j result triton_code = _run_and_compare foo x y xfail_if_use_tensor_descriptor parametrize prefer_nd_tiling False True config patch triton skip_l _cache False test_pointwise_broadcast_nonzero_strides prefer_nd_tiling bool Test we emit tl broadcast_to instead using strides full_shape = col_shape = full_shape device = torch device device full = torch randn full_shape device col = torch as_strided full col_shape full stride Expect block pointers inputs one output result triton_code = _run_and_compare torch add full col expected_num_block_pointers= config_patches= triton prefer_nd_tiling prefer_nd_tiling Check code broadcasts We shouldn t see any strides load_lines store_lines = tuple _get_lines_containing_substr triton_code substr substr tl load tl store prefer_nd_tiling assertExpectedInline load_lines \ tmp = tl load tl make_block_ptr in_ptr shape= strides= block_shape= YBLOCK XBLOCK order= offsets= yoffset xoffset boundary_check= tmp = tl load tl make_block_ptr in_ptr shape= strides= block_shape= YBLOCK order= offsets= yoffset boundary_check= eviction_policy= evict_last None noqa B assertExpectedInline store_lines tl store tl make_block_ptr out_ptr shape= strides= block_shape= YBLOCK XBLOCK order= offsets= yoffset xoffset tl broadcast_to tmp YBLOCK XBLOCK tl float boundary_check= noqa B assertExpectedInline load_lines \ tmp = tl load tl make_block_ptr in_ptr shape= strides= block_shape= XBLOCK order= offsets= xoffset boundary_check= tmp = tl reshape tl broadcast_to tl load tl make_block_ptr in_ptr shape= strides= block_shape= + XBLOCK order= offsets= xoffset boundary_check= eviction_policy= evict_last None None + XBLOCK = + XBLOCK + + XBLOCK + XBLOCK = XBLOCK + XBLOCK XBLOCK XBLOCK noqa B assertExpectedInline store_lines tl store tl make_block_ptr out_ptr shape= strides= block_shape= XBLOCK order= offsets= xoffset tl broadcast_to tmp XBLOCK tl float boundary_check= noqa B parametrize prefer_nd_tiling False True parametrize view_size num_block_pointers num_triton_kernels None Non-power Multiple max block Uses loops subtest arg_values= max_block decorators= test_torchinductor skip_if_triton_cpu Triton CPU slow test max_block Multiple max block Uses loops Test large size loops test_reduction view_size tuple int num_block_pointers int num_triton_kernels int prefer_nd_tiling bool Tests reduction kernel device == cpu all Multiple max block Uses loops view_size == max_block num_block_pointers == num_triton_kernels == prefer_nd_tiling False raise unittest SkipTest Long test raises BrokenProcessPool Error triton CPU device = torch device device view = _discontiguous_tensor view_size device num_triton_kernels == config triton cooperative_reductions fewer kernels cooperative reductions num_triton_kernels = num_block_pointers -= Expect least block pointer input Add more we generate kernels result code = _run_and_compare torch sum view expected_num_block_pointers=num_block_pointers expected_num_triton_kernels=num_triton_kernels config_patches= triton prefer_nd_tiling prefer_nd_tiling parametrize view_size num_block_pointers num_triton_kernels No loops Should supported None None Looped reduction Block pointers yet supported test_mixed_pointwise_reduction view_size tuple int num_block_pointers int num_triton_kernels int Tests mixing pointwise reduction ops foo x y torch sum x + y inputs = _discontiguous_tensor view_size device input_idx range Expect block pointers inputs result code = _run_and_compare foo inputs expected_num_block_pointers=num_block_pointers expected_num_triton_kernels=num_triton_kernels xfail_if_use_tensor_descriptor test_multiple_max_block_non_power_of_ Check we support dims size n MAX_BLOCK where n any positive integer necessarily power foo x x - device = torch device device full_size = max_block view_size = max_block full = torch randn full_size device view = torch as_strided full view_size full stride Check we re using dims aren t all powers have_np _dim = all is_power_of_ dim dim view_size assertTrue have_np _dim Check we need more than one stride represent tensor nontrivial_dims = dim dim view_size dim assertTrue len nontrivial_dims Expect block pointers input output _run_and_compare foo view expected_num_block_pointers= parametrize nd_tiling num_block_pointers subtest True decorators= xfail_if_use_tensor_descriptor With tiling index affine False We can t infer load power test_dynamic_shapes_pointwise nd_tiling bool num_block_pointers int Test pointwise kernel dynamic shapes view_size = view = _discontiguous_tensor view_size device _run_and_compare torch div view view expected_num_block_pointers=num_block_pointers config_patches= triton prefer_nd_tiling nd_tiling compile_kwargs= dynamic True parametrize with_tiling num_block_pointers subtest True decorators= xfail_if_use_tensor_descriptor With tiling index affine False We can t infer load power skipIfXpu msg= Remove after Intel triton issue resolved test_dynamic_shapes_reduction with_tiling bool num_block_pointers int Test reduction kernel dynamic shapes view_size = view = _discontiguous_tensor view_size device _run_and_compare torch prod view expected_num_block_pointers=num_block_pointers config_patches= triton prefer_nd_tiling with_tiling triton tile_reductions with_tiling compile_kwargs= dynamic True unittest skip reason= Dynamo tracing error test_dynamic_shapes_pointwise_multiple_max_block Test dynamic shapes where we know shape multiple max block size We should able generate block pointer case foo x tile_dims = max_block x shape x shape view_size = max_block x shape x shape full = x tile tile_dims view = torch as_strided full view_size full stride view + view device = torch device device x_size = x = torch randn x_size device Expect block pointers input output _run_and_compare x compile_kwargs= dynamic True expected_num_block_pointers= decorateIf xfail_if_use_tensor_descriptor lambda param_kwargs param_kwargs num_block_pointers == param_kwargs num_tiles == parametrize full_size view_size num_block_pointers num_tiles Contiguous D tensor Does require tiling D tensor discontiguous dim D tensor discontiguous dim subtest arg_values= decorators= test_torchinductor skip_if_triton_cpu Triton CPU slow test D tensor discontiguous dim D tensor discontiguous dim D tensor discontiguous dims Block pointers unexpected test_nd_tiling_odd_shapes_pointwise full_size tuple int view_size tuple int num_block_pointers int num_tiles int Test odd shapes ND tiling enabled Uses pointwise op get_input - torch Tensor device = torch device device full = torch randn full_size device torch as_strided full view_size full stride args = get_input arg_idx range Expect up block pointers inputs output result code = _run_and_compare torch add args expected_num_block_pointers=num_block_pointers config_patches= triton prefer_nd_tiling True Check code expected tiling all_tiles = XBLOCK YBLOCK ZBLOCK expected_tiles = set all_tiles num_tiles tile_name all_tiles program code tile_name expected_tiles assertIn tile_name program assertNotIn tile_name program xfail_if_use_tensor_descriptor parametrize view_size num_block_pointers num_triton_kernels reduction_op torch sum Non-power-of shapes torch sum Large size loops torch argmax torch argmax torch var_mean Reduction + pointwise fusion test_ d_reduction_odd_shapes view_size tuple int num_block_pointers int num_triton_kernels int reduction_op Callable Tests D reduction kernels These arise odd shapes which expressible D block pointer view = _discontiguous_tensor view_size device Expect least block pointer input Add more we generate kernels result code = _run_and_compare reduction_op view expected_num_block_pointers=num_block_pointers expected_num_triton_kernels=num_triton_kernels config_patches=tiled_reduction_config Check code multiple Rn_BLOCK s _assert_reduction_ndims code parametrize size expected_num_block_pointers expected_num_triton_kernels expect_fallback True Persistent Welford fallback subtest False decorators= xfail_if_use_tensor_descriptor Looped Welford reduction test_ d_welford_reduction size tuple int expected_num_block_pointers int expected_num_triton_kernels int expect_fallback bool Tests D welford reduction NB input size should nice sense s multiple number processors Otherwise we will get more complex indexing doesn t generate block pointer Since tiling welford reductions depends block pointer analysis those cases would fall back D view = _discontiguous_tensor size device We expect many block pointers one result code = _run_and_compare torch var_mean view expected_num_block_pointers=expected_num_block_pointers expected_num_triton_kernels=expected_num_triton_kernels config_patches=tiled_reduction_config Check Welford reduction assertEqual welford code expect_fallback Check reduction dimensions _assert_reduction_ndims code test_torchinductor skip_if_triton_cpu Triton CPU slow test test_welford_non_block_pointer Tests welford reduction where block pointer analysis fails The main loop will D reduction instead D Use bad size s evenly divisible launch grid This won t decompose into block pointer view = _discontiguous_tensor device We expect many block pointers one result code = _run_and_compare torch var_mean view expected_num_block_pointers= expected_num_triton_kernels= config_patches= triton prefer_nd_tiling True Check Welford reduction assertIn welford code Check single reduction dimension _assert_reduction_ndims code test_reduction_multiple_discontiguous_dims Test reducing tensor more than one discontiguous dimension This case won t generate block pointer since we don allow enough tiling dimensions Use odd shapes frustrate block pointer analysis view = _discontiguous_tensor device result code = _run_and_compare torch sum view expected_num_block_pointers= expected_num_triton_kernels= config_patches=tiled_reduction_config Check reduction dimensions _assert_reduction_ndims code xfail_if_use_tensor_descriptor Cannot use TMA API store no x dimension test_torchinductor skip_if_triton_cpu Illegal instruction File cannot xfail because crashes process test_ d_reduction_multi_kernel Test D reduction multi kernel mode view = _discontiguous_tensor device foo x Reshape D take softmax all trailing dims x = x reshape x shape - torch softmax x - result code = _run_and_compare foo view expected_num_block_pointers= expected_num_triton_kernels= config_patches= triton multi_kernel True tiled_reduction_config Check multi kernel mode assertIn multi_kernel code Check reduction dimensions _assert_reduction_ndims code xfail_if_use_tensor_descriptor test_fused_ d_reduction Tests fusing multiple reductions same input D tiling foo x torch sum x + torch argmax x view_size = view = _discontiguous_tensor view_size device Expect least block pointer input result code = _run_and_compare foo view expected_num_block_pointers= expected_num_triton_kernels= config_patches=tiled_reduction_config Check code multiple Rn_BLOCK s _assert_reduction_ndims code parametrize reduction_op torch sum torch argmax test_ d_reductions_mixed_indexing reduction_op Callable Tests program multiple reductions using different strides These might fused foo args sum reduction_op arg arg args view_size = arg = _discontiguous_tensor view_size device arg = torch empty view_size No guarantees number kernels pointers result code = _run_and_compare foo arg arg config_patches=tiled_reduction_config Check code multiple Rn_BLOCK s _assert_reduction_ndims code parametrize tile_reductions False subtest True decorators= xfail_if_use_tensor_descriptor test_enable_tiled_reductions tile_reductions bool Tests enabling disabling tiled reductions view = _discontiguous_tensor device If tiled we expect block pointer input result code = _run_and_compare torch sum view expected_num_block_pointers= tile_reductions expected_num_triton_kernels= config_patches= triton prefer_nd_tiling True triton tile_reductions tile_reductions Check code multiple Rn_BLOCK s _assert_reduction_ndims code tile_reductions xfail_if_use_tensor_descriptor test_complex_reshape_block_ptr func x y add_ = x + y reshape_ = add_ reshape permute_ = reshape_ permute reshape_ = permute_ reshape clone_ = reshape_ clone memory_format=torch contiguous_format permute_ = clone_ permute clone_ = permute_ clone memory_format=torch contiguous_format clone_ clone_ inps = torch rand device=self device dtype=torch float result code = _run_and_compare func inps expected_num_triton_kernels= expected_num_block_pointers= assertTrue Min code xfail_if_use_tensor_descriptor requires_gpu FIXME test failed Triton-CPU test_ d_permute_tiling Test D tiling permute foo x y z dims = = x permute dims=dims + y b = z + y permute dims=dims + b inps = torch rand device=self device dtype=torch float result code = _run_and_compare foo inps expected_num_triton_kernels= expected_num_block_pointers= config_patches= triton max_tiles triton prefer_nd_tiling True Check D tiling _assert_pointwise_ndims code torch _dynamo config patch capture_scalar_outputs True parametrize num_tile_candidates test_unbacked_size_on_non_contig_dim num_tile_candidates int NUM_REPEAT should determine candidate_tilings NUM_REPEAT = num_tile_candidates == foo x length unbacked = length item repeated = x repeat unbacked NUM_REPEAT permute creates split middle unbacked symint first range ranges unbacked NUM_REPEAT permute = repeated permute permute cos inps = torch rand device=self device dtype=torch float torch scalar_tensor device=self device dtype=torch int torch _dynamo config patch capture_scalar_outputs True _run_and_compare foo inps expected_num_triton_kernels= expected_num_block_pointers= config_patches= triton max_tiles triton prefer_nd_tiling True block_ptr advancements should also deferrered conditional associated buffer being removed case bernoulli operation fused following sum so output buffer needed store immediate result bernoulli operation TODO fails triton CPU Failed convert LLVM IR test_torchinductor xfail_if_triton_cpu Disable split_reductions test now due interaction LOAF config patch split_reductions=False test_removed_buffers torch ops aten fn aten bernoulli sum torch prod torch tensor size p = result code = _run_and_compare fn torch ones device=self device p expected_num_triton_kernels= expected_num_block_pointers= atol=p rtol= xfail_if_use_tensor_descriptor test_pointwise_index_order Test order indices pointwise kernels Expect Z leading dim then Y then X inps = _discontiguous_tensor device=self device _ range result triton_code = _run_and_compare torch add inps expected_num_triton_kernels= expected_num_block_pointers= config_patches= triton max_tiles triton prefer_nd_tiling True Check load store block pointer strides load_lines store_lines index_lines = tuple _get_lines_containing_substr triton_code substr substr tl load tl store index = assertExpectedInline load_lines \ tmp = tl load tl make_block_ptr in_ptr shape= strides= block_shape= ZBLOCK YBLOCK XBLOCK order= offsets= zoffset yoffset xoffset boundary_check= tmp = tl load tl make_block_ptr in_ptr shape= strides= block_shape= ZBLOCK YBLOCK XBLOCK order= offsets= zoffset yoffset xoffset boundary_check= noqa B assertExpectedInline store_lines tl store tl make_block_ptr out_ptr shape= strides= block_shape= ZBLOCK YBLOCK XBLOCK order= offsets= zoffset yoffset xoffset tl broadcast_to tmp ZBLOCK YBLOCK XBLOCK tl float boundary_check= noqa B Check indices These used non-block pointers assertExpectedInline index_lines \ zindex = zoffset + tl arange ZBLOCK None None yindex = yoffset + tl arange YBLOCK None None xindex = xoffset + tl arange XBLOCK None None noqa B test_expand_clone_broadcast Test expand followed clone This uses explicit Triton broadcast base_size = expanded_size = foo x x expand expanded_size clone inps = torch randn base_size device=self device result triton_code = _run_and_compare foo inps expected_num_triton_kernels= expected_num_block_pointers= config_patches= triton max_tiles triton prefer_nd_tiling True We should only need one broadcast num_broadcasts = triton_code count tl broadcast_to assertEqual num_broadcasts test_mul_broadcast_multi_output foo x y z = x y b = c = b d = z e = x z c d e inps = torch randn device=self device torch randn device=self device torch randn device=self device result triton_code = _run_and_compare foo inps expected_num_triton_kernels= expected_num_block_pointers= config_patches= triton max_tiles triton prefer_nd_tiling True Check tiling D even though we allow up D Singleton splits should discarded _assert_pointwise_ndims triton_code Integration test ensure matched dims strides match_mod_div_expr unsigned signed integers respectively This test case has following index = ModularIndexing xindex + ModularIndexing xindex match below candidate invalid match= dim_mod _ dim_mod _ stride_mod _ dim_mod _ dim_mod _ stride_mod _ stride_mod _ stride_mod _ stride_mod _ This now fixed ensuring wild symbols only match integers xfail_if_use_tensor_descriptor skipIfXpu msg= Triton issue exposed new driver will resolved after next triton update test_ensure_integral_dims_and_strides model data args torch nn functional unfold data args data = torch zeros dtype=torch float requires_grad=True device=self device args = _run_and_compare model data args expected_num_triton_kernels= expected_num_block_pointers= compile_kwargs= fullgraph True Integration test test block analysis index expressions using negative strides This test case has following index index_relative_to_xyr_index = - xindex - ModularIndexing xindex - ModularIndexing xindex + subexpr = - xindex - ModularIndexing xindex - ModularIndexing xindex Block analysis should produce following BlockParameters shape= block_shape= XBLOCK + Min XBLOCK + Min XBLOCK strides= - - - offsets= xoffset ModularIndexing xoffset ModularIndexing xoffset constant_offset = xfail_if_use_tensor_descriptor test_negative_strides model x y Slice reverse order via negative stride torch flip x + y x y = _discontiguous_tensor device=self device _ range _run_and_compare model x y expected_num_triton_kernels= expected_num_block_pointers= config patch triton prefer_nd_tiling True config patch triton max_tiles parametrize block_multiple ynumel_exceed_ygrid_size include_z No boundary check all dimensions True False True No xdim boundary check ydim checked since max_ygrid z dim can used since its included True True False Boundary check all dimensions skip triton_cpu very slow test s subtest False False True decorators= test_torchinductor skip_if_triton_cpu xfail_if_use_tensor_descriptor test_boundary_check block_multiple ynumel_exceed_ygrid_size include_z dataclasses dataclass InputShape x int y int z Optional int = None to_list out = y x z None out insert z out BLOCK_SIZE = DIM_SIZE = BLOCK_SIZE block_multiple BLOCK_SIZE + shape = InputShape DIM_SIZE DIM_SIZE DIM_SIZE include_z None ynumel_exceed_ygrid_size shape y = math ceil get_max_y_grid shape y + shape y Use fixed block sizes avoid having generate very large input tensors FixedBlockSizeChoices InductorChoices triton_kernel_kwargs kernel_cls features groups kernel_kwargs block_sizes = f prefix upper BLOCK BLOCK_SIZE prefix size dataclasses asdict shape items size None kernel_kwargs fixed_config = FixedTritonConfig block_sizes kernel_kwargs = _discontiguous_tensor shape to_list device=self device b_shape = shape to_list b_shape - = b = _discontiguous_tensor b_shape device=self device func torch Tensor b torch Tensor - torch Tensor + b V set_choices_handler FixedBlockSizeChoices result code = _run_and_compare func b expected_num_triton_kernels= expected_num_block_pointers= code = code block_multiple ynumel_exceed_ygrid_size assertIn yoffset = tl program_id + tl program_id tl num_programs YBLOCK code Only y dimension should boundary checked b output assertEqual code count boundary_check= No boundary checking assertNotIn boundary_check code Loading assertTrue boundary_check= code Loading b assertTrue boundary_check= code unittest skipIf TRITON_HAS_CPU requires triton CPU backend config patch cpu_backend= triton config patch triton use_block_ptr True TritonBlockPointerTestCPU BlockDescriptorTestBase device = cpu test_torchinductor copy_tests CommonTemplate TritonBlockPointerTestCPU cpu xfail_prop= _expected_failure_triton_cpu unittest skipIf HAS_GPU requires triton GPU backend config patch triton use_block_ptr True TritonBlockPointerTestGPU BlockDescriptorTestBase device = GPU_TYPE test_torchinductor copy_tests CommonTemplate TritonBlockPointerTestGPU GPU_TYPE unittest skipIf HAS_CUDA_AND_TRITON torch cuda get_device_capability = torch version hip None Requires Triton CUDA backend CUDA compute capability = config patch triton use_tensor_descriptor True assume_aligned_inputs True TritonTensorDescriptorTestCUDA BlockDescriptorTestBase block_descriptor_constructor_str = tl make_tensor_descriptor device = GPU_TYPE test_torchinductor copy_tests CommonTemplate TritonTensorDescriptorTestCUDA GPU_TYPE xfail_prop= _expected_failure_use_tensor_descriptor test_failures=TMA_TEST_XFAIL __name__ == __main__ torch _inductor test_case run_tests HAS_GPU TRITON_HAS_CPU run_tests needs= filelock