Owner s module sparse ruff noqa F torch random io itertools unittest functools contextlib redirect_stderr torch testing make_tensor FileCheck torch testing _internal common_cuda SM OrLater SM OrLater TEST_CUSPARSE_GENERIC torch testing _internal common_utils \ TEST_WITH_TORCHINDUCTOR TEST_WITH_ROCM TEST_CUDA_CUDSS TEST_SCIPY TEST_NUMPY TEST_MKL IS_WINDOWS TestCase run_tests load_tests coalescedonoff parametrize subtest skipIfTorchDynamo skipIfRocmVersionLessThan IS_FBCODE IS_REMOTE_GPU suppress_warnings torch testing _internal common_device_type \ ops instantiate_device_type_tests dtypes OpDTypes dtypesIfCUDA onlyCPU onlyCUDA skipCUDAIfNoSparseGeneric precisionOverride skipMeta skipCUDAIf skipCUDAIfRocm skipCPUIfNoMklSparse largeTensorTest torch testing _internal common_methods_invocations \ op_db sparse_csr_unary_ufuncs ReductionOpInfo torch testing _internal common_cuda TEST_CUDA torch testing _internal common_dtype floating_types all_types_and_complex_and floating_and_complex_types floating_types_and all_types_and_complex floating_and_complex_types_and torch testing _internal opinfo definitions linalg sample_inputs_linalg_solve torch testing _internal opinfo definitions sparse validate_sample_input_sparse test_sparse CUSPARSE_SPMM_COMPLEX _SUPPORTED HIPSPARSE_SPMM_COMPLEX _SUPPORTED operator TEST_SCIPY scipy sparse sp TEST_NUMPY numpy np load_tests torch testing _internal common_utils used automatically filter tests sharding sandcastle This line silences flake warnings load_tests = load_tests noqa PLW no_mkl_sparse = IS_WINDOWS TEST_MKL _check_cusparse_spgemm_available cusparseSpGEMM added TEST_WITH_ROCM _sparse_csr_ops = list filter lambda op op supports_sparse_csr op_db _sparse_compressed_ops = list filter lambda op op supports_sparse_csr op supports_sparse_csc op supports_sparse_bsr op supports_sparse_bsc op_db binary_functions_with_dense_output = mm mv binary_ops_with_dense_output = list filter lambda op op name binary_functions_with_dense_output op_db UNARY_EWISE_CSR_ALLOW_AUTOGRAD = abs conj_physical deg rad neg positive frac nn functional relu log p rad deg This should just test_linalg instead code duplication https github com pytorch pytorch pull #discussion_r _test_addmm_addmv test_case f t m v alpha=None beta=None transpose_out=False layout=torch strided mode=None Unified test checking ` f t m v alpha=alpha beta=beta ` computation where f ` torch addmv ` ` torch addmm ` ` transpose_out ` controls whether out argument column-major order ` layout ` controls whether ` m ` converted specified layout Custom behaviour implemented only torch sparse_csr layout dtype = t dtype numpy_dtype = dtype dtype torch bfloat numpy_dtype = torch float dtype is_complex alpha = + j alpha None alpha beta = + j beta None beta alpha = alpha None alpha beta = beta None beta convert_layout mat layout == torch sparse_csr mat to_sparse_csr layout == torch sparse_csc mat to_sparse_csc assert mat layout == layout mat mode == all_sparse res = f map convert_layout t m v alpha=alpha beta=beta test_case assertEqual res layout layout res = res to_dense mode == dense_result res = f t convert_layout m convert_layout v alpha=alpha beta=beta res = f t convert_layout m v alpha=alpha beta=beta res = torch full_like res float nan transpose_out res = res t clone memory_format=torch contiguous_format t f t convert_layout m v alpha=alpha beta=beta out=res res = alpha m numpy_dtype cpu numpy v numpy_dtype cpu numpy beta = res += beta t numpy_dtype cpu numpy res = torch from_numpy res dtype test_case assertEqual res res test_case assertEqual res res TestSparseCSRSampler TestCase test_make_crow_indices Here we test correctness crow_indices algorithm testing CPU int dtype will sufficient device = torch device cpu index_dtype = torch int n_rows range n_cols range nnz range n_rows n_cols + crow_indices = _make_crow_indices n_rows n_cols nnz device=device dtype=index_dtype assertEqual len crow_indices n_rows + counts = crow_indices - crow_indices - assertEqual counts sum nnz assertGreaterEqual counts min assertLessEqual counts max n_cols all_sparse_compressed_layouts test_name= layout parametrize test_name subtest torch sparse_csr name= SparseCSR subtest torch sparse_csc name= SparseCSC subtest torch sparse_bsr name= SparseBSR subtest torch sparse_bsc name= SparseBSC sparse_compressed_nonblock_layouts test_name= layout parametrize test_name subtest torch sparse_csr name= SparseCSR subtest torch sparse_csc name= SparseCSC sparse_compressed_indices_methods = torch sparse_csr torch Tensor crow_indices torch Tensor col_indices torch sparse_csc torch Tensor ccol_indices torch Tensor row_indices torch sparse_bsr torch Tensor crow_indices torch Tensor col_indices torch sparse_bsc torch Tensor ccol_indices torch Tensor row_indices batched_nonbatched test_name= batched parametrize test_name subtest True name= Batched subtest False name= NonBatched hybrid_nonhybrid test_name= hybrid parametrize test_name subtest True name= Hybrid subtest False name= NonHybrid TestSparseCompressed TestCase Testing sparse compressed CSR CSC BSR BSC tensor generic features genTensor size nnz layout device=None dtype=torch float index_dtype=torch int device None device = device_type genSparseCompressedTensor size nnz device=device dtype=dtype index_dtype=index_dtype layout=layout all_sparse_compressed_layouts onlyCPU test_layout layout assertIn str layout torch sparse_csr torch sparse_csc torch sparse_bsr torch sparse_bsc assertEqual type layout torch layout parametrize shape_and_device_inference subtest False name= _ subtest True name= shape_and_device_inference parametrize use_factory_function subtest False name= _ subtest True name= factory parametrize input_kind subtest tensor name= from_tensor subtest list name= from_list all_sparse_compressed_layouts dtypes all_types_and_complex_and torch half torch bool torch bfloat test_sparse_compressed_constructor layout device dtype use_factory_function shape_and_device_inference input_kind input_kind == list shape_and_device_inference torch device device type == cuda list inputs factory constructor function without specifying device will result sparse compressed tensor CPU So skip testing against cuda device unused skipTest nothing test dtype torch float torch complex torch int torch bool skipTest dtype supported list values expected_devices = torch device device TEST_CUDA torch device device type == cuda torch cuda device_count = shape_and_device_inference expected_devices append torch device cuda factory_function = torch sparse_csr torch sparse_csr_tensor torch sparse_csc torch sparse_csc_tensor torch sparse_bsr torch sparse_bsr_tensor torch sparse_bsc torch sparse_bsc_tensor layout compressed_indices_mth plain_indices_mth = sparse_compressed_indices_methods layout input_kind == list index_dtypes = torch int index_dtypes = torch int torch int dtype is_floating_point dtype is_complex requires_grad_lst = False True requires_grad_lst = False index_dtype index_dtypes expected_device expected_devices compressed_indices plain_indices values kwargs generate_simple_inputs layout device=expected_device dtype=dtype index_dtype=index_dtype skip zero-sized tensors list inputs enable_zero_sized=input_kind = list output_tensor=False size = kwargs size shape_and_device_inference size skip shape inference zero-sized tensor inputs because i shape determined empty list ambiguous ii size plain dimension defined max plain_indices undefined plain_indices has no values continue compressed_indices_expect = compressed_indices plain_indices_expect = plain_indices values_expect = values input_kind == list compressed_indices = compressed_indices tolist plain_indices = plain_indices tolist values = values tolist requires_grad requires_grad_lst use_factory_function shape_and_device_inference sparse = factory_function compressed_indices plain_indices values requires_grad=requires_grad sparse = factory_function compressed_indices plain_indices values size dtype=dtype device=expected_device requires_grad=requires_grad shape_and_device_inference sparse = torch sparse_compressed_tensor compressed_indices plain_indices values layout=layout requires_grad=requires_grad sparse = torch sparse_compressed_tensor compressed_indices plain_indices values size dtype=dtype layout=layout device=expected_device requires_grad=requires_grad assertEqual layout sparse layout assertEqual size sparse shape assertEqual compressed_indices_expect compressed_indices_mth sparse assertEqual plain_indices_expect plain_indices_mth sparse assertEqual values_expect sparse values assertEqual sparse device sparse values device assertEqual sparse device expected_device assertEqual sparse values requires_grad requires_grad assertEqual sparse requires_grad requires_grad assertFalse compressed_indices_mth sparse requires_grad assertFalse plain_indices_mth sparse requires_grad skipMeta sparse_compressed_nonblock_layouts dtypes all_types_and_complex_and torch bool torch bfloat torch half test_empty layout device dtype ns = batch_shapes = compressed_dim = torch sparse_csr - torch sparse_csc - layout compressed_indices_mth plain_indices_mth = sparse_compressed_indices_methods layout m n b itertools product ns ns batch_shapes shape = b m n torch sparse check_sparse_tensor_invariants enable=False torch empty may invalid sparse compressed tensors result = torch empty shape dtype=dtype device=device layout=layout assertEqual result shape shape assertEqual result dtype dtype assertEqual result device torch device device assertEqual result layout layout assertEqual compressed_indices_mth result shape b shape compressed_dim + assertEqual plain_indices_mth result shape b assertEqual result values shape b assertEqual result _nnz assertEqual compressed_indices_mth result device torch device device assertEqual plain_indices_mth result device torch device device assertEqual result values device torch device device assertEqual compressed_indices_mth result dtype torch int assertEqual plain_indices_mth result dtype torch int assertEqual result values dtype dtype skipMeta sparse_compressed_nonblock_layouts dtypes all_types_and_complex_and torch bool torch half torch bfloat test_empty_errors layout device dtype assertRaisesRegex RuntimeError torch empty Only batched sparse compressed \\ non-block\\ tensors supported got size torch empty dtype=dtype device=device layout=layout skipMeta all_sparse_compressed_layouts dtypes all_types_and_complex_and torch bool torch bfloat torch half test_sparse_compressed_tensor_with_dims layout device dtype get_sparse_compressed_tensor_properties s layout torch sparse_csr torch sparse_bsr compressed_indices plain_indices = s crow_indices s col_indices compressed_indices plain_indices = s ccol_indices s row_indices values = s values dict shape=s shape dtype=s dtype device=s device nnz=s _nnz layout=s layout compressed_indices_shape=compressed_indices shape compressed_indices_dtype=compressed_indices dtype compressed_indices_device=compressed_indices device plain_indices_shape=plain_indices shape plain_indices_dtype=plain_indices dtype plain_indices_device=plain_indices device values_shape=values shape values_dtype=values dtype values_device=values device index_dtype torch int torch int t generate_simple_inputs layout device=device dtype=dtype index_dtype=index_dtype dense_dim = t dense_dim sparse_dim = t sparse_dim batch_dim = t ndim - sparse_dim - dense_dim nnz = t values shape batch_dim layout torch sparse_bsr torch sparse_bsc blocksize = t values shape batch_dim + batch_dim + + sparse_dim blocksize = e = torch ops aten _sparse_compressed_tensor_with_dims nnz dense_dim t shape blocksize index_dtype dtype=dtype layout=layout device=device e_prop t_prop = get_sparse_compressed_tensor_properties e get_sparse_compressed_tensor_properties t k v e_prop items assertEqual v t_prop k lambda msg f msg when comparing k expected t_prop k got v skipMeta all_sparse_compressed_layouts dtypes all_types_and_complex_and torch bool torch half torch bfloat test_clone layout device dtype sparse generate_simple_inputs layout device=device dtype=dtype index_dtype=torch int cloned_sparse = sparse clone assertEqual sparse cloned_sparse all_sparse_compressed_layouts test_print layout device compressed_indices_mth plain_indices_mth = sparse_compressed_indices_methods layout printed = enable_hybrid False True using local patterns test_print stability patterns = x batch x tensors trivial blocksize non-hybrid hybrid enable_hybrid tensor non-trivial blocksize non-hybrid hybrid enable_hybrid index_dtype torch int torch int dtype torch float torch float compressed_indices plain_indices values kwargs generate_simple_inputs layout device=device dtype=dtype index_dtype=index_dtype enable_hybrid=enable_hybrid enable_non_contiguous_indices=False enable_non_contiguous_values=False enable_zero_sized=False output_tensor=False patterns=patterns size = tuple kwargs size block_ndim = layout torch sparse_bsr torch sparse_bsc base_ndim = batch_ndim = compressed_indices dim - dense_ndim = values dim - batch_ndim - block_ndim - enable_hybrid dense_ndim == non-hybrid cases covered enable_hybrid==False loop continue batchsize = size batch_ndim basesize = size batch_ndim batch_ndim + base_ndim densesize = size batch_ndim + base_ndim assert len densesize == dense_ndim printed append f ########## dtype index_dtype size= batchsize + basesize + densesize ########## x = torch sparse_compressed_tensor compressed_indices plain_indices values size dtype=dtype layout=layout device=device printed append sparse tensor printed append str x printed append f _ compressed_indices_mth __name__ printed append str compressed_indices_mth x printed append f _ plain_indices_mth __name__ printed append str plain_indices_mth x printed append _values printed append str x values printed append printed append orig_maxDiff = maxDiff maxDiff = None try assertExpected \n join printed maxDiff = orig_maxDiff except Exception maxDiff = orig_maxDiff raise skipMeta all_sparse_compressed_layouts dtypes all_types_and_complex_and torch half torch bool torch bfloat test_copy layout device dtype run_test shape blocksize nnz index_type = genSparseCompressedTensor shape nnz dtype=dtype layout=layout device=device index_dtype=index_dtype blocksize=blocksize b = genSparseCompressedTensor shape nnz dtype=dtype layout=layout device=device index_dtype=index_dtype blocksize=blocksize copy_ b assertEqual b ns = number dimensions corresponding block size batch_shapes = m bm n bn b index_dtype zip itertools product ns ns batch_shapes torch int torch int blocksize = bm bn layout torch sparse_bsr torch sparse_bsc run_test b m n blocksize index_dtype run_test b m n blocksize m n index_dtype skipMeta all_sparse_compressed_layouts dtypes all_types_and_complex_and torch half torch bool torch bfloat test_copy_errors layout device dtype blocksize = layout torch sparse_bsr torch sparse_bsc nnz = layout torch sparse_bsr torch sparse_bsc shape = layout torch sparse_bsr torch sparse_bsc index_dtype torch int torch int = genSparseCompressedTensor shape dtype=dtype layout=layout device=device index_dtype=index_dtype blocksize=blocksize assertRaisesRegex RuntimeError copy sparse compressed tensors having different layouts supported copy_ torch empty shape dtype=dtype device=device b = genSparseCompressedTensor shape nnz dtype=dtype layout=layout device=device index_dtype=index_dtype blocksize=blocksize assert _nnz = b _nnz _nnz b _nnz assertRaisesRegex RuntimeError only sparse compressed tensors same number specified elements supported copy_ b shape = tuple reversed shape c = genSparseCompressedTensor shape nnz dtype=dtype layout=layout device=device index_dtype=index_dtype blocksize=blocksize assertRaisesRegex RuntimeError expected shapes src match along dimension b copy_ c blocksize blocksize = tuple reversed blocksize d = genSparseCompressedTensor shape nnz dtype=dtype layout=layout device=device index_dtype=index_dtype blocksize=blocksize assertRaisesRegex RuntimeError copy sparse compressed tensors having different block sizes supported b copy_ d _smallest_divisor n i range int n + n i == i n skipIfTorchDynamo Not TorchDynamo suitable test all_sparse_compressed_layouts ops _sparse_compressed_ops precisionOverride torch bfloat e- torch float e- test_consistency layout device dtype op Checks op strided sparse tensors will produce same results op supports_sparse_layout layout skipTest f op name does support input layout layout FIXME remove followup once integer support landed segment_reduce layout == torch sparse_csr dtype is_floating_point op name masked mean masked amax masked amin skipTest f op name does support input layout layout dtype dtype require_mask = isinstance op ReductionOpInfo masked op name samples = sample op sample_inputs device dtype sample input ndim continue dense_dim = sample input ndim - blocksize = tuple map _smallest_divisor sample input shape layout torch sparse_bsr torch sparse_bsc None _to_sparse x isinstance x torch Tensor blocksize None x ndim = sample input ndim x x ndim = sample input ndim + x shape - blocksize x shape - blocksize x x clone to_sparse layout=layout blocksize=blocksize dense_dim=dense_dim x sparse_sample = sample transform _to_sparse Some strided samples inf nan elements appear share storage so we must clone sample = sample transform lambda x x clone isinstance x torch Tensor x validate_sample_input_sparse op sparse_sample check_validate=False sparse_sample validation returns sparse sample wrapped within ErrorInput instance continue samples append sample sparse_sample Fail early prevent silent success test len samples == raise ValueError Expected least one higher D tensor samples Re-define atol rtol operations result values random hence non-comparable we still want check shape dtype etc attributes results atol = rtol = None op name == randn_like atol = e rtol = sample sparse_sample samples expected = op sample input sample args sample kwargs assert torch is_tensor expected output = op sparse_sample input sparse_sample args sparse_sample kwargs assert torch is_tensor output strided_output = output to_dense require_mask sample kwargs get mask None output_mask = torch masked _output_mask op op sample input sample args sample kwargs expected masked_fill_ ~output_mask assertEqual strided_output expected atol=atol rtol=rtol skipMeta all_sparse_compressed_layouts all_sparse_compressed_layouts layout dtypes all_types_and_complex_and torch bool torch half torch bfloat test_empty_like layout layout device dtype sparse generate_simple_inputs layout layout == layout result = torch empty_like sparse layout=layout compressed_indices_mth plain_indices_mth = sparse_compressed_indices_methods result layout torch _validate_sparse_compressed_tensor_args compressed_indices_mth result plain_indices_mth result result values result shape result layout assertEqual sparse shape result shape assertRaisesRegex RuntimeError empty_like different sparse layout supported lambda torch empty_like sparse layout=layout skipMeta all_sparse_compressed_layouts dtypes all_types_and_complex_and torch half torch bool torch bfloat test_validate layout device dtype make_zero_batched t torch empty + t shape dtype=t dtype device=t device index_dtype torch int torch int compressed_indices plain_indices values kwargs generate_simple_inputs layout device=device dtype=dtype index_dtype=index_dtype output_tensor=False size = kwargs size torch _validate_sparse_compressed_tensor_args compressed_indices plain_indices values size layout check empty batch torch _validate_sparse_compressed_tensor_args make_zero_batched t t compressed_indices plain_indices values + size layout compressed_indices = torch tensor dtype=index_dtype plain_indices = torch tensor dtype=index_dtype torch _validate_compressed_sparse_indices layout torch sparse_csr torch sparse_bsr compressed_indices plain_indices _generate_invalid_input layout device functools partial shape shape basedim= blocksize = layout torch sparse_csc shape = shape basedim + shape basedim + shape basedim + shape basedim + layout torch sparse_bsc shape = shape basedim + shape basedim + blocksize shape basedim blocksize + shape basedim + layout torch sparse_bsr shape = shape basedim + shape basedim blocksize shape basedim + blocksize + shape basedim + shape values lst device=device layout torch sparse_bsr torch sparse_bsc lst = item item lst torch tensor lst device=device tensor = partial torch tensor device=device values = partial values device=device yield incontiguous compressed_indices tensor - - - tensor values shape expected compressed_indices contiguous tensor per batch yield incontiguous plain_indices tensor tensor - - - - values shape expected plain_indices contiguous tensor per batch yield -D compressed_indices tensor tensor values shape compressed_indices must have dimensionality = got yield compressed plain_indices mismatch dimensionalities tensor tensor values shape compressed_indices plain_indices dimensionalities must equal got respectively layout torch sparse_csr torch sparse_csc yield indices values mismatch dimensionalities tensor tensor values shape r values must have dimensionality sum batch block dimensionalities \ = \+ \ got yield indices values mismatch dimensionalities tensor tensor values shape r values must have dimensionality sum batch block dimensionalities \ = \+ \ got yield invalid size tensor tensor values r tensor dimensionality must sum batch base dense dimensionalities \ = \+ \+ \ got yield invalid batchsize tensor tensor values shape r all batch dimensions compressed_indices \ =\ \ \ plain_indices \ =\ \ \ r values \ =\ \ \ must equal tensor batch dimensions \ =\ \ \ layout torch sparse_bsr yield invalid blocksize tensor tensor tensor shape r tensor shape\ \ \ = \ must divisible blocksize\ \ \ = \ defined values shape layout torch sparse_bsc yield invalid blocksize tensor tensor tensor shape r tensor shape\ \ \ = \ must divisible blocksize\ \ \ = \ defined values shape yield invalid compressed_indices shape tensor tensor values shape r compressed_indices shape\ - \ must equal number compressed_indices_names \+ \ = \ got yield invalid compressed_indices shape tensor tensor values shape r plain_indices shape\ - \ must equal nnz \ = \ defined values shape\ \ got yield compressed plain_indices mismatch dtype tensor dtype=torch int tensor dtype=torch int values shape r compressed_indices plain_indices must have same dtype bot got Int Long respectively yield invalid compressed plain_indices dtype tensor dtype=torch int tensor dtype=torch int values shape r compressed_indices plain_indices dtype must Int Long got Short CUDA kernel asserts recoverable so we skip these now torch device device type == cpu yield invalid compressed_indices tensor tensor values shape r ` compressed_indices\ \ == ` satisfied yield invalid compressed_indices when nnz == tensor dtype=torch int tensor dtype=torch int values shape r ` compressed_indices\ \ == ` satisfied yield invalid compressed_indices - tensor tensor values shape r ` compressed_indices\ - \ == nnz ` satisfied yield invalid compressed_indices - when nnz == tensor dtype=torch int tensor dtype=torch int values shape r ` compressed_indices\ - \ == nnz ` satisfied yield invalid compressed_indices diff dim=- tensor tensor values shape r = compressed_indices\ \ - compressed_indices\ \- \ = plain_dim ` satisfied yield invalid compressed_indices diff dim=- tensor tensor values shape r = compressed_indices\ \ - compressed_indices\ \- \ = plain_dim ` satisfied yield invalid min plain_indices tensor tensor - values shape r ` = plain_indices plain_dim ` satisfied yield invalid max plain_indices tensor tensor values shape r ` = plain_indices plain_dim ` satisfied yield non-coalesced tensor tensor values shape r ` plain_indices\ compressed_indices\ i - \ compressed_indices\ i\ \ all i = compressed_dim sorted distinct along last dimension values ` satisfied TEST_CUDA torch device device type == cpu yield indices values mismatch device torch tensor torch tensor values device= cuda shape r device compressed_indices \ =cpu\ must match device values \ =cuda \ yield compressed_indices values mismatch device torch tensor device= cuda torch tensor values shape r Expected all tensors same device found least two devices cuda cpu yield compressed plain_indices mismatch device torch tensor device= cuda torch tensor values device= cuda shape r Expected all tensors same device found least two devices cuda cpu TEST_CUDA torch device device type == cuda torch cuda device_count = yield indices values mismatch device index torch tensor device= cuda torch tensor device= cuda values device= cuda shape r device compressed_indices \ =cuda \ must match device values \ =cuda \ yield compressed_indices values mismatch device index torch tensor device= cuda torch tensor device= cuda values device= cuda shape r Expected all tensors same device found least two devices cuda cuda skipMeta all_sparse_compressed_layouts parametrize target subtest validate_sparse_compressed_tensor_args subtest sparse_compressed_tensor subtest sparse_compressed_tensor_no_size test_invalid_input layout device target label compressed_indices plain_indices values size errmsg _generate_invalid_input layout device layout torch sparse_bsr errmsg = errmsg replace compressed_indices_name row block replace plain_indices_name column block layout torch sparse_bsc errmsg = errmsg replace compressed_indices_name column block replace plain_indices_name row block layout torch sparse_csr errmsg = errmsg replace compressed_indices_name row replace plain_indices_name column layout torch sparse_csc errmsg = errmsg replace compressed_indices_name column replace plain_indices_name row layout torch sparse_csr torch sparse_bsr errmsg = errmsg replace compressed_indices crow_indices \ replace plain_indices col_indices \ replace plain_dim ncols \ replace compressed_dim nrows errmsg = errmsg replace compressed_indices ccol_indices \ replace plain_indices row_indices \ replace plain_dim nrows \ replace compressed_dim ncols target == sparse_compressed_tensor_no_size label invalid size invalid batchsize invalid compressed_indices shape invalid max plain_indices invalid blocksize Skip invalid size input valid size estimated other inputs continue assertRaisesRegex RuntimeError errmsg target == validate_sparse_compressed_tensor_args torch _validate_sparse_compressed_tensor_args compressed_indices plain_indices values size layout target == sparse_compressed_tensor torch sparse_compressed_tensor compressed_indices plain_indices values size layout=layout target == sparse_compressed_tensor_no_size torch sparse_compressed_tensor compressed_indices plain_indices values layout=layout raise NotImplementedError target skipMeta onlyCPU largeTensorTest GB cpu test_invalid_input_csr_large rows = assertRaisesRegex RuntimeError -bit integer overflow row dimension torch sparse_csr_tensor torch arange rows + dtype=torch int rows torch tensor dtype=torch int torch tensor rows torch sparse_csr_tensor torch arange rows + dtype=torch int rows torch tensor dtype=torch int torch tensor rows cols = assertRaisesRegex RuntimeError -bit integer overflow column dimension torch sparse_csr_tensor torch arange dtype=torch int torch tensor dtype=torch int torch tensor cols torch sparse_csr_tensor torch arange dtype=torch int torch tensor dtype=torch int torch tensor cols nnz = assertRaisesRegex RuntimeError -bit integer overflow nnz nnz cannot stored int crow_indices ` crow_indices - == nnz ` ` check happens after overflow validation So we can use ` nnz - ` here avoid ` value cannot converted type int without overflow ` during construction crow_indices torch sparse_csr_tensor torch tensor nnz nnz - dtype=torch int torch arange nnz dtype=torch int repeat torch ones nnz dtype=torch int nnz torch sparse_csr_tensor torch tensor nnz nnz dtype=torch int torch arange nnz dtype=torch int repeat torch ones nnz dtype=torch int nnz skipMeta onlyCPU all_sparse_compressed_layouts test_dim layout compressed_indices plain_indices values kwargs generate_simple_inputs layout output_tensor=False size = kwargs size batch_dim = compressed_indices dim - sparse_dim = block_dim = layout torch sparse_bsr torch sparse_bsc dense_dim = values dim - batch_dim - block_dim - sparse = torch sparse_compressed_tensor compressed_indices plain_indices values size layout=layout assertEqual sparse sparse_dim sparse_dim assertEqual sparse dense_dim dense_dim skipMeta all_sparse_compressed_layouts dtypes all_types_and_complex_and torch bool torch half torch bfloat test_to_dtype layout device dtype to_dense does support hybrid inputs sparse generate_simple_inputs layout dtype=dtype device=device enable_hybrid=False to_dtype all_types_and_complex_and torch bool torch half torch bfloat sparse_to_dtype = sparse to_dtype dense_to_dtype = sparse to_dense to_dtype assertEqual sparse_to_dtype to_dense dense_to_dtype skipMeta all_sparse_compressed_layouts dtypes torch double test_pickle layout dtype device pickle sparse generate_simple_inputs layout device=device dtype=dtype serialized = pickle dumps sparse sparse_loaded = pickle loads serialized assertEqual sparse sparse_loaded all_sparse_compressed_layouts parametrize index_dtype torch int torch int dtypes all_types_and_complex_and torch half torch bfloat torch bool test_select_copy device dtype index_dtype layout is_view_of base other shameless copy TestViewOps is_view_of other _is_view other base other _base base base device = other device False base device type cpu cuda base untyped_storage data_ptr = other untyped_storage data_ptr False True kwargs = dict device=device dtype=dtype index_dtype=index_dtype sparse dense zip generate_simple_inputs layout kwargs generate_simple_inputs torch strided kwargs layout torch sparse_csr torch sparse_bsr n_batchdim = sparse crow_indices ndim - layout torch sparse_csc torch sparse_bsc n_batchdim = sparse ccol_indices ndim - assert unreachable assertEqual sparse dense dim range sparse ndim sparse shape dim == assertRaisesRegex IndexError index out range tensor size torch select_copy sparse dim assertRaisesRegex IndexError index out range tensor size torch select_copy dense dim n_batchdim dim = n_batchdim dim n_batchdim + assertRaisesRegex RuntimeError selecting sparse dimensions supported batched sparse compressed tensors torch select_copy sparse dim index sparse shape dim sparse shape dim - dense_select = torch select_copy dense dim index sparse_select = torch select_copy sparse dim index assertEqual sparse_select dense_select assertFalse is_view_of sparse_select values sparse values _npref_block_addmm_addmv c b alpha beta alpha b + beta c TestSparseCSR TestCase test_csr_stride = genSparseCSRTensor dtype=torch float device=self device_type index_dtype=torch int assertRaisesRegex RuntimeError Sparse CSR tensors do have strides stride assertRaisesRegex RuntimeError Sparse CSR tensors do have strides stride - test_csr_storage = genSparseCSRTensor dtype=torch float device=self device_type index_dtype=torch int assertRaisesRegex RuntimeError Cannot access storage SparseCsrTensorImpl storage test_csr_is_contiguous = genSparseCSRTensor dtype=torch float device=self device_type index_dtype=torch int assertRaisesRegex RuntimeError Sparse CSR tensors do have is_contiguous is_contiguous onlyCPU largeTensorTest GB cpu test_csr_nnz Tests limits number specified elements CSR tensors see gh- nnz rows cols = max nnz crow_indices = torch tensor nnz dtype=torch int col_indices = torch arange nnz dtype=torch int values = torch ones nnz dtype=torch int = torch sparse_csr_tensor crow_indices col_indices values rows cols assertEqual _nnz nnz test_csr_double_to_sparse_csr = genSparseCSRTensor dtype=torch float device=self device_type index_dtype=torch int to_sparse_csr to_sparse_csr all_sparse_compressed_layouts parametrize index_dtype torch int torch int dtypes all_types_and_complex_and torch half torch bfloat torch bool test_select device dtype index_dtype layout compressed_indices_mth = torch sparse_csr torch Tensor crow_indices torch sparse_bsr torch Tensor crow_indices torch sparse_csc torch Tensor ccol_indices torch sparse_bsc torch Tensor ccol_indices layout plain_indices_mth = torch sparse_csr torch Tensor col_indices torch sparse_bsr torch Tensor col_indices torch sparse_csc torch Tensor row_indices torch sparse_bsc torch Tensor row_indices layout create_tensor_mth = torch sparse_csr torch sparse_csr_tensor torch sparse_bsr torch sparse_bsr_tensor torch sparse_csc torch sparse_csc_tensor torch sparse_bsc torch sparse_bsc_tensor layout shape = nnz = blocksize = layout torch sparse_bsr torch sparse_bsc sparse = genSparseCompressedTensor shape nnz device=device layout=layout dtype=dtype index_dtype=index_dtype blocksize=blocksize comp_indices = compressed_indices_mth sparse plain_indices = plain_indices_mth sparse values = sparse values select batch dimensions sparse_selected = sparse select expected_sparse_selected = create_tensor_mth comp_indices select contiguous plain_indices select contiguous values select contiguous size= dtype=dtype device=device assertEqual expected_sparse_selected sparse_selected selecting rows col batch dims allowed sparse_non_batched = sparse select sparse dimensions select_args sparse_selected = sparse_non_batched select select_args dense_selected = sparse_non_batched to_dense select select_args assertEqual dense_selected sparse_selected assertEqual sparse sparse to_dense assigning sparse through indexing disabled assertRaisesRegex TypeError Cannot assign sparse tensor sparse = select sparse dimensions without removing batch dims msg = selecting sparse dimensions supported batched sparse compressed tensors assertRaisesRegex RuntimeError msg sparse select - assertRaisesRegex RuntimeError msg sparse select - skipMeta dtypes all_types_and_complex_and torch half torch bool torch bfloat test_resize device dtype numel tensor r = s tensor shape r = s r batch_shapes = index_dtype b zip torch int torch int batch_shapes shape = b nnz = = genSparseCSRTensor shape nnz dtype=dtype device=device index_dtype=index_dtype assertEqual numel numel new_shape = b resize_ new_shape assertEqual shape new_shape resize larger shape doesn t add specified elements assertEqual _nnz nnz assertEqual numel numel new_shape = b resize_ new_shape assertEqual shape new_shape resize smaller shape trims specified elements assertEqual _nnz assertEqual numel numel trim batched dimensions resize_ new_shape - new_shape - assertEqual shape new_shape - new_shape - assertEqual _nnz assertEqual numel numel skipMeta dtypes torch float torch bool all_sparse_compressed_layouts test_resize_as_sparse_compressed device dtype layout _check_resize_b_as_a b br = b clone br resize_as_sparse_ shape inherited assertEqual shape br shape other metadata affected assertEqual b layout br layout assertEqual b device br device assertEqual b dtype br dtype _get_compressed_plain_inds t compressed_indices_mth plain_indices_mth = sparse_compressed_indices_methods t layout compressed_indices_mth t plain_indices_mth t br_compressed_indices br_plain_indices = _get_compressed_plain_inds br br_values = br values b_compressed_indices b_plain_indices = _get_compressed_plain_inds b a_compressed_indices a_plain_indices = _get_compressed_plain_inds assertEqual a_plain_indices shape br_plain_indices shape assertEqual a_compressed_indices shape br_compressed_indices shape We don t check content br_plain_indices br_compressed_indices because well-defined content depends original shape ` b ` ` resize_as ` ought discard nor needed subsequent operation likely updates indices values ` b ` anyway device dtype indices should always unaffected assertEqual b_plain_indices dtype br_plain_indices dtype assertEqual b_plain_indices device br_plain_indices device assertEqual b_compressed_indices dtype br_compressed_indices dtype assertEqual b_compressed_indices device br_compressed_indices device values generated empty shape updated assertEqual values shape br_values shape device dtype indices should always unaffected b_values = b values assertEqual b_values dtype br_values dtype assertEqual b_values device br_values device nnz will picked up via new shape values assertEqual _nnz br _nnz post resize invariants layout respected torch _validate_sparse_compressed_tensor_args br_compressed_indices br_plain_indices br_values br shape br layout block_sparse = layout torch sparse_bsr torch sparse_bsc shape = nnz = blocksize = block_sparse index_dtype torch int torch int = genSparseCompressedTensor shape layout=layout device=device index_dtype=index_dtype dtype=dtype nnz=nnz blocksize=blocksize same size resize should trigger b = genSparseCompressedTensor shape layout=layout device=device index_dtype=index_dtype dtype=dtype nnz=nnz blocksize=blocksize This test will always trigger resize layouts same nothing should happen b The invariants function checked should still hold _check_resize_b_as_a b same ndim bigger more nnz different dtype different blocksize blocked b = genSparseCompressedTensor tuple s s shape layout=layout device=device dtype=torch chalf index_dtype=torch int index_dtype == torch int torch int nnz=nnz blocksize=tuple bi bi blocksize _check_resize_b_as_a b different device only check cuda pass we know we testing environment has multiple devices TODO cpu does seem work correctly sparse Causes call ` copy_ ` which complains about incompatible nnz between src torch device device type == cuda layout torch sparse_bsc torch sparse_bsr a_cpu = genSparseCompressedTensor shape layout=layout device= cpu index_dtype=index_dtype dtype=dtype nnz=nnz blocksize=blocksize _check_resize_b_as_a b error strided a_strided = to_dense assertRaisesRegex RuntimeError r resize_as_sparse_compressed_ src expected sparse compressed tensor layout b resize_as_sparse_ a_strided error b strided b_strided = b to_dense assertRaisesRegex RuntimeError r resize_as_sparse_compressed_ expected sparse compressed tensor layout b_strided resize_as_sparse_ error layout does match transpose induces layout flip assertRaisesRegex RuntimeError r resize_as_sparse_compressed_tensor_ src must have same layout b transpose - - resize_as_sparse_ assertRaisesRegex RuntimeError r resize_as_sparse_compressed_tensor_ src must have same layout b resize_as_sparse_ transpose - - skipMeta dtypes all_types_and_complex_and torch half torch bool torch bfloat test_resize_errors device dtype index_dtype torch int torch int shape = nnz = = genSparseCSRTensor shape nnz dtype=dtype device=device index_dtype=index_dtype assertRaisesRegex RuntimeError torch resize_ Only batched sparse CSR matrices supported new_shape = resize_ new_shape resizing columns smaller size implemented assertRaisesRegex RuntimeError torch resize_ Resizing columns sparse CSR tensors smaller value supported new_shape = resize_ new_shape skipIfTorchDynamo dtypes all_types_and_complex_and torch half torch bool torch bfloat test_sparse_csr_from_dense device dtype dense = torch tensor dtype=dtype device=device sparse = dense to_sparse_csr assertEqual torch tensor dtype=torch int sparse crow_indices assertEqual torch tensor dtype=torch int sparse col_indices assertEqual torch tensor dtype=dtype sparse values dense = torch tensor dtype=dtype device=device sparse = dense to_sparse_csr assertEqual torch tensor dtype=torch int sparse crow_indices assertEqual torch tensor dtype=torch int sparse col_indices assertEqual torch tensor dtype=dtype sparse values dense = torch tensor dtype=dtype device=device sparse = dense to_sparse_csr assertEqual torch tensor dtype=torch int sparse crow_indices assertEqual torch tensor dtype=torch int sparse col_indices assertEqual torch tensor dtype=dtype sparse values _test_sparse_compressed_to_dense device dtype layout compressed_format_str = str layout - to_compressed t getattr t f to_sparse_ compressed_format_str compressed_constructor input kwargs constructor = getattr torch f sparse_ compressed_format_str _tensor constructor input kwargs get_dense_shape shape batch_ndim layout torch sparse_csc compressed_dims_slice = slice batch_ndim + batch_ndim - - compressed_dims_slice = slice batch_ndim batch_ndim + shape batch_ndim + shape compressed_dims_slice + shape batch_ndim + transpose t batch_ndim layout torch sparse_csc t transpose batch_ndim batch_ndim + t mn = m n itertools product mn mn size = m n dense = make_tensor size dtype=dtype device=device sparse = to_compressed dense assertEqual sparse to_dense dense batch_shape = compressed_indices = torch tensor device=device repeat reshape batch_shape - plain_indices = torch tensor device=device repeat reshape batch_shape - values = torch tensor device=device dtype=dtype repeat reshape batch_shape - sparse = compressed_constructor compressed_indices plain_indices values dtype=dtype device=device dense_shape = get_dense_shape sparse shape len batch_shape dense = torch tensor dtype=dtype device=device repeat reshape dense_shape assertEqual sparse to_dense transpose dense len batch_shape dtypes all_types_and_complex_and torch half torch bool torch bfloat test_sparse_csr_to_dense device dtype _test_sparse_compressed_to_dense device dtype torch sparse_csr dtypes all_types_and_complex_and torch half torch bool torch bfloat test_sparse_csc_to_dense device dtype _test_sparse_compressed_to_dense device dtype torch sparse_csc skipMeta skipCPUIfNoMklSparse coalescedonoff dtypes torch double test_coo_to_csr_convert device dtype coalesced assertRaisesRegex RuntimeError Input supposed vector torch _convert_indices_from_coo_to_csr torch randint device=device size= size = sparse_dim = nnz = sparse_coo _ _ = genSparseTensor size sparse_dim nnz coalesced device dtype sparse_csr = sparse_coo to_sparse_csr assertTrue sparse_csr is_sparse_csr assertEqual sparse_csr to_dense sparse_coo to_dense vec = torch randn dtype=dtype device=device coo_product = sparse_coo matmul vec csr_product = sparse_csr matmul vec assertEqual coo_product csr_product vec = torch randn dtype=dtype device=device index = torch tensor dtype=torch int values = torch tensor dtype=dtype device=device coo = torch sparse_coo_tensor index values torch Size dtype=dtype device=device csr = coo to_sparse_csr assertEqual coo matmul vec csr matmul vec col_indices = torch tensor dtype=torch int device=device assertEqual csr col_indices col_indices values = torch tensor dtype=dtype device=device assertEqual csr values values parametrize blocksize dtypes torch double torch int torch double torch int unittest skipIf TEST_SCIPY SciPy found skipMeta test_csr_to_block_csr device dtypes blocksize shape dtype index_dtype = dtypes m k = shape nnz = random randint m k t = genSparseCSRTensor m blocksize k blocksize nnz dtype=dtype device=device index_dtype=index_dtype st = sp csr_matrix t values cpu t col_indices cpu t crow_indices cpu shape=tuple t size block_t = t to_sparse_bsr blocksize blocksize assertEqual block_t values dim assertTrue block_t layout == torch sparse_bsr block_st = st tobsr blocksize= blocksize blocksize block_st sort_indices assertEqual block_t values cpu block_st data assertEqual block_t col_indices cpu torch tensor block_st indices index_dtype assertEqual block_t crow_indices cpu torch tensor block_st indptr index_dtype dtypes torch double unittest skipIf TEST_SCIPY SciPy found test_csr_to_block_csr_errors device dtype index_dtype torch int torch int nnz = t = genSparseCSRTensor nnz dtype=dtype device=device index_dtype=index_dtype assertRaisesRegex RuntimeError r tensor sparse size \ \ must divisible given blocksize \ \ block_t = t to_sparse_bsr TODO Support auto generation device check sparse tensors See https github com pytorch pytorch issues onlyCUDA dtypes torch double test_matmul_device_mismatch device dtype cpu = torch rand cuda = cpu cuda s m m itertools product cpu cuda repeat= csr = m to_sparse s device == csr device == m device torch addmm s csr m assertRaisesRegex RuntimeError Expected all tensors same device torch addmm s csr m skipCPUIfNoMklSparse skipCUDAIfNoSparseGeneric dtypes floating_and_complex_types dtypesIfCUDA floating_and_complex_types_and torch half SM OrLater torch bfloat SM OrLater test_csr_matvec device dtype TEST_WITH_ROCM dtype == torch half dtype == torch bfloat skipTest ROCm doesn t work half dtypes correctly side = index_dtype torch int torch int csr = genSparseCSRTensor side side device=device dtype=dtype index_dtype=index_dtype vec = torch randn side dtype=dtype device=device res = csr matmul vec expected = csr to_dense matmul vec atol rtol = e- e- dtype == torch half None None assertEqual res expected atol=atol rtol=rtol bad_vec = torch randn side + dtype=dtype device=device err_msg = size mismatch got assertRaisesRegex RuntimeError err_msg csr matmul bad_vec onlyCUDA dtypes torch float torch float torch complex torch complex test_baddbmm device dtype TODO disable invariant checks within torch baddbmm constructs unconventional csr tensors leading RuntimeError tensor dimensionality must sum batch base dense dimensionalities = + + got when invariant checking enabled When done undecorate run_test torch sparse check_sparse_tensor_invariants enable=False run_test c a_batched b op_b=False op_out=False dtype=None device=None alpha = complex random random random random dtype is_complex random random beta = complex random random random random dtype is_complex random random b = b mH op_b shape == b shape b actual = torch baddbmm c a_batched b alpha=alpha beta=beta out = torch empty_like c mH op_out shape == b shape c torch baddbmm c a_batched b alpha=alpha beta=beta out=out expected = torch addmm c i b i alpha=alpha beta=beta i range c shape expected = torch stack expected assertEqual actual out assertEqual actual expected index_dtype torch int torch int m n k batch_size noncontiguous zip itertools product repeat= True False nnz = random randint m k = genSparseCSRTensor m k nnz dtype=dtype device=device index_dtype=index_dtype a_batched regular CSR tensor batch dimension shape a_batched = torch sparse_csr_tensor crow_indices col_indices values batch_size m k check_invariants=False b = make_tensor batch_size k n dtype=dtype device=device noncontiguous=noncontiguous c = make_tensor batch_size m n dtype=dtype device=device noncontiguous=noncontiguous op_b op_out itertools product True False repeat= run_test c a_batched b op_b op_out dtype=dtype device=device onlyCUDA skipCUDAIfNoSparseGeneric skipIfRocmVersionLessThan dtypes torch float torch float torch complex torch complex test_bmm device dtype run_test a_batched b op_b=False op_out=False dtype=None device=None b = b mH op_b shape == b shape b actual = torch bmm a_batched b out = torch empty_like actual mH op_out shape == b shape actual torch bmm a_batched b out=out expected = torch mm b i i range b shape expected = torch stack expected assertEqual actual out assertEqual actual expected index_dtype torch int torch int m n k batch_size noncontiguous zip itertools product repeat= True False nnz = random randint m k = genSparseCSRTensor m k nnz dtype=dtype device=device index_dtype=index_dtype a_batched regular CSR tensor batch dimension shape It unorthodox PyTorch represent batch sparse tensor way hence checking tensor invariants locally turned off a_batched = torch sparse_csr_tensor crow_indices col_indices values batch_size m k check_invariants=False b = make_tensor batch_size k n dtype=dtype device=device noncontiguous=noncontiguous op_b op_out itertools product True False repeat= run_test a_batched b op_b op_out dtype=dtype device=device run_test_block_addmm_addmv addmv_addmm c b op_b=False op_out=False dtype=None device=None ref=_npref_block_addmm_addmv alpha = complex random random random random dtype is_complex random random beta = complex random random random random dtype is_complex random random b = b mH op_b shape == b shape b actual = addmv_addmm c b alpha=alpha beta=beta out = torch empty_like c mH op_out shape == b shape c addmv_addmm c b alpha=alpha beta=beta out=out expected = ref c b alpha beta assertEqual actual out assertEqual actual expected lambda msg f msg \na= \nc= c \nb= b \nalpha= alpha beta= beta TODO block_size broken parametrize block_size parametrize index_dtype torch int torch int parametrize noncontiguous True False skipCPUIfNoMklSparse unittest skipIf TEST_SCIPY SciPy found skipIfTorchDynamo raises sparse matrix length ambiguous use getnnz dtypes floating_and_complex_types dtypesIfCUDA floating_and_complex_types_and torch half SM OrLater torch bfloat SM OrLater precisionOverride torch float e- torch complex e- torch float e- torch complex e- torch float e- torch bfloat e- test_block_addmm device dtype index_dtype block_size noncontiguous make_transposed_addmm_op f tt t isinstance t torch Tensor t transpose - - assume numpy scipy spmatrix t transpose functools wraps f wrapper c b alpha=None beta=None out=None out None ref takes no out kwarg assert isinstance out torch Tensor transpose inplace propagate out checking context out transpose_ - - f tt c tt b tt alpha=alpha beta=beta out=out f tt c tt b tt alpha=alpha beta=beta wrapper ref_sp_numpy c b alpha=None beta=None out=None prep_input t to_sp_block_compressed t t layout torch sparse_bsc tt = t transpose - - tt = t t_sp_bsr = sp bsr_matrix tt values cpu numpy tt col_indices cpu numpy tt crow_indices cpu numpy shape=tt shape t layout torch sparse_bsc t_sp_bsr transpose t_sp_bsr t layout torch strided to_sp_block_compressed t t cpu resolve_conj numpy res = _npref_block_addmm_addmv prep_input t t c b alpha beta out None out copy_ res out res ref_half_bfloat c b alpha=None beta=None out=None res = alpha to_dense torch float b to_dense torch float dtype + beta c out None out copy_ res out res dtype torch half torch bfloat ref = ref_half_bfloat ref = ref_sp_numpy m n k itertools product repeat= nnz = random randint m k = genSparseCSRTensor m k nnz dtype=dtype device=device index_dtype=index_dtype a_data = make_tensor nnz block_size block_size dtype=dtype device=device a_data = a_data mT noncontiguous a_data = torch sparse_bsr_tensor crow_indices col_indices a_data m block_size k block_size check_invariants=False b = make_tensor k block_size n block_size dtype=dtype device=device noncontiguous=noncontiguous c = make_tensor m block_size n block_size dtype=dtype device=device noncontiguous=noncontiguous op_b op_out itertools product True False repeat= run_test_block_addmm_addmv torch addmm c b op_b op_out dtype=dtype device=device ref=ref run_test_block_addmm_addmv make_transposed_addmm_op torch addmm c b op_b op_out dtype=dtype device=device ref=make_transposed_addmm_op ref parametrize block_size parametrize index_dtype torch int torch int parametrize noncontiguous True False unittest skipIf TEST_SCIPY SciPy found dtypes torch float torch float torch complex torch complex test_block_addmv device dtype index_dtype block_size noncontiguous TODO Explicitly disable block size support TEST_WITH_ROCM TEST_CUSPARSE_GENERIC block_size == ref_block_addmv c b alpha beta _npref_block_addmm_addmv c to_dense b alpha beta m k itertools product repeat= nnz = random randint m k noncontiguous = genSparseCSRTensor m block_size k block_size nnz dtype=dtype device=device index_dtype=index_dtype = to_sparse_bsr block_size block_size = genSparseCSRTensor m k nnz dtype=dtype device=device index_dtype=index_dtype a_data = make_tensor nnz block_size block_size dtype=dtype device=device a_data = a_data mT noncontiguous a_data Test column-major blocks = torch sparse_bsr_tensor crow_indices col_indices a_data m block_size k block_size check_invariants=False b = make_tensor k block_size dtype=dtype device=device noncontiguous=noncontiguous c = make_tensor m block_size dtype=dtype device=device noncontiguous=noncontiguous run_test_block_addmm_addmv torch addmv c b dtype=dtype device=device ref=ref_block_addmv parametrize matrix_shape name_fn=lambda x shape_ x format x dtypes torch float torch float torch complex torch complex onlyCPU test_addmv device dtype matrix_shape mat = torch randn matrix_shape dtype=dtype device=device mat mat real = sparse_mat = mat to_sparse_csr mvec = torch randn mat size dtype=dtype device=device avec = torch randn mat size dtype=torch float device=device ref_output = torch addmv avec mat mvec output = torch addmv avec sparse_mat mvec assertEqual ref_output output parametrize block_size parametrize index_dtype torch int torch int parametrize noncontiguous True False skipCPUIfNoMklSparse unittest skipIf TEST_SCIPY SciPy found dtypes torch float torch float torch complex torch complex test_block_triangular_solve device dtype index_dtype block_size noncontiguous run_test b upper transpose unitriangular op_out unitriangular device_type == cpu TODO When unitriangular=True results correct CPU upper device_type == cpu TODO When upper=False some generated inputs might crash CPU actual = torch triangular_solve b upper=upper unitriangular=unitriangular transpose=transpose actual_X = actual solution actual_A_clone = actual cloned_coefficient assertTrue actual_A_clone numel == _nnz == assertTrue actual_X isnan all TODO replace torch method when implemented to_dense block sparse tensor a_bsr = sp bsr_matrix values cpu numpy col_indices cpu numpy crow_indices cpu numpy shape=a shape expected_X _ = torch triangular_solve b torch tensor a_bsr todense device=device transpose=transpose upper=upper unitriangular=unitriangular expected_X isnan any TODO zeros diagonal handled CPU path there s no way query info MKL device_type == cuda TEST_WITH_ROCM assertTrue actual_X isnan any actual_X isinf any assertEqual actual_X expected_X out = torch empty_like b mH op_out shape == b shape b torch triangular_solve b upper=upper unitriangular=unitriangular transpose=transpose out= out actual_A_clone assertEqual out actual_X assertEqual out expected_X m k itertools product nnz = random randint m m noncontiguous = genSparseCSRTensor m block_size m block_size nnz dtype=dtype device=device index_dtype=index_dtype = to_sparse_bsr block_size block_size = genSparseCSRTensor m m nnz dtype=dtype device=device index_dtype=index_dtype a_data = make_tensor nnz block_size block_size dtype=dtype device=device a_data = a_data mT noncontiguous a_data Test column-major blocks = torch sparse_bsr_tensor crow_indices col_indices a_data m block_size m block_size check_invariants=False b = make_tensor m block_size k dtype=dtype device=device noncontiguous=noncontiguous upper unitriangular transpose op_out itertools product True False repeat= run_test b upper unitriangular transpose op_out skipCPUIfNoMklSparse skipIfRocmVersionLessThan dtypes torch double test_mm device dtype test_shape di dj dk nnz =None nnz =None index_dtype torch int torch int alpha = random random beta = random random _test_addmm t x y TODO addmm doesn t support strided result sparse inputs res = beta t + alpha x y res = torch addmm t x y beta=beta alpha=alpha expected = torch addmm t x to_dense y to_dense beta=beta alpha=alpha assertEqual res expected res = torch addmm t x y expected = torch addmm t x to_dense y to_dense assertEqual res expected _test_mm x y res = torch mm x y expected = torch mm x to_dense y to_dense x layout torch strided y layout torch strided assertEqual res layout torch strided assertEqual res layout torch sparse_csr assertEqual res to_dense expected _test t x y _test_addmm t x y _test_mm x y nnz None nnz = random randint di dk di dk t = torch randn di dj dtype=dtype device=device x = genSparseCSRTensor di dk nnz device=device dtype=dtype index_dtype=index_dtype y = torch randn dk dj dtype=dtype device=device _test t x y t = torch randn di dj dtype=dtype device=device x = genSparseCSCTensor di dk nnz device=device dtype=dtype index_dtype=index_dtype y = torch randn dk dj dtype=dtype device=device _test t x y nnz None nnz = random randint dk dj dk dj t = torch randn di dj dtype=dtype device=device x = torch randn di dk dtype=dtype device=device y = genSparseCSRTensor dk dj nnz device=device dtype=dtype index_dtype=index_dtype _test t x y t = torch randn di dj dtype=dtype device=device x = torch randn di dk dtype=dtype device=device y = genSparseCSCTensor dk dj nnz device=device dtype=dtype index_dtype=index_dtype _test t x y x_shape y_shape = x shape y shape gen_csr_csc = genSparseCSRTensor genSparseCSCTensor Test mm CSR CSC CSR CSC gen_x gen_y itertools product gen_csr_csc gen_csr_csc x = gen_x x_shape nnz device=device dtype=dtype index_dtype=index_dtype y = gen_y y_shape nnz device=device dtype=dtype index_dtype=index_dtype _test_mm x y test_empty_inputs lhs_layout rhs_layout xd = torch rand device=device dtype=dtype yd = xd transpose - - zd = torch rand device=device dtype=dtype xls yls zls = t to_sparse layout=lhs_layout t xd yd zd xrs yrs zrs = t to_sparse layout=rhs_layout t xd yd zd ls rs ld rd xls yrs xd yd xls zrs xd zd zls yrs zd yd zls zrs zd zd res_sparse = ls rs res_dense = ld rd assertEqual res_sparse to_dense res_dense test_orthogonal_inputs lhs_layout rhs_layout ones = torch ones device=device dtype=dtype zeros = torch zeros device=device dtype=dtype x = torch cat ones zeros - to_sparse layout=lhs_layout y = torch cat zeros ones - to_sparse layout=rhs_layout res = x y res_expected = torch zeros res shape device=device dtype=dtype layout=res layout assertEqual res res_expected lhs_layout rhs_layout itertools product torch sparse_csr torch sparse_csc repeat= test_empty_inputs lhs_layout rhs_layout test_orthogonal_inputs lhs_layout rhs_layout i j k test_shape i j k test_shape skipCPUIfNoMklSparse dtypes floating_and_complex_types dtypesIfCUDA floating_and_complex_types_and torch half SM OrLater TEST_CUSPARSE_GENERIC torch bfloat SM OrLater TEST_CUSPARSE_GENERIC precisionOverride torch bfloat e- torch float e- test_sparse_mm device dtype test_shape d d d nnz transposed index_dtype transposed D = torch randn d d dtype=dtype device=device t_ D = torch randn d d dtype=dtype device=device S = genSparseCSRTensor d d nnz device=device dtype=dtype index_dtype=index_dtype S_dense = S to_dense assertEqual torch sparse mm S D torch mm S_dense D index_dtype torch int torch int test_shape False index_dtype test_shape True index_dtype dtypes floating_and_complex_types dtypesIfCUDA floating_and_complex_types_and torch half SM OrLater TEST_CUSPARSE_GENERIC torch bfloat SM OrLater TEST_CUSPARSE_GENERIC precisionOverride torch bfloat e- torch float e- test_sparse_addmm device dtype test_shape m n p nnz broadcast index_dtype alpha_beta=None alpha_beta None alpha = random random beta = random random alpha beta = alpha_beta broadcast D = make_tensor dtype=dtype device=device D = make_tensor n p dtype=dtype device=device D = make_tensor m p dtype=dtype device=device S = genSparseCSRTensor n m nnz dtype=dtype device=device index_dtype=index_dtype S_dense = S to_dense Y = torch sparse addmm D S D beta=beta alpha=alpha Y_dense = torch addmm D S_dense D beta=beta alpha=alpha assertEqual Y Y_dense index_dtype torch int torch int test_shape False index_dtype None test_shape True index_dtype None test_shape False index_dtype test_shape True index_dtype test_shape False index_dtype test_shape True index_dtype skipCPUIfNoMklSparse skipIfRocmVersionLessThan dtypes floating_and_complex_types precisionOverride torch double e- torch float e- torch bfloat torch half e- torch cfloat e- torch cdouble e- dtypesIfCUDA floating_types_and torch complex torch bfloat SM OrLater TEST_WITH_ROCM torch half SM OrLater TEST_WITH_ROCM torch complex CUSPARSE_SPMM_COMPLEX _SUPPORTED HIPSPARSE_SPMM_COMPLEX _SUPPORTED sparse_compressed_nonblock_layouts test_addmm_all_sparse_csr device dtype layout M = torch randn device=device dtype m = torch randn device=device dtype m = torch randn device=device dtype _test_addmm_addmv torch addmm M m m layout=layout mode= all_sparse Test -strided M = torch randn device=device dtype expand m = torch randn device=device dtype expand m = torch randn device=device dtype _test_addmm_addmv torch addmm M m m layout=layout mode= all_sparse Test beta= M=nan M = torch full float nan device=device dtype m = torch randn device=device dtype m = torch randn device=device dtype _test_addmm_addmv torch addmm M m m beta= layout=layout mode= all_sparse Test transpose t t t t itertools product True False repeat= maybe_transpose cond m cond m m t clone memory_format=torch contiguous_format t M = maybe_transpose t torch randn device=device dtype m = maybe_transpose t torch randn device=device dtype m = maybe_transpose t torch randn device=device dtype _test_addmm_addmv torch addmm M m m transpose_out=t layout=layout mode= all_sparse onlyCPU skipCPUIfNoMklSparse dtypes floating_and_complex_types sparse_compressed_nonblock_layouts test_addmm_dense_result device dtype layout M = torch randn device=device dtype m = torch randn device=device dtype m = torch randn device=device dtype _test_addmm_addmv torch addmm M m m layout=layout mode= dense_result Test -strided M = torch randn device=device dtype expand m = torch randn device=device dtype expand m = torch randn device=device dtype _test_addmm_addmv torch addmm M m m layout=layout mode= dense_result Test beta= M=nan M = torch full float nan device=device dtype m = torch randn device=device dtype m = torch randn device=device dtype _test_addmm_addmv torch addmm M m m beta= layout=layout mode= dense_result Test transpose t t t t itertools product True False repeat= maybe_transpose cond m cond m m t clone memory_format=torch contiguous_format t M = maybe_transpose t torch randn device=device dtype m = maybe_transpose t torch randn device=device dtype m = maybe_transpose t torch randn device=device dtype _test_addmm_addmv torch addmm M m m transpose_out=t layout=layout mode= dense_result parametrize k parametrize n parametrize m skipCPUIfNoMklSparse dtypes floating_and_complex_types dtypesIfCUDA floating_types_and torch complex torch bfloat SM OrLater TEST_WITH_ROCM torch half SM OrLater TEST_WITH_ROCM torch complex CUSPARSE_SPMM_COMPLEX _SUPPORTED HIPSPARSE_SPMM_COMPLEX _SUPPORTED precisionOverride torch double e- torch float e- torch bfloat torch half e- torch cfloat e- torch cdouble e- test_addmm_sizes_all_sparse_csr device dtype m n k M = torch randn n m device=device dtype m = torch randn n k device=device dtype m = torch randn k m device=device dtype _test_addmm_addmv torch addmm M m m layout=torch sparse_csr mode= all_sparse M = torch randn n m device=device dtype to_sparse_csr m = torch randn n k + device=device dtype to_sparse_csr m = torch randn k m device=device dtype to_sparse_csr assertRaisesRegex RuntimeError f n x k + k x m lambda torch addmm M m m assertRaisesRegex RuntimeError f n x k + k x m lambda torch mm m m skipCPUIfNoMklSparse dtypes torch float test_addmm_errors device dtype test errors same dense sparse versions re test is_sparse shapes must compatible matrix multiplication = make_tensor dtype=dtype device=device is_sparse a_sparse = to_sparse_csr torch addmm a_sparse torch addmm test is_sparse mat must matrix = make_tensor dtype=dtype device=device is_sparse a_sparse = to_sparse_csr torch addmm a_sparse unsqueeze torch addmm unsqueeze test is_sparse first input needs D D = make_tensor dtype=dtype device=device is_sparse a_sparse = to_sparse_csr torch addmm unsqueeze a_sparse torch addmm unsqueeze test test test test try test is_sparse=False except RuntimeError msg assertRaisesRegex RuntimeError re escape str msg test is_sparse=True skipCPUIfNoMklSparse dtypes torch float test_mm_errors device dtype test errors same dense sparse versions re test is_sparse shapes must compatible matrix multiplication = make_tensor dtype=dtype device=device is_sparse a_sparse = to_sparse_csr torch mm a_sparse torch mm test is_sparse mat must matrix = make_tensor dtype=dtype device=device is_sparse a_sparse = to_sparse_csr torch mm a_sparse unsqueeze torch mm unsqueeze test test test try test is_sparse=False except RuntimeError msg assertRaisesRegex RuntimeError re escape str msg test is_sparse=True sparse_compressed_nonblock_layouts dtypes torch float torch double test_add device layout dtype _test_spadd_shape nnz shape sparse to_dense uses torch add internally so torch add wrong dense tensor will wrong test would still pass there s separate test checks correctness to_dense call x = genSparseCompressedTensor shape nnz dtype=dtype device=device index_dtype=torch int layout=layout blocksize= y = torch randn shape dtype=dtype device=device r = random random res = torch add y x alpha=r expected = y + r x to_dense assertEqual res expected res_perm = torch add x y alpha=r assertEqual res_perm expected Non contiguous dense tensor s = list shape s = shape - s - = shape y = torch randn s dtype=torch double device=device y transpose_ len s - r = random random res = torch add y x alpha=r expected = y + r x to_dense res_perm = torch add x y alpha=r assertEqual res expected assertEqual res_perm expected ns = batch_shapes = b m n itertools product batch_shapes ns ns _test_spadd_shape b m n _test_spadd_shape m n b m n _test_spadd_shape m n b m n dtypes torch float torch double test_mul device dtype TODO This whole test should migrated OpInfos _test_spadd_shape fn nnz shape x = genSparseCSRTensor shape nnz dtype=dtype device=device index_dtype=torch int y = genSparseCSRTensor shape nnz dtype=dtype device=device index_dtype=torch int Forward comparison res_sparse_sparse = fn y x res_dense_sparse = fn y to_dense x res_sparse_dense = fn y x to_dense expected = fn y to_dense x to_dense assertEqual res_sparse_sparse expected TODO While result mul dense csr csr fully compressed That means may contain materialized zeros since dense argument converted according sparsity pattern csr In future we might require result fully compressed assertEqual res_dense_sparse expected assertEqual res_sparse_dense expected Grad comparison x = genSparseCSRTensor shape nnz dtype=dtype device=device index_dtype=torch int y = genSparseCSRTensor shape nnz dtype=dtype device=device index_dtype=torch int z = genSparseCSRTensor shape nnz dtype=dtype device=device index_dtype=torch int csr csr - csr csr csr gradients x_a = x clone requires_grad_ y_a = y clone requires_grad_ fn y_a x_a backward z x_dense_a = x to_dense requires_grad_ y_dense_a = y to_dense requires_grad_ fn y_dense_a x_dense_a backward z to_dense assertEqual x_a grad layout torch sparse_csr assertEqual y_a grad layout torch sparse_csr assertEqual x_a grad to_dense x_dense_a grad assertEqual y_a grad to_dense y_dense_a grad TODO Currently strided Tensors cannot have csr gradients dense csr - csr csr dense gradients x_a = x clone requires_grad_ y_a = y to_dense clone requires_grad_ err_msg = Function MulBackward returned invalid gradient index - expected layout Strided got SparseCsr assertRaisesRegex RuntimeError err_msg fn y_a x_a backward z csr dense - csr dense csr gradients x_a = x to_dense clone requires_grad_ y_a = y clone requires_grad_ err_msg = Function MulBackward returned invalid gradient index - expected layout Strided got SparseCsr assertRaisesRegex RuntimeError err_msg fn y_a x_a backward z _test_spadd_shape torch mul _test_spadd_shape torch mul _test_spadd_shape torch mul _test_spadd_shape torch mul TODO enable hybrid once to_dense supports parametrize enable_hybrid False all_sparse_compressed_layouts dtypes all_types_and_complex_and torch bool torch bfloat torch half test_mul_scalar layout device dtype enable_hybrid sparse generate_simple_inputs layout device=device dtype=dtype index_dtype=torch int enable_hybrid=enable_hybrid scalar_dtype all_types_and_complex_and torch bool torch bfloat torch half ComplexHalf experimental dtype torch half scalar_dtype is_complex continue scalar_t = torch tensor dtype=scalar_dtype scalar scalar_t scalar_t item res_out = sparse mul scalar assertEqual res_out scalar sparse res_dense_out = sparse to_dense mul scalar BUG dispatcher ignores mul Scalar Tensor Scalar This issues circumvented mul Tensor Tensor kernel assertEqual res_out res_dense_out dtype == torch result_type sparse scalar res_in_dense = sparse to_dense mul_ scalar res_in = sparse clone mul_ scalar assertEqual res_in res_in_dense assertEqual res_out res_in skipCPUIfNoMklSparse dtypes torch float torch float torch complex torch complex test_sparse_add device dtype run_test m n index_dtype alpha = random random nnz = random randint m n nnz = random randint m n nnz = random randint m n TEST_WITH_ROCM ROCm fails when nnz = nnz nnz nnz = max nnz max nnz max nnz S = genSparseCSRTensor m n nnz dtype=dtype device=device index_dtype=index_dtype S = genSparseCSRTensor m n nnz dtype=dtype device=device index_dtype=index_dtype S = genSparseCSRTensor m n nnz dtype=dtype device=device index_dtype=index_dtype sparse_args = S S S dense_args = t to_dense t sparse_args arg_idx = list range len sparse_args out_idx = arg_idx + None idx idx idx itertools product arg_idx arg_idx out_idx s = sparse_args idx s = sparse_args idx s = None idx None sparse_args idx d = dense_args idx d = dense_args idx d = None idx None dense_args idx expected = torch add d d alpha=alpha out=d actual = torch add s s alpha=alpha out=s assertEqual actual crow_indices dtype index_dtype assertEqual actual col_indices dtype index_dtype assertEqual actual expected assertEqual s d s None assertEqual s crow_indices dtype index_dtype assertEqual s col_indices dtype index_dtype index_dtype torch int torch int m n itertools product run_test m n index_dtype dtypes torch float torch float torch complex torch complex test_sparse_add_errors device dtype run_test index_type = genSparseCSRTensor dtype=dtype device=device index_dtype=index_dtype b = genSparseCSRTensor dtype=dtype device=device index_dtype=index_dtype assertRaisesRegex RuntimeError Expected input tensors have same shape torch add b index_dtype torch int torch int run_test index_dtype skipCPUIfNoMklSparse skipCUDAIfRocm msg= needs HIPSPARSE_GENERIC_SPSV SPSM dtypes torch float torch float torch complex torch complex precisionOverride torch float e- torch complex e- torch float e- torch complex e- test_sparse_triangular_solve device dtype run_test n k upper unitriangular transpose zero unitriangular triangle_function = torch triu upper torch tril Make sure diagonal elements materialized This exercise ` unitriangular=True ` relying explicit presence these indices upper remove_diagonal t t triu - remove_diagonal t t tril - triangle_function = remove_diagonal make_A = torch zeros zero make_tensor A = make_A n n dtype=dtype device=device A = triangle_function A A_sparse = A to_sparse_csr B = make_tensor n k dtype=dtype device=device expected = torch triangular_solve B A upper=upper unitriangular=unitriangular transpose=transpose expected_X = expected solution actual = torch triangular_solve B A_sparse upper=upper unitriangular=unitriangular transpose=transpose actual_X = actual solution actual_A_clone = actual cloned_coefficient assertTrue actual_A_clone numel == A_sparse _nnz == assertTrue actual_X isnan all assertEqual actual_X expected_X test out C contiguous strides out = torch empty_strided n k k dtype=dtype device=device torch triangular_solve B A_sparse upper=upper unitriangular=unitriangular transpose=transpose out= out actual_A_clone assertEqual out expected_X test out F contiguous strides out = torch empty_strided n k n dtype=dtype device=device torch triangular_solve B A_sparse upper=upper unitriangular=unitriangular transpose=transpose out= out actual_A_clone assertEqual out expected_X assertEqual out stride n test out discontiguous strides out = torch empty_strided n k n dtype=dtype device=device n k assertFalse out is_contiguous assertFalse out t is_contiguous before_stride = out stride torch triangular_solve B A_sparse upper=upper unitriangular=unitriangular transpose=transpose out= out actual_A_clone assertEqual out expected_X assertEqual out stride before_stride ks = ns = k n upper unitriangular transpose zero itertools product itertools product ks ns itertools product True False repeat= run_test n k upper unitriangular transpose zero dtypes torch float torch float torch complex torch complex precisionOverride torch float e- torch complex e- torch float e- torch complex e- test_sampled_addmm device dtype run_test c b op_a op_b alpha=None beta=None dtype is_complex alpha = random random + j alpha None alpha beta = random random + j beta None beta alpha = random random alpha None alpha beta = random random beta None beta op_a shape == b shape = mH op_b shape == b shape b = b mH actual = torch sparse sampled_addmm c b alpha=alpha beta=beta out = torch sparse_csr_tensor map torch clone actual crow_indices actual col_indices torch empty_like actual values size=actual shape torch sparse sampled_addmm c b alpha=alpha beta=beta out=out spy_c = torch sparse_csr_tensor c crow_indices c col_indices torch ones_like c values size=c shape expected = alpha b spy_c to_dense + beta c to_dense assertEqual actual to_dense out to_dense assertEqual actual to_dense expected mnk = list itertools product repeat= Add test case size b tensors mnk = mnk + batch_shapes = tf = True False index_dtype torch int torch int m n k b noncontiguous bcast_c itertools product mnk batch_shapes tf tf bcast_c len b == continue nnz = random randint m n c_batch = bcast_c b c = genSparseCSRTensor c_batch m n nnz dtype=dtype device=device index_dtype=index_dtype = make_tensor b m k dtype=dtype device=device noncontiguous=noncontiguous b = make_tensor b k n dtype=dtype device=device noncontiguous=noncontiguous op_a op_b itertools product True False repeat= run_test c b op_a op_b dtypes torch float torch float torch complex torch complex test_sampled_addmm_autograd device dtype torch testing _internal common_methods_invocations sample_inputs_sparse_sampled_addmm samples = list sample_inputs_sparse_sampled_addmm None device dtype requires_grad=True sample dense_covector zip samples True False c = sample input = sample args b = sample args Compute sparse result output = torch sparse sampled_addmm c b sample kwargs covector = torch randn_like output to_dense dense_covector torch randn_like output output backward covector Compute dense result compare sparse result c b = x detach to_dense requires_grad_ True x c b dense_output = sample kwargs alpha b torch ones_like c to_dense + sample kwargs beta c assertEqual output dense_output dense_covector = covector to_dense dense_output backward dense_covector assertEqual c grad c grad assertEqual grad grad assertEqual b grad b grad skipCUDAIfRocm onlyCUDA skipCUDAIf True Causes CUDA memory exception see https github com pytorch pytorch issues dtypes torch float torch float torch complex torch complex precisionOverride torch float e- torch complex e- torch float e- torch complex e- test_sampled_addmm_zero_sized device dtype run_test c b actual = torch sparse sampled_addmm c b assertEqual actual shape c shape m n k itertools product repeat= c = torch empty m n dtype=dtype device=device layout=torch sparse_csr = make_tensor m k dtype=dtype device=device b = make_tensor k n dtype=dtype device=device run_test c b onlyCUDA dtypes torch float torch float torch complex torch complex test_sampled_addmm_errors device dtype test errors same dense sparse sampled versions re shapes must compatible matrix multiplication = make_tensor dtype=dtype device=device a_sparse = to_sparse_csr assertRaisesRegex RuntimeError r cannot multiplied torch sparse sampled_addmm a_sparse mat must matrix assertRaisesRegex RuntimeError r Expected mat matrix torch sparse sampled_addmm a_sparse mat must matrix assertRaisesRegex RuntimeError r Expected mat matrix torch sparse sampled_addmm a_sparse = make_tensor dtype=dtype device=device b = make_tensor dtype=dtype device=device b_sparse = b to_sparse_csr assertRaisesRegex RuntimeError r shape\ - \ must match mat shape\ - \ torch sparse sampled_addmm b_sparse b = make_tensor dtype=dtype device=device b_sparse = b to_sparse_csr assertRaisesRegex RuntimeError r shape\ - \ must match mat shape\ - \ torch sparse sampled_addmm b_sparse = make_tensor dtype=dtype device=device a_sparse = to_sparse_csr assertRaisesRegex RuntimeError r Expected mat have strided layout torch sparse sampled_addmm a_sparse a_sparse a_sparse assertRaisesRegex RuntimeError r Expected mat have strided layout torch sparse sampled_addmm a_sparse a_sparse onlyCPU dtypes torch float torch float torch bfloat torch float precisionOverride torch bfloat test_sparse_mm_reduce_sum device dtype run_test m n k nnz train sparse = genSparseCSRTensor m k nnz dtype=dtype device=device index_dtype=torch int dense = sparse to_dense mat = torch randn k n dtype=dtype ref_mat = mat clone train sparse requires_grad_ mat requires_grad_ dense requires_grad_ ref_mat requires_grad_ ref_out = torch mm dense ref_mat out = torch sparse mm sparse mat sum assertEqual out ref_out train ref_out sum backward out sum backward grad_input = sparse grad ref_grad_input = dense grad grad_mat = mat grad ref_grad_mat = ref_mat grad assertEqual grad_input to_dense ref_grad_input assertEqual grad_mat ref_grad_mat run_test False run_test True skipIfTorchDynamo onlyCPU dtypes torch float torch float torch bfloat torch float precisionOverride torch bfloat torch float test_sparse_mm_reduce device dtype run_test m n k nnz reduce_type index_dtype train csr = genSparseCSRTensor m n nnz dtype=dtype device=device index_dtype=index_dtype mat = torch randn n k dtype=dtype ref_mat = mat clone ref_values = csr values clone out_int = index_dtype == torch int coo_indices = torch _convert_indices_from_csr_to_coo csr crow_indices csr col_indices out_int =out_int row col = coo_indices coo_indices ref row col val mat out = torch zeros m k dtype=dtype weight = mat index_select col src = weight mul val view - index = row view - expand_as weight index = index dtype=torch int scatter_reduce expect index int out scatter_reduce_ index src reduce=reduce_type include_self=False out train csr requires_grad_ mat requires_grad_ ref_values requires_grad_ ref_mat requires_grad_ ref_out = ref row col ref_values ref_mat out = torch sparse mm csr mat reduce_type assertEqual out ref_out train dtype torch bfloat torch float ref_out sum backward out sum backward grad_values = csr grad values grad_weight = mat grad ref_grad_values = ref_values grad ref_grad_weight = ref_mat grad assertEqual grad_values ref_grad_values assertEqual grad_weight ref_grad_weight train False True index_dtype torch int torch int reduce_type sum mean amax amin setting nnz M create empty rows run_test reduce_type index_dtype train run_test reduce_type index_dtype train run_test reduce_type index_dtype train we doing blocking x vector length kernel so need test when K x vector length run_test reduce_type index_dtype train skipMeta dtypes all_types_and_complex_and torch half torch bool torch bfloat test_coo_csr_conversion device dtype m n itertools product size = m n dense = make_tensor size dtype=dtype device=device coo_sparse = dense to_sparse csr_sparse = coo_sparse to_sparse_csr assertEqual csr_sparse to_dense dense skipMeta dtypes all_types_and_complex_and torch half torch bool torch bfloat test_csr_coo_conversion device dtype m n itertools product size = m n dense = make_tensor size dtype=dtype device=device csr_sparse = dense to_sparse_csr coo_sparse = csr_sparse to_sparse assertEqual coo_sparse to_dense dense Currently there no rule PyTorch filling zeros outputs operations Sparse CSR tensors Hence only those operators supported which have - correspondence example sin = tan = cos = hence s supported Note here we do test only unary operators ops sparse_csr_unary_ufuncs test_zero_to_zero_correspondence_unary device dtype op zero = torch zeros dtype=dtype device=device tensor_explicit_zeros = torch sparse_csr_tensor dtype=dtype device=device output_zero = op zero expected_zero = zero output_zero dtype output_explicit_zeros = op tensor_explicit_zeros to_dense expected_explicit_zeros = tensor_explicit_zeros to_dense output_explicit_zeros dtype output expected output_zero expected_zero output_explicit_zeros expected_explicit_zeros assertEqual output expected f This operator op name should supported Sparse CSR breaks - correspondence inp zero to_sparse_csr tensor_explicit_zeros assertEqual op inp values numel inp values numel f op name fails preserve sparsity pattern ops sparse_csr_unary_ufuncs test_sparse_csr_unary_out device dtype op samples = op sample_inputs device dtype op supports_out skipTest Skipped Out supported sample samples assert torch is_tensor sample input Sparse CSR only supports D tensors inputs Fail early prevent silent success test sample input ndim = raise ValueError f Expected D tensor got tensor dimension sample input ndim sample input = sample input to_sparse_csr expect = op sample input sample args sample kwargs out = genSparseCSRTensor sample input size sample input _nnz device=sample input device dtype=expect dtype index_dtype=sample input crow_indices dtype op sample input sample args sample kwargs out=out assertEqual out expect ops sparse_csr_unary_ufuncs test_sparse_csr_unary_inplace device dtype op samples = op sample_inputs device dtype op inplace_variant None skipTest Skipped Inplace variant supported sample samples assert torch is_tensor sample input Sparse CSR only supports D tensors inputs Fail early prevent silent success test sample input ndim = raise ValueError f Expected D tensor got tensor dimension sample input ndim sample input = sample input to_sparse_csr expect = op sample input sample args sample kwargs torch can_cast expect dtype dtype assertRaisesRegex RuntimeError result type op inplace_variant sample input sample args sample kwargs continue sample input is_complex op name == abs assertRaisesRegex RuntimeError supported op inplace_variant sample input sample args sample kwargs continue actual = op inplace_variant sample input sample args sample kwargs assertIs actual sample input assertEqual actual expect skipIfTorchDynamo Not TorchDynamo suitable test ops sparse_csr_unary_ufuncs dtypes=OpDTypes supported allowed_dtypes= torch double torch cdouble test_autograd_sparse_csr_unary device dtype op op name UNARY_EWISE_CSR_ALLOW_AUTOGRAD skipTest f Skipped Unary op op name supported CSR input autograd samples = list op sample_inputs device dtype Fail early prevent silent success test ndims_equals_ d = s input ndim == s samples any ndims_equals_ d raise ValueError Expected least one D tensor samples sample samples We must skip samples low dimensionality we can t convert them sparsed compressed layouts sample input ndim continue sparse_input = sample input to_sparse_csr requires_grad_ True fn input output = op gradcheck_wrapper op get_op input sample args sample kwargs sample output_process_fn_grad None sample output_process_fn_grad output output Compute sparse result output = fn sparse_input covector = torch randn_like output output backward covector assertTrue torch is_tensor sparse_input grad assertTrue sparse_input grad is_sparse_csr Compute dense result compare sparse result dense_input = sparse_input detach to_dense requires_grad_ True dense_output = fn dense_input dense_covector = covector to_dense dense_output backward dense_covector assertEqual sparse_input grad dense_input grad dtypes torch float test_autograd_dense_output_addmm device dtype torch testing _internal common_methods_invocations sample_inputs_addmm samples = list sample_inputs_addmm None device dtype requires_grad=True Fail early prevent silent success test ndims_equals_ d = s args ndim == s samples any ndims_equals_ d raise ValueError Expected least one D tensor samples convert sparse sample samples = sample args relu to_sparse_csr sample args shape == sample args shape warnings warnings warn Broken square matrices see https github com pytorch pytorch issues continue This path tests autograd path wrt dense inputs addmm torch addmm torch sparse addmm fn c b output = addmm c b sample kwargs sample output_process_fn_grad None sample output_process_fn_grad output output assertTrue torch autograd gradcheck fn sample input sample args fast_mode=True noncontiguous c = make_tensor sample input shape device=device dtype=dtype noncontiguous=True requires_grad=True b = make_tensor sample args shape device=device dtype=dtype noncontiguous=True requires_grad=True assertTrue torch autograd gradcheck fn c b fast_mode=True Now test autograd path wrt sparse inputs reverse True False c b = sample input sample args reverse shape = b shape continue fn inputs = c b reverse c b output = addmm inputs sample kwargs sample output_process_fn_grad None sample output_process_fn_grad output output gradcheck doesn t work sparse CSR yet compare against dense path Compute sparse result = detach requires_grad_ True output = fn covector = torch randn_like output output backward covector assertTrue torch is_tensor grad addmm == torch sparse addmm assertTrue grad is_sparse_csr assertTrue grad layout == torch strided Compute dense result compare sparse result dense_a = detach to_dense requires_grad_ True dense_output = fn dense_a assertEqual output dense_output dense_covector = covector to_dense dense_output backward dense_covector addmm == torch sparse addmm assertEqual grad dense_a grad sparse_mask assertEqual grad dense_a grad skipCPUIfNoMklSparse dtypes torch float test_autograd_dense_output_addmv device dtype torch testing _internal common_methods_invocations sample_inputs_addmv samples = list sample_inputs_addmv None device dtype requires_grad=True Fail early prevent silent success test ndims_equals_ d = s args ndim == s samples any ndims_equals_ d raise ValueError Expected least one D tensor samples convert sparse sample samples TODO Remove detach once we have autograd support CSR input = sample args to_sparse_csr detach fn c b output = torch addmv c b sample kwargs sample output_process_fn_grad None sample output_process_fn_grad output output assertTrue torch autograd gradcheck fn sample input sample args fast_mode=True noncontiguous c = make_tensor sample input shape device=device dtype=dtype noncontiguous=True requires_grad=True b = make_tensor sample args shape device=device dtype=dtype noncontiguous=True requires_grad=True assertTrue torch autograd gradcheck fn c b fast_mode=True skipIfTorchDynamo Not TorchDynamo suitable test ops binary_ops_with_dense_output dtypes=OpDTypes supported allowed_dtypes= torch double test_autograd_dense_output device dtype op op name == mv no_mkl_sparse device_type == cpu skipTest MKL Sparse available samples = list op sample_inputs device dtype requires_grad=True Fail early prevent silent success test ndims_equals_ d = s input ndim == s samples any ndims_equals_ d raise ValueError Expected least one D tensor samples Here we assume signature op sparse_input dense_input - dense_output sample samples TODO Remove detach once we have autograd support CSR input sparse_input = sample input to_sparse_csr detach fn args output = op gradcheck_wrapper op get_op sparse_input args sample kwargs sample output_process_fn_grad None sample output_process_fn_grad output output assertTrue torch autograd gradcheck fn sample args fast_mode=True noncontiguous args = make_tensor shape device=device dtype=dtype noncontiguous=True requires_grad=True sample args assertTrue torch autograd gradcheck fn args fast_mode=True dtypes all_types_and_complex test_direct_coo_csr_conversion device dtype m n itertools product size = m n dense = make_tensor size dtype=dtype device=device coo_sparse = dense to_sparse_coo assertEqual coo_sparse to_sparse_csr to_sparse_coo coo_sparse skipMeta dtypes all_types_and_complex_and torch half torch bool torch bfloat test_sum device dtype run_test shape nnz index_type = genSparseCSRTensor shape nnz dtype=dtype device=device index_dtype=index_dtype assertEqual sum values sum dtype floating_types requires_grad_ True sum backward assertEqual grad torch ones shape dtype=dtype device=device shape index_dtype itertools product torch int torch int run_test shape index_dtype run_test shape max shape index_dtype run_test shape shape shape index_dtype skipIfTorchDynamo skipMeta dtypes all_types_and_complex_and torch half torch bool torch bfloat all_sparse_compressed_layouts test_transpose device dtype layout _check_transpose_view subject transpose assertTrue transpose values _is_view assertTrue transpose _is_view assertTrue transpose _base subject _check_layout_invariants transpose assertEqual transpose device torch device device compressed_indices_mth plain_indices_mth = sparse_compressed_indices_methods transpose layout compressed_indices plain_indices = compressed_indices_mth transpose plain_indices_mth transpose torch _validate_sparse_compressed_tensor_args compressed_indices plain_indices transpose values transpose shape transpose layout check_good_transpose subject subject_dense dim dim expected_layout transpose = subject transpose dim dim correct layout assertEqual transpose layout expected_layout transpose must view _check_transpose_view subject transpose result uses unsafe construction so we check invariants _check_layout_invariants transpose assertEqual transpose to_dense subject_dense transpose dim dim round_trip = transpose transpose dim dim assertEqual round_trip layout subject layout transpose must view _check_transpose_view subject round_trip result uses unsafe construction so we check invariants _check_layout_invariants round_trip assertEqual round_trip to_dense subject_dense check_same_dim_transpose subject subject_dense dim transpose = subject transpose dim dim correct layout assertEqual transpose layout subject layout transpose must view _check_transpose_view subject transpose result uses unsafe construction so we check invariants _check_layout_invariants transpose assertEqual transpose to_dense subject_dense check_dim_type_mismatch_throws subject name dim name dim mismatch_name = f dim \\ name \\ dim \\ name \\ err = r transpose\ \ can only transpose dimensions same type \ Batch Sparse Dense\ got + mismatch_name assertRaisesRegex RuntimeError err subject transpose dim dim run_test shape nnz index_type n_dense blocksize= subject = genSparseCompressedTensor shape nnz layout=layout device=device index_dtype=index_type blocksize=blocksize dense_dims=n_dense dtype=dtype sparse = len shape - n_dense - sparse = sparse - dense = sparse + n_dense None dense = dense + n_dense None n_batch = len shape - n_dense - batch = sparse - n_batch None batch = n_batch None sparse_dims = sparse sparse dense_dims = dense dense batch_dims = batch batch named = name d name d zip Batch Sparse Dense batch_dims sparse_dims dense_dims named = name d name d zip Batch Sparse Dense batch_dims sparse_dims dense_dims flipped_layout = torch sparse_csr torch sparse_csc torch sparse_csc torch sparse_csr torch sparse_bsr torch sparse_bsc torch sparse_bsc torch sparse_bsr layout n_dense expect all transpose throw name dim name dim itertools product named named msg = r transpose\ \ hybrid sparse compressed tensors dense dimensions supported dim None dim None assertRaisesRegex RuntimeError msg subject transpose dim dim subject_dense = subject to_dense name dim name dim itertools product named named dim None check_same_dim_transpose subject subject_dense dim dim None name == name expected_layout = flipped_layout name == Sparse layout check_good_transpose subject subject_dense dim dim expected_layout check_dim_type_mismatch_throws subject name dim name dim batch sparse sparse dense only full hybrid cases shape_ndense = list itertools product sparse only cases shape_ndense += shape n_dense index_dtype itertools product shape_ndense torch int torch int n_batch = len shape - n_dense - sparse_shape = shape n_batch n_batch + layout torch sparse_bsr torch sparse_bsc blocked all combinations should valid blocksizes run_test shape index_dtype n_dense blocksize= run_test shape max sparse_shape index_dtype n_dense blocksize= run_test shape sparse_shape sparse_shape index_dtype n_dense blocksize= repeat realistic sparseity case varried block sizes run_test shape max sparse_shape index_dtype n_dense blocksize= run_test shape max sparse_shape index_dtype n_dense blocksize= run_test shape max sparse_shape index_dtype n_dense blocksize= run_test shape index_dtype n_dense run_test shape max sparse_shape index_dtype n_dense run_test shape sparse_shape sparse_shape index_dtype n_dense TODO This stopgap rigorous extension our autograd tests test functionality detach skipMeta dtypes all_types_and_complex_and torch half torch bool torch bfloat test_exercise_detach device dtype shape = nnz = index_dtype torch int torch int inp = genSparseCSRTensor shape nnz dtype=dtype device=device index_dtype=index_dtype detached_inp = inp detach assertEqual inp detached_inp _construct_sp_matrix tensor layout blocksize= tensor layout torch sparse_coo torch sparse_csr torch sparse_csc torch strided tensor = tensor to_dense raise NotImplementedError repr tensor layout torch sparse_csr sp csr_matrix tensor cpu numpy layout torch sparse_csc sp csc_matrix tensor cpu numpy layout torch sparse_bsr sp bsr_matrix tensor cpu numpy blocksize=blocksize sorted_indices layout torch sparse_bsc SciPy doesn t have native BSC support - our tests don t need full functionality so fake using transposed BSR matrix FakeBscMatrix __init__ matrix _matrix = matrix shape = tuple reversed matrix shape indptr = matrix indptr indices = matrix indices data = x transpose x matrix data staticmethod from_matrix matrix blocksize blocksize = tuple reversed blocksize matrix = matrix transpose FakeBscMatrix sp bsr_matrix matrix blocksize=blocksize sorted_indices sub = _matrix sorted_indices FakeBscMatrix sub FakeBscMatrix from_matrix tensor cpu numpy blocksize=blocksize sorted_indices raise NotImplementedError repr tensor skipMeta all_sparse_compressed_layouts to_layout all_sparse_compressed_layouts from_layout test_compressed_layout_conversions_coverage device from_layout to_layout This test performs smoke test covered conversion verifies exception thrown unsupported conversions TODO This test covers subset TestSparseAny test_to_sparse tests can eliminated Keeping test until new ` Tensor to_sparse layout blocksize ` has landed allowed_pairwise_layouts_sets = frozenset torch sparse_csc frozenset torch sparse_csr frozenset torch sparse_csc torch sparse_csr frozenset torch sparse_csc torch sparse_bsc frozenset torch sparse_csc torch sparse_bsr frozenset torch sparse_csr torch sparse_bsc frozenset torch sparse_csr torch sparse_bsr frozenset torch sparse_bsc frozenset torch sparse_bsr frozenset torch sparse_bsc torch sparse_bsr block_layouts = torch sparse_bsr torch sparse_bsc _to_from_layout layout_a layout_b expect_error = True layout_a layout_b allowed_pairwise_layouts_sets expect_error = False BSR - CSR yet supported layout_a layout_b == torch sparse_bsr torch sparse_csr expect_error = True BSR - CSC yet supported layout_a layout_b == torch sparse_bsr torch sparse_csc expect_error = True BSC - CSR yet supported layout_a layout_b == torch sparse_bsc torch sparse_csr expect_error = True BSC - CSC yet supported layout_a layout_b == torch sparse_bsc torch sparse_csc expect_error = True CSR - BSR only works non-batched inputs layout_a layout_b == torch sparse_csr torch sparse_bsr dim expect_error = True CSR - BSC only works non-batched inputs layout_a layout_b == torch sparse_csr torch sparse_bsc dim expect_error = True CSC - BSR only works non-batched inputs layout_a layout_b == torch sparse_csc torch sparse_bsr dim expect_error = True CSC - BSC only works non-batched inputs layout_a layout_b == torch sparse_csc torch sparse_bsc dim expect_error = True blocksize_a = layout_a torch sparse_bsr torch sparse_bsc None blocksize_b = layout_b torch sparse_bsr torch sparse_bsc None b = to_sparse layout=layout_a blocksize=blocksize_a expect_error assertRaises RuntimeError b to_sparse layout=layout_b blocksize=blocksize_b c = b to_sparse layout=layout_b blocksize=blocksize_b assertEqual to_dense c to_dense change blocksize upon conversion yet supported b layout block_layouts block_layout block_layouts assertRaisesRegex RuntimeError conversion blocksize changed supported b to_sparse layout=block_layout blocksize= batch_dims = sparse_dims = batch_dim batch_dims = make_tensor batch_dim + sparse_dims dtype=torch float device=device _to_from_layout from_layout to_layout skipMeta all_sparse_compressed_layouts batched_nonbatched hybrid_nonhybrid unittest skipIf TEST_SCIPY SciPy found test_dense_to_from_sparse_compressed device hybrid batched layout This test tests conversion dense CSR CSC comparing SciPy s implementation Here we test only those conversion combinations SciPy supports ensure PyTorch conversions same page SciPy Independent SciPy all conversion combinations tested TestSparseAny test_to_sparse blocked_layouts = torch sparse_bsr torch sparse_bsc helpers _check_against_scipy_matrix pt_matrix dense blocksize kwargs scipy has no bsc layout so we check against bsr layout transposed dense layout == torch sparse_bsc sp_matrix = _construct_sp_matrix dense t layout=torch sparse_bsr blocksize=blocksize - sp_matrix = _construct_sp_matrix dense layout=layout blocksize=blocksize compressed_indices_mth plain_indices_mth = sparse_compressed_indices_methods layout assertEqual layout pt_matrix layout layout == torch sparse_bsc assertEqual sp_matrix shape - pt_matrix shape assertEqual sp_matrix shape pt_matrix shape assertEqual torch tensor sp_matrix indptr dtype=torch int compressed_indices_mth pt_matrix assertEqual torch tensor sp_matrix indices dtype=torch int plain_indices_mth pt_matrix layout == torch sparse_bsc we must transpose blocks before comparing assertEqual torch tensor sp_matrix data pt_matrix values transpose - - assertEqual torch tensor sp_matrix data pt_matrix values _check_hybrid_matrix pt_matrix dense blocksize kwargs Calculate COO indices sparse matrix compressed_indices_mth plain_indices_mth = sparse_compressed_indices_methods layout compressed_indices = compressed_indices_mth pt_matrix plain_indices = plain_indices_mth pt_matrix coo_indices = torch _convert_indices_from_csr_to_coo compressed_indices plain_indices row_indices col_indices = torch sparse_csr coo_indices coo_indices torch sparse_csc coo_indices coo_indices torch sparse_bsr coo_indices coo_indices torch sparse_bsc coo_indices coo_indices pt_matrix layout If sparse matrix layout blocked rearrange dense matrix so shape past first two dimensions match shape sparse matrix values dense_to_check = dense blocksize dense_shape = dense shape dense_to_check_shape = dense shape blocksize blocksize dense shape blocksize blocksize + dense shape dense_to_check = dense_to_check reshape dense_to_check_shape transpose Verify non-zero values sparse matrix equal corresponding values dense matrix assertEqual pt_matrix values dense_to_check row_indices col_indices Verify remaining elements dense matrix i e dense sparse matrix fully equal mask = torch ones_like dense_to_check dtype=torch bool mask row_indices col_indices = False assertTrue torch all torch masked_select dense_to_check mask == _check_batched pt_tensor dense check_batch=None batch_shape= blocksize= kwargs assertEqual layout pt_tensor layout assertEqual pt_tensor shape dense shape compressed_indices_mth plain_indices_mth = sparse_compressed_indices_methods layout batch_index np ndindex batch_shape pt_matrix = pt_tensor batch_index dense_matrix = dense batch_index dense_dim = pt_matrix dim - dense_matrix_pt = dense_matrix to_sparse layout=layout blocksize=blocksize None dense_dim=dense_dim sanity check selecting batch to_ layout dense batch to_ layout should give same result assertEqual pt_matrix dense_matrix_pt check_batch pt_matrix dense_matrix blocksize kwargs _generate_subject sparse_shape batch_shape hybrid_shape shape = batch_shape + sparse_shape + hybrid_shape n_batch_dim = len batch_shape n_hybrid_dim = len hybrid_shape generate dense tensor dense = make_tensor shape dtype=torch float device=device introduce some sparsty mask sparse shape element applies entire dense sub-tensor hybrid applied each batch mask = make_tensor sparse_shape dtype=torch bool device=device manually expand match hybrid shape hybrid mask = mask view sparse_shape + tuple _ range n_hybrid_dim mask = mask expand sparse_shape + hybrid_shape mask will broadcast over batch dims present dense mask note order important here hybrid-ness decides inner content check which used build batched checker needed check_content = _check_against_scipy_matrix hybrid check_content = _check_hybrid_matrix batched check_content = functools partial _check_batched check_batch=check_content sparse_sizes = blocksizes = layout blocked_layouts batch_sizes = batched hybrid_sizes = hybrid general cases always run sparse_shape blocksize batch_shape hybrid_shape itertools product sparse_sizes blocksizes batch_sizes hybrid_sizes dense = _generate_subject sparse_shape batch_shape hybrid_shape sparse = dense to_sparse layout=layout blocksize=blocksize None dense_dim=len hybrid_shape check_content sparse dense blocksize=blocksize batch_shape=batch_shape hybrid_shape=hybrid_shape dense_back = sparse to_dense assertEqual dense dense_back special cases batched tensors batched batched sparse tensors need only have same number non-zeros each batch necessarily same sparsity pattern each batch sparse_shape = sparse_sizes hybrid_shape = hybrid_sizes batch_shape = batch_sizes shape = batch_shape + sparse_shape + hybrid_shape dense = make_tensor shape dtype=torch float device=device blocksize = blocksizes number elements blocks each batch total nnz batch_mask_shape = sparse_shape layout blocked_layouts we blocked mask generated block valued elements batch_mask_shape = sparse_shape blocksize sparse_shape blocksize random bool vector w length equal max possible nnz sparse_shape mask_source = make_tensor batch_mask_shape dtype=torch bool device=device flatten n_batch = functools reduce operator mul batch_shape stack random permutations source each batch mask = torch stack mask_source torch randperm mask_source numel _ range n_batch dim= reshape batch_shape + batch_mask_shape layout blocked_layouts blocked we need do bit extra work expand mask blocked-space element-space mask_shape = mask shape mask = mask view mask_shape + mask = mask expand mask_shape + blocksize mask = mask transpose - - mask = mask flatten - - flatten - - mask_shape = mask shape mask = mask view mask_shape + len hybrid_shape mask = mask expand mask_shape + hybrid_shape dense = dense mask sparse = dense to_sparse layout=layout blocksize=blocksize None dense_dim=len hybrid_shape check_content sparse dense blocksize=blocksize batch_shape=batch_shape hybrid_shape=hybrid_shape dense_back = sparse to_dense assertEqual dense dense_back batches have different nnz we expect conversion throw mask_ = mask mask_ = mask clone fill_ True mask_ = mask clone fill_ False mask_true = mask_source clone fill_ True mask_false = mask_source clone fill_ False mask = torch stack mask_ mask_ mask_ i i range n_batch dim= reshape batch_shape + mask_ shape dense = make_tensor shape dtype=torch float device=device dense = dense mask msg = Expect same number specified elements per batch assertRaisesRegex RuntimeError msg dense to_sparse layout=layout blocksize=blocksize None Should throw there zero batch size dense = make_tensor + shape dtype=torch float device=device layout_code = str layout split _ - msg = f to_sparse_ layout_code Expected product batch dimensions non-zero assertRaisesRegex RuntimeError msg dense to_sparse layout=layout blocksize=blocksize None skipMeta all_sparse_compressed_layouts coalescedonoff dtypes torch double unittest skipIf TEST_SCIPY SciPy found test_sparse_to_sparse_compressed device dtype coalesced layout This test tests conversion COO CSR CSC CSC CSR CSC comparing SciPy s implementation Here we test only those conversion combinations SciPy supports ensure PyTorch conversions same page SciPy Independent SciPy all conversion combinations tested TestSparseAny test_to_sparse blocksize_kw = layout torch sparse_bsc torch sparse_bsr blocksize_kw blocksize = block modes don t support width height shapes = layout torch sparse_csc torch sparse_csr shapes = raise NotImplementedError unhandled layout layout torch sparse_bsc torch sparse_csc compressed_indices_mth = torch Tensor ccol_indices plain_indices_mth = torch Tensor row_indices layout torch sparse_bsr torch sparse_csr compressed_indices_mth = torch Tensor crow_indices plain_indices_mth = torch Tensor col_indices raise NotImplementedError unhandled layout shape shapes sparse_dim = nnz = shape shape sparse _ _ = genSparseTensor shape sparse_dim nnz coalesced device dtype sp_matrix = _construct_sp_matrix sparse layout pt_matrix = sparse to_sparse layout=layout blocksize_kw assertEqual layout pt_matrix layout assertEqual sp_matrix shape pt_matrix shape assertEqual torch tensor sp_matrix indptr dtype=torch int compressed_indices_mth pt_matrix assertEqual torch tensor sp_matrix indices dtype=torch int plain_indices_mth pt_matrix assertEqual torch tensor sp_matrix data pt_matrix values sparse_csc = sparse to_sparse_csc sp_matrix = _construct_sp_matrix sparse_csc layout pt_matrix = sparse_csc to_sparse layout=layout blocksize_kw assertEqual layout pt_matrix layout assertEqual sp_matrix shape pt_matrix shape assertEqual torch tensor sp_matrix indptr dtype=torch int compressed_indices_mth pt_matrix assertEqual torch tensor sp_matrix indices dtype=torch int plain_indices_mth pt_matrix assertEqual torch tensor sp_matrix data pt_matrix values unittest skipIf TEST_CUDA_CUDSS The test requires cudss dtypes floating_types test_linalg_solve_sparse_csr_cusolver device dtype https github com krshrimali pytorch blob f ee dd c e ba bfd ea cabdf b test test_sparse_csr py try spd = torch rand A = spd T spd b = torch rand cuda A = A to_sparse_csr cuda x = torch sparse spsolve A b except RuntimeError e Calling linear solver sparse tensors requires compiling str e skipTest PyTorch built cuDSS support samples = sample_inputs_linalg_solve None device dtype sample samples sample input ndim = continue out = torch zeros sample args size dtype=dtype device=device sample args ndim = sample args size - = assertRaisesRegex RuntimeError b must D tensor out = torch linalg solve sample input to_sparse_csr sample args sample kwargs break sample args numel assertRaisesRegex RuntimeError Expected non-empty other tensor found empty tensor torch linalg solve sample input to_sparse_csr sample args sample kwargs out=out break expect = torch linalg solve sample input sample args sample kwargs sample input = sample input to_sparse_csr sample args ndim = sample args size - == expect = expect squeeze - sample args = sample args squeeze - out = torch linalg solve sample input sample args sample kwargs assertEqual expect out skipIfNoTriton cls torch utils _triton has_triton no-op triton present has_triton cls functools wraps cls updated= skipped_cls cls setUp skipTest Triton available skipped_cls skipIfNoTriton TestSparseCompressedTritonKernels TestCase _to_block_triangular_inplace d row_block col_block This function modifies ` d ` become upper lower block-triangular in-place It assumed ` d shape - ` divisible ` row_block ` ` d shape - ` divisible ` col_block ` torch sparse _triton_ops tile_to_blocksize m n = d shape - d_tiled = tile_to_blocksize d row_block col_block d_tiled = d_tiled moveaxis - - moveaxis - - m row_block n col_block d_tiled tril_ d_tiled triu_ d onlyCUDA dtypes torch half torch bfloat torch float dtypesIfCUDA torch half torch bfloat SM OrLater torch float unittest skipIf IS_FBCODE IS_REMOTE_GPU Test requires Triton test_triton_bsr_softmax device dtype functools partial torch sparse _triton_ops bsr_softmax tensor = partial make_tensor device=device dtype=dtype low= high= NOTE batch dims zero sizes supported ` to_sparse_bsr ` batches = size = block_size = General correctness row_block col_block b m n itertools product block_size block_size batches size size input = tensor b + m n input diagonal dim =- dim =- fill_ m n input = _to_block_triangular_inplace input row_block col_block bsr = input to_sparse_bsr row_block col_block coo = input to_sparse torch float res_tri = bsr_softmax bsr res_coo = torch sparse softmax coo - assertEqual res_tri res_coo input dtype Test long rows which exceed Triton s max numel limit set input = tensor b + bsr = input to_sparse_bsr assertEqual input softmax - bsr_softmax bsr parametrize block_size parametrize index_dtype torch int torch int onlyCUDA dtypes torch half torch bfloat torch float dtypesIfCUDA torch half torch bfloat SM OrLater torch float unittest skipIf TEST_WITH_TORCHINDUCTOR IS_FBCODE IS_REMOTE_GPU Skipped internal remote GPUs test_triton_bsr_dense_bmm device dtype index_dtype block_size functools partial torch sparse _triton_ops bsr_dense_mm kernel_impl args kwargs bsr_dense_mm args skip_checks=True kwargs kernel = torch _TritonLibrary registerOp _triton_bsr_dense_mm_out _triton_bsr_dense_mm_out Tensor bsr Tensor dense Tensor out - Tensor kernel_impl SparseCsrCUDA kernel = kernel_impl means dispatch already registered This exactly what we need assertTrue kernel kernel_impl Note each value non-zero block range block_size low^ high^ tensor = partial make_tensor device=device dtype=dtype low= high= NOTE batch dims zero sizes supported ` to_sparse_bsr ` batches = size = Whether make inputs orthogonal so product zero make_orthogonal = True False bd bs m n k is_ortho itertools product batches batches size size size make_orthogonal bsr = tensor bs + m k NOTE do get confused will transposed dense = tensor bd + n k is_ortho bsr = torch cat bsr torch zeros_like bsr dim=- dense = torch cat torch zeros_like dense dense dim=- bsr = bsr to_sparse_bsr block_size bsr dim == dtype = torch float Test against linear check dispatch which takes place torch half torch bfloat res_dense = torch nn functional linear dense bsr to_dense res_tri_out = torch empty_like res_dense res_tri = torch nn functional linear dense bsr out=res_tri_out Check dispatch worked non-trivial outputs m n k assertTrue kernel kernel_invoked kernel kernel_invoked = False Otherwise check correctness against bmm since nn linear does support bsr dim res_dense = bsr to_dense dense transpose - - res_tri_out = torch empty_like res_dense res_tri = kernel bsr dense transpose - - out=res_tri_out assertTrue res_tri res_tri_out assertEqual res_tri res_dense res_dense = bsr to_dense dense transpose - - check whether bsr_dense_mm handles different grid sizes None means max possible grid size which CUDA-dependent grid_size = None grid_gen = itertools product grid_size repeat= grid grid_gen res_tri = torch sparse _triton_ops bsr_dense_mm bsr dense transpose - - max_grid=grid assertEqual res_tri res_dense onlyCUDA dtypes torch half unittest skipIf IS_FBCODE IS_REMOTE_GPU Skipped internal remote GPUs test_triton_bsr_dense_bmm_error_messages device dtype torch sparse _triton_ops bsr_dense_mm rhs = torch rand dtype=dtype device=device lhs = rhs to_sparse_bsr assertRaisesRegex ValueError only BSR sparse format supported bsr_dense_mm lhs to_sparse_bsc rhs assertRaisesRegex ValueError same GPU device bsr_dense_mm lhs rhs cpu torch cuda device_count assertRaisesRegex ValueError same GPU device bsr_dense_mm lhs cuda rhs cuda assertRaisesRegex ValueError all inputs expected same dtype bsr_dense_mm lhs rhs torch float assertRaisesRegex ValueError r one \ half bfloat float \ bsr_dense_mm lhs torch double rhs torch double assertRaisesRegex ValueError all inputs involved matrix product expected least D bsr_dense_mm lhs torch rand dtype=dtype device=device assertRaisesRegex ValueError sizes involved matrix product compatible matrix multiplication bsr_dense_mm lhs torch rand dtype=dtype device=device assertRaisesRegex ValueError r dense size\ - \ == should divisible bsr_dense_mm lhs torch rand dtype=dtype device=device Blocksizes check blocksize n = blocksize rhs = torch rand n n dtype=dtype device=device lhs = rhs to_sparse_bsr blocksize assertRaisesRegex ValueError should least power bsr_dense_mm lhs rhs out check rhs = torch rand dtype=dtype device=device lhs = rhs to_sparse_bsr assertRaisesRegex ValueError r ` out ` argument has wrong shape out = torch rand dtype=dtype device=device bsr_dense_mm lhs rhs out=out assertRaisesRegex ValueError r only row-major col-major ` out ` out = torch rand dtype=dtype device=device transpose - bsr_dense_mm lhs rhs out=out parametrize block_size onlyCUDA dtypes torch half torch bfloat torch float dtypesIfCUDA torch half torch bfloat SM OrLater torch float unittest skipIf IS_FBCODE IS_REMOTE_GPU Test requires Triton precisionOverride torch float e- test_triton_scaled_dot_product_attention device dtype block_size functools partial torch sparse _triton_ops _scaled_dot_product_attention Note each value non-zero block range block_size low^ high^ tensor = partial make_tensor device=device dtype=dtype low= high= broadcast_input ts batch_dims = torch broadcast_shapes t shape - t ts yield torch broadcast_to t batch_dims + t shape - t ts NOTE batch dims zero sizes supported ` to_sparse_bsr ` batches = size = bam bq bk bv m n k itertools product batches batches batches batches size size size query = tensor bq + m k key = tensor bk + n k value = tensor bv + n k We make attn_mask block lower upper triangular so BSR Strided function variants directly comparable attn_mask = torch ones bam + m n device=device dtype=torch bool attn_mask = _to_block_triangular_inplace attn_mask block_size block_size attn_mask_bsr = attn_mask to_sparse_bsr block_size NOTE only boolean mask directly compatible Strided version without any pre- post-processing Hence we test against boolean mask scale None scale None query size - == scale = We cast double here dispatches MATH backend which introduces additional rounding steps over fused implementations expected = torch nn functional scaled_dot_product_attention broadcast_input query double key double value double attn_mask scale=scale dtype mask_dtype torch bool dtype res = _scaled_dot_product_attention query key value attn_mask_bsr mask_dtype scale=scale assertEqual res expected parametrize block_size onlyCUDA dtypes torch half torch bfloat torch float dtypesIfCUDA torch half torch bfloat SM OrLater torch float unittest skipIf IS_FBCODE IS_REMOTE_GPU Test requires Triton test_triton_sampled_addmm device dtype block_size functools partial torch sparse _triton_ops sampled_addmm broadcast_batch_dims_bsr Note each value non-zero block range block_size low^ high^ tensor = partial make_tensor device=device dtype=dtype low= high= NOTE batch dims zero sizes supported ` to_sparse_bsr ` batches = size = delta_k = - bi bm bm m n k dk itertools product batches batches batches size size size delta_k Test powers ks well k = max k + dk Non-trivial sparsity pattern Plus tril inputs result also tril so we can compare BSR CSR implementations input = tensor bi + m n tril_ bsr = input to_sparse_bsr block_size mat = tensor bm + m k tril_ mat = tensor bm + k n tril_ batch_dim = torch broadcast_shapes input shape - mat shape - mat shape - csr = input broadcast_to batch_dim + input shape - to_sparse_csr torch float mat csr = mat broadcast_to batch_dim + mat shape - torch float mat csr = mat broadcast_to batch_dim + mat shape - torch float input_broadcasted_clone = broadcast_batch_dims_bsr test_triton_sampled_addmm bsr mat mat clone input_broadcasted_clone = torch sparse_compressed_tensor input_broadcasted_clone crow_indices input_broadcasted_clone col_indices For testing ` out= ` let s make values have weird strides so kernel modifies values s needs result being copied into out values input_broadcasted_clone values transpose - - contiguous transpose - - layout=input_broadcasted_clone layout size=input_broadcasted_clone shape scalars = alpha beta out itertools product scalars scalars None input_broadcasted_clone res_tri = sampled_addmm bsr mat mat alpha=alpha beta=beta out=out out None assertTrue res_tri out batch_broadcasted_shape = torch broadcast_shapes t shape - t input mat mat assertTrue res_tri shape == batch_broadcasted_shape + m n res_csr = torch sparse sampled_addmm csr mat csr mat csr alpha=alpha beta=beta input dtype assertEqual res_tri to_dense res_csr to_dense Check different grid sizes make sure input slicing works input larger than grid grid_size = None grid_gen = itertools product grid_size repeat= grid grid_gen res_tri_grid = sampled_addmm bsr mat mat alpha=alpha beta=beta max_grid=grid assertEqual res_tri res_tri_grid onlyCUDA dtypes torch half torch bfloat torch float dtypesIfCUDA torch half torch bfloat SM OrLater torch float unittest skipIf IS_FBCODE IS_REMOTE_GPU Test requires Triton test_triton_scatter_mm device dtype torch sparse _triton_ops scatter_mm functools partial tensor = partial make_tensor device=device dtype=dtype low= high= sizes = m k n itertools product sizes sizes sizes blocks = torch stack tensor m k tensor m k others = torch stack tensor k n tensor k n expected = torch stack blocks others + blocks others blocks others blocks others indices_data = scatter_mm torch tensor dtype=torch int device=device torch tensor dtype=torch int device=device result = scatter_mm blocks others indices_data=indices_data assertEqual result expected indices_data = bsr_strided_mm torch tensor dtype=torch int device=device torch tensor n n m n m + n dtype=torch int device=device torch tensor dtype=torch int device=device torch tensor k n n k n + n k n k n + n dtype=torch int device=device dict SPLIT_N= is_compressed=False TILE_M=m TILE_N=n GROUP_SIZE= bsize other = tensor bsize k n expected = torch cat torch cat blocks blocks dim= torch cat torch zeros_like blocks blocks dim= dim= other result = scatter_mm blocks other indices_data=indices_data assertEqual result expected parametrize blocksize x x onlyCUDA dtypes torch half torch bfloat torch float dtypesIfCUDA torch half torch bfloat SM OrLater torch float unittest skipIf IS_FBCODE IS_REMOTE_GPU Test requires Triton test_triton_bsr_scatter_mm device dtype blocksize triton torch sparse _triton_ops bsr_scatter_mm bsr_scatter_mm_indices_data functools partial isinstance blocksize str blocksize = tuple map int blocksize split x blocksize = blocksize Note each value non-zero block range blocksize low^ high^ tensor = partial make_tensor device=device dtype=dtype low= high= NOTE batch dims zero sizes supported ` to_sparse_bsr ` batches = sizes = blocksize blocksize blocksize sizes_K = blocksize blocksize bd bs M K N has_zero_row_block itertools product batches batches sizes sizes_K sizes False True bsr_dense = tensor bs + M K has_zero_row_block M blocksize bsr_dense blocksize zero_ continue bsr = bsr_dense to_sparse_bsr blocksize dense = tensor bd + K N expected = bsr to_dense dense indices_format bsr_strided_mm bsr_strided_mm_compressed scatter_mm indices_format bsr_strided_mm bsr_strided_mm_compressed SPLIT_N_list = N while SPLIT_N_list - SPLIT_N_list append max SPLIT_N_list - SPLIT_N_list = SPLIT_N SPLIT_N_list indices_data = bsr_scatter_mm_indices_data bsr dense indices_format=indices_format SPLIT_N=SPLIT_N try result = bsr_scatter_mm bsr dense indices_data=indices_data except triton compiler OutOfResources ensure there least one successful test assert SPLIT_N SPLIT_N_list break assertEqual result expected torch sparse _triton_ops _bsr_scatter_mm_indices_data cache_clear test_TensorAsKey device torch sparse _triton_ops TensorAsKey assertEqualOptions = dict exact_dtype=True exact_device=True exact_layout=True t = torch tensor dtype=torch int device=device key = TensorAsKey t assertTrue key == TensorAsKey t assertTrue key obj t t = t key = TensorAsKey t assertTrue key == key assertEqual key obj t assertEqualOptions deleting object leads dead key del t assertTrue key obj None assertTrue key obj t key different storage offset shape assertFalse key == TensorAsKey t key different strides assertFalse key == TensorAsKey t when object dies make sure key represents dead object well del t assertTrue key obj None Storing tensor dict key d = t = torch tensor dtype=torch int device=device key = TensorAsKey t d key = assertTrue d get key == t _ = t assertTrue d get TensorAsKey t _ == assertTrue d get TensorAsKey t clone None d TensorAsKey t _ = assertTrue d get key == t t _ reference same data so key becomes dead its obj property returns None until all references deleted del t assertTrue key obj None assertTrue d get key == del t _ assertTrue key obj None assertTrue d get key == Storing tensor dict key value d = t = torch tensor dtype=torch int device=device key = TensorAsKey t d key = t assertEqual d get key t assertEqualOptions when object deleted key represents alive object because object referenced dict item value del t assertTrue key obj None This also means life time tensor same life time corresponding dict item del d key assertTrue key obj None Storing tensor dict key value wrapped TensorAsKey d = t = torch tensor dtype=torch int device=device key = TensorAsKey t d key = key assertEqual d get key key assertEqualOptions assertTrue key obj None when object deleted will dead wrapped value hold tensor instance weakref del t assertTrue key obj None key still valid assertEqual d get key key assertEqualOptions suppress_warnings parametrize op bsr_dense_addmm bsr_dense_mm bsr_dense_linear _int_bsr_dense_addmm parametrize blocksize x parametrize out_dtype unspecified int onlyCUDA dtypes torch half torch bfloat torch float torch int dtypesIfCUDA torch half torch bfloat SM OrLater torch float torch int precisionOverride torch float e- unittest skipIf IS_FBCODE IS_REMOTE_GPU Test requires Triton test_triton_kernel op device dtype blocksize out_dtype torch sparse _triton_ops bsr_dense_addmm bsr_dense_mm _int_bsr_dense_addmm torch sparse _triton_ops_meta create_blocked_tensor get_meta optimize_bsr_dense_addmm dump out_dtype == unspecified out_dtype = None op == bsr_dense_addmm out_dtype = getattr torch out_dtype out_dtype is_floating_point = dtype is_floating_point skipTest incompatible out dtype skipTest out dtype implemented bsr_dense_linear input weights bias=None torch nn functional linear input weights bias=bias transpose - - operation = dict bsr_dense_addmm=bsr_dense_addmm bsr_dense_mm=bsr_dense_mm bsr_dense_linear=bsr_dense_linear _int_bsr_dense_addmm=_int_bsr_dense_addmm op reference input mat mat beta= alpha= left_alpha=None right_alpha=None op=op assert mat layout torch strided assert mat layout torch strided dtype torch int op == _int_bsr_dense_addmm mat = torch _int_mm mat mat workaround RuntimeError addmm_cuda implemented Char out_dtype None mat = torch _int_mm mat mat out_dtype mat = torch _int_mm mat mat torch int mat = mat mat alpha = mat = alpha left_alpha None mat = left_alpha reshape left_alpha shape - - mat right_alpha None mat = mat right_alpha reshape right_alpha shape - - beta input + mat op == _int_bsr_dense_addmm _int_bsr_dense_addmm same bsr_dense_addmm except int inputs _int_bsr_dense_addmm returns int result This covered operation reference definitions above all other definitions below identical between _int_bsr_dense_addmm bsr_dense_addmm dtype is_floating_point dtype is_complex skipTest f Redundant test op dtype tensors op = bsr_dense_addmm nc_copy t axes= - Return copy input The returned copy will non-contiguous tensor t layout torch strided shape = list t shape axes shape = r = torch empty shape dtype=t dtype device=t device s = r tuple slice None None t shape i = r shape i None i range t ndim s copy_ t s t layout torch sparse_bsr compressed_indices = t crow_indices plain_indices = t col_indices torch sparse_compressed_tensor compressed_indices plain_indices nc_copy t values t shape layout=t layout raise NotImplementedError t layout isinstance blocksize str BM BK = tuple map int blocksize split x BM BK = blocksize op bsr_dense_linear BM = BK todo eliminate skip skipTest f op does support non-square blocks op bsr_dense_linear dtype torch int todo eliminate skip skipTest f op does support int dtype torch int min BM BK skipTest triton kernel does support support int blocks smaller than beta_lst = dict bsr_dense_addmm= bsr_dense_mm= bsr_dense_linear= op alpha_lst = dict bsr_dense_addmm= bsr_dense_mm= bsr_dense_linear= op sparsity_lst = blocks_per_row_lst = blocks_per_col_lst = result_cols_lst = has_left_alpha_lst = dict bsr_dense_addmm= False True bsr_dense_mm= False bsr_dense_linear= False op has_right_alpha_lst = dict bsr_dense_addmm= False True bsr_dense_mm= False bsr_dense_linear= False op high = + int dtype torch int beta alpha sparsity blocks_per_row blocks_per_col N has_left_alpha has_right_alpha itertools product beta_lst alpha_lst sparsity_lst blocks_per_row_lst blocks_per_col_lst result_cols_lst has_left_alpha_lst has_right_alpha_lst M = BM blocks_per_row K = BK blocks_per_col mat = create_blocked_tensor M K BM BK sparsity dtype device=device bsr = mat to_sparse_bsr BM BK mat = make_tensor K N dtype=dtype device=device low= high=high input = make_tensor M N dtype=dtype device=device low= high=high left_alpha = make_tensor M dtype=dtype device=device low= high=high has_left_alpha None right_alpha = make_tensor N dtype=dtype device=device low= high=high has_right_alpha None op == bsr_dense_addmm noqa SIM Find optimal kernel parameters speed-up about x running test Enable if-block when test method updated run test finally disable if-block key = M K N BM BK beta == beta == alpha == meta = get_meta op key version= dtype meta None optimize_bsr_dense_addmm M K N BM BK beta=beta alpha=alpha dtype=dtype sparsity= assert meta None dump will update torch sparse _triton_ops_meta py expected = reference input mat mat beta=beta alpha=alpha left_alpha=left_alpha right_alpha=right_alpha out_dtype None expected = expected out_dtype out = expected new_empty input shape dtype=out_dtype out = None kwargs = dict bsr_dense_addmm=dict beta=beta alpha=alpha out=out left_alpha=left_alpha right_alpha=right_alpha bsr_dense_mm= bsr_dense_linear=dict bias=input transpose - - op args = dict bsr_dense_addmm= input bsr mat bsr_dense_mm= bsr mat bsr_dense_linear= mat transpose - - bsr op result = operation args kwargs assertEqual result expected Test non-contiguous input tensors nc_mat = nc_copy mat nc_input = nc_copy input nc_bsr = nc_copy bsr args = dict bsr_dense_addmm= input bsr nc_mat bsr_dense_mm= bsr nc_mat bsr_dense_linear= nc_mat transpose - - bsr op result = operation args kwargs assertEqual result expected todo add bsr_dense_linear set below currently nn linear has unnecessarily restrictive arguments checks op bsr_dense_addmm bsr_dense_mm args = dict bsr_dense_addmm= input nc_bsr mat bsr_dense_mm= nc_bsr mat bsr_dense_linear= mat transpose - - nc_bsr op result = operation args kwargs assertEqual result expected op bsr_dense_addmm bsr_dense_linear args = dict bsr_dense_addmm= nc_input bsr nc_mat bsr_dense_linear= nc_mat transpose - - bsr op kwargs = dict bsr_dense_addmm=dict beta=beta alpha=alpha left_alpha=left_alpha right_alpha=right_alpha out=out bsr_dense_linear=dict bias=nc_input transpose - - op result = operation args kwargs assertEqual result expected parametrize op bsr_dense_addmm _int_bsr_dense_addmm onlyCUDA parametrize out_dtype unspecified int dtypes torch half torch bfloat torch float torch int dtypesIfCUDA torch half torch bfloat SM OrLater torch float torch int unittest skipIf IS_FBCODE IS_REMOTE_GPU Test requires Triton test_triton_tune op device dtype out_dtype torch sparse _triton_ops bsr_dense_addmm _int_bsr_dense_addmm torch sparse _triton_ops_meta create_blocked_tensor tune_bsr_dense_addmm tune__int_bsr_dense_addmm get_meta out_dtype == unspecified out_dtype = None op == bsr_dense_addmm out_dtype = getattr torch out_dtype out_dtype is_floating_point = dtype is_floating_point skipTest incompatible out dtype skipTest out dtype implemented operation = dict bsr_dense_addmm=bsr_dense_addmm _int_bsr_dense_addmm=_int_bsr_dense_addmm op tuner = dict bsr_dense_addmm=tune_bsr_dense_addmm _int_bsr_dense_addmm=tune__int_bsr_dense_addmm op op == _int_bsr_dense_addmm M K N = blocksize = M K N = blocksize = sparsity = bsr = create_blocked_tensor M K blocksize sparsity dtype device to_sparse_bsr blocksize sparsity = - bsr _nnz blocksize blocksize M K input = make_tensor K N dtype=dtype device=device dense = make_tensor K N dtype=dtype device=device version_dtype = dtype out_dtype None out = None out = input new_empty input shape dtype=out_dtype dtype out_dtype version_dtype = dtype out_dtype op bsr_dense_addmm _int_bsr_dense_addmm args = input bsr dense get_current_meta version = version_dtype sparsity meta_key = M K N blocksize False True True get_meta op meta_key version=version exact=True raise NotImplementedError op assertEqual get_current_meta None meta = tuner args dict store=True verbose=False out=out assertEqual get_current_meta meta expected = operation args dict out=None out_dtype None out clone result = operation args dict meta=meta out=out assertEqual result expected onlyCUDA unittest skipIf IS_FBCODE IS_REMOTE_GPU Test requires Triton test_triton_bsr_dense_addmm_meta device torch sparse _triton_ops bsr_dense_addmm_meta torch sparse _triton_ops_meta update update_bsr_dense_addmm_meta dtype = torch float Ms = Ks = beta = alpha = get_meta M K N sparsity=None bsr_dense_addmm_meta M K N Ms Ks beta alpha dtype=dtype sparsity=sparsity _version= test_triton_bsr_dense_addmm_meta update_meta M K N value sparsity= key = M K N Ms Ks beta == beta == alpha == update_bsr_dense_addmm_meta bsr_dense_addmm torch cuda get_device_name test_triton_bsr_dense_addmm_meta dtype sparsity key value get_meta_with_checks M K N warn_count= sparsity=None f = io StringIO redirect_stderr f result = get_meta M K N sparsity=sparsity msg = f getvalue FileCheck check_count str=f UserWarning bsr_dense_addmm uses non-optimal triton kernel parameters M= M K= K N= N count=warn_count exactly=True run msg result Test warn_once when requesting non-existing tuned parameters multiple times f = io StringIO redirect_stderr f _ range get_meta _ range get_meta msg = f getvalue FileCheck check_count str= UserWarning bsr_dense_addmm uses non-optimal triton kernel parameters M= K= N= count= exactly=True run msg FileCheck check_count str= UserWarning bsr_dense_addmm uses non-optimal triton kernel parameters M= K= N= count= exactly=True run msg Test warn_once when tuned parameters missing default_meta = dict GROUP_SIZE_ROW= SPLIT_N= num_stages= num_warps= assertEqual get_meta_with_checks warn_count= default_meta Test no warn_once when tuned parameters available update_meta expected_meta = dict GROUP_SIZE_ROW= SPLIT_N= num_stages= num_warps= assertEqual get_meta_with_checks warn_count= expected_meta Test non-existing tuned parameters non-default sparsity while default sparsity parameters available assertEqual get_meta_with_checks warn_count= sparsity= expected_meta Test non-existing tuned parameters while there exists parameters consistent N SPLIT_N ratio assertEqual get_meta_with_checks warn_count= dict GROUP_SIZE_ROW= SPLIT_N= num_stages= num_warps= assertEqual get_meta_with_checks warn_count= dict GROUP_SIZE_ROW= SPLIT_N= num_stages= num_warps= e g TestSparseCSRCPU TestSparseCSRCUDA instantiate_device_type_tests TestSparseCSR globals instantiate_device_type_tests TestSparseCompressed globals instantiate_device_type_tests TestSparseCompressedTritonKernels globals __name__ == __main__ run_tests