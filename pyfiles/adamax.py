mypy allow-untyped-defs typing cast Optional Union torch torch Tensor optimizer _capturable_doc _default_to_fused_or_foreach _differentiable_doc _disable_dynamo_if_unsupported _foreach_doc _get_capturable_supported_devices _get_scalar_dtype _get_value _maximize_doc _params_doc _to_scalar _use_grad_for_differentiable _view_as_real Optimizer ParamsT __all__ = Adamax adamax Adamax Optimizer __init__ params ParamsT lr Union float Tensor = e- betas tuple float float = eps float = e- weight_decay float = foreach Optional bool = None maximize bool = False differentiable bool = False capturable bool = False isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Invalid learning rate lr = eps raise ValueError f Invalid epsilon value eps = betas raise ValueError f Invalid beta parameter index betas = betas raise ValueError f Invalid beta parameter index betas = weight_decay raise ValueError f Invalid weight_decay value weight_decay defaults = lr lr betas betas eps eps weight_decay weight_decay foreach foreach maximize maximize differentiable differentiable capturable capturable super __init__ params defaults __setstate__ state super __setstate__ state group param_groups group setdefault foreach None group setdefault maximize False group setdefault differentiable False group setdefault capturable False p group params p_state = state get p len p_state = torch is_tensor p_state step step_val = float p_state step p_state step = torch tensor step_val dtype=_get_scalar_dtype device=p device group capturable torch tensor step_val dtype=_get_scalar_dtype _init_group group params_with_grad grads exp_avgs exp_infs state_steps has_complex = False p group params p grad None continue has_complex &#124; = torch is_complex p params_with_grad append p p grad is_sparse raise RuntimeError Adamax does support sparse gradients grads append p grad state = state p State initialization len state == state step = torch zeros dtype=_get_scalar_dtype device=p device group capturable torch tensor dtype=_get_scalar_dtype state exp_avg = torch zeros_like p memory_format=torch preserve_format state exp_inf = torch zeros_like p memory_format=torch preserve_format exp_avgs append state exp_avg exp_infs append state exp_inf state_steps append state step has_complex _use_grad_for_differentiable step closure=None Performs single optimization step Args closure Callable optional A closure reevaluates model returns loss _cuda_graph_capture_health_check loss = None closure None torch enable_grad loss = closure group param_groups params_with_grad list Tensor = grads list Tensor = exp_avgs list Tensor = exp_infs list Tensor = state_steps list Tensor = beta beta = group betas eps = group eps lr = group lr weight_decay = group weight_decay foreach = group foreach maximize = group maximize differentiable = group differentiable capturable = group capturable has_complex = _init_group group params_with_grad grads exp_avgs exp_infs state_steps adamax params_with_grad grads exp_avgs exp_infs state_steps eps=eps beta =beta beta =beta lr=lr weight_decay=weight_decay foreach=foreach maximize=maximize differentiable=differentiable capturable=capturable has_complex=has_complex loss Adamax __doc__ = r Implements Adamax algorithm variant Adam based infinity norm math \begin aligned \rule mm pt \\ \textbf input \gamma \text lr \beta_ \beta_ \text betas \theta_ \text params f \theta \text objective \ \lambda \text weight decay \\ \hspace mm \epsilon \text epsilon \\ \textbf initialize m_ \leftarrow \text first moment u_ \leftarrow \text infinity norm \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \ \lambda \neq \\ \hspace mm g_t \leftarrow g_t + \lambda \theta_ t- \\ \hspace mm m_t \leftarrow \beta_ m_ t- + - \beta_ g_t \\ \hspace mm u_t \leftarrow \mathrm max \beta_ u_ t- &#124; g_ t &#124; +\epsilon \\ \hspace mm \theta_t \leftarrow \theta_ t- - \frac \gamma m_t -\beta^t_ u_t \\ \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned For further details regarding algorithm we refer ` Adam A Method Stochastic Optimization ` _ + rf Args _params_doc lr float Tensor optional learning rate default e- betas Tuple float float optional coefficients used computing running averages gradient its square eps float optional term added denominator improve numerical stability default e- weight_decay float optional weight decay L penalty default _foreach_doc _maximize_doc _differentiable_doc _capturable_doc _Adam\ A Method Stochastic Optimization https arxiv org abs _single_tensor_adamax params list Tensor grads list Tensor exp_avgs list Tensor exp_infs list Tensor state_steps list Tensor eps float beta float beta float lr float weight_decay float maximize bool differentiable bool capturable bool has_complex bool torch jit is_scripting lr = _to_scalar lr i param enumerate params grad = grads i grad = grad maximize -grad exp_avg = exp_avgs i exp_inf = exp_infs i step_t = state_steps i If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices param device type == step_t device type param device type capturable_supported_devices raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices update step step_t += weight_decay = grad = grad add param alpha=weight_decay torch is_complex param param = torch view_as_real param grad = torch view_as_real grad exp_avg = torch view_as_real exp_avg exp_inf = torch view_as_real exp_inf Update biased first moment estimate exp_avg lerp_ grad - beta Update exponentially weighted infinity norm differentiable torch maximum exp_inf mul_ beta grad abs add_ eps out=exp_inf norm_buf = torch cat exp_inf mul_ beta unsqueeze grad abs add_ eps unsqueeze_ exp_inf copy_ torch amax norm_buf keepdim=False capturable why jump through extra hoops negate bias_correction check out once fixed we should use bias_correction addcdiv value=- readability neg_bias_correction = beta step_t - neg_bias_correction div_ lr denom = exp_inf neg_bias_correction param addcdiv_ exp_avg denom bias_correction = - beta _get_value step_t clr = lr bias_correction param addcdiv_ exp_avg exp_inf value=-clr _multi_tensor_adamax params list Tensor grads list Tensor exp_avgs list Tensor exp_infs list Tensor state_steps list Tensor eps float beta float beta float lr float weight_decay float maximize bool differentiable bool capturable bool has_complex bool differentiable raise AssertionError _foreach ops don t support autograd len params == If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices supports_xla=False all p device type == step device type p device type capturable_supported_devices p step zip params state_steps strict=True raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices lr = _to_scalar lr grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads exp_avgs exp_infs state_steps type ignore list-item grouped_params_ grouped_grads_ grouped_exp_avgs_ grouped_exp_infs_ grouped_state_steps_ _ grouped_tensors values grouped_params = cast list Tensor grouped_params_ grouped_grads = cast list Tensor grouped_grads_ grouped_exp_avgs = cast list Tensor grouped_exp_avgs_ grouped_exp_infs = cast list Tensor grouped_exp_infs_ grouped_state_steps = cast list Tensor grouped_state_steps_ has_complex _view_as_real grouped_params grouped_grads grouped_exp_avgs grouped_exp_infs maximize grouped_grads = torch _foreach_neg grouped_grads type ignore assignment Update steps If steps CPU foreach will fall back slow path which for-loop calling t add over over will then wrapped into Tensor over over again which slower than we just wrapped once now The alpha required assure we go right overload torch compiler is_compiling grouped_state_steps is_cpu torch _foreach_add_ grouped_state_steps torch tensor device= cpu alpha= torch _foreach_add_ grouped_state_steps weight_decay = maximize Reuse intermediate memory grouped_grads already allocated maximize torch _foreach_add_ grouped_grads grouped_params alpha=weight_decay grouped_grads = torch _foreach_add type ignore assignment grouped_grads grouped_params alpha=weight_decay Update biased first moment estimate torch _foreach_lerp_ grouped_exp_avgs grouped_grads - beta Update exponentially weighted infinity norm torch _foreach_mul_ grouped_exp_infs beta case we need introduce copy grads since one has been introduced previously maximize weight_decay == grouped_grads = torch _foreach_abs grouped_grads type ignore assignment torch _foreach_abs_ grouped_grads torch _foreach_add_ grouped_grads eps torch _foreach_maximum_ grouped_exp_infs grouped_grads bias_corrections Union tuple Tensor list Tensor capturable bias_corrections = torch _foreach_pow beta grouped_state_steps foreach_sub doesn t allow scalar first arg torch _foreach_sub_ bias_corrections torch _foreach_div_ bias_corrections lr denom = torch _foreach_mul grouped_exp_infs bias_corrections torch _foreach_addcdiv_ grouped_params grouped_exp_avgs denom bias_corrections = - beta _get_value step step grouped_state_steps step_size = _get_value lr bc - bc bias_corrections torch _foreach_addcdiv_ grouped_params grouped_exp_avgs grouped_exp_infs step_size _disable_dynamo_if_unsupported single_tensor_fn=_single_tensor_adamax adamax params list Tensor grads list Tensor exp_avgs list Tensor exp_infs list Tensor state_steps list Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim foreach Optional bool = None maximize bool = False differentiable bool = False capturable bool = False has_complex bool = False eps float beta float beta float lr float weight_decay float r Functional API performs adamax algorithm computation See ` ~torch optim Adamax ` details torch compiler is_compiling all isinstance t torch Tensor t state_steps raise RuntimeError API has changed ` state_steps ` argument must contain list singleton tensors foreach None _ foreach = _default_to_fused_or_foreach params differentiable use_fused=False foreach torch jit is_scripting raise RuntimeError torch jit script supported foreach optimizers foreach torch jit is_scripting func = _multi_tensor_adamax func = _single_tensor_adamax func params grads exp_avgs exp_infs state_steps eps=eps beta =beta beta =beta lr=lr weight_decay=weight_decay maximize=maximize differentiable=differentiable has_complex=has_complex capturable=capturable