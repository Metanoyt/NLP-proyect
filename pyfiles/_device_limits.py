torch torch _C dtype __all__ = GPULimits GPULimits r Utility provides theoretical limits Nvidia GPU devices The limits don t take into account thermal throttling assume GPU run its peak rated frequency This because user hardware configuration may influence power behavior __init__ target_device torch device The device properties object obtained calling cudaGetDeviceProperties CUDA runtime function We need total memory bus width memory clock rate calculate memory bandwidth device_properties = torch cuda get_device_properties target_device The compute capability needed determine number FLOPs per cycle per SM compute_capability = int f device_properties major device_properties minor FLOPs per cycle information derived Table https resources nvidia com en-us-hopper-architecture nvidia-h -tensor-c Returns number FMA instructions retired per cycle per SM given data type when tensor cores NOT used get_fma_per_cycle_per_sm_cuda_cores data_type dtype - int hardcoded_device_values = Ampere Architecture fp _ fp _ fp _ Hopper Architecture fp _ fp _ fp _ Blackwell Architecture fp _ fp _ fp _ dict_key = data_type torch float dict_key = f fp _ compute_capability data_type torch float dict_key = f fp _ compute_capability data_type torch float dict_key = f fp _ compute_capability dict_key = unknown dict_key hardcoded_device_values keys raise RuntimeError f No data sm_ compute_capability data_type hardcoded_device_values dict_key Returns number FMA instructions retired per cycle per SM given data type when tensor cores ARE used get_fma_per_cycle_per_sm_tensor_cores data_type dtype - int hardcoded_device_values = Ampere Architecture int _ fp _ fp _ fp _ Hopper Architecture int _ fp _ fp _ fp _ fp _ Blackwell Architecture int _ fp _ fp _ fp _ dict_key = data_type torch float dict_key = f fp _ compute_capability data_type torch bfloat FP BF equivalent terms FLOPs per cycle per SM dict_key = f fp _ compute_capability data_type torch float dict_key = f fp _ compute_capability data_type torch int dict_key = f int _ compute_capability data_type torch float dict_key = f fp _ compute_capability dict_key = unknown dict_key hardcoded_device_values keys raise RuntimeError f No data sm_ compute_capability data_type hardcoded_device_values dict_key get_tflops_per_second data_type dtype use_tensor_cores bool = True - float num_sms = device_properties multi_processor_count clock_rate = device_properties clock_rate KHz fma_per_cycle = use_tensor_cores fma_per_cycle = get_fma_per_cycle_per_sm_tensor_cores data_type fma_per_cycle = get_fma_per_cycle_per_sm_cuda_cores data_type FMA counts floating point operations Clock rate KHz tflops_per_second = num_sms fma_per_cycle clock_rate e tflops_per_second get_memory_bandwidth_Bps - int DRAM devices Double-Data which means they provide output both fronts clock beat bus_bytes_per_cycle = int device_properties memory_bus_width mem_clock_rate_Hz = device_properties memory_clock_rate bytes_per_second = bus_bytes_per_cycle mem_clock_rate_Hz bytes_per_second get_shared_memory_bandwidth_Bps - int Each warp can LD ST x bytes per cycle To calculate device s throughput we need multiply frequency number SMs num_sms = device_properties multi_processor_count bytes_per_cycle_per_sm = bytes_per_cycle_per_device = num_sms bytes_per_cycle_per_sm bytes_per_second = bytes_per_cycle_per_device device_properties clock_rate bytes_per_second