Owner s oncall jit ruff noqa F typing List torch torch testing _internal common_utils raise_on_run_directly skipIfTorchDynamo torch testing _internal jit_utils JitTestCase skipIfTorchDynamo TestAutodiffJit JitTestCase test_undefined_tensor_lists fn tensor_list List torch Tensor add_tensor cat = torch cat tensor_list dim= r = torch sin cat + add_tensor r fn_s = torch jit script fn = torch rand requires_grad=True b = torch rand requires_grad=True x = b y = torch rand requires_grad=True ret = fn_s x y ret sum backward ret = fn_s x y ret sum backward ret = fn_s x y s = ret sum backward_fn expects inputs grad_output current_grad_r current_grad_r provided because we need add contribution grad_r when we backward_fn = s grad_fn next_functions check behavior defined tensor grad_out = torch rand grad_inputs = backward_fn grad_out None expect tensors grad_y grad_a grad_b assertEqual len grad_inputs x grad_inputs assertTrue isinstance x torch Tensor now test undefined grad_out grad_inputs = backward_fn None None expect all them None assertEqual len grad_inputs x grad_inputs x None assertEqual torch max torch abs x item test_requires_grad_outputs outputs should require_grad only eager outputs would require_grad fn b c relu + b relu c relu = torch rand requires_grad=False b = torch rand requires_grad=False c = torch rand requires_grad=True fn_s = torch jit script fn _ range x y = fn_s b c assertFalse x requires_grad assertTrue y requires_grad test_requires_grad_outputs_profiled_twice value r used twice gammaln entr so profiled twice So during autodiff graph formation profile nodes unmerged because they aliasing Then DifferentiableGraph doesn t have profile node output The requires_grad info should then added onto output value otherwise autodiff will make output require_grad Note relies gammaln entr having autodiff implementations fn b c r = relu relu torch special gammaln r torch special entr r c cos relu fn_s = torch jit script fn = torch rand requires_grad=False b = torch rand requires_grad=False c = torch rand requires_grad=True _ range x_s y_s z_s = fn_s b c x y z = fn b c assertEqual x_s requires_grad x requires_grad assertEqual y_s requires_grad y requires_grad assertEqual z_s requires_grad z requires_grad test_requires_grad_outputs_side_effects same above also add CallFunction between torch jit ignore python_fn x x relu fn b c r = relu relu z = python_fn r torch relu r torch nn functional gelu r c cos relu fn_s = torch jit script fn = torch rand requires_grad=False b = torch rand requires_grad=False c = torch rand requires_grad=True _ range x_s y_s z_s = fn_s b c x y z = fn b c assertEqual x_s requires_grad x requires_grad assertEqual y_s requires_grad y requires_grad assertEqual z_s requires_grad z requires_grad test_autodiff_requires_grad_nograd torch jit ignore python_fn x x relu fn b c x = sin relu y = python_fn b torch no_grad z = x + c x y z fn_s = torch jit script fn = torch rand requires_grad=True b = torch rand requires_grad=True c = torch rand requires_grad=True _ range x_s y_s z_s = fn_s b c x y z = fn b c assertEqual x_s requires_grad x requires_grad assertEqual y_s requires_grad y requires_grad assertEqual z_s requires_grad z requires_grad __name__ == __main__ raise_on_run_directly test test_jit py