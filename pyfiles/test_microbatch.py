Copyright c Meta Platforms Inc affiliates Owner s oncall distributed model_registry ModelWithKwargs torch torch distributed pipelining pipeline torch distributed pipelining microbatch merge_chunks split_args_kwargs_into_chunks TensorChunkSpec torch nn attention flex_attention create_block_mask flex_attention torch testing _internal common_device_type instantiate_device_type_tests skipXPUIf torch testing _internal common_utils run_tests TestCase d_hid = torch manual_seed MicrobatchTests TestCase test_split_and_merge x = torch randn d_hid x = torch randn d_hid x = torch randn d_hid args = x x x kwargs = x x x x x x Default chunking dim arg_chunks kwarg_chunks = split_args_kwargs_into_chunks args kwargs assert len arg_chunks == assert len kwarg_chunks == assert arg_chunks shape == torch Size d_hid assert arg_chunks shape == torch Size d_hid assert arg_chunks shape == torch Size d_hid assert arg_chunks shape == torch Size d_hid assert kwarg_chunks x shape == torch Size d_hid assert kwarg_chunks x shape == torch Size d_hid assert kwarg_chunks x shape == torch Size d_hid Merge chunks back together merged_args = merge_chunks arg_chunks TensorChunkSpec TensorChunkSpec TensorChunkSpec torch testing assert_close merged_args args merged_kwargs = merge_chunks kwarg_chunks x TensorChunkSpec x TensorChunkSpec x TensorChunkSpec torch testing assert_close merged_kwargs kwargs print Microbatch test passed test_split_block_mask device B = H = SEQ_LEN = DIM = DOC_LEN = create_block_causal_mask batch eos_id int mask = batch == eos_id mask - = True acc_mask = torch cumsum torch where mask dim= seq_idx = torch zeros_like acc_mask dtype=torch int seq_idx = acc_mask - block_causal_mask b torch Tensor h torch Tensor q_idx torch Tensor kv_idx torch Tensor seq_idx b q_idx == seq_idx b kv_idx q_idx = kv_idx block_causal_mask Create fake batch which packs several documents together which each has DOC_LEN tokens last token EOS token DOC_LEN - total_elements = B SEQ_LEN batch = torch arange total_elements device=device DOC_LEN batch = batch reshape B SEQ_LEN q k v = torch randn B H SEQ_LEN DIM device=device requires_grad=True i range block_mask_fn = torch compile create_block_mask fullgraph=True block_mask = block_mask_fn create_block_causal_mask batch DOC_LEN - B=B H=H Q_LEN=SEQ_LEN KV_LEN=SEQ_LEN device=device block_mask = block_mask_fn create_block_causal_mask batch DOC_LEN - B=B H=H Q_LEN=SEQ_LEN KV_LEN=SEQ_LEN device=device device == cuda flex_fn = torch compile flex_attention It s unclear why CPU + torch compile + flex_attention can cause issue flex_fn = flex_attention out = flex_fn q k v block_mask=block_mask out sum backward q_clone k_clone v_clone = target clone detach target q k v arg_split _ = split_args_kwargs_into_chunks q_clone k_clone v_clone unused_block_mask block_mask block_mask block_mask chunks= args_chunk_spec=None kwargs_chunk_spec=None assert len arg_split == q_total_chunks = dq_total_chunks = k_total_chunks = dk_total_chunks = v_total_chunks = dv_total_chunks = block_mask_total_chunks = out_total_chunks = i range len arg_split q_chunk k_chunk v_chunk block_mask_chunk = arg_split i chunk total_chunks zip q_chunk k_chunk v_chunk q_total_chunks k_total_chunks v_total_chunks chunk requires_grad = True total_chunks append chunk out_chunk = flex_fn q_chunk k_chunk v_chunk block_mask=block_mask_chunk block_mask out_chunk sum backward dq_total_chunks append q_chunk grad dk_total_chunks append k_chunk grad dv_total_chunks append v_chunk grad block_mask_total_chunks append block_mask_chunk block_mask out_total_chunks append out_chunk concat_q = torch cat q_total_chunks dim= concat_dq = torch cat dq_total_chunks dim= concat_k = torch cat k_total_chunks dim= concat_dk = torch cat dk_total_chunks dim= concat_v = torch cat v_total_chunks dim= concat_dv = torch cat dv_total_chunks dim= concat_kv_indices = torch cat bm kv_indices bm block_mask_total_chunks dim= concat_kv_num_blocks = torch cat bm kv_num_blocks bm block_mask_total_chunks dim= concat_kv_full_num_blocks = torch cat bm full_kv_num_blocks bm block_mask_total_chunks dim= concat_kv_full_indices = torch cat bm full_kv_indices bm block_mask_total_chunks dim= concat_out = torch cat out_total_chunks dim= assertEqual concat_q q assertEqual concat_dq q grad assertEqual concat_k k assertEqual concat_dk k grad assertEqual concat_v v assertEqual concat_dv v grad assertEqual concat_kv_indices block_mask kv_indices assertEqual concat_kv_num_blocks block_mask kv_num_blocks assertEqual concat_kv_full_num_blocks block_mask full_kv_num_blocks assertEqual concat_kv_full_indices block_mask full_kv_indices assertEqual concat_out out test_split_block_mask_batch_size_one device B = H = SEQ_LEN = DIM = create_causal_mask causal_mask b torch Tensor h torch Tensor q_idx torch Tensor kv_idx torch Tensor q_idx = kv_idx causal_mask q k v = torch randn B H SEQ_LEN DIM device=device i range block_mask_fn = torch compile create_block_mask fullgraph=True block_mask = block_mask_fn create_causal_mask B= H=H Q_LEN=SEQ_LEN KV_LEN=SEQ_LEN device=device device == cuda flex_fn = torch compile flex_attention It s unclear why CPU + torch compile + flex_attention can cause issue flex_fn = flex_attention out = flex_fn q k v block_mask=block_mask q_clone k_clone v_clone = target clone detach target q k v arg_split _ = split_args_kwargs_into_chunks q_clone k_clone v_clone block_mask block_mask chunks= args_chunk_spec=None kwargs_chunk_spec=None assert len arg_split == out_total_chunks = i range len arg_split q_chunk k_chunk v_chunk block_mask_chunk = arg_split i out_chunk = flex_fn q_chunk k_chunk v_chunk block_mask=block_mask_chunk block_mask out_total_chunks append out_chunk concat_out = torch cat out_total_chunks dim= assertEqual concat_out out test_split_block_mask_none device B = H = SEQ_LEN = DIM = q k v = torch randn B H SEQ_LEN DIM device=device i range arg_split kwarg_split = split_args_kwargs_into_chunks q k v None attention_mask None chunks= args_chunk_spec=None kwargs_chunk_spec=None assert len arg_split == i range len arg_split assertIsNone arg_split i assertIsNone kwarg_split i attention_mask skipXPUIf True https github com intel torch-xpu-ops issues test_chunk_spec device mod = ModelWithKwargs device batch_size = ModelWithKwargs DEFAULT_BATCH_SIZE x = torch randn batch_size d_hid device=device y = torch randn batch_size d_hid device=device num_chunks = args_chunk_spec = TensorChunkSpec from_tuple kwargs_chunk_spec = TensorChunkSpec from_dict y args_split kwargs_split = split_args_kwargs_into_chunks x y y num_chunks args_chunk_spec kwargs_chunk_spec pipe = pipeline mod mb_args=args_split mb_kwargs=kwargs_split device ref = mod x y out = pipe x y torch testing assert_close out ref print f equivalence test passed torch sum out ref torch sum ref devices = cpu cuda hpu xpu instantiate_device_type_tests MicrobatchTests globals only_for=devices allow_xpu=True __name__ == __main__ run_tests