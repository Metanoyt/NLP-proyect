mypy allow-untyped-defs Tracing This module contains functionality support JIT s tracing frontend notably torch jit trace torch jit trace_module This intended imported directly please use exposed functionalities ` torch jit ` contextlib copy functools inspect os re warnings collections abc Callable enum Enum typing Any Optional TypeVar typing_extensions ParamSpec torch torch _jit_internal _get_model_id _qualified_name get_callable_argument_names is_scripting torch autograd function torch jit _script _CachedForward script ScriptModule torch jit _state _enabled _python_cu torch nn Module torch testing _comparison default_tolerances _flatten = torch _C _jit_flatten _unflatten = torch _C _jit_unflatten R = TypeVar R covariant=True type always covariant P = ParamSpec P _create_interpreter_name_lookup_fn frames_up= _get_interpreter_name_for_var var frame = inspect currentframe frame raise RuntimeError failed inspect frame i = while i frames_up + frame = frame f_back frame raise RuntimeError failed get frame i += f_locals = frame f_locals k v f_locals items isinstance v torch Tensor var v k k = _get_interpreter_name_for_var _unique_state_dict module keep_vars=False since Parameter detach always creates new torch Tensor instance id v doesn t work So we always get Parameter Buffer values deduplicate params using Parameters Buffers state_dict = module state_dict keep_vars=True filtered_dict = type state_dict seen_ids set int = set k v state_dict items id v seen_ids continue seen_ids add id v keep_vars filtered_dict k = v filtered_dict k = v detach filtered_dict ONNXTracedModule torch nn Module __init__ inner strict=True force_outplace=False return_inputs=False return_inputs_states=False super __init__ inner may Module may arbitrary callable If s Module we get its parameters automatically which lets us avoid special casing functions versus modules inner = inner strict = strict _force_outplace = force_outplace _return_inputs = return_inputs _return_inputs_states = return_inputs_states forward args torch Tensor in_vars in_desc = _flatten args NOTE use full state because we need BatchNorm export This differs compiler path which doesn t support moment module_state = list _unique_state_dict keep_vars=True values ret_inputs = inputs_states = outs = wrapper args in_args list torch Tensor = i range len in_vars isinstance args i torch Tensor raise RuntimeError Expected Tensor argument in_args append args i trace_inputs = _unflatten in_args in_desc _return_inputs ret_inputs append tuple x clone memory_format=torch preserve_format x args _return_inputs_states inputs_states append _unflatten in_args in_desc outs append inner trace_inputs _return_inputs_states inputs_states = inputs_states trace_inputs out_vars _ = _flatten outs len out_vars == out_vars tuple out_vars graph _out = torch _C _create_graph_by_tracing wrapper in_vars + module_state _create_interpreter_name_lookup_fn strict _force_outplace _return_inputs graph outs ret_inputs _return_inputs_states graph outs inputs_states graph outs _clone_inputs args clone_input None None isinstance torch Tensor TODO figure out one liner clone set requires_grad v = detach clone memory_format=None is_mkldnn torch preserve_format requires_grad_ requires_grad grad None v grad = clone_input v grad v clone memory_format=torch preserve_format pyrefly ignore missing-attribute function _nested_map lambda x isinstance x torch Tensor clone_input condition_msg= tensors args This purely developer debugging We going advertise _JIT_TIME = os environ get PYTORCH_JIT_TIME False CUDA-only timing _JIT_DISABLE = os environ get PYTORCH_JIT_DISABLE False _JIT_STATS = os environ get PYTORCH_JIT_STATS False contextlib contextmanager _time trace_name name time=True _JIT_TIME time torch cuda is_available yield stream = torch cuda current_stream start = torch cuda Event enable_timing=True end = torch cuda Event enable_timing=True stream record_event start try yield finally stream record_event end end synchronize print f trace_name name time start elapsed_time end ms verify model args loss_fn=torch sum devices=None Verify JIT compiled model has same behavior its uncompiled version along its backwards pass If your model returns multiple outputs you must also specify ` loss_fn ` produce loss which backwards will computed This function has side-effects e g executes your model saves loads parameters so don t expect model come out exactly same what you passed Args model compiled torch nn Module function module function verified The module function definition MUST have been decorated ` torch jit compile ` args tuple Tensor positional arguments pass compiled function module verified A non-tuple assumed single positional argument passed model loss_fn function optional loss function applied output model before backwards invoked By default we assume model returns single result we func ` torch sum ` before calling backwards inappropriate you can pass your own loss function Note model returns tuple results these passed separate positional arguments ` loss_fn ` devices iterable device IDs optional GPU devices which compiled module will run This determines RNG state we must save when running both compiled uncompiled versions model TODO In principle we track device information our trace so should possible check our execution actually obeyed devices user provided TODO Consider adding utility function torch jit test case isinstance model torch _C CompiledFunction type ignore attr-defined raise TypeError Cannot verify uncompiled module Add torch jit compile compile is_module = isinstance model Module isinstance args tuple args = args is_module saved_state = copy deepcopy model state_dict run_fwd_bwd args force_trace=False assert_compiled=False params = list model parameters is_module in_vars _ = _flatten args params We use special API reset trace compile scratch compiled_fn = model force_trace compiled_fn clear_cache assert_compiled hits = compiled_fn hits out = model args assert_compiled compiled_fn hits == hits type ignore possibly-undefined raise RuntimeError failed use compiled function isinstance out tuple out = out loss_fn == torch sum len out = raise ValueError f Model returns len out outputs default loss function torch sum can only handle single output out_vars _ = _flatten out saved_outs = v detach clone memory_format=torch preserve_format v out_vars loss = loss_fn out grads = torch autograd grad loss in_vars TODO I m sure clone here necessary safer saved_grads = v detach clone memory_format=torch preserve_format v grads saved_outs saved_grads torch random fork_rng devices _caller= torch jit verify uncompiled_outs uncompiled_grads = run_fwd_bwd args force_trace=True assert model has_trace_for args is_module model load_state_dict saved_state type ignore possibly-undefined compiled_outs compiled_grads = run_fwd_bwd args assert_compiled=True _verify_equal uncompiled_outs compiled_outs _verify_equal uncompiled_grads compiled_grads _verify_equal xs ys x y zip xs ys x sub y abs max e- raise RuntimeError JIT real computation mismatch indent s \n join \t + line line s splitlines TracingCheckError Exception __init__ graph_diff_error tensor_compare_error extra_msg=None message = Tracing failed sanity checks \n extra_msg None message += extra_msg + \n graph_diff_error None message += ERROR Graphs differed across invocations \n message += indent graph_diff_error + \n tensor_compare_error None message += ERROR Tensor-valued Constant nodes differed value across invocations This often indicates tracer has encountered untraceable code \n message += indent tensor_compare_error + \n super __init__ message Check traced module against set user-provided validation inputs torch no_grad _check_trace check_inputs func traced_func check_tolerance strict force_outplace is_trace_module _module_class example_inputs_is_kwarg=False Note tracing independent optimizations which consume trace inputs check_inputs isinstance inputs torch Tensor inputs = inputs is_trace_module copied_dict = pyrefly ignore missing-attribute name data inputs items copied_dict name = _clone_inputs data check_mod = torch jit trace_module getattr func __self__ func copied_dict check_trace=False strict=strict _force_outplace=force_outplace _module_class=_module_class _compilation_unit=torch _C CompilationUnit example_inputs_is_kwarg=example_inputs_is_kwarg _store_inputs=False check_mod_func = check_mod _c _get_method traced_func name inputs = inputs traced_func name isinstance inputs torch Tensor isinstance inputs dict example_inputs_is_kwarg inputs = inputs example_inputs_is_kwarg check_mod = torch jit trace func check_trace=False strict=strict _force_outplace=force_outplace _module_class=_module_class example_kwarg_inputs=_clone_inputs inputs _store_inputs=False check_mod = torch jit trace func _clone_inputs inputs check_trace=False strict=strict _force_outplace=force_outplace _module_class=_module_class _store_inputs=False check_mod_func = check_mod graph_diagnostic_info mod_canonicalized = torch _C _jit_pass_canonicalize traced_func graph torch _C _jit_pass_inline mod_canonicalized torch _C _jit_pass_erase_shape_information mod_canonicalized mod_str = str mod_canonicalized mod_str = re sub r ___torch_mangle_ - +\ mod_str check_canonicalized = torch _C _jit_pass_canonicalize check_mod_func graph torch _C _jit_pass_inline check_canonicalized torch _C _jit_pass_erase_shape_information check_canonicalized check_str = str check_canonicalized check_str = re sub r ___torch_mangle_ - +\ check_str graph_diff_errors = None mod_str = check_str difflib graph_diff = difflib ndiff mod_str splitlines True check_str splitlines True graph_diff_errors = Graph diff \n + indent join graph_diff + \n n_mod n_check zip mod_canonicalized nodes check_canonicalized nodes str n_mod = str n_check graph_diff_errors += First diverging operator \n node_diff = difflib ndiff str n_mod splitlines True str n_check splitlines True source_printout = Node diff \n + indent join node_diff + \n mod_stack = n_mod sourceRange mod_stack source_printout += Trace source location \n + indent mod_stack + \n check_stack = n_check sourceRange check_stack source_printout += Check source location \n + indent check_stack + \n graph_diff_errors += source_printout break For now only print out first pair nodes diverges tensor_compare_errors = None Check Tensor-valued constant nodes n_mod n_check zip mod_canonicalized nodes check_canonicalized nodes n_mod kind = n_check kind break Graphs have already diverged n_mod kind == prim Constant n_mod mustBeNone n_check mustBeNone n_mod hasAttribute value continue n_mod kindOf value = t n_check kindOf value = t continue mod_tensor_val = n_mod t value check_tensor_val = n_check t value try torch testing assert_close mod_tensor_val check_tensor_val equal_nan=True except RuntimeError AssertionError e tensor_compare_errors None tensor_compare_errors = tensor_compare_errors += Node \n + indent str n_mod + \n compare_stack = n_mod sourceRange compare_stack tensor_compare_errors += Source Location \n + indent compare_stack + \n tensor_compare_errors += Comparison exception + indent str e break For now only print first diverging pair graph_diff_errors tensor_compare_errors wrap_retval x x isinstance x tuple x run_mod_and_filter_tensor_outputs mod inputs running_what try isinstance inputs dict example_inputs_is_kwarg outs = wrap_retval mod inputs outs = wrap_retval mod _clone_inputs inputs outs = out out outs isinstance out torch Tensor outs except Exception e graph_diff_errors tensor_compare_errors = graph_diagnostic_info msg = f encountered exception while running running_what test inputs \nException \n indent str e raise TracingCheckError graph_diff_errors tensor_compare_errors extra_msg=msg e has_warned = False maybe_warn_nondeterministic has_warned has_warned = True nondeterm_ops = op op traced_func graph nodes op isNondeterministic len nondeterm_ops nondeterministic_ops_warning = Trace had nondeterministic nodes nondeterministic_ops_warning += Did you forget call eval your model Nodes \n nondeterministic_ops_warning += \n join indent str op op nondeterm_ops nondeterministic_ops_warning += \nThis may cause errors trace checking To disable trace checking pass check_trace=False torch jit trace warnings warn nondeterministic_ops_warning category=TracerWarning stacklevel= compare_outputs original reference match_what all_ok = True i orig ref enumerate zip original reference try orig is_quantized orig = orig dequantize ref is_quantized ref = ref dequantize orig is_mkldnn orig = orig to_dense ref is_mkldnn ref = ref to_dense ref is_complex orig is_complex torch testing assert_close orig torch cdouble ref torch cdouble rtol=check_tolerance atol=default_tolerances orig ref equal_nan=True orig is_mps ref is_mps torch testing assert_close orig float ref float rtol=check_tolerance atol=default_tolerances orig ref equal_nan=True getattr orig is_nested None getattr ref is_nested None assert getattr orig is_nested None == getattr ref is_nested None t_orig t_ref zip orig unbind ref unbind torch testing assert_close t_orig double t_ref double rtol=check_tolerance atol=default_tolerances t_orig t_ref equal_nan=True torch testing assert_close orig double ref double rtol=check_tolerance atol=default_tolerances orig ref equal_nan=True except AssertionError e maybe_warn_nondeterministic warnings warn Output nr + str i + + traced function does match corresponding output + match_what + Detailed error \n + str e category=TracerWarning stacklevel= all_ok = False all_ok traced_outs = run_mod_and_filter_tensor_outputs traced_func inputs trace fn_outs = run_mod_and_filter_tensor_outputs func inputs Python function compare_outputs traced_outs fn_outs Python function check_outs = run_mod_and_filter_tensor_outputs check_mod_func inputs repeated trace compare_outputs traced_outs check_outs repeated trace diag_info = graph_diagnostic_info any info None info diag_info raise TracingCheckError diag_info TracerWarning Warning staticmethod ignore_lib_warnings We ignore warnings all submodules excluding JIT because we need them e g _check_trace warnings filterwarnings ignore category=TracerWarning module= torch jit warnings filterwarnings ignore torch jit fuser cuda We ignore tracer warnings coming form inside library because all our shape checks nn will trigger them TracerWarning ignore_lib_warnings torch _C _tracer_warn_use_python make_tuple example_inputs isinstance example_inputs torch Tensor dict example_inputs done primarily so weird iterables fail here pybind code isinstance example_inputs tuple tuple example_inputs example_inputs make_module mod _module_class _compilation_unit isinstance mod ScriptModule mod torch _jit_internal module_has_exports mod infer_methods_stubs_fn = torch jit _recursive make_stubs_from_exported_methods torch jit _recursive create_script_module mod infer_methods_stubs_fn share_types=False is_tracing=True _module_class None _module_class = TopLevelTracedModule _module_class mod _compilation_unit=_compilation_unit wrap_check_inputs check_inputs check_inputs None None forward c c check_inputs analyze_ts_result_with_export_result export trace torch utils _pytree pytree flat_export = pytree tree_leaves export flat_trace = pytree tree_leaves trace orig loaded zip flat_export flat_trace orig layout = loaded layout False mkldnn supported torch allclose orig layout == torch _mkldnn type ignore attr-defined True type orig type loaded False isinstance orig torch _subclasses FakeTensor Skip FakeTensor True isinstance orig torch Tensor orig dtype = loaded dtype False torch allclose orig loaded False orig = loaded False True _trace_impl func example_inputs=None optimize=None check_trace=True check_inputs=None check_tolerance= e- strict=True _force_outplace=False _module_class=None _compilation_unit=_python_cu example_kwarg_inputs=None _store_inputs=True isinstance func torch jit ScriptModule hard trace because forward method ScriptModule already defined so would result error warnings warn The input trace already ScriptModule tracing no-op Returning object stacklevel= func isinstance func torch nn Module example_inputs None isinstance example_kwarg_inputs dict example_inputs = example_kwarg_inputs raise RuntimeError example_kwarg_inputs should dict trace_module func forward example_inputs None check_trace wrap_check_inputs check_inputs check_tolerance strict _force_outplace _module_class example_inputs_is_kwarg=isinstance example_kwarg_inputs dict _store_inputs=_store_inputs hasattr func __self__ isinstance func __self__ torch nn Module func __name__ == forward example_inputs None isinstance example_kwarg_inputs dict example_inputs = example_kwarg_inputs raise RuntimeError example_kwarg_inputs should dict trace_module func __self__ forward example_inputs None check_trace wrap_check_inputs check_inputs check_tolerance strict _force_outplace _module_class example_inputs_is_kwarg=isinstance example_kwarg_inputs dict _store_inputs=_store_inputs Special case common case passing single Tensor isinstance example_inputs torch Tensor dict example_kwarg_inputs None example_inputs = example_inputs done primarily so weird iterables fail here pybind code example_kwarg_inputs None isinstance example_inputs tuple pyrefly ignore bad-argument-type example_inputs = tuple example_inputs var_lookup_fn = _create_interpreter_name_lookup_fn hasattr func __self__ isinstance func __self__ torch nn Module raise AttributeError trace doesn t support compiling individual module s functions \n Please use trace_module name = _qualified_name func isinstance example_kwarg_inputs dict example_inputs = example_kwarg_inputs traced = torch _C _create_function_from_trace_with_dict name func example_kwarg_inputs var_lookup_fn strict _force_outplace get_callable_argument_names func traced = torch _C _create_function_from_trace name func pyrefly ignore bad-argument-type example_inputs var_lookup_fn strict _force_outplace get_callable_argument_names func Check trace against new traces created user-specified inputs check_trace check_inputs None _check_trace check_inputs func traced check_tolerance strict _force_outplace False _module_class example_inputs_is_kwarg=isinstance example_kwarg_inputs dict _check_trace example_inputs func traced check_tolerance strict _force_outplace False _module_class example_inputs_is_kwarg=isinstance example_kwarg_inputs dict Allow torch compile inline traced _torchdynamo_inline = func type ignore attr-defined traced _ExportType str Enum DIRECT_EXPORT = DIRECT_EXPORT TRACE_AND_EXPORT = TRACE_AND_EXPORT SOURCE_TO_SOURCE = SOURCE_TO_SOURCE __str__ - str value _ExportOutcome str Enum SUCCESS = SUCCESS FAILED_TO_EXPORT = FAILED_TO_EXPORT FAILED_TO_RUN = FAILED_TO_RUN ACCURACY_ERROR = ACCURACY_ERROR __str__ - str value trace func example_inputs=None optimize=None check_trace=True check_inputs=None check_tolerance= e- strict=True _force_outplace=False _module_class=None _compilation_unit=_python_cu example_kwarg_inputs=None _store_inputs=True r Trace function executable ` ScriptFunction ` will optimized using just-in-time compilation Tracing ideal code operates only ` ` Tensor ` ` \\s lists dictionaries tuples ` ` Tensor ` ` \\s Using ` torch jit trace ` ` torch jit trace_module ` you can turn existing module Python function into TorchScript ` ScriptFunction ` ` ScriptModule ` You must provide example inputs we run function recording operations performed all tensors The resulting recording standalone function produces ` ScriptFunction ` The resulting recording ` nn Module forward ` ` nn Module ` produces ` ScriptModule ` This module also contains any parameters original module had well Warning Tracing only correctly records functions modules which data dependent e g do have conditionals data tensors do have any untracked external dependencies e g perform input output access global variables Tracing only records operations done when given function run given tensors Therefore returned ` ScriptModule ` will always run same traced graph any input This has some important implications when your module expected run different sets operations depending input module state For example Tracing will record any control-flow like if-statements loops When control-flow constant across your module fine often inlines control-flow decisions But sometimes control-flow actually part model itself For instance recurrent network loop over possibly dynamic length input sequence In returned ` ScriptModule ` operations have different behaviors ` ` training ` ` ` ` eval ` ` modes will always behave mode during tracing no matter which mode ` ScriptModule ` In cases like these tracing would appropriate func ` scripting torch jit script ` better choice If you trace such models you may silently get incorrect results subsequent invocations model The tracer will try emit warnings when doing something may cause incorrect trace produced Args func callable torch nn Module A Python function ` torch nn Module ` will run ` example_inputs ` ` func ` arguments values must tensors possibly nested tuples contain tensors When module passed ` torch jit trace ` only ` ` forward ` ` method run traced see func ` torch jit trace torch jit trace_module ` details Keyword arguments example_inputs tuple torch Tensor None optional A tuple example inputs will passed function while tracing Default ` ` None ` ` Either argument ` ` example_kwarg_inputs ` ` should specified The resulting trace can run inputs different types shapes assuming traced operations support those types shapes ` example_inputs ` may also single Tensor which case automatically wrapped tuple When value None ` ` example_kwarg_inputs ` ` should specified check_trace ` ` bool ` ` optional Check same inputs run through traced code produce same outputs Default ` ` True ` ` You might want disable example your network contains non- deterministic ops you sure network correct despite checker failure check_inputs list tuples optional A list tuples input arguments should used check trace against what expected Each tuple equivalent set input arguments would specified ` ` example_inputs ` ` For best results pass set checking inputs representative space shapes types inputs you expect network see If specified original ` ` example_inputs ` ` used checking check_tolerance float optional Floating-point comparison tolerance use checker procedure This can used relax checker strictness event results diverge numerically known reason such operator fusion strict ` ` bool ` ` optional run tracer strict mode default ` ` True ` ` Only turn off when you want tracer record your mutable container types currently ` ` list ` ` ` ` dict ` ` you sure container you using your problem ` ` constant ` ` structure does get used control flow conditions example_kwarg_inputs dict optional This parameter pack keyword arguments example inputs will passed function while tracing Default ` ` None ` ` Either argument ` ` example_inputs ` ` should specified The dict will unpacking arguments name traced function If keys dict don t match traced function s arguments name runtime exception will raised Returns If ` func ` ` nn Module ` ` ` forward ` ` ` nn Module ` ` trace ` returns ` ScriptModule ` object single ` ` forward ` ` method containing traced code The returned ` ScriptModule ` will have same set sub-modules parameters original ` ` nn Module ` ` If ` ` func ` ` standalone function ` ` trace ` ` returns ` ScriptFunction ` Example tracing function testcode torch foo x y x + y Run ` foo ` provided inputs record tensor operations traced_foo = torch jit trace foo torch rand torch rand ` traced_foo ` can now run TorchScript interpreter saved loaded Python-free environment Example tracing existing module torch torch nn nn Net nn Module __init__ - None super __init__ conv = nn Conv d forward x conv x n = Net example_weight = torch rand example_forward_input = torch rand Trace specific method construct ` ScriptModule ` single ` forward ` method module = torch jit trace n forward example_forward_input Trace module implicitly traces ` forward ` construct ` ScriptModule ` single ` forward ` method module = torch jit trace n example_forward_input _enabled func optimize None warnings warn ` optimize ` deprecated has no effect Use ` torch jit optimized_execution ` instead FutureWarning stacklevel= torch _utils_internal log_torchscript_usage traced_func = _trace_impl func example_inputs optimize check_trace check_inputs check_tolerance strict _force_outplace _module_class _compilation_unit example_kwarg_inputs _store_inputs log_torchscript_usage trace model_id=_get_model_id traced_func traced_func _trace_module_map Optional dict Any Any = None trace_module mod inputs optimize=None check_trace=True check_inputs=None check_tolerance= e- strict=True _force_outplace=False _module_class=None _compilation_unit=_python_cu example_inputs_is_kwarg=False _store_inputs=True Trace module executable ` ScriptModule ` will optimized using just-in-time compilation When module passed func ` torch jit trace torch jit trace ` only ` ` forward ` ` method run traced With ` ` trace_module ` ` you can specify dictionary method names example inputs trace see ` ` inputs ` ` argument below See func ` torch jit trace torch jit trace ` more information tracing Args mod torch nn Module A ` ` torch nn Module ` ` containing methods whose names specified ` ` inputs ` ` The given methods will compiled part single ` ScriptModule ` inputs dict A dict containing sample inputs indexed method names ` ` mod ` ` The inputs will passed methods whose names correspond inputs keys while tracing ` ` forward example_forward_input method example_method _input ` ` Keyword arguments check_trace ` ` bool ` ` optional Check same inputs run through traced code produce same outputs Default ` ` True ` ` You might want disable example your network contains non- deterministic ops you sure network correct despite checker failure check_inputs list dicts optional A list dicts input arguments should used check trace against what expected Each tuple equivalent set input arguments would specified ` ` inputs ` ` For best results pass set checking inputs representative space shapes types inputs you expect network see If specified original ` ` inputs ` ` used checking check_tolerance float optional Floating-point comparison tolerance use checker procedure This can used relax checker strictness event results diverge numerically known reason such operator fusion example_inputs_is_kwarg ` ` bool ` ` optional This parameter indicate whether example inputs pack pack keyword arguments Default ` ` False ` ` Returns A ` ScriptModule ` object single ` ` forward ` ` method containing traced code When ` ` func ` ` ` ` torch nn Module ` ` returned ` ScriptModule ` will have same set sub-modules parameters ` ` func ` ` Example tracing module multiple methods torch torch nn nn Net nn Module __init__ - None super __init__ conv = nn Conv d forward x conv x weighted_kernel_sum weight weight conv weight n = Net example_weight = torch rand example_forward_input = torch rand Trace specific method construct ` ScriptModule ` single ` forward ` method module = torch jit trace n forward example_forward_input Trace module implicitly traces ` forward ` construct ` ScriptModule ` single ` forward ` method module = torch jit trace n example_forward_input Trace specific methods module specified ` inputs ` constructs ` ScriptModule ` ` forward ` ` weighted_kernel_sum ` methods inputs = forward example_forward_input weighted_kernel_sum example_weight module = torch jit trace_module n inputs _enabled mod optimize None warnings warn ` optimize ` deprecated has no effect Use ` torch jit optimized_execution ` instead FutureWarning stacklevel= var_lookup_fn = _create_interpreter_name_lookup_fn isinstance mod torch nn Module raise AttributeError expected torch nn Module first argument isinstance inputs dict raise AttributeError expected dictionary method_name input pairs old_module_map = torch jit _trace _trace_module_map try trace_module_map dict Any Any = register_submods mod prefix name child mod named_children submod_qualname = prefix + + name trace_module_map child = submod_qualname register_submods child submod_qualname trace_module_map __module = mod torch jit _trace _trace_module_map = trace_module_map register_submods mod __module module = make_module mod _module_class _compilation_unit method_name example_inputs inputs items method_name == forward forward special case because we need trace ` Module __call__ ` which sets up some extra tracing uses argument names real ` Module forward ` method func = mod forward_method = getattr mod method_name argument_names = get_callable_argument_names forward_method func = getattr mod method_name argument_names = get_callable_argument_names func isinstance example_inputs dict example_inputs_is_kwarg Raise exception when user provided key names aligned forward method s arguments name key example_inputs key argument_names valid_arguments = + join argument_names + raise NameError f key forward method s arguments valid arguments name valid_arguments module _c _create_method_from_trace_with_dict method_name func example_inputs var_lookup_fn strict _force_outplace argument_names _store_inputs example_inputs = make_tuple example_inputs module _c _create_method_from_trace method_name func example_inputs var_lookup_fn strict _force_outplace argument_names _store_inputs check_trace_method = module _c _get_method method_name Check trace against new traces created user-specified inputs check_trace check_inputs None _check_trace check_inputs func check_trace_method check_tolerance strict _force_outplace True _module_class example_inputs_is_kwarg=example_inputs_is_kwarg _check_trace inputs func check_trace_method check_tolerance strict _force_outplace True _module_class example_inputs_is_kwarg=example_inputs_is_kwarg finally torch jit _trace _trace_module_map = old_module_map module is_tracing Return boolean value Returns ` ` True ` ` tracing function called during tracing code ` ` torch jit trace ` ` ` ` False ` ` otherwise is_scripting False torch _C _is_tracing TracedModule ScriptModule _disable_script_meta = True __init__ orig id_set=None _compilation_unit=None XXX orig can nn Module function super __init__ assert isinstance orig torch nn Module Copy subset ` orig ` temporary nn Module This way customize what will actually get compiled create_script_module id_set = set This allows us preserve original module s qualified name defining new type attribute _jit_override_qualname In torch _jit_internal _qualified_name we have special case will look up attribute override whatever qualname we would get python type system QualnameWrapper torch nn Module pass QualnameWrapper _jit_override_qualname = torch _jit_internal _qualified_name type ignore attr-defined type orig tmp_module = QualnameWrapper check_unique param param id_set raise ValueError TracedModules don t support parameter sharing between modules id_set add param tmp_module training = orig training name param orig _parameters items param None tmp_module _parameters name = param check_unique param name buf orig _buffers items buf None tmp_module _buffers name = buf check_unique buf name val orig __dict__ items torch _C _jit_is_script_object val name orig _parameters name orig _buffers setattr tmp_module name val orig _backward_hooks raise ValueError Modules have backward hooks assigned can t compiled + str orig name submodule orig _modules items submodule None continue tmp_module _modules name = make_module submodule TracedModule _compilation_unit=None script_module = torch jit _recursive create_script_module tmp_module lambda module share_types=False is_tracing=True __dict__ _name = type orig __name__ __dict__ _actual_script_module = script_module name _parameters _buffers _modules training delattr name forward args kwargs raise RuntimeError Trace submodules cannot called __getattr__ attr _actual_script_module __dict__ super __getattr__ attr getattr _actual_script_module attr __setattr__ attr value _actual_script_module __dict__ super __setattr__ attr value setattr _actual_script_module attr value _get_name _name extra_repr f original_name= _name TopLevelTracedModule TracedModule forward Callable Any = _CachedForward type ignore assignment _reconstruct cpp_module Re-construct instance TopLevelTracedModule using instance C++ module Args cpp_module The C++ module TopLevelTracedModule will rebuilt around __dict__ _actual_script_module _reconstruct cpp_module _script_if_tracing fn Callable P R - Callable P R functools wraps fn wrapper args P args kwargs P kwargs - R is_tracing Not tracing don t do anything fn args kwargs compiled_fn Callable P R = script wrapper __original_fn type ignore attr-defined compiled_fn args kwargs wrapper __original_fn = fn type ignore attr-defined wrapper __script_if_tracing_wrapper = True type ignore attr-defined wrapper _get_trace_graph f args= kwargs=None strict=True _force_outplace=False return_inputs=False _return_inputs_states=False Return tuple tracing function model warning This function internal-only should only used ONNX exporter If you trying get graph through tracing please go through public API instead trace = torch jit trace nn LSTMCell input hidden trace_graph = trace graph Trace function model returning tuple consisting both trace execution well original value If return_inputs also returns trace inputs part tuple Tracing guaranteed change semantics function module traced Args f torch nn Module function function module traced args tuple Tensor positional arguments pass function module traced A non-tuple assumed single positional argument passed model kwargs dict keyword arguments pass function module traced Example trace cell testcode trace = torch jit trace nn LSTMCell input hidden kwargs None kwargs = isinstance args tuple args = args outs = ONNXTracedModule f strict _force_outplace return_inputs _return_inputs_states args kwargs outs