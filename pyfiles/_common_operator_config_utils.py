mypy allow-untyped-defs copy operator collections namedtuple collections abc Callable typing Union torch torch ao nn intrinsic nni torch ao nn intrinsic qat nniqat torch ao nn qat nnqat torch ao nn quantized reference nnqr torch nn nn torch nn functional F torch ao quantization fuser_method_mappings _sequential_wrapper fuse_conv_bn fuse_conv_bn_relu fuse_convtranspose_bn fuse_linear_bn backend_config BackendPatternConfig DTypeConfig DTypeWithConstraints ObservationType __all__ list str = TODO rename more explicit e g qat_conv_relu _ConvMetadata = namedtuple _ConvMetadata root transpose bn reference transpose_reference fused_conv_relu fused_conv_bn fused_conv_bn_relu qat relu_qat bn_qat bn_relu_qat func func_transpose _Conv dMetadata = _ConvMetadata nn Conv d nn ConvTranspose d nn BatchNorm d nnqr Conv d nnqr ConvTranspose d nni ConvReLU d nni ConvBn d nni ConvBnReLU d nnqat Conv d nniqat ConvReLU d nniqat ConvBn d nniqat ConvBnReLU d F conv d F conv_transpose d _Conv dMetadata = _ConvMetadata nn Conv d nn ConvTranspose d nn BatchNorm d nnqr Conv d nnqr ConvTranspose d nni ConvReLU d nni ConvBn d nni ConvBnReLU d nnqat Conv d nniqat ConvReLU d nniqat ConvBn d nniqat ConvBnReLU d F conv d F conv_transpose d _Conv dMetadata = _ConvMetadata nn Conv d nn ConvTranspose d nn BatchNorm d nnqr Conv d nnqr ConvTranspose d nni ConvReLU d nni ConvBn d nni ConvBnReLU d nnqat Conv d nniqat ConvReLU d nniqat ConvBn d nniqat ConvBnReLU d F conv d F conv_transpose d Add constraints fixed qparams ops like sigmoid tanh ensure values fall within proper ranges e g sigmoid - tanh _FIXED_QPARAM_OP_ TO _CONSTRAINTS = DTypeWithConstraints dtype=torch quint quant_min_lower_bound= quant_max_upper_bound= scale_exact_match= zero_point_exact_match= _FIXED_QPARAM_OP_NEG TO _CONSTRAINTS = DTypeWithConstraints dtype=torch quint quant_min_lower_bound= quant_max_upper_bound= scale_exact_match= zero_point_exact_match= _FIXED_QPARAMS_OP_TO_CONSTRAINTS dict Union Callable str DTypeWithConstraints = torch nn Hardsigmoid _FIXED_QPARAM_OP_ TO _CONSTRAINTS torch nn functional hardsigmoid _FIXED_QPARAM_OP_ TO _CONSTRAINTS hardsigmoid _FIXED_QPARAM_OP_ TO _CONSTRAINTS hardsigmoid_ _FIXED_QPARAM_OP_ TO _CONSTRAINTS torch nn Sigmoid _FIXED_QPARAM_OP_ TO _CONSTRAINTS torch sigmoid _FIXED_QPARAM_OP_ TO _CONSTRAINTS sigmoid _FIXED_QPARAM_OP_ TO _CONSTRAINTS sigmoid_ _FIXED_QPARAM_OP_ TO _CONSTRAINTS torch nn Softmax _FIXED_QPARAM_OP_ TO _CONSTRAINTS torch nn Tanh _FIXED_QPARAM_OP_NEG TO _CONSTRAINTS torch tanh _FIXED_QPARAM_OP_NEG TO _CONSTRAINTS tanh _FIXED_QPARAM_OP_NEG TO _CONSTRAINTS tanh_ _FIXED_QPARAM_OP_NEG TO _CONSTRAINTS _get_binary_op_configs dtype_configs list DTypeConfig - list BackendPatternConfig binary_op_configs list BackendPatternConfig = num_tensor_args_to_observation_type_mapping = TODO used right now since we have extra check prepare will need change NO_OBSERVER later after we implemented Tensor dtype inference properly ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT ObservationType OUTPUT_SHARE_OBSERVER_WITH_INPUT ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT op_with_quantized_bop_scalar_variant operator add torch add operator mul torch mul bop_patterns = op_with_quantized_bop_scalar_variant nn ReLU op_with_quantized_bop_scalar_variant F relu op_with_quantized_bop_scalar_variant torch relu op_with_quantized_bop_scalar_variant binary_op_configs extend BackendPatternConfig bop_pattern set_dtype_configs dtype_configs noqa E _set_num_tensor_args_to_observation_type num_tensor_args_to_observation_type_mapping bop_pattern bop_patterns matmul binary_op_configs append BackendPatternConfig torch matmul set_dtype_configs dtype_configs noqa E binary_op_configs _get_linear_configs dtype_configs list DTypeConfig - list BackendPatternConfig Return all configs related linear modules ops observation_type = ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT linear_configs list BackendPatternConfig = Single linear modules functions ------------------------------------- linear module linear_configs append BackendPatternConfig torch nn Linear set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module torch nn Linear set_reference_quantized_module nnqr Linear set_qat_module nnqat Linear linear qat module linear_configs append BackendPatternConfig nnqat Linear set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module torch nn Linear set_reference_quantized_module nnqr Linear functional linear linear_configs append BackendPatternConfig torch nn functional linear set_observation_type observation_type noqa E set_dtype_configs dtype_configs _set_input_type_to_index weight bias Linear + relu ------------------- linear module + relu fusion config linear relu linear module + relu module linear_configs append BackendPatternConfig torch nn Linear torch nn ReLU set_dtype_configs dtype_configs noqa E set_fuser_method _sequential_wrapper nni LinearReLU set_fused_module nni LinearReLU linear relu linear module + functional relu linear_configs append BackendPatternConfig torch nn Linear torch nn functional relu set_dtype_configs dtype_configs noqa E set_fuser_method _sequential_wrapper nni LinearReLU set_fused_module nni LinearReLU linear module + relu fused module configs linear relu fused module linear_configs append BackendPatternConfig nni LinearReLU set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module torch nn Linear set_reference_quantized_module nnqr Linear set_qat_module nniqat LinearReLU linear relu qat fused module linear_configs append BackendPatternConfig nniqat LinearReLU set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module torch nn Linear set_reference_quantized_module nnqr Linear functional linear + relu configs linear relu functional linear + relu module linear_configs append BackendPatternConfig F linear torch nn ReLU set_observation_type observation_type noqa E set_dtype_configs dtype_configs linear relu functional linear + functional relu linear_configs append BackendPatternConfig F linear F relu set_observation_type observation_type noqa E set_dtype_configs dtype_configs Linear + batchnorm ------------------------ linear bn fusion linear_configs append BackendPatternConfig nn Linear nn BatchNorm d set_dtype_configs dtype_configs noqa E set_fuser_method fuse_linear_bn set_fused_module nni LinearBn d linear bn fused linear bn fused module linear_configs append BackendPatternConfig nni LinearBn d set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module torch nn Linear set_reference_quantized_module nnqr Linear set_qat_module nniqat LinearBn d linear bn qat fused module linear_configs append BackendPatternConfig nniqat LinearBn d set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module torch nn Linear set_reference_quantized_module nnqr Linear linear_configs _get_conv_configs dtype_configs Return all configs related conv modules ops conv_configs = observation_type = ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT convs _Conv dMetadata _Conv dMetadata _Conv dMetadata Single conv modules functions ----------------------------------- conv module conv_configs append BackendPatternConfig convs root set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module convs root set_reference_quantized_module convs reference set_qat_module convs qat conv qat module conv_configs append BackendPatternConfig convs qat set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module convs root set_reference_quantized_module convs reference functional conv conv_configs append BackendPatternConfig convs func set_observation_type observation_type noqa E set_dtype_configs dtype_configs _set_input_type_to_index weight bias Conv + relu ----------------- conv module + relu fusion configs conv relu fusion conv module + relu module conv_configs append BackendPatternConfig convs root torch nn ReLU set_dtype_configs dtype_configs noqa E set_fuser_method _sequential_wrapper convs fused_conv_relu set_fused_module convs fused_conv_relu conv relu fusion conv module + functional relu conv_configs append BackendPatternConfig convs root F relu set_dtype_configs dtype_configs noqa E set_fuser_method _sequential_wrapper convs fused_conv_relu set_fused_module convs fused_conv_relu conv module + relu fused module configs conv relu fused module conv_configs append BackendPatternConfig convs fused_conv_relu set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module convs root set_reference_quantized_module convs reference set_qat_module convs relu_qat conv relu qat fused module conv_configs append BackendPatternConfig convs relu_qat set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module convs root set_reference_quantized_module convs reference functional conv + relu configs conv relu functional conv + relu module conv_configs append BackendPatternConfig convs func torch nn ReLU set_observation_type observation_type noqa E set_dtype_configs dtype_configs conv relu functional conv + functional relu conv_configs append BackendPatternConfig convs func F relu set_observation_type observation_type noqa E set_dtype_configs dtype_configs fused conv relu conv_configs append BackendPatternConfig convs fused_conv_relu set_dtype_configs dtype_configs noqa E set_qat_module convs relu_qat conv_configs append BackendPatternConfig convs relu_qat set_dtype_configs dtype_configs noqa E set_root_module convs root set_reference_quantized_module convs reference Conv + batchnorm + relu ------------------------------- conv bn fusion configs conv + bn fusion conv_configs append BackendPatternConfig convs root convs bn set_dtype_configs dtype_configs noqa E set_fuser_method fuse_conv_bn set_fused_module convs fused_conv_bn conv + bn + relu module fusion conv_configs append BackendPatternConfig convs root convs bn nn ReLU set_dtype_configs dtype_configs noqa E set_fuser_method fuse_conv_bn_relu set_fused_module convs fused_conv_bn_relu conv + bn + relu functional fusion conv_configs append BackendPatternConfig convs root convs bn F relu set_dtype_configs dtype_configs noqa E set_root_module convs root set_fuser_method fuse_conv_bn_relu set_fused_module convs fused_conv_bn_relu TODO we can add fusion torch relu well conv + bn + relu fused module configs fused conv bn conv_configs append BackendPatternConfig convs fused_conv_bn set_dtype_configs dtype_configs noqa E set_qat_module convs bn_qat fused conv bn relu conv_configs append BackendPatternConfig convs fused_conv_bn_relu set_dtype_configs dtype_configs noqa E set_qat_module convs bn_relu_qat conv bn qat fused module conv_configs append BackendPatternConfig convs bn_qat set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module convs root set_reference_quantized_module convs reference conv bn relu qat fused module conv_configs append BackendPatternConfig convs bn_relu_qat set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module convs root set_reference_quantized_module convs reference conv transpose its fusion conv transpose config conv_configs append BackendPatternConfig convs transpose set_dtype_configs dtype_configs noqa E set_root_module convs transpose set_reference_quantized_module convs transpose_reference conv transpose + bn fusion conv_configs append BackendPatternConfig convs transpose convs bn set_dtype_configs dtype_configs noqa E set_fuser_method fuse_convtranspose_bn set_root_module convs transpose set_reference_quantized_module convs transpose_reference functional conv transpose conv_configs append BackendPatternConfig convs func_transpose set_dtype_configs dtype_configs noqa E _set_input_type_to_index weight bias conv_configs _get_cat_config dtype_configs list DTypeConfig - BackendPatternConfig BackendPatternConfig torch cat set_observation_type ObservationType OUTPUT_SHARE_OBSERVER_WITH_INPUT set_dtype_configs dtype_configs _get_ln_configs dtype_configs list DTypeConfig - list BackendPatternConfig ln_configs = ln_configs append BackendPatternConfig torch nn LayerNorm set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs dtype_configs ln_configs append BackendPatternConfig torch nn functional layer_norm set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs dtype_configs _set_input_type_to_index weight bias ln_configs _get_default_op_configs dtype_configs list DTypeConfig - list BackendPatternConfig default_ops = torch nn ELU torch nn LeakyReLU torch nn Hardswish torch nn InstanceNorm d torch nn InstanceNorm d torch nn InstanceNorm d torch nn Dropout torch nn PReLU torch nn functional elu torch nn functional hardswish torch nn functional leaky_relu torch nn functional dropout configs = BackendPatternConfig op set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs dtype_configs op default_ops configs append BackendPatternConfig torch nn functional group_norm set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs dtype_configs _set_input_type_to_index weight bias configs append BackendPatternConfig torch nn functional instance_norm set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs dtype_configs _set_input_type_to_index weight bias configs _add_fixed_qparams_to_dtype_configs dtype_configs list DTypeConfig constraints DTypeWithConstraints - list DTypeConfig Return copy list DTypeConfigs where activations subject specified constraints required fixed qparams ops If data type doesn t match one constraints simply leave corresponding DTypeConfig unchanged If ` scale_min_lower_bound ` ` scale_max_upper_bound ` specified activations throw exception since these settings incompatible fixed qparams ops new_dtype_configs = dtype_config dtype_configs dc = copy deepcopy dtype_config orig_constraints dc input_dtype_with_constraints dc output_dtype_with_constraints orig_constraints dtype = constraints dtype continue orig_constraints scale_min_lower_bound None raise ValueError f scale_min_lower_bound invalid fixed qparams ops dtype_config orig_constraints scale_max_upper_bound None raise ValueError f scale_max_upper_bound invalid fixed qparams ops dtype_config orig_constraints quant_min_lower_bound = constraints quant_min_lower_bound orig_constraints quant_max_upper_bound = constraints quant_max_upper_bound orig_constraints scale_exact_match = constraints scale_exact_match orig_constraints zero_point_exact_match = constraints zero_point_exact_match new_dtype_configs append dc new_dtype_configs _get_fixed_qparams_op_configs dtype_configs list DTypeConfig - list BackendPatternConfig fixed_qparams_op_configs = fixed_qparam_op constraints _FIXED_QPARAMS_OP_TO_CONSTRAINTS items new_dtype_configs = _add_fixed_qparams_to_dtype_configs dtype_configs constraints fixed_qparams_op_configs append BackendPatternConfig fixed_qparam_op set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs new_dtype_configs fixed_qparams_op_configs _get_share_qparams_op_configs dtype_configs Get operator config operators works both float quantized input input quantized output Tensor shares same quantization parameter input Example operator avgpool d reshape transpose maxpool d Example observed operator observer_ - avgpool d - observer_ same observer instance input _get_share_qprams_op_backend_config op BackendPatternConfig op set_observation_type ObservationType OUTPUT_SHARE_OBSERVER_WITH_INPUT set_dtype_configs dtype_configs share_qparams_ops = torch nn AdaptiveAvgPool d torch nn AdaptiveAvgPool d torch nn AdaptiveAvgPool d torch nn AvgPool d torch nn AvgPool d torch nn AvgPool d torch nn Hardtanh torch nn Identity torch nn MaxPool d torch nn MaxPool d torch nn MaxPool d torch nn PixelShuffle torch nn PixelUnshuffle torch nn ReLU torch nn ReLU torch adaptive_avg_pool d torch nn functional adaptive_avg_pool d torch nn functional adaptive_avg_pool d torch nn functional hardtanh torch nn functional hardtanh_ torch nn functional interpolate torch nn functional max_pool d torch nn functional max_pool d torch nn functional max_pool d torch nn functional pixel_shuffle torch nn functional pixel_unshuffle torch nn functional relu torch nn functional relu torch avg_pool d torch _C _nn avg_pool d torch _C _nn avg_pool d torch clamp torch flatten torch mean torch narrow torch repeat_interleave torch transpose torch squeeze torch stack torch unsqueeze operator floordiv contiguous clamp detach detach_ mean permute repeat repeat_interleave reshape resize_ relu relu_ squeeze squeeze_ transpose unsqueeze unsqueeze_ view _get_share_qprams_op_backend_config op op share_qparams_ops _get_bn_configs dtype_configs list DTypeConfig - list BackendPatternConfig Get configs related batchnorm bn_configs = bn_to_fused_bn = torch nn BatchNorm d nni BNReLU d torch nn BatchNorm d nni BNReLU d bn bn_to_fused_bn keys fused_bn = bn_to_fused_bn bn bn module + relu module fusion config bn_configs append BackendPatternConfig bn nn ReLU set_dtype_configs dtype_configs noqa E set_fuser_method _sequential_wrapper fused_bn set_fused_module fused_bn bn module + F relu fusion config bn_configs append BackendPatternConfig bn F relu set_dtype_configs dtype_configs noqa E set_fuser_method _sequential_wrapper fused_bn set_fused_module fused_bn bn_configs append BackendPatternConfig bn set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs dtype_configs fused bn configs fused_bn bn_to_fused_bn values bn_configs append BackendPatternConfig fused_bn set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs dtype_configs bn_configs _get_rnn_op_configs dtype_configs list DTypeConfig - list BackendPatternConfig rnn_op_configs = rnn_op ref_rnn_op nn GRUCell nnqr GRUCell nn LSTMCell nnqr LSTMCell nn RNNCell nnqr RNNCell nn LSTM nnqr LSTM nn GRU nnqr GRU rnn_op_configs append BackendPatternConfig rnn_op set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs dtype_configs set_root_module rnn_op set_reference_quantized_module ref_rnn_op rnn_op_configs _get_embedding_op_configs dtype_configs list DTypeConfig - list BackendPatternConfig embedding_op_configs = embedding_op qat_embedding_op ref_embedding_op nn Embedding nnqat Embedding nnqr Embedding nn EmbeddingBag nnqat EmbeddingBag nnqr EmbeddingBag embedding_op_configs append BackendPatternConfig embedding_op set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs dtype_configs set_qat_module qat_embedding_op set_root_module embedding_op set_reference_quantized_module ref_embedding_op config qat op embedding_op_configs append BackendPatternConfig qat_embedding_op set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E set_dtype_configs dtype_configs set_root_module embedding_op set_reference_quantized_module ref_embedding_op embedding_op_configs _get_tensor_info_op_configs dtype_configs These ops work tensors different dtypes non-tensors containing information about input tensor _get_config op BackendPatternConfig op set_observation_type ObservationType INPUT_OUTPUT_NOT_OBSERVED set_dtype_configs dtype_configs _get_config op op shape size