file contains ` _LoadBalancer ` its family implementation different load-balancing strategies tensor sharding functools abc ABC abstractmethod typing Optional torch torch Tensor torch nn attention flex_attention BlockMask make private since s still prototype _LoadBalancer ABC abstractmethod _generate_indices restore bool = False - Optional Tensor Generate indices load balancing Args restore bool Returns The generated indices shape ` seq_len ` load-balancing identical within batch ` batch_size seq_len ` load-balancing should vary within batch Warning For Multi-Head Attention we require masks over head dimension identical i e value ` _generate_indices ` does have ` heads ` dimension Example Here causal mask attention where q_len == kv_len == KV_index Q_index This mask matrix also represents computation required compute masked Q K^T - mask i j == computation Q i dot K j required - mask i j == computation should skipped Therefore number s matrix represents amount computation required Assume we want distribute Q K^T computation devices then matrix also distributed KV_index rank Q_index ------------------------ rank An imbalance computation observed these ranks could make rank straggler when performing Context Parallel In order balance computation we need rearrange QKV tensors before sharding such way result mask matrix evenly distributed over devices each rank has number s close possible This method defines strategy how rearrange QKV tensor better load-balance - when ` restore == False ` method returns indices tensor ` rearrange_idx ` such Q rearrange_idx desired Q tensor after rearranging - when ` restore == True ` method returns indices tensor ` restore_idx ` such Q rearrange_idx restore_idx == Q i e restoring rearranged tensor back original status before rearranging _HeadTailLoadBalancer _LoadBalancer __init__ seq_length int world_size int device str &#124; torch device seq_length = seq_length world_size = world_size device = device _generate_indices restore bool = False - Tensor Generate head-and-tail load balancing indices restore indices Args restore If True generate restore indices map head-and-tail rearranged positions back original positions If False generate load balance indices rearrange original positions head-and-tail pattern Returns The generated indices shape ` seq_len ` because load-balancing identical within batch Warning For Multi-Head Attention we require masks over head dimension identical i e value ` _generate_indices ` does have ` heads ` dimension Example Here causal mask attention where q_len == kv_len == KV_index Q_index Head-tail load-balance strategy rearranges Q tensor combining Q k seq dim Q -k rank Q k k Q - k -k rank so In python code looks like k = Q size cp_world_size rank range cp_world_size reordered_Q rank k rank + k = torch cat Q rank k rank + k Q - rank + k -rank k This can also done tensor slicing For above example indices tensor slicing slice_indices = Tensor After reordering QKV using ` slice_indices ` corresponding mask matrix distributing over devices becomes well-balanced KV_index rank Q_index ------------------------ rank To restore reordering putting tensor back slicing op can do trick ` restore_indices ` such slice_indices restore_indices == Tensor In way ` reordered_Q restore_indices ` will just original Q seq_length = seq_length world_size = world_size assert seq_length world_size == chunk_size = seq_length world_size all_indices = rank range world_size Generate indices first chunk cp rank first_chunk_start = rank chunk_size first_chunk_indices = list range first_chunk_start first_chunk_start + chunk_size Second chunk positions complementary chunk second_chunk_idx = world_size - rank - second_chunk_start = second_chunk_idx chunk_size second_chunk_indices = list range second_chunk_start second_chunk_start + chunk_size combine indices rank all_indices extend first_chunk_indices + second_chunk_indices all_indices_tensor = torch tensor all_indices dtype=torch int device=self device restore all_indices_tensor = torch argsort all_indices_tensor all_indices_tensor unsqueeze add batch dim _PerDocumentHeadTailLoadBalancer _LoadBalancer __init__ seq_length_per_doc list list int world_size int device str &#124; torch device ` seq_length_per_doc ` has size B seq_len load-balancing should vary within batch Otherwise ` seq_length_per_doc ` should have size seq_len seq_length_per_doc = seq_length_per_doc world_size = world_size device = device _generate_indices restore bool = False - Tensor Generate per-document head-and-tail rearrange indices so after rearranging input load-balanced per-document head-and-tail style Args restore If True generate restore indices map per-document head-and-tail rearranged positions back original positions If False generate load balance indices rearrange original positions per-document head-and-tail pattern Returns The generated indices shape ` batch_size seq_len ` load-balancing should vary within batch Otherwise should have shape ` seq_len ` Warning For Multi-Head Attention we require masks over head dimension identical i e ` seq_length_per_doc ` must have size B seq_len seq_len Example Here document causal mask attention where q_len == kv_len == KV_index Q_index The per-document head-and-tail load-balancer will apply head-and-tail reordering within each document After load-balancing context-parallel devices above mask matrix will look like KV_index Q_index ------------------------------------------------ torch stack _generate_indices_for_batch seq_lengths restore seq_lengths seq_length_per_doc _generate_indices_for_batch seq_length_per_doc restore - Tensor type ignore no-untyped-def world_size = world_size device = device assert all seq_length world_size == seq_length seq_length_per_doc chunk_length_per_doc = seq_length world_size seq_length seq_length_per_doc indices = document_start_idx = seq_length chunk_length zip seq_length_per_doc chunk_length_per_doc Generate indices current document rank range world_size head_chunk_start_idx = document_start_idx + chunk_length rank tail_chunk_end_idx = document_start_idx + chunk_length world_size - rank indices append torch arange head_chunk_start_idx head_chunk_start_idx + chunk_length device=device indices append torch arange tail_chunk_end_idx - chunk_length tail_chunk_end_idx device=device document_start_idx += seq_length indices_tensor = torch cat indices restore indices_tensor = torch argsort indices_tensor indices_tensor _PTRRLoadBalancer _LoadBalancer Processing-Time based Round-Robin PTRR load balancer This load balancer should only used flex_attention since leverages ` BlockMask ` __init__ block_mask BlockMask world_size int ` block_mask ` must have shape B seq_len seq_len seq_len seq_len block_mask = block_mask world_size = world_size staticmethod ptrr_scheduling process_time Tensor group_size int - Tensor Separate tasks into ` group_size ` groups using PTRR scheduling process_time D tensor size n where n number tasks The value process time task Size ` n ` must divisible ` group_size ` group_size number groups Returns tasks_in_group list list int A collection list int each list should have size ` n group_size ` ` group_size ` lists total Each element index input ` process_time ` i e len process_time - Example process_time = tasks_in_group = values = sum = values = sum = values = sum = values = sum = assert process_time ndim == num_tasks = process_time size num_tasks group_size = raise NotImplementedError f num_tasks num_tasks must divisible group_size group_size device = process_time device _ sorted_indices_descending = torch sort process_time descending=True stable=True process time tied order preserved sorted_indices_descending_reversed = torch flip sorted_indices_descending view - group_size dims= view - tasks_in_group = torch where torch arange num_tasks device=device group_size == sorted_indices_descending sorted_indices_descending_reversed tasks_in_group = tasks_in_group view - group_size transpose group_size n group_size sort each group This step should have impact correctness nor execution run time helps users visualize mask tasks_in_group _ = torch sort tasks_in_group dim= tasks_in_group _generate_indices restore bool = False - Tensor Generate PTRR reorder indices shape ` seq_len ` ` batch_size seq_len ` Args restore If True generate restore indices map Processing-Time based Round-Robin PTRR rearranged positions back original positions If False generate load balance indices rearrange original positions PTRR pattern Returns The generated indices shape ` seq_len ` load-balancing identical within batch i e ` BlockMask shape == ` ` batch_size seq_len ` load-balancing should vary within batch Warning For Multi-Head Attention we require masks over head dimension identical i e ` block_mask ` must have shape B seq_len seq_len seq_len seq_len Example Here document causal mask attention whereq_len == kv_len == BLOCK_SIZE each entry block KV_index - row value = - row value = - row value = - row value = - row value = - row value = - row value = Q_index - row value = - row value = - row value = - row value = - row value = - row value = - row value = - row value = - row value = The reorder indices will mask matrix will look like KV_index - row value = - row value = - row value = - row value = - row value = rank sum= - row value = - row value = - row value = ------------------------------------------------ - row value = - row value = - row value = - row value = - row value = rank sum= - row value = - row value = - row value = block_mask = block_mask kv_num_blocks = block_mask kv_num_blocks full_kv_num_blocks = block_mask full_kv_num_blocks non_sparse_kv_num_blocks = kv_num_blocks + full_kv_num_blocks full_kv_num_blocks None kv_num_blocks B H Q = non_sparse_kv_num_blocks shape requirement masking identical across heads i e H == BlockMask non_sparse_kv_num_blocks = non_sparse_kv_num_blocks view - Q B Q_BLK batch_ptrr = torch vmap functools partial _PTRRLoadBalancer ptrr_scheduling group_size=self world_size ptrr_indices = batch_ptrr non_sparse_kv_num_blocks B group_size num_blks_in_group ptrr_indices = ptrr_indices reshape B - B num_blocks NOTE only support case where qkv block size equal q_blk_size kv_blk_size = block_mask BLOCK_SIZE assert q_blk_size == kv_blk_size now only support q_blk_size == kv_blk_size indices = torch arange q_blk_size ptrr_indices size device=ptrr_indices device view - q_blk_size NUM_BLOCKS BLOCK_SIZE indices = indices ptrr_indices view B - B qkv_size restore indices = torch vmap torch argsort indices indices _create_default_load_balancer seq_length int world_size int device str &#124; torch device - Optional _LoadBalancer _attention _cp_options _cp_options enable_load_balance _HeadTailLoadBalancer seq_length world_size device None