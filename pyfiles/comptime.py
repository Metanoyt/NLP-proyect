This module provides public comptime interface TorchDynamo enabling users execute arbitrary Python code during symbolic evaluation their programs The comptime interface allows inspection modification TorchDynamo s compilation process while running This can useful - Debugging compilation issues - Inspecting intermediate state - Adding custom guards graph breaks - Analyzing symbolic shapes values Example usage torch torch _dynamo comptime comptime my_model x Print compile-time known information about x comptime print x Print current FX graph being constructed comptime print_graph Force value treated static comptime lambda ctx ctx get_local x is_dynamic comptime force_static x Add manual graph break comptime graph_break Note While API provides significant flexibility intentionally avoids exposing internal implementation details TorchDynamo maintain compatibility across versions builtins dis time traceback collections abc Callable Sequence typing Any Optional TextIO Union torch torch _dynamo symbolic_convert InstructionTranslatorBase torch _dynamo variables base VariableTracker torch _subclasses fake_tensor FakeTensor torch fx experimental symbolic_shapes free_symbols exc unimplemented_v variables CellVariable variables constant ConstantVariable variables tensor SymNodeVariable ComptimeVar A ComptimeVar represents Python value some particular point time Python code we symbolically evaluating torchdynamo This must distinguished runtime value compile-time there some properties variable we do know example ComptimeVar represents Tensor we only know metadata about tensor we do NOT know what actual data Tensor __init__ v VariableTracker - None __variable = v as_proxy - Union VariableTracker Sequence VariableTracker Returns fx Proxy tuple list fx Proxy representing variable FX graph we assembling pass user compiler This method only works variables we actually track FX graph aka Tensors ints you compiling dynamic shapes In particular you have list tuple tensors you will get list tuple proxies single proxy representing entire list tuple __variable as_proxy is_proxy - bool Returns True as_proxy would succeed __variable is_proxy as_fake - Union FakeTensor torch SymInt Returns fake value either FakeTensor SymInt representing variable question This only works variables denote Tensor int You can use query metadata e g v as_fake size will tell you compile-time known size tensor WARNING Do NOT mutate returned tensor __variable as_proxy node meta example_value size dim Optional int = None - Union int torch SymInt Returns size tensor dim None size dimension dim The returned size may SymInt as_fake size dim type ignore union-attr return-value python_type - type Returns what type v would have returned variable compile time __variable python_type as_python_constant - Any Returns Python value variable would have only completely known compile-time e g constant WARNING Do NOT mutate returned constant The returned constant may may correspond actual value variable may take runtime example variable question constant list we may copy list __variable as_python_constant is_python_constant - bool Returns True as_python_constant would succeed __variable is_python_constant is_dynamic - bool isinstance __variable SymNodeVariable fs = free_symbols __variable sym_num bool fs False force_static - None Forces value static inducing guard its specific value isinstance __variable SymNodeVariable __variable evaluate_expr isinstance __variable ConstantVariable TODO Maybe complain isn t int bool float variable pass raise AssertionError f cannot force __variable type __variable static _i_will_not_complain_if_bc_breaks_VariableTracker - VariableTracker Returns internal data structure VariableTracker Dynamo uses represent variables compile time There no BC guarantees API WE RESERVE THE RIGHT TO BREAK YOUR CODE you rely __variable __repr__ - str __variable debug_repr TODO API adding custom guard ComptimeContext This context provides access public API Dynamo s internals If there something here you would find useful missing please file feature request https github com pytorch pytorch __init__ tx InstructionTranslatorBase - None __tx = tx get_local name str stacklevel int = - ComptimeVar Retrieve compile-time known information about local tx = __get_tx stacklevel var = tx symbolic_locals name Auto-dereference when accessing cell locals python isinstance var CellVariable ComptimeVar tx output side_effects load_cell var ComptimeVar var graph_break msg str = ComptimeContext graph_break - None Manually trigger graph break unimplemented_v gb_type= ComptimeContext graph break context=msg explanation=f Manually triggered ComptimeContext graph break message msg hints= graph - torch fx Graph Retrieve partially constructed FX graph would passed user compiler after compilation __tx output graph assert_static val ComptimeVar - None Asserts int static dynamic per dynamic shapes assert val is_dynamic expected static got dynamic run TORCH_LOGS=dynamic more info print_graph verbose bool = True file Optional TextIO = None - None Print partially constructed FX graph would passed user compiler after compilation print __tx output graph python_code verbose=verbose src file=file parent - ComptimeContext ComptimeContext __tx parent type ignore arg-type __get_tx stacklevel int - Any tx = __tx _ range stacklevel tx = tx parent type ignore assignment tx print val Any file Optional TextIO = None - None print repr val file=file print_disas file Optional TextIO = None stacklevel int = - None Print current series opcodes being executed including parent frames including where you particular opcode stream tx = __get_tx stacklevel print dis Bytecode tx f_code current_offset=tx instructions tx instruction_pointer offset dis file=file print_value_stack file Optional TextIO = None stacklevel int = - None Print current Python value stack Note NOT same traceback use print_bt print Note stacklevel= will typically empty comptime cannot currently used expression context where there would intermediates stack If you would find useful please file bug https github com pytorch pytorch NB Stack grows downwards our print tx = __get_tx stacklevel s tx stack print f - s debug_repr file=file print_locals file Optional TextIO = None stacklevel int = - None Print all locals available current context By default view very limited you can get more information about any individual local using get_local tx = __get_tx stacklevel k v tx symbolic_locals items print f k = v debug_repr file=file print_bt file Optional TextIO = None stacklevel int = - None Print user code backtrace starting beginning frame Dynamo started evaluating Note MAY NOT go all way torch compile invocation we may have done graph break compiling intermediate frame starting point If you think other behavior would better file bug https github com pytorch pytorch stack = tx = __get_tx stacklevel while tx None stack append tx frame_summary tx = getattr tx parent None print join traceback StackSummary from_list reversed stack format file=file print_guards file Optional TextIO = None - None Print currently installed guards Dynamo context This does NOT include guards associated variables may may installed future those variables used TODO improve print format current guard format extremely verbose print \n join f repr guard guard sorted __tx output guards file=file _i_will_not_complain_if_bc_breaks_InstructionTranslator - InstructionTranslatorBase Returns internal data structure InstructionTranslator Dynamo uses track state symbolic evaluation There no BC guarantees API WE RESERVE THE RIGHT TO BREAK YOUR CODE you rely __tx sleep sec Union int float - None time sleep sec _Comptime staticmethod __call__ fn Callable ComptimeContext Any fallback_fn Callable Any = lambda None - Any fn gets called compile time TorchDynamo calls fallback_fn otherwise fallback_fn Convenience wrappers more compact use staticmethod graph_break - None comptime lambda ctx ctx graph_break staticmethod print e Any - None comptime lambda ctx ctx print ctx get_local e lambda print e staticmethod print_graph - None comptime lambda ctx ctx print_graph staticmethod print_disas stacklevel int = - None comptime lambda ctx ctx print_disas stacklevel=ctx get_local stacklevel as_python_constant + staticmethod print_value_stack stacklevel int = - None comptime lambda ctx ctx print_value_stack stacklevel=ctx get_local stacklevel as_python_constant + This more useful variant print_value_stack can used expression context e g x + print_value_stack_and_return y + z you will see x stack prior addition operation staticmethod print_value_stack_and_return e Any stacklevel int = - Any comptime lambda ctx ctx print_value_stack stacklevel=ctx get_local stacklevel as_python_constant + e staticmethod print_locals stacklevel int = - None comptime lambda ctx ctx print_locals stacklevel=ctx get_local stacklevel as_python_constant + staticmethod print_bt stacklevel int = - None comptime lambda ctx ctx print_bt stacklevel=ctx get_local stacklevel as_python_constant + staticmethod print_guards - None comptime lambda ctx ctx print_guards staticmethod assert_static val Any - None comptime lambda ctx ctx assert_static ctx get_local val staticmethod force_static val Any - None comptime lambda ctx ctx get_local val force_static staticmethod breakpoint - None Like pdb breakpoint drop into pdb whenever line code compiled dynamo Use putting your model code torch _dynamo comptime comptime comptime breakpoint And then inside pdb you can access ctx query things about compilation context Pdb ctx print_bt Pdb ctx print_locals Pdb p ctx get_local attention as_fake inner inner_ctx ComptimeContext - None ctx = inner_ctx parent noqa F builtins breakpoint comptime inner staticmethod sleep sec Union int float - None comptime lambda ctx ctx sleep ctx get_local sec as_python_constant comptime = _Comptime