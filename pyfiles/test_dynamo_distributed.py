Owner s module dynamo contextlib copy functools random unittest contextlib contextmanager datetime timedelta io StringIO unittest mock patch numpy np torch torch _dynamo torch _dynamo logging torch _dynamo test_case torch distributed dist torch optim optim torch nn torch _C FileCheck torch _dynamo config torch _dynamo backends distributed DDPOptimizer torch _dynamo comptime comptime torch _dynamo testing collect_results torch _dynamo utils same torch _higher_order_ops wrap tag_activation_checkpoint torch compiler set_enable_guard_collectives torch distributed _functional_collectives _maybe_wrap_tensor torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp wrap lambda_auto_wrap_policy transformer_auto_wrap_policy torch nn attention flex_attention flex_attention torch nn parallel DistributedDataParallel DDP torch testing _internal common_cuda PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION torch testing _internal common_distributed _dynamo_dist_per_rank_init DynamoDistributedMultiProcTestCase DynamoDistributedSingleProcTestCase import_transformers_or_skip requires_accelerator_dist_backend skip_if_lt_x_gpu torch testing _internal common_utils skipIfXpu torch testing _internal inductor_utils HAS_GPU torch testing _internal triton_utils requires_cuda_and_triton reset_rng_state torch manual_seed random seed np random seed init_weights m isinstance m nn Linear nn init xavier_uniform_ m weight m bias data fill_ contextmanager enable_guard_collectives old = set_enable_guard_collectives True try yield finally set_enable_guard_collectives old ToyModel nn Module __init__ in_feat= hidden_feat= out_feat= ctx_manager=None super __init__ ctx_manager = ctx_manager net = nn Sequential nn Linear in_feat hidden_feat nn ReLU + nn Linear hidden_feat hidden_feat nn ReLU + nn Linear hidden_feat hidden_feat nn ReLU + nn Linear hidden_feat out_feat nn ReLU forward inputs ctx_manager None ctx_manager net inputs net inputs get_model device bsz= in_feat= hidden_feat= out_feat= ctx_manager=None m = ToyModel in_feat=in_feat hidden_feat=hidden_feat out_feat=out_feat ctx_manager=ctx_manager device m apply init_weights inputs = torch rand bsz in_feat device outputs = m inputs m inputs outputs MutatingModel nn Module __init__ in_feat= hidden_feat= out_feat= ctx_manager=None super __init__ ctx_manager = ctx_manager net = nn Sequential nn Linear in_feat hidden_feat nn ReLU + nn Linear hidden_feat hidden_feat nn ReLU + nn Linear hidden_feat hidden_feat nn ReLU + nn Linear hidden_feat out_feat nn ReLU state = forward inputs state = net inputs state get_mutating_model device bsz= in_feat= hidden_feat= out_feat= ctx_manager=None m = MutatingModel in_feat=in_feat hidden_feat=hidden_feat out_feat=out_feat ctx_manager=ctx_manager device m apply init_weights inputs = torch rand bsz in_feat device outputs = m inputs m inputs outputs ForcedGetAttrMod torch nn Module __init__ device super __init__ linear = torch nn Linear __dict__ forced_linear = torch nn Linear device=device counter = forward x counter += x linear x forced_linear weight get_forced_getattr_module device mod = ForcedGetAttrMod device device=device x = torch randn device=device mod x mod x ToyInnerModel nn Module __init__ - None super __init__ layers = nn Linear nn Linear layers = nn Sequential layers forward inputs layers inputs ToyOuterModel nn Module __init__ device super __init__ layers = ToyInnerModel device _ range layers = nn Sequential layers nn ReLU layers nn ReLU forward inputs layers inputs get_toy_model_for_activation_checkpointing device m = ToyOuterModel device device m apply init_weights inputs = torch rand device m inputs find_first_node gm func node gm graph nodes node target func node None apply_fsdp_with_checkpointing model wrap_policy checkpoint_policy use_activation_checkpointing=True torch distributed algorithms _checkpoint checkpoint_wrapper apply_activation_checkpointing checkpoint_wrapper CheckpointImpl model = FSDP copy deepcopy model auto_wrap_policy=wrap_policy use_orig_params=True use_activation_checkpointing checkpoint_wrapper_fn = functools partial checkpoint_wrapper checkpoint_impl=CheckpointImpl NO_REENTRANT apply_activation_checkpointing model checkpoint_wrapper_fn=checkpoint_wrapper_fn check_fn=checkpoint_policy model get_custom_model device MyCustomLinear torch nn Module __init__ - None super __init__ weight = nn Parameter torch randn forward x tmp = torch mm x weight t test edge case where torch where scalar decomposed aten where tensor tensor tensor tensors T T wrapped FakeTensors during DDPOptimizer compilation tmp + torch where tmp MyLinear torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x MyModule torch nn Module __init__ - None super __init__ mods = MyLinear torch nn ReLU sandwich custom middle so comes before after MyCustomLinear torch nn ReLU MyLinear torch nn ReLU seq = torch nn Sequential x items mods x items forward x y test special case where th bucket layers close graph input capacity which would trigger new bucket there only trivial ops without parameters put into new bucket optimize case fusing empty bucket back together previous full one seq x + y m = MyModule device m apply init_weights inputs = torch rand device test duplicated inputs inputs = inputs inputs correct_outputs = m inputs m inputs correct_outputs get_hf_bert rank Note use import_transformers_or_skip your test case you use multiprocessing test try transformers AutoModelForMaskedLM BertConfig except ImportError e raise unittest SkipTest Unable transformers e device_type = acc type acc = torch accelerator current_accelerator cpu batch_size max_length config device = BertConfig f device_type rank model = AutoModelForMaskedLM from_config config device input_ids = torch randint config vocab_size batch_size max_length device decoder_ids = torch randint config vocab_size batch_size max_length device inputs = input_ids input_ids labels decoder_ids model train model inputs CheckSplitsCompiler __init__ - None compiler_called = compile_fn gm example_inputs compiler_called += gm This simulates DDP doesn t actually do any process communication just has enough properties so dynamo distributed optimization able optimize Feel free simulate more properties necessary The other important thing patching _active_ddp_module which what actually triggers DDP optimization FakeDDP nn Module __init__ module bucket_cap_mb= super __init__ module = module bucket_bytes_cap = int bucket_cap_mb contextmanager _inside_ddp_forward DDP _active_ddp_module = try yield finally DDP _active_ddp_module = None forward inputs kwargs DDP _active_ddp_module _inside_ddp_forward module forward inputs kwargs module forward inputs kwargs run_hf_bert_ddp model inputs backend reset_rng_state correct_outputs = model inputs correct_loss = correct_outputs loss correct_loss backward reset_rng_state opt_model = torch compile model backend=backend opt_outputs = opt_model inputs opt_loss = opt_outputs loss opt_loss backward inputs_flat = inputs k k inputs correct_results = collect_results model correct_outputs logits correct_loss inputs_flat opt_results = collect_results opt_model opt_outputs logits opt_loss inputs_flat assertTrue same correct_results opt_results TestFakeDistributedSingleProc torch _dynamo test_case TestCase unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch patch object config optimize_ddp True patch object torch _inductor config fallback_random True unittest skipIf torch _inductor config triton native_matmul FIXME native matmul fails RuntimeError Cannot access data pointer Tensor test_hf_bert_ddp_inductor model inputs = get_hf_bert model = FakeDDP model run_hf_bert_ddp model inputs inductor patch object config optimize_ddp True test_hf_bert_ddp_aot_eager model inputs = get_hf_bert model = FakeDDP model run_hf_bert_ddp model inputs aot_eager patch object config optimize_ddp True test_issue Model nn Module forward torch randn torch randn model = Model model = FakeDDP model opt_model = torch compile model backend= aot_eager opt_model patch object config optimize_ddp True test_symbol_splitting Model nn Module __init__ - None super __init__ weight = nn Parameter torch randn weight = nn Parameter torch randn forward x x = torch cat x x y = x weight z = x + y weight z model = Model model = FakeDDP model opt_model = torch compile dynamic=True model opt_model torch randn patch object config optimize_ddp True test_ddp_optimizer_inductor_strides_dont_specialize Model nn Module __init__ super __init__ fc_ = nn Linear fc_ = nn Linear forward x x = fc_ x x = fc_ x x model = Model model = FakeDDP model inp = torch randn inp = torch randn torch _dynamo mark_dynamic inp torch _dynamo mark_dynamic inp torch _dynamo utils clear_compilation_metrics torch _dynamo reset try DDP _active_ddp_module = model opt_model = torch compile model assertEqual len torch _dynamo utils get_compilation_metrics opt_model inp compile_count_before = len torch _dynamo utils get_compilation_metrics opt_model inp compile_count_after = len torch _dynamo utils get_compilation_metrics no recompiles assertEqual compile_count_before compile_count_after finally DDP _active_ddp_module = None config patch optimize_ddp=True capture_scalar_outputs=True test_unbacked_symbol_splitting_direct Model nn Module __init__ - None super __init__ weight = nn Parameter torch randn weight = nn Parameter torch randn forward x y u _ = y tolist x = torch cat x x y = x weight z = x + y weight u z model = Model model = FakeDDP model opt_model = torch compile dynamic=True model opt_model torch randn torch tensor config patch optimize_ddp=True capture_scalar_outputs=True test_unbacked_symbol_splitting_indirect Model nn Module __init__ - None super __init__ weight = nn Parameter torch randn weight = nn Parameter torch randn forward x y u _ = y tolist = torch ones u x = torch cat x x y = x weight z = x + y weight sum z model = Model model = FakeDDP model opt_model = torch compile dynamic=True model opt_model torch randn torch tensor config patch optimize_ddp=True capture_scalar_outputs=True test_unbacked_symbol_splitting_torture_multi Model nn Module __init__ - None super __init__ weight = nn Parameter torch randn weight = nn Parameter torch randn weight = nn Parameter torch randn forward x y partition one contains u u _ = y tolist x = torch cat x x y = x weight partition two contains variable y = y weight = torch ones u partition three z = x + y weight sum z model = Model model = FakeDDP model bucket_cap_mb= opt_model = torch compile dynamic=True model opt_model torch randn torch tensor config patch optimize_ddp=True capture_dynamic_output_shape_ops=True test_unbacked_symbol_splitting_no_binding Model nn Module __init__ - None super __init__ weight = nn Parameter torch randn weight = nn Parameter torch randn forward x y nz = y nonzero x = torch cat x x y = x weight z = x + y weight nz + sum z model = Model model = FakeDDP model opt_model = torch compile dynamic=True model opt_model torch randn torch tensor patch object config optimize_ddp True test_call_method_forward Model nn Module __init__ super __init__ layers = _ range layer = nn ModuleList nn LayerNorm nn MultiheadAttention embed_dim= num_heads= batch_first=True layers append layer layers = nn ModuleList layers forward x torch Tensor - torch Tensor x Batch Freq Time Feature B F T H = x shape m layers x = x reshape B F T H x = m x x _ = m forward x x x x = x reshape B F T H x model = Model model = FakeDDP model opt_model = torch compile model opt_model torch randn Are these tests failing Check see TestFakeDistributedSingleProc has single process version s just problem Dynamo distributed optimizer you should able repro single process requires_accelerator_dist_backend nccl xccl TestMultiProc DynamoDistributedMultiProcTestCase Note MultiProcTestCase spawns processes per test slow Prefer MultiThreadedTestCase most tests Perhaps use one sparingly integration tests device_type = acc type acc = torch accelerator current_accelerator cpu skip_if_lt_x_gpu config patch optimize_ddp=False enable_compiler_collectives=True test_ddp_baseline_aot_eager_multiprocess _dynamo_dist_per_rank_init rank world_size assertFalse config optimize_ddp m inputs correct_outputs = get_model f device_type rank m = DDP m device_ids= rank m = torch compile m backend= aot_eager outputs = m inputs assertTrue same correct_outputs outputs _test_hf_bert_ddp_inductor static_graph _dynamo_dist_per_rank_init rank world_size model inputs = get_hf_bert rank model = DDP model static_graph=static_graph run_hf_bert_ddp model inputs inductor skip_if_lt_x_gpu import_transformers_or_skip unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch optimize_ddp=True enable_compiler_collectives=True patch object torch _inductor config fallback_random True test_hf_bert_ddp_inductor _test_hf_bert_ddp_inductor static_graph=False skip_if_lt_x_gpu import_transformers_or_skip unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch optimize_ddp=True enable_compiler_collectives=True patch object torch _inductor config fallback_random True test_hf_bert_ddp_inductor_static_graph _test_hf_bert_ddp_inductor static_graph=True _test_hf_bert_aot_eager static_graph _dynamo_dist_per_rank_init rank world_size model inputs = get_hf_bert rank model = DDP model static_graph=static_graph run_hf_bert_ddp model inputs aot_eager skip_if_lt_x_gpu import_transformers_or_skip config patch optimize_ddp=True enable_compiler_collectives=True test_hf_bert_ddp_aot_eager _test_hf_bert_aot_eager static_graph=False skip_if_lt_x_gpu import_transformers_or_skip config patch optimize_ddp=True enable_compiler_collectives=True test_hf_bert_ddp_aot_eager_static_graph _test_hf_bert_aot_eager static_graph=True skip_if_lt_x_gpu unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch optimize_ddp=False enable_compiler_collectives=True test_ddp_activation_checkpointing torch distributed algorithms _checkpoint checkpoint_wrapper apply_activation_checkpointing checkpoint_wrapper CheckpointImpl MyModel torch nn Module __init__ - None super __init__ fc = torch nn Linear fc = torch nn Linear fc = torch nn Linear forward inp fc fc fc inp _dynamo_dist_per_rank_init rank world_size assertFalse config optimize_ddp model = MyModel device=self device_type Activation checkpointing Linear layers non_reentrant_wrapper = functools partial checkpoint_wrapper checkpoint_impl=CheckpointImpl NO_REENTRANT check_fn = lambda submodule isinstance noqa E submodule torch nn Linear apply_activation_checkpointing model checkpoint_wrapper_fn=non_reentrant_wrapper check_fn=check_fn model = DDP model x = torch randn device_type correct_outputs = model x opt_model = torch compile model outputs = opt_model x assertTrue same correct_outputs outputs config patch enable_compiler_collectives=True skip_if_lt_x_gpu test_fsdp_aot_eager _dynamo_dist_per_rank_init rank world_size Test basic FSDP wrapping outer wrap around whole model m inputs correct_outputs = get_model f device_type rank fsdp_m = FSDP m use_orig_params=True fsdp_m = torch compile fsdp_m backend= aot_eager outputs = fsdp_m inputs assertTrue same correct_outputs outputs Test recursive wrapping nested FSDP around each Linear m inputs correct_outputs = get_model f device_type rank fsdp_m = FSDP m auto_wrap_policy=functools partial transformer_auto_wrap_policy transformer_layer_cls= nn Linear use_orig_params=True fsdp_m = torch compile fsdp_m backend= aot_eager outputs = fsdp_m inputs assertTrue same correct_outputs outputs skip_if_lt_x_gpu unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch requires_cuda_and_triton test_ddp_optimizer_cudagraph Net nn Module __init__ super __init__ need large channel trigger ddp optimizer split module CHANNELS = convi = nn Conv d CHANNELS padding= bias=False convp = nn Conv d CHANNELS CHANNELS padding= bias=False bni = nn BatchNorm d CHANNELS forward bitmap_channels x = convi bitmap_channels x = bni x x = convp x x _dynamo_dist_per_rank_init rank world_size net = Net rank optimizer = torch optim SGD net parameters lr= e- net = DDP net device_ids= rank opt_net = torch compile net mode= reduce-overhead opt_net train _ range optimizer zero_grad data = torch randn dtype=torch float device= cuda opt_net data sum backward fwd bwd graph such graphs total graph_id = torch _inductor cudagraph_trees get_container rank tree_manager new_graph_id id assertTrue graph_id == config patch enable_compiler_collectives=True skip_if_lt_x_gpu test_fsdp_setattr _dynamo_dist_per_rank_init rank world_size Test basic FSDP wrapping outer wrap around whole model torch _dynamo utils counters counters clear m inputs correct_outputs = get_mutating_model f device_type rank fsdp_m = FSDP m use_orig_params=True fsdp_m = torch compile fsdp_m backend= eager fullgraph=False outputs = fsdp_m inputs assertTrue same correct_outputs outputs assertEqual len counters graph_break first_graph_break = list counters graph_break keys noqa RUF assertIn setattr Tensor requires_grad first_graph_break config patch inline_inbuilt_nn_modules=False config patch enable_compiler_collectives=True skip_if_lt_x_gpu test_fsdp_unspecialized_forced_getattr_no_inline _dynamo_dist_per_rank_init rank world_size Test basic FSDP wrapping outer wrap around whole model torch _dynamo utils counters counters clear m inputs correct_outputs = get_forced_getattr_module f device_type rank fsdp_m = FSDP m use_orig_params=True fsdp_m = torch compile fsdp_m backend= eager fullgraph=False outputs = fsdp_m inputs assertTrue same correct_outputs outputs config patch enable_compiler_collectives=True skip_if_lt_x_gpu test_fsdp_unspecialized_forced_getattr_inline _dynamo_dist_per_rank_init rank world_size Test basic FSDP wrapping outer wrap around whole model torch _dynamo utils counters counters clear m inputs correct_outputs = get_forced_getattr_module f device_type rank fsdp_m = FSDP m use_orig_params=True fsdp_m = torch compile fsdp_m backend= eager fullgraph=False outputs = fsdp_m inputs assertTrue same correct_outputs outputs config patch enable_compiler_collectives=True skip_if_lt_x_gpu unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_fsdp_inductor _dynamo_dist_per_rank_init rank world_size Test basic FSDP wrapping outer wrap around whole model m inputs correct_outputs = get_model f device_type rank fsdp_m = FSDP m use_orig_params=True fsdp_m = torch compile fsdp_m backend= inductor outputs = fsdp_m inputs assertTrue same correct_outputs outputs Test recursive wrapping nested FSDP around each Linear m inputs correct_outputs = get_model f device_type rank fsdp_m = FSDP m auto_wrap_policy=functools partial transformer_auto_wrap_policy transformer_layer_cls= nn Linear use_orig_params=True fsdp_m = torch compile fsdp_m backend= inductor outputs = fsdp_m inputs assertTrue same correct_outputs outputs config patch enable_compiler_collectives=True skip_if_lt_x_gpu unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_fsdp_activation_checkpointing _dynamo_dist_per_rank_init rank world_size model inputs = get_toy_model_for_activation_checkpointing f device_type rank is_inner = lambda module isinstance module ToyInnerModel noqa E wrap_policy = functools partial lambda_auto_wrap_policy lambda_fn=is_inner model = apply_fsdp_with_checkpointing model wrap_policy is_inner correct_outputs = model inputs cnt = torch _dynamo testing CompileCounterWithBackend inductor opt_model = torch compile model backend=cnt outputs = opt_model inputs assertTrue same correct_outputs outputs Each FSDP module separate graph assertEqual cnt frame_count assertTrue find_first_node cnt graphs tag_activation_checkpoint None import_transformers_or_skip unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch TODO whc Investigate why cudagraphs breaks inductor+fsdp hf_bert patch object torch _inductor config triton cudagraphs False patch object torch _inductor config fallback_random True config patch enable_compiler_collectives=True unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Inaccurate results fused SDPA kernels test_hf_bert_fsdp apply_fsdp model wrap_policy model = FSDP copy deepcopy model auto_wrap_policy=wrap_policy use_orig_params=True model _dynamo_dist_per_rank_init rank world_size wrap_policy test_instance None FSDP without recursive wrapping print f Running hf_bert test test_instance model inputs = get_hf_bert rank reset_rng_state eager_model = apply_fsdp model wrap_policy correct_outputs = eager_model inputs correct_loss = correct_outputs loss correct_loss backward reset_rng_state opt_model = apply_fsdp model wrap_policy opt_model = torch compile opt_model backend= inductor opt_outputs = opt_model inputs opt_loss = opt_outputs loss opt_loss backward inputs_flat = inputs k k inputs correct_results = collect_results eager_model correct_outputs logits correct_loss inputs_flat opt_results = collect_results opt_model opt_outputs logits opt_loss inputs_flat assertTrue same correct_results opt_results import_transformers_or_skip unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch TODO whc Investigate why cudagraphs breaks inductor+fsdp hf_bert patch object torch _inductor config triton cudagraphs False patch object torch _inductor config fallback_random True config patch guard_nn_modules=True enable_compiler_collectives=True test_hf_bert_fsdp_activation_checkpointing transformers models bert modeling_bert BertLayer _dynamo_dist_per_rank_init rank world_size wrap_policy test_instance functools partial transformer_auto_wrap_policy transformer_layer_cls= BertLayer FSDP recursive wrapping BertLayer instances print f Running hf_bert_activation_checkpointing test test_instance model inputs = get_hf_bert rank check_fn = lambda submodule isinstance noqa E submodule BertLayer reset_rng_state eager_model = apply_fsdp_with_checkpointing model wrap_policy check_fn correct_outputs = eager_model inputs correct_loss = correct_outputs loss correct_loss backward reset_rng_state opt_model = apply_fsdp_with_checkpointing model wrap_policy check_fn opt_model = torch compile opt_model backend= inductor opt_outputs = opt_model inputs opt_loss = opt_outputs loss opt_loss backward inputs_flat = inputs k k inputs correct_results = collect_results eager_model correct_outputs logits correct_loss inputs_flat opt_results = collect_results opt_model opt_outputs logits opt_loss inputs_flat assertTrue same correct_results opt_results unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch enable_compiler_collectives=True test_compiler_collectives_automatic_dynamic_tensor _dynamo_dist_per_rank_init rank world_size SimpleModel nn Module __init__ input_size output_size super __init__ linear = nn Linear input_size output_size forward x linear x torch _dynamo utils clear_compilation_metrics model = SimpleModel rank model forward = torch compile model forward ddp_model = DDP model device_ids= rank loss_fn = nn CrossEntropyLoss optimizer = optim SGD ddp_model parameters lr= B s torch randn s torch randint s rank == dataloader = B B B dataloader = B B B data labels dataloader data labels = data rank labels rank optimizer zero_grad output = ddp_model data loss = loss_fn output labels loss backward optimizer step metrics = torch _dynamo utils get_compilation_metrics Number compiles same all nodes res = None world_size torch distributed all_gather_object res len metrics r res assertEqual res r unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch enable_compiler_collectives=True test_compiler_collectives_automatic_dynamic_scalar _dynamo_dist_per_rank_init rank world_size torch _dynamo utils clear_compilation_metrics TODO This should possible do inside function device = f device_type rank torch compile f x y x + torch ones y device=device sum rank == dataloader = dataloader = data dataloader f torch randn device=self rank data metrics = torch _dynamo utils get_compilation_metrics Number compiles same all nodes res = None world_size torch distributed all_gather_object res len metrics r res assertEqual res r unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch enable_compiler_collectives=True test_compiler_collectives_automatic_dynamic_speculation_divergence _dynamo_dist_per_rank_init rank world_size torch _dynamo utils clear_compilation_metrics torch compile f x y zx = x shape noqa F zy = y shape noqa F x sum + y sum rank == dataloader = dataloader = data dataloader f torch randn data device=self rank torch randn data device=self rank metrics = torch _dynamo utils get_compilation_metrics Number compiles same all nodes res = None world_size torch distributed all_gather_object res len metrics r res assertEqual res r unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch enable_compiler_collectives=True test_compiler_collectives_graph_break_empty_graph_still_collective _dynamo_dist_per_rank_init rank world_size torch _dynamo utils clear_compilation_metrics torch compile f x y z = y noqa F print woof zx = x shape noqa F zy = y shape noqa F x sum + y sum rank == dataloader = dataloader = data dataloader f torch randn data device=self rank torch randn data device=self rank metrics = torch _dynamo utils get_compilation_metrics Number compiles same all nodes res = None world_size torch distributed all_gather_object res len metrics r res assertEqual res r unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch enable_compiler_collectives=True test_compiler_collectives_dim_mismatch _dynamo_dist_per_rank_init rank world_size torch _dynamo utils clear_compilation_metrics torch compile f x y zx = x shape noqa F zy = y shape noqa F x sum + y sum rank == dataloader = dataloader = data dataloader f torch randn data device=self rank torch randn data device=self rank metrics = torch _dynamo utils get_compilation_metrics res = None world_size torch distributed all_gather_object res len metrics r res assertEqual res r unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch enable_compiler_collectives=True test_compiler_collectives_missing_source _dynamo_dist_per_rank_init rank world_size torch _dynamo utils clear_compilation_metrics torch compile f rank xs xs rank sum xs = _ range world_size xs append torch randn device=self rank f rank xs metrics = torch _dynamo utils get_compilation_metrics res = None world_size torch distributed all_gather_object res len metrics r res assertEqual res r unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch enable_compiler_collectives=True test_compiler_collectives_scalar_missing_source _dynamo_dist_per_rank_init rank world_size torch _dynamo utils clear_compilation_metrics torch compile f rank xs torch tensor xs rank device=self rank xs = i range world_size xs append + i f rank xs metrics = torch _dynamo utils get_compilation_metrics res = None world_size torch distributed all_gather_object res len metrics r res assertEqual res r unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch config patch enable_compiler_collectives=True test_compiler_collectives_type_mismatch _dynamo_dist_per_rank_init rank world_size torch _dynamo utils clear_compilation_metrics torch compile f x isinstance x int torch tensor x device=self rank x sum rank == x = torch randn device=self rank x = f x This deadlocks I guess we don t support rank == x = torch randn device=self rank x = f x metrics = torch _dynamo utils get_compilation_metrics res = None world_size torch distributed all_gather_object res len metrics r res assertEqual res r unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch enable_guard_collectives test_guard_collective _dynamo_dist_per_rank_init rank world_size torch _dynamo utils clear_compilation_metrics torch compile f x x sum x = torch randn device=self rank f x rank == x = torch randn device=self rank x = torch randn device=self rank recompile one rank f x metrics = torch _dynamo utils get_compilation_metrics res = None world_size torch distributed all_gather_object res len metrics r res assertEqual res r unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_get_pg_attr _dynamo_dist_per_rank_init rank world_size pg = dist distributed_c d _get_default_group device = f device_type rank torch compile fullgraph=True f x dist distributed_c d _rank_not_in_group pg x + x - x = torch ones device=device assertEqual f x x - pg = dist distributed_c d GroupMember NON_GROUP_MEMBER assertEqual f x x + skipIfXpu ProcessGroupXCCL doesn t support _set_default_timeout yet unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch patch object torch _inductor config fx_graph_cache False patch object torch _inductor config fx_graph_remote_cache False test_asymmetric_compilation torch _dynamo comptime comptime _dynamo_dist_per_rank_init rank world_size torch _dynamo utils clear_compilation_metrics device = f device_type rank pg = dist distributed_c d _get_default_group cnt = torch _dynamo testing CompileCounter sleep_time = torch compile backend=cnt f x rank == comptime sleep sleep_time y = x y sum backend = pg _get_backend torch device device backend _set_default_timeout timedelta seconds=sleep_time - x = torch ones device=device NCCL startup lazy w = pg allreduce x w wait f x rank = test fails NCCL timeout without line dist distributed_c d _add_ephemeral_timeout_for_all_pgs timedelta seconds=sleep_time w = pg allreduce x w wait torch accelerator synchronize device metrics = torch _dynamo utils get_compilation_metrics Number compiles same all nodes res = None world_size torch distributed all_gather_object res len metrics r res assertEqual res r skipIfXpu ProcessGroupXCCL doesn t support _set_default_timeout yet unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch patch object torch _inductor config fx_graph_cache True patch object torch _inductor config fx_graph_remote_cache False patch object torch _inductor config sleep_sec_TESTING_ONLY test_asymmetric_compilation_with_fx_cache torch _dynamo utils counters torch _inductor utils fresh_cache fresh_cache _dynamo_dist_per_rank_init rank world_size torch _dynamo utils clear_compilation_metrics device = f device_type rank pg = dist distributed_c d _get_default_group torch compile f x y = x y sum backend = pg _get_backend torch device device backend _set_default_timeout timedelta seconds= counters clear x = torch ones device=device f x assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass w = pg allreduce x w wait torch accelerator synchronize device torch _dynamo reset rank == fresh_cache f x assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass f x assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass w = pg allreduce x w wait torch accelerator synchronize device requires_accelerator_dist_backend nccl xccl unittest skipUnless torch accelerator is_available Requires accelerator TestSingleProc DynamoDistributedSingleProcTestCase Test harness initializes dist process group Test simple things here since they simpler debug Use TestMultiProc things really need run multiple nodes device_type = acc type acc = torch accelerator current_accelerator cpu get_model bsz= in_feat= hidden_feat= out_feat= ctx_manager=None m = ToyModel in_feat=in_feat hidden_feat=hidden_feat out_feat=out_feat ctx_manager=ctx_manager device m apply init_weights inputs = torch rand bsz in_feat device outputs = m inputs m inputs outputs patch object config optimize_ddp False test_ddp_baseline_aot_eager torch nn parallel DistributedDataParallel DDP m inputs correct_outputs = get_model ddp_m = DDP m device_ids=self device_ids ddp_m = torch compile ddp_m backend= aot_eager outputs = ddp_m inputs assertTrue same correct_outputs outputs unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch patch object config optimize_ddp False test_ddp_baseline_inductor torch nn parallel DistributedDataParallel DDP m inputs correct_outputs = get_model ddp_m = DDP m device_ids=self device_ids ddp_m = torch compile ddp_m backend= inductor outputs = ddp_m inputs assertTrue same correct_outputs outputs patch object config optimize_ddp True test_graph_split assert config optimize_ddp Just ensures appropriate number splits happen based bucket size model parameters - verifies number times user-provided compiler called DDPOptimizer which doing graph splitting m inputs correct_outputs = get_model ddp_m = DDP m device_ids=self device_ids bucket_cap_mb= check_splits_compiler = CheckSplitsCompiler torch compile backend=check_splits_compiler compile_fn opt_fn inputs ddp_m inputs opt_outputs = opt_fn inputs assertTrue same correct_outputs opt_outputs assertEqual check_splits_compiler compiler_called ensure compatibility dynamo explain explain_out = torch _dynamo explain ddp_m inputs break_reasons = explain_out break_reasons assertEqual len break_reasons assertTrue all DDPOptimizer r reason r break_reasons patch object config optimize_ddp True test_graph_split_ctx_manager Ensures we get right number splits respective context managers effects applied computation get_compiler lambda CheckSplitsCompiler lambda None ctx_manager output_test lambda torch autocast torch device device type torch float lambda out assertEqual out dtype torch float torch enable_grad lambda out assertTrue out requires_grad torch no_grad lambda out assertTrue out requires_grad m inputs correct_outputs = get_model out_feat= hidden_feat= in_feat= ctx_manager=ctx_manager inp - matrix float bytes = MB hidden - matrix float bytes = MB bucket_cap_mb = MB ddp_m = DDP m device_ids=self device_ids bucket_cap_mb=bucket_cap_mb compiler = get_compiler torch compile backend=compiler compile_fn compiler aot_eager opt_fn inputs ddp_m inputs opt_outputs = opt_fn inputs assertTrue same correct_outputs opt_outputs compiler assertEqual compiler compiler_called output_test opt_outputs ensure compatibility dynamo explain explain_out = torch _dynamo explain ddp_m inputs break_reasons = explain_out break_reasons assertEqual len break_reasons assertTrue all DDPOptimizer r reason r break_reasons skipIfXpu XPU device doesn t support flex_attention yet patch object config optimize_ddp True test_compiled_flex_attention_full_model_ddp Model torch nn Module __init__ S H D super __init__ S = S H = H D = D alibi_bias = generate_alibi_bias H register_buffer alibi_bias alibi_bias persistent=True attention = flex_attention project_qk = torch nn Linear H D H D project_v = torch nn Linear H D H D forward hidden_states batch_size _ _ = hidden_states size query key = project_qk hidden_states chunk dim= query = query view S batch_size H D query = query permute key = key view S batch_size H D key = key permute value = project_v hidden_states value = value view S batch_size H D value = value permute attention query key value score_mod=self alibi_score_mod generate_alibi_bias num_heads alibi_bias = - i + num_heads i range num_heads torch tensor alibi_bias alibi_score_mod score b h q_idx kv_idx bias = q_idx - kv_idx alibi_bias h score + bias B = H = S = D = model = Model S H D model device_type model = torch compile model model = DDP model device_ids=self device_ids hidden_states = torch randn B S H D device_type model hidden_states torch accelerator synchronize skipIfXpu XPU device doesn t support flex_attention yet patch object config optimize_ddp True test_compiled_flex_attention_local_ddp Model torch nn Module __init__ S H D super __init__ S = S H = H D = D alibi_bias = generate_alibi_bias H register_buffer alibi_bias alibi_bias persistent=True attention = torch compile flex_attention project_qk = torch nn Linear H D H D project_v = torch nn Linear H D H D forward hidden_states batch_size _ _ = hidden_states size query key = project_qk hidden_states chunk dim= query = query view S batch_size H D query = query permute key = key view S batch_size H D key = key permute value = project_v hidden_states value = value view S batch_size H D value = value permute attention query key value score_mod=self alibi_score_mod generate_alibi_bias num_heads alibi_bias = - i + num_heads i range num_heads torch tensor alibi_bias alibi_score_mod score b h q_idx kv_idx bias = q_idx - kv_idx alibi_bias h score + bias B = H = S = D = model = Model S H D model device_type model = torch compile model model = DDP model device_ids=self device_ids hidden_states = torch randn B S H D device_type model hidden_states torch accelerator synchronize patch object config optimize_ddp True unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_graph_split_inductor assert config optimize_ddp Same above using inductor backend We observed issues inductor fx interface past m inputs correct_outputs = get_model ddp_m = DDP m device_ids=self device_ids bucket_cap_mb= torch compile backend= inductor opt_fn inputs ddp_m inputs opt_outputs = opt_fn inputs assertTrue same correct_outputs opt_outputs torch _inductor config patch layout_optimization True keep_output_stride False patch object config optimize_ddp True _test_graph_split_inductor_layout_optimizations_impl context assert config optimize_ddp channel_dim = channel dim must inductor do layout optimization use NHWC ToyModelConv nn Module __init__ - None super __init__ net = nn Sequential nn Conv d channel_dim channel_dim stride= bias=False nn ReLU + nn Conv d channel_dim channel_dim stride= bias=False nn ReLU + nn Conv d channel_dim channel_dim stride= bias=False nn ReLU + nn Conv d channel_dim channel_dim stride= bias=False nn ReLU forward inputs net inputs get_model m = ToyModelConv device m apply init_weights inputs = torch rand channel_dim channel_dim device outputs = m inputs m inputs outputs context m inputs correct_outputs = get_model ddp_m = DDP m device_ids=self device_ids bucket_cap_mb= torch compile backend= inductor opt_fn inputs ddp_m inputs opt_outputs = opt_fn inputs assertTrue same correct_outputs opt_outputs unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_graph_split_inductor_layout_optimizations_training _test_graph_split_inductor_layout_optimizations_impl contextlib nullcontext unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_graph_split_inductor_layout_optimizations_inference _test_graph_split_inductor_layout_optimizations_impl torch no_grad patch object config optimize_ddp True unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_graph_split_inductor_transpose assert config optimize_ddp B = N = D = K = Foo nn Module __init__ - None super __init__ linear = nn Linear N K linear = torch nn Linear D K forward x xt = x transpose xt = linear xt flatten linear xt mod = Foo device compiled_mod = torch compile mod backend= inductor ddp_compiled_mod = DDP compiled_mod device_ids=self device_ids x = torch randn B N D dtype=torch float device=self device assertTrue same mod x ddp_compiled_mod x x_ = torch randn B N D dtype=torch float device=self device assertTrue same mod x_ ddp_compiled_mod x_ x_ = torch randn B N D dtype=torch float device=self device assertTrue same mod x_ ddp_compiled_mod x_ patch object config optimize_ddp True test_no_split Ensures DDPOptimizer returns correct compiled module without introducing graph splits Based model parameters fitting bucket DDP will always do first bucket really small size so only tiny model will escape m inputs correct_outputs = get_model hidden_feat= ddp_m = DDP m device_ids=self device_ids bucket_cap_mb= check_splits_compiler = CheckSplitsCompiler torch compile backend=check_splits_compiler compile_fn opt_fn inputs ddp_m inputs opt_outputs = opt_fn inputs assertTrue same correct_outputs opt_outputs assertEqual check_splits_compiler compiler_called patch object config optimize_ddp True test_aot_autograd Explicitly check AotAutograd family compilers work since they require example inputs propagated between graph splits m inputs correct_outputs = get_model ddp_m = DDP m device_ids=self device_ids bucket_cap_mb= torch compile backend= aot_eager opt_fn inputs ddp_m inputs opt_outputs = opt_fn inputs opt_outputs sum backward assertTrue same correct_outputs opt_outputs patch object config optimize_ddp True test_custom_layer Just ensures appropriate number splits happen based bucket size model parameters - verifies number times user-provided compiler called DDPOptimizer which doing graph splitting m inputs correct_outputs = get_custom_model device ddp_m = DDP m device_ids=self device_ids bucket_cap_mb= check_splits_compiler = CheckSplitsCompiler torch compile backend=check_splits_compiler compile_fn opt_fn inputs ddp_m inputs opt_outputs = opt_fn inputs assertTrue same correct_outputs opt_outputs assertEqual check_splits_compiler compiler_called unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_empty_graph_inductor fn get_world_size = torch distributed distributed_c d get_world_size get_world_size opt_fn = torch compile fn backend= inductor res = None try res = opt_fn except Exception pass assertEqual res patch object config optimize_ddp False test_ignored_parameters Verifies ddp graph-split logic ignores parameters marked ignore DDP module Hooks up graph-split optimizer manually so can peek internal state m inputs correct_outputs = get_custom_model device parameters_to_ignore = seq weight seq linear bias DDP _set_params_and_buffers_to_ignore_for_model m parameters_to_ignore ddp_m = DDP m device_ids=self device_ids bucket_cap_mb= parameter_ids_to_ignore = id ddp_m module get_parameter p p ddp_m parameters_to_ignore check_splits_compiler = CheckSplitsCompiler ddp_optimizer = DDPOptimizer bucket_bytes_cap=ddp_m bucket_bytes_cap backend_compile_fn=check_splits_compiler compile_fn torch compile backend=ddp_optimizer compile_fn opt_fn inputs ddp_m inputs opt_outputs = opt_fn inputs assertTrue same correct_outputs opt_outputs assertEqual check_splits_compiler compiler_called b ddp_optimizer buckets p_id b param_ids assertFalse p_id parameter_ids_to_ignore patch object config optimize_ddp True test_higher_order_op torch utils checkpoint checkpoint N = InnerModule torch nn Module __init__ - None super __init__ linear = torch nn Linear N N linear = torch nn Linear N N forward x = linear x = linear MockModule torch nn Module __init__ - None super __init__ inner_mod = InnerModule inner_mod = InnerModule forward x = checkpoint inner_mod x use_reentrant=False = torch cos = checkpoint inner_mod use_reentrant=False = torch cos mod = MockModule device_type mod = DDP mod bucket_cap_mb= x = torch randn N N device=self device_type requires_grad=True args = x backend = aot_eager cnt = torch _dynamo testing CompileCounterWithBackend backend torch compile mod backend=cnt args test_fsdp_orig_params_assert Test basic FSDP wrapping outer wrap around whole model m inputs _ = get_model f device_type rank fsdp_m = FSDP m use_orig_params=False Test function call does throw exception fsdp_m = torch compile fsdp_m test_fsdp_skip_guards It s currently difficult test dynamo guards Most guards tests indirect- modify something observe guard question failed In case since FSDP guards already deemed useless skipping them expected have no practical effect s pretty contrived even try make those guards fail Instead we observe guard source printed dynamo s comptime print_guards function Note comptime prints guards before time they get installed installed so both cases skip no skip same guards get printed The difference skip case they show up special guard source which will cause them installed So all we check expected guard source local_fsdp_module global GUARDS_FILE GUARDS_FILE = StringIO skip_guards expected_guard_source True local_fsdp_module False local_unspecialized_nn_module torch _dynamo reset ToyModel nn Module __init__ in_feat= hidden_feat= out_feat= super __init__ net = nn Sequential nn Linear in_feat hidden_feat nn ReLU + nn Linear hidden_feat hidden_feat nn ReLU + nn Linear hidden_feat hidden_feat nn ReLU + nn Linear hidden_feat out_feat nn ReLU forward inputs out = net inputs comptime _ ctx ctx print_guards file=GUARDS_FILE out device = f device_type rank m = ToyModel in_feat= hidden_feat= out_feat= device inputs = torch rand device m apply init_weights correct_outputs = m inputs fsdp_m = FSDP m use_orig_params=True torch _dynamo config patch skip_fsdp_guards=skip_guards opt_m = torch compile fsdp_m backend= aot_eager outputs = opt_m inputs far exhaustive check all expected guards just check couple them FileCheck check local L TYPE_MATCH check f expected_guard_source L _modules net TYPE_MATCH check f expected_guard_source L _modules net _modules TYPE_MATCH run GUARDS_FILE getvalue assertTrue same correct_outputs outputs test_fsdp_skip_register_attr_or_module ensure FSDP module registered attributes fx graph see ` source guard_source is_fsdp_module ` before calling ` register_attr_or_module ` variables builder py ToyModel nn Module __init__ in_feat= hidden_feat= out_feat= super __init__ net = nn Sequential nn Linear in_feat hidden_feat nn ReLU + nn Linear hidden_feat hidden_feat nn ReLU forward inputs out = net inputs out torch _dynamo reset device = f device_type rank m = ToyModel in_feat= hidden_feat= out_feat= device inputs = torch rand device m apply init_weights correct_outputs = m inputs fsdp_m = FSDP m use_orig_params=True debug_compiler gm _ node gm graph nodes node op == get_attr name l__self___net_ _weight l__self___net_ _bias l__self___net_ _weight l__self___net_ _bias assertFalse name node name f FSDP module name should registered attributes gm opt_m = torch compile fsdp_m backend=debug_compiler outputs = opt_m inputs assertTrue same correct_outputs outputs test_fsdp_dup_tensors_same_source Tests FSDP-managed modules parameters buffers same source de-duplicated meaning they each only passed once graph input DuplicateModule nn Module __init__ - None super __init__ device_type = acc type acc = torch accelerator current_accelerator cpu _param = torch randn device=device_type _buf = torch nn Buffer torch randn requires_grad=False device=device_type forward x torch Tensor - torch Tensor Use ` _param ` ` _buf ` each twice compiled forward exercise they de-duplicated TorchDynamo z = x + _buf + _buf z += _param + _param z model = DuplicateModule fsdp_model = FSDP copy deepcopy model use_orig_params=True fsdp_model = torch compile fsdp_model backend= aot_eager inp = torch randn device=self device_type local_out = model inp fsdp_out = fsdp_model inp assertEqual local_out fsdp_out patch object config guard_nn_modules True test_fsdp_dup_tensors_diff_source Tests FSDP-managed modules parameters buffers different source do result incorrect AOTAutograd de-dup guards like ` ` b ` ` where ` ` ` ` ` ` b ` ` certainly same We check checking per-invocation recompiles BufModule nn Module __init__ - None super __init__ device_type = acc type acc = torch accelerator current_accelerator cpu _buf = nn Buffer torch randn requires_grad=False device=device_type forward x torch Tensor - torch Tensor x + _buf Model nn Module __init__ - None super __init__ device_type = acc type acc = torch accelerator current_accelerator cpu _param = nn Parameter torch randn device=device_type _buf_module = BufModule Share buffer meaning same tensor different source _buf = _buf_module _buf forward x torch Tensor - torch Tensor Use same buffer tensor twice compiled forward including data mutation trigger de-dup logic _buf mul_ z = x + _buf z = _buf_module z z += _param z fsdp_model = FSDP Model use_orig_params=True cnt = torch _dynamo testing CompileCounterWithBackend aot_eager fsdp_model = torch compile fsdp_model backend=cnt inp = torch randn device=self device_type _ range fsdp_model inp Check no recompiles there incorrect de-dup guards then frame count would equal number forward calls assertEqual cnt frame_count test_fsdp_staticmethod Tests Dynamo compiles staticmethods FSDP-managed modules correctly both when staticmethod invoked object itself ModuleWithStaticMethod nn Module __init__ use_self bool super __init__ _use_self = use_self torch manual_seed force ` _param ` deterministic device_type = acc type acc = torch accelerator current_accelerator cpu _param = nn Parameter torch randn device=device_type forward x torch Tensor - torch Tensor _use_self z = _add x _param z = ModuleWithStaticMethod _add x _param z = z staticmethod _add x torch Tensor y torch Tensor - torch Tensor x + y model = ModuleWithStaticMethod False x = torch randn device=self device_type ref_out = model x test_outs list torch Tensor = use_self False True model = ModuleWithStaticMethod use_self fsdp_model = FSDP model use_orig_params=True cnt = torch _dynamo testing CompileCounterWithBackend aot_eager fsdp_model = torch compile fsdp_model backend=cnt test_outs append fsdp_model x Check no recompiles which could happen incorrectly passing args staticmethod e g doubly passing ` ` expected here forward Graph should add imul assertEqual cnt frame_count test_out test_outs assertEqual test_out ref_out test_async_subclass_no_specialize cnt = torch _dynamo testing CompileCounterWithBackend eager torch compile backend=cnt fullgraph=True dynamic=True f x x + f _maybe_wrap_tensor torch randn f _maybe_wrap_tensor torch randn assertEqual cnt frame_count __name__ == __main__ torch _dynamo test_case run_tests run_tests