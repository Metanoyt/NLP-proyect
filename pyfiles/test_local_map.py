Owner s module higher order operators flake noqa B functools unittest contextlib contextmanager ExitStack typing Any Callable Optional torch torch _dynamo torch _functorch torch _inductor torch _inductor decomposition torch fx traceback fx_traceback torch nn functional F torch nn torch _dynamo variables higher_order_ops LocalMapWrappedHigherOrderVariable torch _functorch aot_autograd aot_export_joint_with_descriptors torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor make_fx torch fx experimental symbolic_shapes ShapeEnv torch nn attention sdpa_kernel SDPBackend torch utils checkpoint create_selective_checkpoint_contexts torch distributed is_available torch distributed _tensor experimental local_map torch distributed tensor placement_types Replicate Shard torch testing _internal common_utils run_tests TEST_WITH_CROSSREF TEST_WITH_TORCHDYNAMO TEST_WITH_TORCHINDUCTOR TestCase nested_compile_region = torch compiler nested_compile_region contextmanager enable_local_map_wrapping torch _dynamo variables higher_order_ops LocalMapWrappedHigherOrderVariable vt_cls torch _higher_order_ops local_map local_map_module vt_cls enable local_map_module defer_inlining yield _export model torch nn Module inputs tuple Any - torch nn Module torch _dynamo functional_export _dynamo_graph_capture_for_export torch export _trace _restore_state_dict Thin wrapper around graph capture output restores original calling convention attribute fqn TODO Use bytecode calling convention instead pytree more seamless UX Attach guards Be more careful about tensor constants names torch _dynamo config patch install_free_tensors=True gm = _dynamo_graph_capture_for_export model inputs _restore_state_dict model gm gm ap_style_initial_capture model torch nn Module inputs_fn Callable - torch nn Module Similar AP s initial capture - no dtype casting - no AP decomps - no inductor fake_mode = FakeTensorMode fake_mode shape_env = ShapeEnv fake_mode static_shapes = False fake_mode inputs = inputs_fn assert isinstance inputs tuple enable_local_map_wrapping torch _dynamo utils _disable_saved_tensors_hooks_during_tracing torch_ir_with_fqn = _export model inputs unused = ExitStack joint_with_descriptors = aot_export_joint_with_descriptors unused torch_ir_with_fqn inputs decompositions=torch _inductor decomposition select_decomp_table unused close joint_with_descriptors graph_module get_skip_reasons msg = torch distributed is_available msg += Torch distributed available TEST_WITH_TORCHINDUCTOR TEST_WITH_TORCHDYNAMO msg += Already manually torch compile d msg = msg MyTransform torch autograd Function staticmethod forward ctx x x + staticmethod backward ctx grad grad + context_parallel_attention query key value out = F scaled_dot_product_attention query=query key=key value=value is_causal=False out NOTE we use function directly node checks save_scalar_muls ctx op args kwargs op == torch ops aten mul Scalar torch utils checkpoint CheckpointPolicy MUST_SAVE torch utils checkpoint CheckpointPolicy MUST_RECOMPUTE save_mm ctx op args kwargs op == torch ops aten mm default torch utils checkpoint CheckpointPolicy MUST_SAVE torch utils checkpoint CheckpointPolicy MUST_RECOMPUTE create_model attention_fn nheads dim dim sac_policy=None LocalMapTransformerBlock nn Module __init__ nheads dim dim super __init__ nheads = nheads bias = False wq = nn Linear dim dim bias=bias wk = nn Linear dim dim bias=bias wv = nn Linear dim dim bias=bias wo = nn Linear dim dim bias=bias w = nn Linear dim dim bias=bias w = nn Linear dim dim bias=bias sac_policy sac_context_fn = functools partial create_selective_checkpoint_contexts sac_policy sac_context_fn = None _forward x q = wq x k = wk x v = wv x q = q unflatten - nheads - permute k = k unflatten - nheads - permute v = v unflatten - nheads - permute o = attention_fn q k v o = o permute flatten - o = wo o o = o + x o = w o o = torch nn functional relu o o = w o o = o + o o forward x sac_context_fn None torch utils checkpoint checkpoint _forward x use_reentrant=False context_fn=self sac_context_fn _forward x LocalMapTransformerBlock nheads dim dim get_local_mapped_functions mesh assert torch distributed is_available local_map out_placements= Shard Shard Shard in_placements= Shard Shard Shard query Shard Shard Replicate key Shard Shard Replicate value redistribute_inputs=True in_grad_placements=None device_mesh=mesh cp_decorated query key value context_parallel_attention query key value cp_function = local_map context_parallel_attention out_placements= Shard Shard Shard in_placements= Shard Shard Shard query Shard Shard Replicate key Shard Shard Replicate value redistribute_inputs=True in_grad_placements=None device_mesh=mesh cp_decorated cp_function TestLocalMap TestCase setUp torch _dynamo reset exit_stack = ExitStack exit_stack enter_context sdpa_kernel backends= SDPBackend MATH torch distributed is_available torch testing _internal distributed fake_pg FakeStore fake_store = FakeStore world_size = torch distributed init_process_group fake store=self fake_store rank= world_size=self world_size mesh = torch distributed device_mesh init_device_mesh cpu mesh_dim_names= dp tp cp ep tearDown exit_stack close torch distributed is_available torch distributed destroy_process_group unittest skipIf get_skip_reasons test_simple cp_decorated cp_function = get_local_mapped_functions mesh bs = dim = dim = dim nheads = seq_len = torch _dynamo testing EagerAndRecordGraphs normalize_gm backend = EagerAndRecordGraphs model = create_model cp_decorated nheads dim dim inputs = torch randn bs seq_len dim requires_grad=True LocalMapWrappedHigherOrderVariable enable out = torch compile model backend=backend inputs out sum backward model = create_model cp_function nheads dim dim inputs = torch randn bs seq_len dim requires_grad=True LocalMapWrappedHigherOrderVariable enable out = torch compile model backend=backend inputs out sum backward TEST_WITH_CROSSREF assertEqual len backend graphs assertEqual normalize_gm backend graphs print_readable print_output=False normalize_gm backend graphs print_readable print_output=False assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_self_modules_wq_parameters_weight_ f L_x_ f L_self_modules_wk_parameters_weight_ f L_self_modules_wv_parameters_weight_ f L_self_modules_wo_parameters_weight_ f L_self_modules_w _parameters_weight_ f L_self_modules_w _parameters_weight_ f l_self_modules_wq_parameters_weight_ = L_self_modules_wq_parameters_weight_ l_x_ = L_x_ l_self_modules_wk_parameters_weight_ = L_self_modules_wk_parameters_weight_ l_self_modules_wv_parameters_weight_ = L_self_modules_wv_parameters_weight_ l_self_modules_wo_parameters_weight_ = L_self_modules_wo_parameters_weight_ l_self_modules_w _parameters_weight_ = L_self_modules_w _parameters_weight_ l_self_modules_w _parameters_weight_ = L_self_modules_w _parameters_weight_ q f = torch _C _nn linear l_x_ l_self_modules_wq_parameters_weight_ None l_self_modules_wq_parameters_weight_ = None k f = torch _C _nn linear l_x_ l_self_modules_wk_parameters_weight_ None l_self_modules_wk_parameters_weight_ = None v f = torch _C _nn linear l_x_ l_self_modules_wv_parameters_weight_ None l_self_modules_wv_parameters_weight_ = None unflatten f = q unflatten - - q = None q_ f = unflatten permute unflatten = None unflatten_ f = k unflatten - - k = None k_ f = unflatten_ permute unflatten_ = None unflatten_ f = v unflatten - - v = None v_ f = unflatten_ permute unflatten_ = None subgraph_ = subgraph_ local_map_hop = torch ops higher_order local_map_hop subgraph_ q_ k_ v_ subgraph_ = q_ = k_ = v_ = None o f = local_map_hop local_map_hop = None permute_ f = o permute o = None o_ f = permute_ flatten - permute_ = None o_ f = torch _C _nn linear o_ l_self_modules_wo_parameters_weight_ None o_ = l_self_modules_wo_parameters_weight_ = None o f = o_ + l_x_ o_ = l_x_ = None o_ f = torch _C _nn linear o l_self_modules_w _parameters_weight_ None l_self_modules_w _parameters_weight_ = None o_ f = torch nn functional relu o_ o_ = None o_ f = torch _C _nn linear o_ l_self_modules_w _parameters_weight_ None o_ = l_self_modules_w _parameters_weight_ = None o_ f = o + o_ o = o_ = None o_ subgraph_ torch nn Module forward q_ f k_ f v_ f out f = torch _C _nn scaled_dot_product_attention query = q_ key = k_ value = v_ is_causal = False q_ = k_ = v_ = None out ignore_empty_lines=True unittest skipIf get_skip_reasons test_sac cp_decorated cp_function = get_local_mapped_functions mesh bs = dim = dim = dim nheads = seq_len = torch _dynamo testing AotEagerAndRecordGraphs normalize_gm backend = AotEagerAndRecordGraphs model = create_model cp_decorated nheads dim dim sac_policy=save_scalar_muls inputs = torch randn bs seq_len dim requires_grad=True LocalMapWrappedHigherOrderVariable enable out = torch compile model backend=backend inputs out sum backward model = create_model cp_function nheads dim dim sac_policy=save_scalar_muls inputs = torch randn bs seq_len dim requires_grad=True LocalMapWrappedHigherOrderVariable enable out = torch compile model backend=backend inputs out sum backward TEST_WITH_CROSSREF assertEqual len backend graphs assertEqual normalize_gm backend graphs print_readable print_output=False normalize_gm backend graphs print_readable print_output=False assertEqual normalize_gm backend fw_graphs print_readable print_output=False normalize_gm backend fw_graphs print_readable print_output=False assertEqual normalize_gm backend bw_graphs print_readable print_output=False normalize_gm backend bw_graphs print_readable print_output=False assertEqual len backend graphs graph find_nodes op= call_function target=torch _higher_order_ops wrap tag_activation_checkpoint TODO add joint testing compile backend fw_outs = n name n backend fw_graphs graph find_nodes op= output args bw_ins = n name n backend bw_graphs graph find_nodes op= placeholder node backend fw_graphs graph nodes recompute node meta expected = save_scalar_muls None node target None None actual = node meta recompute assertEqual expected actual actual == torch utils checkpoint CheckpointPolicy MUST_SAVE assertTrue node name fw_outs node name bw_ins actual == torch utils checkpoint CheckpointPolicy MUST_RECOMPUTE can still fw_outs post-graph bytecode assertFalse node name bw_ins unittest skipIf get_skip_reasons test_sac_deferred This test bit weird state needs compositional compile API so we can defer inlining up until AOTAutograd stage Then we should inlined stage But we can t do today cp_decorated cp_function = get_local_mapped_functions mesh bs = dim = dim = dim nheads = seq_len = torch _dynamo testing AotEagerAndRecordGraphs normalize_gm backend = AotEagerAndRecordGraphs model = create_model cp_decorated nheads dim dim sac_policy=save_scalar_muls torch bfloat inputs = torch randn bs seq_len dim requires_grad=True dtype=torch bfloat try enable_local_map_wrapping out = torch compile model backend=backend inputs out sum backward except AttributeError e TODO get rid when we can install subgraph assertTrue module torch _higher_order_ops local_map has no attribute call_local_map str e model = create_model cp_function nheads dim dim sac_policy=save_scalar_muls torch bfloat inputs = torch randn bs seq_len dim requires_grad=True dtype=torch bfloat try enable_local_map_wrapping out = torch compile model backend=backend inputs out sum backward except AttributeError e TODO get rid when we can install subgraph assertTrue module torch _higher_order_ops local_map has no attribute call_local_map str e TODO re-enable tests backward when we can install subgraph TEST_WITH_CROSSREF assertEqual len backend graphs assertEqual normalize_gm backend graphs print_readable print_output=False normalize_gm backend graphs print_readable print_output=False assertEqual normalize_gm backend fw_graphs print_readable print_output=False normalize_gm backend fw_graphs print_readable print_output=False assertEqual normalize_gm backend bw_graphs print_readable print_output=False normalize_gm backend bw_graphs print_readable print_output=False assertEqual len backend graphs graph find_nodes op= call_function target=torch _higher_order_ops wrap tag_activation_checkpoint TODO add joint testing compile backend fw_outs = n name n backend fw_graphs graph find_nodes op= output args bw_ins = n name n backend bw_graphs graph find_nodes op= placeholder node backend fw_graphs graph nodes recompute node meta expected = save_scalar_muls None node target None None actual = node meta recompute assertEqual expected actual actual == torch utils checkpoint CheckpointPolicy MUST_SAVE assertTrue node name fw_outs assertTrue node name fw_outs node name bw_ins actual == torch utils checkpoint CheckpointPolicy MUST_RECOMPUTE can still fw_outs post-graph bytecode assertFalse node name bw_ins unittest skipIf get_skip_reasons test_local_map_dynamo_mismatch_placements local_map out_placements= Shard Shard Shard in_placements= Shard Shard Shard redistribute_inputs=True in_grad_placements=None device_mesh=self mesh mismatch_input x y x + y x = torch randn requires_grad=True y = torch randn requires_grad=True LocalMapWrappedHigherOrderVariable enable assertRaisesRegex AssertionError Expecting inputs local_map function based placements found torch compile mismatch_input backend= eager fullgraph=True x y local_map out_placements= Shard Shard Shard purposefully mismatched outputs in_placements= Shard Shard Shard redistribute_inputs=True in_grad_placements=None device_mesh=self mesh mismatch_outputs x x + x + x = torch randn requires_grad=True LocalMapWrappedHigherOrderVariable enable assertRaisesRegex AssertionError Expecting outputs local_map function based placements found torch compile mismatch_outputs backend= eager fullgraph=True x unittest skipIf get_skip_reasons test_local_map_dynamo_reordered_inputs local_map out_placements= Shard Shard in_placements= Shard Shard Replicate Shard redistribute_inputs=True in_grad_placements=None device_mesh=self mesh reorder_inputs first_input second_input second_input sum + first_input dynamo will reorder inputs x = torch randn requires_grad=True y = torch randn requires_grad=True LocalMapWrappedHigherOrderVariable enable assertRaisesRegex AssertionError r Dynamo changed order inputs local_map function please adjust order inputs input_placements \ l_args_ _ l_args_ _\ \ l_args_ _ l_args_ _\ torch compile reorder_inputs backend= eager fullgraph=True x y unittest skipIf get_skip_reasons test_local_map_with_local_shapes_hop_tracing fn x assert x shape == expected local shapes force view specialization ops out = x view - + out view x shape x = torch randn gm = make_fx fn x gm meta = local_map_kwargs in_placements Shard Replicate Replicate out_placements Shard Replicate Replicate device_mesh mesh FakeTensorMode global_tensor = torch randn requires_grad=True torch _higher_order_ops local_map defer_inlining out = torch _higher_order_ops local_map_hop gm global_tensor out sum backward assertEqual global_tensor shape unittest skipIf get_skip_reasons test_local_map_with_local_shapes_dynamo_tracing local_map out_placements= Shard Replicate Replicate in_placements= Shard Replicate Replicate redistribute_inputs=True in_grad_placements=None device_mesh=self mesh fn x out = x view - + out view x shape MyModule torch nn Module forward x fn x model = MyModule inputs_fn torch randn requires_grad=True gm = ap_style_initial_capture model inputs_fn fw_node bw_node = n n gm graph nodes call_local_map n name Graph should aware Fake key used local shapes fw_inputs = fw_node args assert len fw_inputs == assertEqual fw_inputs meta val shape fw_outputs = fw_node args assert len fw_outputs == assertEqual fw_outputs meta val shape bw_inputs = bw_node args assert len bw_inputs == assertEqual bw_inputs meta val shape bw_outputs = bw_node meta val assert len bw_outputs == assertEqual bw_outputs shape unittest skipIf get_skip_reasons test_none_gradients local_map out_placements= Replicate Replicate Replicate in_placements= Replicate Replicate Replicate Replicate Replicate Replicate redistribute_inputs=True in_grad_placements=None device_mesh=self mesh replicate_linear w x x does requires_grad so will have None gradients torch matmul x w t MyModule torch nn Module __init__ super __init__ w = nn Linear forward x replicate_linear w weight x model = MyModule inputs_fn torch randn ap_style_initial_capture model inputs_fn unittest skipIf get_skip_reasons test_none_placements ScalarHolder torch nn Module __init__ scalar super __init__ scalar = scalar forward x x + scalar local_map out_placements= Replicate Replicate Replicate in_placements= Replicate Replicate Replicate None None redistribute_inputs=True in_grad_placements=None device_mesh=self mesh fn_with_non_tensors x scalar module x + + scalar + module scalar MyModule torch nn Module __init__ super __init__ module = ScalarHolder forward x fn_with_non_tensors x module inputs_fn torch randn requires_grad=True model = MyModule ap_style_initial_capture model inputs_fn unittest skipIf get_skip_reasons test_filtered_gradients local_map out_placements= Replicate Replicate Replicate Replicate Replicate Replicate in_placements= Replicate Replicate Replicate Replicate Replicate Replicate redistribute_inputs=True in_grad_placements=None device_mesh=self mesh returns_non_param w x x does requires_grad output so its corresponding tangent filtered out torch matmul x w t x + MyModule torch nn Module __init__ super __init__ w = nn Linear forward x b = returns_non_param w weight x sum + b sum model = MyModule inputs_fn torch randn ap_style_initial_capture model inputs_fn unittest skipIf get_skip_reasons test_symint_activations torch distributed distributed_c d c d _get_group_name_from_axis_name mesh_name mesh = mesh group = mesh get_group mesh_name group group_name axis_size axis_name mesh = mesh assert axis_name mesh mesh_dim_names axis_dim = mesh mesh_dim_names index axis_name mesh size axis_dim _all_to_all torch Tensor output_split_sizes Optional list int input_split_sizes Optional list int group_name str group_size = c d _get_group_size_by_name group_name output_split_sizes None input_split_sizes None assert output_split_sizes None input_split_sizes None output_split_sizes input_split_sizes must either specified together both set None output_split_sizes = shape group_size group_size input_split_sizes = output_split_sizes tensor = torch ops _c d_functional all_to_all_single output_split_sizes input_split_sizes group_name res = torch ops _c d_functional wait_tensor tensor res _AllToAll torch autograd Function staticmethod forward ctx Any x torch Tensor output_split_sizes Optional list int input_split_sizes Optional list int axis_name str group_name = _get_group_name_from_axis_name axis_name ctx group_name = group_name ctx output_split_sizes = output_split_sizes ctx input_split_sizes = input_split_sizes _all_to_all x output_split_sizes input_split_sizes group_name staticmethod backward ctx Any grad_output torch Tensor type ignore override _all_to_all grad_output ctx input_split_sizes ctx output_split_sizes ctx group_name all_to_all = _AllToAll apply local_map out_placements= Replicate Replicate Replicate in_placements= Replicate Replicate Replicate Replicate Replicate Replicate None redistribute_inputs=True in_grad_placements=None device_mesh=self mesh _forward num_tokens_per_expert x axis_name ep_size = axis_size axis_name generate input splits output splits all-to-all torch no_grad num_tokens_per_expert_group = all_to_all num_tokens_per_expert None None axis_name input_splits = num_tokens_per_expert view ep_size - sum dim= torch device cpu non_blocking=True NOTE would incur device-to-host sync output_splits = num_tokens_per_expert_group view ep_size - sum dim= torch device cpu non_blocking=False input_splits = input_splits tolist output_splits = output_splits tolist perform all-to-all routed_inputs = all_to_all x output_splits input_splits axis_name noop routed experts routed_outputs = routed_inputs noop shared experts out = x clone out add_ routed_outputs MyModule torch nn Module __init__ super __init__ forward num_tokens_per_expert routed_input _forward num_tokens_per_expert routed_input ep model = MyModule inputs_fn x = torch randn requires_grad=True num_tokens_per_expert = torch ones dtype=torch int num_tokens_per_expert x ap_style_initial_capture model inputs_fn unittest skipIf get_skip_reasons test_fx_annotations local_map out_placements= Replicate Replicate Replicate in_placements= Replicate Replicate Replicate Replicate Replicate Replicate None redistribute_inputs=True in_grad_placements=None device_mesh=self mesh fn w x id fx_traceback annotate inside_local_map id torch matmul x w t context_fn = functools partial create_selective_checkpoint_contexts save_mm MyModule torch nn Module __init__ super __init__ w = nn Linear forward x = fn w weight x b = torch utils checkpoint checkpoint fn w weight x use_reentrant=False context_fn=context_fn sum + b sum model = MyModule inputs_fn torch randn fx_traceback preserve_node_meta joint_gm_deferred = ap_style_initial_capture model inputs_fn joint_inputs = n meta val n joint_gm_deferred graph nodes n op == placeholder TODO need local shape interpreter cases where graph specializes shapes interp = torch fx Interpreter joint_gm_deferred joint_gm_inlined = make_fx interp run joint_inputs mm_nodes = joint_gm_inlined graph find_nodes op= call_function target=torch ops aten mm default assertEqual len mm_nodes assertNotIn partitioner_tag mm_nodes meta assertNotIn partitioner_tag mm_nodes meta assertEqual mm_nodes meta partitioner_tag is_backward assertEqual mm_nodes meta partitioner_tag is_backward assertEqual mm_nodes meta custom inside_local_map assertEqual mm_nodes meta custom inside_local_map assertEqual mm_nodes meta custom inside_local_map assertEqual mm_nodes meta custom inside_local_map __name__ == __main__ run_tests