mypy allow-untyped-defs disable-error-code= attr-defined valid-type functools logging random dataclasses asdict dataclass typing Any torch torch _inductor config torch _inductor codegen rocm ck_tile_template CKTileTemplate torch _inductor codegen rocm rocm_kernel ROCmTemplateKernel torch _inductor codegen rocm rocm_template ArgInfo torch _inductor ir Buffer Layout torch utils _ordered_set OrderedSet utils IndentedBuffer log = logging getLogger __name__ is_static_int number sympy isinstance number int sympy Integer torch_layout_to_ck_layout torch_layout torch_layout stride - == Row torch_layout stride - == Col None dataclass CKTileGemmOperation layout_a str layout_b str layout_c str datatype_a str datatype_b str datatype_c str tile_m int tile_n int tile_k int warp_m int warp_n int warp_k int warp_tile_m int warp_tile_n int warp_tile_k int m_is_padded str n_is_padded str k_is_padded str pipeline str scheduler str epilogue str layout_repr f layout_a layout_b layout_c dtype_repr f datatype_a datatype_b datatype_c tile_sizes _ join f tile_m tile_n tile_k f warp_m warp_n warp_k f warp_tile_m warp_tile_n warp_tile_k name ck_tile_gemm_universal_ + _ join f layout_repr f dtype_repr f tile_sizes f pipeline f scheduler f epilogue dict_items asdict items functools cache ops Generate supported instance dataclasses itertools compute_v _instances = CKTileGemmOperation layout_a=layout_a layout_b=layout_b layout_c=layout_c datatype_a=datatype_a datatype_b=datatype_b datatype_c=datatype_c tile_m=tile_m tile_n=tile_n tile_k=tile_k warp_m=warp_m warp_n=warp_n warp_k=warp_k warp_tile_m=warp_tile_m warp_tile_n=warp_tile_n warp_tile_k=warp_tile_k m_is_padded=m_is_padded n_is_padded=n_is_padded k_is_padded=k_is_padded pipeline= CompV scheduler= Intrawave epilogue=epilogue layout_a layout_b layout_c Row Row Row Row Col Row datatype_a datatype_b datatype_c FP BF tile_m tile_n tile_k warp_m warp_n warp_k warp_tile_m warp_tile_n warp_tile_k m_is_padded true false n_is_padded true false k_is_padded true false epilogue Default CShuffle compute_v _instances = CKTileGemmOperation layout_a=layout_a layout_b=layout_b layout_c=layout_c datatype_a=datatype_a datatype_b=datatype_b datatype_c=datatype_c tile_m=tile_m tile_n=tile_n tile_k=tile_k warp_m=warp_m warp_n=warp_n warp_k=warp_k warp_tile_m=warp_tile_m warp_tile_n=warp_tile_n warp_tile_k=warp_tile_k m_is_padded=m_is_padded n_is_padded=n_is_padded k_is_padded=k_is_padded pipeline= CompV scheduler= Intrawave epilogue=epilogue layout_a layout_b layout_c Row Row Row Row Col Row datatype_a datatype_b datatype_c FP BF tile_m tile_n tile_k half tile size since has double buffering warp_m warp_n warp_k warp_tile_m warp_tile_n warp_tile_k m_is_padded true false n_is_padded true false k_is_padded true false epilogue Default CShuffle mem_instances = CKTileGemmOperation layout_a=layout_a layout_b=layout_b layout_c=layout_c datatype_a=datatype_a datatype_b=datatype_b datatype_c=datatype_c tile_m=tile_m tile_n=tile_n tile_k=tile_k warp_m=warp_m warp_n=warp_n warp_k=warp_k warp_tile_m=warp_tile_m warp_tile_n=warp_tile_n warp_tile_k=warp_tile_k m_is_padded=m_is_padded n_is_padded=n_is_padded k_is_padded=k_is_padded pipeline= Mem scheduler=scheduler epilogue=epilogue layout_a layout_b layout_c Row Row Row Row Col Row datatype_a datatype_b datatype_c FP BF tile_m tile_n tile_k warp_m warp_n warp_k warp_tile_m warp_tile_n warp_tile_k m_is_padded true false n_is_padded true false k_is_padded true false scheduler Intrawave Interwave epilogue Default CShuffle list itertools chain compute_v _instances compute_v _instances mem_instances CKTileGemmTemplate CKTileTemplate This used rendering CK-Tile Universal GEMM kernels gemm_template = r version_comment headers globals instance_definition extern C PT_EXPORT kernel_definition using instance_namespace BaseGemmPipeline using instance_namespace TilePartitioner constexpr auto TileK = instance_namespace TileK constexpr auto kPrefetchStages = BaseGemmPipeline PrefetchStages const auto BiasTerms = std array const void const auto BiasStrides = std array int _t auto kargs = ck_tile UniversalGemmKernelArgs X W BiasTerms Y M N K LDA LDB BiasStrides LDC kBatch workspace_size workspace_size = run kernel const auto dispatch = const auto has_hot_loop_ const auto tail_number_ constexpr using Kernel = instance_namespace Kernel has_hot_loop_ value tail_number_ value Kernel IsSupportedArgument kargs we do our best statically avoid case ` filter_op ` throw std runtime_error invalid argument auto stream_config = ck_tile stream_config stream auto grid_size = Kernel GridSize M N kBatch constexpr auto block_size = Kernel BlockSize constexpr auto lds_bytes = constexpr auto kBlockPerCU = auto gemm = ck_tile make_kernel block_size x kBlockPerCU Kernel grid_size block_size lds_bytes kargs float elapsed_time = ck_tile launch_kernel stream_config gemm const ck_tile index_t k_grain = kBatch TileK const ck_tile index_t K_split = K + k_grain - k_grain TileK const ck_tile index_t num_loop = TilePartitioner GetLoopNum K_split const bool has_hot_loop = BaseGemmPipeline BlockHasHotloop num_loop const ck_tile TailNumber tail_num = BaseGemmPipeline GetBlockLoopTailNum num_loop rendered_dispatch kernel definition extern C __init__ input_nodes list Buffer layout Layout - None super __init__ ck_tile_gemm_template input_nodes=input_nodes layout=layout header - IndentedBuffer res = super header res splice CK GEMM header s #include ck_tile ops gemm hpp #include ck_tile ops epilogue hpp res globals - IndentedBuffer res = super globals res splice CK GEMM globals using Row = ck_tile tensor_layout gemm RowMajor using Col = ck_tile tensor_layout gemm ColumnMajor template ck_tile index_t PrefetchStages typename Dispatcher void dispatch_memory_pipeline_hot_loop const ck_tile TailNumber tail_num Dispatcher dispatch tail_num == ck_tile TailNumber One dispatch ck_tile bool_constant true ck_tile integral_constant ck_tile TailNumber ck_tile TailNumber One tail_num == ck_tile TailNumber Full dispatch ck_tile bool_constant true ck_tile integral_constant ck_tile TailNumber ck_tile TailNumber Full constexpr PrefetchStages tail_num == ck_tile TailNumber Two dispatch ck_tile bool_constant true ck_tile integral_constant ck_tile TailNumber ck_tile TailNumber Two constexpr PrefetchStages tail_num == ck_tile TailNumber Three dispatch ck_tile bool_constant true ck_tile integral_constant ck_tile TailNumber ck_tile TailNumber Three constexpr PrefetchStages tail_num == ck_tile TailNumber Four dispatch ck_tile bool_constant true ck_tile integral_constant ck_tile TailNumber ck_tile TailNumber Four constexpr PrefetchStages tail_num == ck_tile TailNumber Five dispatch ck_tile bool_constant true ck_tile integral_constant ck_tile TailNumber ck_tile TailNumber Five constexpr PrefetchStages tail_num == ck_tile TailNumber Six dispatch ck_tile bool_constant true ck_tile integral_constant ck_tile TailNumber ck_tile TailNumber Six constexpr PrefetchStages tail_num == ck_tile TailNumber Seven dispatch ck_tile bool_constant true ck_tile integral_constant ck_tile TailNumber ck_tile TailNumber Seven res check_dtypes op CKTileGemmOperation X_dtype W_dtype out_dtype = T get_layout dtype T input_nodes output_node op datatype_a = _TORCH_DTYPE_TO_CK X_dtype False op datatype_b = _TORCH_DTYPE_TO_CK W_dtype False op datatype_c = _TORCH_DTYPE_TO_CK out_dtype False True check_layouts op CKTileGemmOperation X_layout W_layout out_layout = torch_layout_to_ck_layout T get_layout T input_nodes output_node op layout_a = X_layout False op layout_b = W_layout False op layout_c = out_layout False True get_gemm_problem_size X_size W_size = T get_layout size T input_nodes M K = X_size _ N = W_size M N K check_block_tiles op CKTileGemmOperation The contiguous dimension tensor must divisible block tile size This helper function enforces inputs output M N K = get_gemm_problem_size check dim_size tile_size is_padded is_static_int dim_size dim_size tile_size = is_padded == false False True op layout_a == Row handle kBatch check True op layout_a == Col check M op tile_m op m_is_padded False raise AssertionError f Invalid layout op layout_a= op layout_b == Row check N op tile_n op n_is_padded False op layout_b == Col handle kBatch check True raise AssertionError f Invalid op layout_b= op layout_c == Row check N op tile_n op n_is_padded False op layout_c == Col check M op tile_m op m_is_padded False raise AssertionError f Invalid layout op layout_c= True check_alignments op CKTileGemmOperation The contiguous dimension tensor must divisible vector load size M N K = get_gemm_problem_size max_alignment contiguous_elements_per_tile elements_per_thread ck_dtype vector_load_bytes alignment = vector_load_bytes ck_dtype_to_size ck_dtype alignment contiguous_elements_per_tile alignment == elements_per_thread alignment == alignment threads_per_block = op warp_m op warp_n op warp_k gfx _threads_per_warp a_elements_per_thread = op tile_m op tile_k threads_per_block b_elements_per_thread = op tile_n op tile_k threads_per_block op layout_a == Row K contiguous tensor dimension a_max_vector_size = max_alignment op tile_k a_elements_per_thread op datatype_a is_static_int K K a_max_vector_size = False op layout_a == Col M contiguous tensor dimension a_max_vector_size = max_alignment op tile_m a_elements_per_thread op datatype_a is_static_int M M a_max_vector_size = False raise AssertionError f Invalid layout op layout_a= op layout_b == Row N contiguous tensor dimension b_max_vector_size = max_alignment op tile_n b_elements_per_thread op datatype_b is_static_int N N b_max_vector_size = False op layout_b == Col K contiguous tensor dimension b_max_vector_size = max_alignment op tile_k b_elements_per_thread op datatype_b is_static_int K K b_max_vector_size = False raise AssertionError f Invalid layout op layout_b= ` default ` epilogue writes C memory tensor element divisibility check necessary ` cshuffle ` epilogue writes C memory bytes so contiguous C dimension size must divisible number tensor elements bytes op epilogue == CShuffle op layout_c == Row is_static_int N N ck_dtype_to_size op datatype_c = False True check_warp_tiles op CKTileGemmOperation op tile_m op warp_m op warp_tile_m = False op tile_n op warp_n op warp_tile_n = False op tile_k op warp_k op warp_tile_k = False True check_block_tile_size op CKTileGemmOperation assuming LDS size KB op pipeline == CompV max_block_tile_size = max_block_tile_size = block_tile_size = ck_dtype_to_size op datatype_a op tile_m op tile_k + ck_dtype_to_size op datatype_b op tile_n op tile_k block_tile_size max_block_tile_size False True filter_op op CKTileGemmOperation Determines whether given op definition suitable current input output operation template implements Filter based inputs dtype layout statically inferred size Returns None op suitable otherwise returns op used check_dtypes op None check_layouts op None check_block_tiles op None check_alignments op None op emit_ck_instance op CKTileGemmOperation This method used generate code which defines type alias generated kernel template_definition = r Gemm operator operation_name namespace operation_name block tile constexpr int _t TileM = tile_m constexpr int _t TileN = tile_n constexpr int _t TileK = tile_k warps per block constexpr int _t WarpM = warp_m constexpr int _t WarpN = warp_n constexpr int _t WarpK = warp_k xdl tile constexpr int _t WarpTileM = warp_tile_m constexpr int _t WarpTileN = warp_tile_n constexpr int _t WarpTileK = warp_tile_k constexpr bool kPadM = m_is_padded constexpr bool kPadN = n_is_padded constexpr bool kPadK = k_is_padded using ALayout = layout_a using BLayout = layout_b using CLayout = layout_c using ADataType = datatype_a using BDataType = datatype_b using CDataType = datatype_c using AccDataType = F constexpr bool permuteA = false constexpr bool permuteB = false constexpr bool DoubleSmemBuffer = has_double_smem_buffer constexpr bool TransposeC = false constexpr int kBlockPerCu = constexpr ck_tile index_t TilePartitionerGroupNum = constexpr ck_tile index_t TilePartitionerM = using GemmShape = ck_tile TileGemmShape ck_tile sequence TileM TileN TileK ck_tile sequence WarpM WarpN WarpK ck_tile sequence WarpTileM WarpTileN WarpTileK permuteA permuteB using TilePartitioner = ck_tile GemmSpatiallyLocalTilePartitioner GemmShape TilePartitionerGroupNum TilePartitionerM using Traits = ck_tile TileGemmTraits kPadM kPadN kPadK ALayout BLayout CLayout using GemmUniversalTraits = ck_tile TileGemmUniversalTraits kPadM kPadN kPadK DoubleSmemBuffer ALayout BLayout CLayout TransposeC using GemmPipelineProblem = ck_tile GemmPipelineProblem ADataType BDataType AccDataType GemmShape Traits rendered_scheduler template bool has_hot_loop_v ck_tile TailNumber tail_number_v using UniversalGemmProblem = ck_tile UniversalGemmPipelineProblem ADataType BDataType AccDataType GemmShape GemmUniversalTraits scheduler has_hot_loop_v tail_number_v rendered_pipeline rendered_epilogue template bool has_hot_loop_v ck_tile TailNumber tail_number_v using Kernel = ck_tile GemmKernel TilePartitioner GemmPipeline has_hot_loop_v tail_number_v GemmEpilogue render_epilogue epilogue_type epilogue_type == Default r using EpilogueProblem = ck_tile DefaultGemm DEpilogueProblem ADataType BDataType AccDataType CDataType CLayout kPadM kPadN WarpTileM WarpTileN WarpTileK TransposeC using GemmEpilogue = ck_tile DefaultGemm DEpilogue EpilogueProblem epilogue_type == CShuffle r constexpr auto kMemoryOperation = ck_tile memory_operation_enum set using DsDataType = ck_tile tuple no bias terms vanilla GEMM using DsLayout = ck_tile tuple constexpr auto ELayout = CLayout using CDEElementWise = ck_tile element_wise PassThrough no-op using EpilogueProblem = ck_tile CShuffleEpilogueProblem ADataType BDataType DsDataType AccDataType CDataType DsLayout ELayout CDEElementWise GemmPipelineProblem kBlockSize TileM TileN WarpM WarpN WarpTileM WarpTileN WarpTileK TransposeC kMemoryOperation using GemmEpilogue = ck_tile CShuffleEpilogue EpilogueProblem raise AssertionError Epilogue must set render_pipeline pipeline_type rf using BaseGemmPipeline = ck_tile BaseGemmPipelineAgBgCr pipeline_type GemmPipelineProblem template bool has_hot_loop_v ck_tile TailNumber tail_number_v using GemmPipeline = ck_tile GemmPipelineAgBgCr pipeline_type UniversalGemmProblem has_hot_loop_v tail_number_v render_scheduler scheduler_type rf constexpr auto scheduler = ck_tile GemmPipelineScheduler scheduler_type rendered_definition = _template_from_string template_definition render operation_name=op name asdict op rendered_scheduler=render_scheduler op scheduler rendered_pipeline=render_pipeline op pipeline rendered_epilogue=render_epilogue op epilogue has_double_smem_buffer= true op pipeline == CompV false rendered_definition render type ignore override kernel ROCmTemplateKernel op CKTileGemmOperation kwargs - str The primary entry point code rendering process used template epilogue_nodes = kwargs get epilogue_nodes assert epilogue_nodes None == len epilogue_nodes template_buffer_node = kwargs get template_buffer_node template_buffer_node None output_node = template_buffer_node assert == len input_nodes X W = input_nodes Y = output_node instance_definition = emit_ck_instance op version_comment = rf Generated code CK inductor backend See type __module__ type __qualname__ Template instance op torch __version__= torch version git_version= getattr torch version git_version None render_dispatch pipeline_type op_name switch_tailnum_template = r switch tail_num tail_num valid_tailnums case ck_tile TailNumber tail_num dispatch has_hot_loop ck_tile integral_constant ck_tile TailNumber ck_tile TailNumber tail_num break endfor default std ostringstream err err Unsupported dispatch Pipeline pipeline Prefetch stages kPrefetchStages Tail num tail_num throw std runtime_error err str switch tail_num dispatch_template = r has_hot_loop rendered_with_hot_loop has_hot_loop == false rendered_without_hot_loop has_hot_loop pipeline_type == CompV _template_from_string dispatch_template render rendered_with_hot_loop=self _template_from_string switch_tailnum_template render has_hot_loop= ck_tile integral_constant bool true valid_tailnums= Full Odd Even pipeline=pipeline_type rendered_without_hot_loop=self _template_from_string switch_tailnum_template render has_hot_loop= ck_tile integral_constant bool false valid_tailnums= Full Odd Even pipeline=pipeline_type pipeline_type == Mem _template_from_string dispatch_template render rendered_with_hot_loop= dispatch_memory_pipeline_hot_loop kPrefetchStages tail_num dispatch rendered_without_hot_loop=self _template_from_string switch_tailnum_template render has_hot_loop= ck_tile integral_constant bool false valid_tailnums= Full Odd Even pipeline=pipeline_type pipeline_type == CompV _template_from_string dispatch_template render rendered_with_hot_loop=self _template_from_string switch_tailnum_template render has_hot_loop= ck_tile integral_constant bool true valid_tailnums= Two Three pipeline=pipeline_type rendered_without_hot_loop=self _template_from_string switch_tailnum_template render has_hot_loop= ck_tile integral_constant bool false valid_tailnums= Full Odd Even pipeline=pipeline_type raise AssertionError f Pipeline pipeline_type supported _template_from_string gemm_template render headers=self header getvalue globals=self globals getvalue instance_definition=instance_definition kernel_definition=kernel def_kernel inputs= X W type ignore list-item outputs= Y names_str= X W Y size_args= f int _t arg arg M N K LDA LDB LDC instance_namespace=op name version_comment=version_comment rendered_dispatch=render_dispatch op pipeline op name gen_ops Creates list ` CKTileGemmOperation ` instances match GEMM operation template represents The instances guaranteed have correct layout dtype dimension padding GEMM input arguments An instance may invalidate GEMM configuration runtime Such instances will assigned +inf runtime autotune process instances = ops instances raise AssertionError No Composable Kernel Universal GEMM instances found Please check library installed filtered_instances = list filter filter_op instances NB when using fixed list order most likely we will pick subset instances which very similar each other Randomizing choice seems solve random seed - chosen_instances = random sample filtered_instances min len filtered_instances config rocm ck_tile_max_profiling_configs config rocm ck_tile_max_profiling_configs filtered_instances log debug generated d ck instances after sample s len chosen_instances chosen_instances chosen_instances staticmethod add_choices choices layout input_nodes Add Composable Kernel Universal GEMM instance choices auto-tuning list template = CKTileGemmTemplate input_nodes layout ops = template gen_ops op ops k_batch template k_batch_choices op template maybe_append_choice choices op=op kBatch=k_batch k_batch_choices op CKTileGemmOperation - tuple int Returns list k_batch choices template default_choices = check dim_size tile_size is_padded is_static_int dim_size dim_size tile_size = is_padded == false False True _ _ K _ _ _ = size_args op layout_a == Row op layout_b == Col choices = tuple filter lambda k_batch check K op tile_k k_batch op k_is_padded default_choices choices = default_choices op epilogue == Default choices = choices size_args Sizes strides used kernel call X = input_nodes W = input_nodes Y = output_node M = X get_size K = X get_size N = W get_size LDA = X get_stride X get_stride == LDB = W get_stride W get_stride == LDC = Y get_stride Y get_stride == M N K LDA LDB LDC get_runtime_arg_info - list ArgInfo ArgInfo kBatch int _t get_runtime_arg_values kwargs Any - list Any maybe_append_choice kwarg k_batch must match name argument arg_names = OrderedSet arg name arg get_runtime_arg_info arg_names issubset kwargs raise ValueError Missing runtime arguments + join arg_names - kwargs keys kwargs k k arg_names