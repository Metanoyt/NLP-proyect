mypy allow-untyped-defs typing NamedTuple torch torch utils _pytree pytree torch _C _functorch _unwrap_for_grad _wrap_for_grad current_level TransformType torch _functorch apis vmap torch _functorch utils enable_single_level_autograd_function torch _functorch vmap _add_batch_dim _broadcast_to_and_flatten restore_vmap unwrap_batched wrap_batched torch _ops HigherOrderOperator torch autograd forward_ad _set_fwd_grad_enabled autograd Function technically runs before regular PyTorch dispatcher This how features like autocast torch_dispatch e g PythonTLSSnapshot work One day we might decide change until then we need give illusion autograd Function runs before those things We do using creating custom HigherOrderOperator only functorch dispatches specially CustomFunctionHigherOrderOperator HigherOrderOperator __init__ - None super __init__ custom_function_call __call__ autograd_function args kwargs When custom_function_call done dispatching through functorch should just invoke autograd Function This consistent autograd Function behavior being invoked before PyTorch dispatcher This will lead us into trouble later down line pre-existing There invariant function traced make_fx should have same behavior when provided same Tensor However make_fx sees autograd Function composite because autograd Function happens before Python dispatch key only traces forward pass torch _C _are_functorch_transforms_active super __call__ autograd_function args kwargs autograd_function apply args kwargs custom_function_call This mechanism autograd Function works functorch transforms It wraps autograd Function interactions functorch transforms defined via PyDispatcher HigherOrderOperator rather than through traditional PyTorch dispatcher custom_function_call = CustomFunctionHigherOrderOperator The grad rule custom_function_call construct new _SingleLevelFunction autograd Function only works single layer level functorch - unwraps inputs - redispatches custom_function_call - wraps outputs whose backward pass calls original autograd Function s backward Why do we need redispatch custom_function_call ----------------------------------------------------- This consistent how ATen operators work functorch s grad transform they always redispatch original operator Consider torch sin let s say we do grad grad torch sin x grad will - set up autograd graph - unwrap inputs - redispatch sin - rewrap outputs On redispatch grad will - set up autograd graph - unwrap inputs - redispatch sin - rewrap outputs To set up autograd graph we generate _SingleLevelFunction apply custom_function_call py_impl TransformType Grad custom_function_call py_impl TransformType Jvp custom_function_call_grad interpreter autograd_function operands Generated = generate_single_level_function interpreter autograd_function enable_single_level_autograd_function flat_out = Generated apply operands flat_out generate_single_level_function interpreter autograd_function level = interpreter level forward operands unwrapped_operands = pytree tree_map_only torch Tensor lambda x _unwrap_for_grad x level operands Both enable_grad _set_fwd_grad_enabled necessary no matter transform _SingleLevelFunction will turn off both fwd bwd gradient computation we need turn back here torch enable_grad _set_fwd_grad_enabled True interpreter lower unwrapped_output = custom_function_call autograd_function unwrapped_operands See NOTE mark_dirty object identity check wrap_fn output _wrap_for_grad output level wrap_outputs_maintaining_identity unwrapped_output unwrapped_operands operands wrap_fn setup_context ctx inputs output autograd_function setup_context ctx inputs output backward only used transform TransformType Grad backward ctx grads result = autograd_function backward ctx grads result jvp only used transform TransformType Jvp jvp ctx tangents result = autograd_function jvp ctx tangents result This sequence magic words dynamically generate Subclass given name A Tensor s grad_fn field has name original autograd Function s name + Backward so we do generate some meaningful name name = f autograd_function __name__ Generated Generated = type name torch autograd function _SingleLevelFunction forward staticmethod forward backward staticmethod backward jvp staticmethod jvp setup_context staticmethod setup_context Generated wrap_outputs_maintaining_identity handles outputs vmap backward vjp jvp staticmethod The way distinguishes between vmap case backward jvp case out_dims specified NB we cannot use out_dims=None deciding factor This because out_dims=None can still happen vmap staticmethod What user saying case their output does have dimension being vmapped over which valid NO_OUT_DIMS = specified NOTE mark_dirty object identity check autograd Function s ctx mark_dirty expect returned input have same object identity input Mode-only functorch will greatly simplify logic wrap_outputs_maintaining_identity outputs unwrapped_inputs orig_inputs wrap_fn out_dims=NO_OUT_DIMS flat_unwrapped_inputs = pytree arg_tree_leaves unwrapped_inputs flat_orig_inputs = pytree arg_tree_leaves orig_inputs unwrapped_input_to_orig_input = id unwrapped orig unwrapped orig zip flat_unwrapped_inputs flat_orig_inputs flat_outputs spec = pytree tree_flatten outputs result = out_dims_specified = out_dims = NO_OUT_DIMS out_dims_specified flat_out_dims = _broadcast_to_and_flatten out_dims spec _broadcast_to_and_flatten returns None unable broadcast TODO update following link master stable once s out flat_out_dims None raise RuntimeError f The autograd Function s vmap staticmethod returned f incompatible output out_dims tuple f Expected out_dims= out_dims f compatible structure ` output ` f out_dims has structure pytree tree_flatten out_dims f output has structure spec f For more details please see f https pytorch org docs main notes extending func html i output enumerate flat_outputs isinstance output torch Tensor result append output continue id output unwrapped_input_to_orig_input result append unwrapped_input_to_orig_input id output continue out_dims_specified result append wrap_fn output flat_out_dims i type ignore possibly-undefined index result append wrap_fn output pytree tree_unflatten result spec NOTE functorch vjp autograd interaction There s edge case functorch vjp autograd interaction will eventually fixed mode-only functorch The TL DR there s no way unwrap dead GradTensorWrapper so we framework need do manually Regular PyTorch operators automatically do so consistent MyExp torch autograd Function staticmethod forward x x exp staticmethod setup_context ctx inputs output y = output ctx save_for_backward y staticmethod backward gy y = ctx saved_tensors MyMul apply gy y x = torch randn requires_grad=True gy = torch randn requires_grad=True _ vjp_fn = vjp MySin apply x result = vjp_fn gy MyMul autograd Function shown here It saves ` y ` backward since gy requires grad vjp_fn gy we get MyMul apply gy GradTensorWrapper y level=dead Because y saved backward MyExp GradTensorWrapper now dead since we outside vjp context PyTorch dispatcher operations upon seeing dead GradTensorWrapper will automatically unwrap GradTensorWrapper when applied But since autograd Function technically sits above regular PyTorch dispatcher doesn t get treatment So we manually do unwrapping consistent regular PyTorch dispatcher operations VmapInfo NamedTuple batch_size int randomness str has_overridden_vmap_rule autograd_function autograd_function vmap torch autograd Function vmap validate_vmap_returns_tuple_of_two_elements result base_error_msg = Expected vmap staticmethod have two returns output out_dims pytree structure compatible output isinstance result tuple raise RuntimeError base_error_msg + f Got type result instead len result == raise RuntimeError base_error_msg + f Got len result returns instead custom_function_call py_impl TransformType Vmap custom_function_call_vmap interpreter autograd_function operands kwargs any isinstance val torch Tensor val torch utils _pytree tree_flatten kwargs raise NotImplementedError f Run vmap autograd Function kwarg-only Tensor args f Please do pass kwarg-only Tensors autograd Function f Got kwargs autograd_function generate_vmap_rule has_overridden_vmap_rule autograd_function TODO Update link stable once s out https github com pytorch pytorch issues raise RuntimeError f You tried vmap over autograd_function __name__ f has both generate_vmap_rule=True overridden vmap f staticmethod Please set generate_vmap_rule=False delete f overridden vmap staticmethod avoid ambiguity f For more details please see f https pytorch org docs main notes extending func html custom_function_call_vmap_generate_rule interpreter autograd_function operands has_overridden_vmap_rule autograd_function TODO Update link stable once s out https github com pytorch pytorch issues raise RuntimeError f You tried vmap over autograd_function __name__ f does have vmap support Please override implement f vmap staticmethod set generate_vmap_rule=True f For more details please see f https pytorch org docs main notes extending func html custom_function_call_vmap_helper interpreter autograd_function vmap autograd_function operands kwargs custom_function_call_vmap_helper interpreter vmap_function op operands kwargs current_level = interpreter level info = VmapInfo batch_size=interpreter batch_size randomness=interpreter randomness We re either autograd Function case vmap staticmethod torch library register_vmap case autograd_function_case = isinstance op torch autograd function FunctionMeta lower_to_next autograd_function_case interpreter lower torch _C _ExcludeDispatchKeyGuard torch _C DispatchKeySet torch _C DispatchKey FuncTorchBatched unwrapped_operands in_dims = unwrap_batched operands current_level If none tensors batched current level then we skip current level This saves user needing handle case their vmap staticmethod consistent our C++ batching rule API pytree tree_all lambda dim dim None in_dims lower_to_next autograd_function_case custom_function_call op operands op operands kwargs lower_to_next result = vmap_function info in_dims unwrapped_operands kwargs validate_vmap_returns_tuple_of_two_elements result unwrapped_output out_dims = result See NOTE mark_dirty object identity check wrap_fn output out_dim output out_dim None _add_batch_dim output out_dim current_level wrap_outputs_maintaining_identity unwrapped_output unwrapped_operands operands wrap_fn out_dims=out_dims unpack_outputs outputs out_dims = outputs - isinstance out_dims tuple outputs = outputs - outputs = outputs outputs out_dims custom_function_call_vmap_generate_rule interpreter autograd_function operands unwrapped_operands in_dims = unwrap_batched operands interpreter level vmapped_function = vmapify_autograd_function autograd_function in_dims interpreter batch_size interpreter randomness interpreter lower outputs = custom_function_call vmapped_function unwrapped_operands assert isinstance outputs tuple outputs out_dims = unpack_outputs outputs wrap_batched outputs out_dims interpreter level custom_function_call py_impl TransformType Functionalize custom_function_call_functionalize interpreter autograd_function generate_vmap_rule operands raise RuntimeError NYI Functionalize rule custom_function_call vmapify_autograd_function autograd_function in_dims batch_size randomness forward operands outputs out_dims = restore_vmap autograd_function forward in_dims batch_size randomness operands isinstance outputs torch Tensor outputs out_dims outputs out_dims setup_context ctx inputs outputs outputs out_dims = unpack_outputs outputs key = id Generated inner inputs outputs wrapped_ctx save_for_backward will - unwrap batchedtensors into tensor bdim - save_for_backward unwrapped_tensors - assign bdims wrapped_ctx _pt_saved_tensors_bdims wrapped_ctx = CtxCustomSave ctx current_level autograd_function setup_context wrapped_ctx inputs outputs input_shapes used reductify later reduce expanded gradients correct shape See NOTE Why can t we rely autograd reduce expanded gradients more details input_shapes = tuple inp shape isinstance inp torch Tensor None inp inputs hasattr ctx _pt_input_shapes ctx _pt_input_shapes = ctx _pt_input_shapes update key input_shapes hasattr ctx _pt_saved_tensors_bdims_stack ctx _pt_saved_tensors_bdims_stack = ctx _pt_saved_tensors_bdims_stack update key wrapped_ctx _pt_saved_tensors_bdims See NOTE Why do we need run setup_context under vmap restore_vmap inner in_dims out_dims batch_size randomness inputs outputs hasattr ctx _pt_out_dims ctx _pt_out_dims = ctx _pt_out_dims update key out_dims jvp ctx tangents key = id Generated jvp_no_context saved_tensors tangents wrapped_ctx = CtxWithSavedTensors ctx saved_tensors autograd_function jvp wrapped_ctx tangents tangent_in_dims = get_tangents_in_dims in_dims tangents out_tangents out_tangents_dims = restore_vmap jvp_no_context ctx _pt_saved_tensors_bdims_stack key tangent_in_dims batch_size randomness ctx saved_tensors tangents result = reductify out_tangents out_tangents_dims ctx _pt_out_dims key batch_size isinstance result torch Tensor result None result None backward ctx grad_outputs key = id Generated grad_outputs_ = grad_outputs - grad_outputs_in_dims = ctx _pt_out_dims key isinstance grad_outputs_in_dims tuple grad_outputs_in_dims = grad_outputs_in_dims grad_outputs_in_dims = tuple in_dim grad_output None None grad_output in_dim zip grad_outputs_ grad_outputs_in_dims backward_no_context inputs saved_tensors grad_outputs = inputs wrapped_ctx = CtxWithSavedTensors ctx saved_tensors autograd_function backward wrapped_ctx grad_outputs grad_ins grad_ins_dims = restore_vmap backward_no_context ctx _pt_saved_tensors_bdims_stack key grad_outputs_in_dims batch_size randomness ctx saved_tensors grad_outputs_ result = reductify grad_ins grad_ins_dims in_dims batch_size ctx _pt_input_shapes key result name = f Vmapped autograd_function __name__ Generated = type name torch autograd Function forward staticmethod forward backward staticmethod backward jvp staticmethod jvp setup_context staticmethod setup_context generate_vmap_rule True Generated tangents might None so we need replace corresponding in_dims None get_tangents_in_dims input_dims tangents flat_in_dims spec = pytree tree_flatten input_dims flat_tangents = pytree arg_tree_leaves tangents result = None tangent None in_dim in_dim tangent zip flat_in_dims flat_tangents pytree tree_unflatten result spec NOTE Why do we need run setup_context under vmap Consider following autograd Function Sum torch autograd Function staticmethod forward x x sum staticmethod setup_context ctx inputs outputs ctx x_shape = inputs staticmethod backward ctx gy gy expand ctx x_shape x = torch randn B in_dims = vmap Sum apply in_dims x Let s assume moment we didn t vmap setup_context VmappedSum VmappedSum torch autograd Function staticmethod forward x vmap Sum forward in_dims x staticmethod setup_context ctx inputs outputs Sum setup_context ctx inputs outputs staticmethod backward ctx gy backward_no_context gy gy expand ctx x_shape dims = gx = vmap backward_no_context dims gy gx We end up saving B x_shape In backward gy has shape B we re doing backward_no_context gy gy expand B gx = vmap backward_no_context dims gy Tensor B This gives us wrong result gx has shape B B should have shape Performing vmap over setup_context means shape saved has shape leads correct result shape gx Wraps ctx object Forwards all attr accesses underlying object except attrs _pt_attrs WrappedCtx _pt_reserved_attrs tuple str = _pt_reserved_attrs _pt_inner_ctx __init__ ctx isinstance ctx WrappedCtx reserved_attrs = type _pt_reserved_attrs name reserved_attrs hasattr ctx name continue raise RuntimeError f PyTorch reserves reserved_attrs field ctx Please name your fields ctx something avoid name collision _pt_inner_ctx = ctx __getattr__ name getattr _pt_inner_ctx name __setattr__ name value name type _pt_reserved_attrs __dict__ name = value setattr _pt_inner_ctx name value Wraps ctx create new ctx object overrides saved_tensors CtxWithSavedTensors WrappedCtx _pt_reserved_attrs = _pt_new_saved_tensors WrappedCtx _pt_reserved_attrs __init__ ctx new_saved_tensors super __init__ ctx _pt_new_saved_tensors = new_saved_tensors property saved_tensors _pt_new_saved_tensors CtxCustomSave WrappedCtx _pt_reserved_attrs = _pt_saved_tensors_bdims _pt_current_level WrappedCtx _pt_reserved_attrs __init__ ctx current_level super __init__ ctx _pt_saved_tensors_bdims = _pt_current_level = current_level save_for_backward tensors unwrapped_tensors bdims = unwrap_batched tensors _pt_current_level _pt_inner_ctx save_for_backward unwrapped_tensors _pt_saved_tensors_bdims = bdims save_for_forward tensors unwrapped_tensors bdims = unwrap_batched tensors _pt_current_level _pt_inner_ctx save_for_forward unwrapped_tensors _pt_saved_tensors_bdims = bdims reductify grad_input grad_input_bdim input_bdim batch_size target_shape_without_bdim_to_reduce_to=None isinstance grad_input tuple grad_input = grad_input isinstance grad_input_bdim tuple grad_input_bdim = grad_input_bdim isinstance input_bdim tuple input_bdim = input_bdim target_shape_without_bdim_to_reduce_to None target_shape_without_bdim_to_reduce_to = len grad_input None result = tuple reductify_leaf gi gi_bdim i_bdim batch_size maybe_ishape gi gi_bdim i_bdim maybe_ishape zip grad_input grad_input_bdim input_bdim target_shape_without_bdim_to_reduce_to result reductify_leaf grad_input grad_input_bdim input_bdim batch_size target_shape_without_bdim_to_reduce_to=None grad_input None None grad_input_bdim None input_bdim None grad_input grad_input_bdim None input_bdim None grad_input sum grad_input_bdim NOTE Why can t we rely autograd reduce expanded gradients For reverse-mode AD given grad_input input valid user grad_input has broadcasted shape when compared input In situation autograd automatically reduces grad_input shape input However when input_bdim None we have problems example grad_input Tensor input Tensor B We can expand grad_input Tensor B isn t broadcastable B example grad_input Tensor B input Tensor B We can swizzle grad_input Tensor B isn t broadcastable B This means we need also reduce grad_input shape input This behavior controlled ` target_shape_without_bdim_to_reduce_to ` flag not-None then we do reducing manually otherwise we do do reduction assert input_bdim None grad_input_bdim None grad_input = grad_input unsqueeze input_bdim new_shape = list grad_input shape new_shape input_bdim = batch_size grad_input = grad_input expand new_shape grad_input_bdim = input_bdim target_shape_without_bdim_to_reduce_to None vmap torch Tensor sum_to_size in_dims= grad_input_bdim None out_dims=input_bdim grad_input target_shape_without_bdim_to_reduce_to input_bdim = grad_input_bdim grad_input = grad_input movedim grad_input_bdim input_bdim grad_input autograd_function_forward_rewritten original_forward original_setup_context new_forward ctx args kwargs output = original_forward args kwargs original_setup_context ctx args output output new_forward AutogradFunctionApply HigherOrderOperator __init__ - None super __init__ autograd_function_apply __call__ fwd bwd fwd_args fwd_kwargs saved_values = None args_tensor_mask = fwd_kwargs args_tensor_mask non_differentiable_idx = fwd_kwargs non_differentiable_idx length_of_tensor_args = sum args_tensor_mask Filter out original tensor args fwd_args lifted freevars should args ApplyTemplate apply since we don t need calculate gradients them new_fwd_args = fwd_args length_of_tensor_args ApplyTemplate torch autograd Function staticmethod pyrefly ignore bad-override forward ctx args nonlocal saved_values output saved_values = fwd None fwd_args If users call ctx mark_non_differentiable original fwd function len non_differentiable_idx non_differentiable_output = i x enumerate output i non_differentiable_idx non_differentiable_output append x ctx mark_non_differentiable non_differentiable_output output staticmethod backward ctx grad bwd None grad saved_values ApplyTemplate apply new_fwd_args autograd_function_apply = AutogradFunctionApply