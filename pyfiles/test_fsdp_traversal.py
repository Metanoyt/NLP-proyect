Owner s oncall distributed sys torch torch distributed dist torch distributed fsdp FullyShardedDataParallel FSDP torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp DEVICEInitMode FSDPInitMode FSDPTest NestedWrappedModule torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TestTraversal FSDPTest property world_size torch torch accelerator is_available gpu_cnt = torch accelerator device_count gpu_cnt gpu_cnt skip_if_lt_x_gpu test_fsdp_modules nested_wrapped_module = NestedWrappedModule init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE modules = FSDP fsdp_modules nested_wrapped_module assertEqual modules nested_wrapped_module module get_submodule nested_wrapped_module module get_submodule get_submodule nested_wrapped_module module get_submodule modules = FSDP fsdp_modules nested_wrapped_module root_only=True assertEqual modules nested_wrapped_module module get_submodule nested_wrapped_module module get_submodule devices = cuda hpu xpu instantiate_device_type_tests TestTraversal globals only_for=devices allow_xpu=True __name__ == __main__ run_tests