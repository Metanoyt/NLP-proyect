mypy allow-untyped-defs logging operator collections defaultdict dataclasses dataclass field math prod typing Any cast torch torch utils _ordered_set OrderedSet config inductor_prims pattern_matcher CallFunction Ignored KeywordArg ListOf Match MULTIPLE PatternExpr PatternMatcherPass log = logging getLogger __name__ aten = torch ops aten patterns = PatternMatcherPass _is_last_dim t torch Tensor dim int - bool dim == t ndim - dim == - _is_backward graph torch fx Graph - bool placeholders = node graph nodes node op = placeholder break placeholders append node all node name startswith primal node placeholders _compute_mm_arithmetic_intensity M int N int K int - float M N K M K + N K + M N _filter_nodes_by_target nodes list torch fx Node target - list torch fx Node x x nodes x target == target _find_ancestors node torch fx Node - OrderedSet torch fx Node ancestors = OrderedSet torch fx Node ancestors add node cur_nodes = node while len cur_nodes new_nodes = node cur_nodes inp node all_input_nodes inp ancestors ancestors add inp new_nodes append inp cur_nodes = new_nodes OrderedSet node node ancestors node op = placeholder _get_tensor node torch fx Node - torch Tensor val = node meta val assert isinstance val torch Tensor val dataclass _AllGatherMatch match Match shard_node torch fx Node ag_node torch fx Node res_node torch fx Node gather_dim int group_name str replace_with new_node torch fx Node - None res_node replace_all_uses_with new_node erase - None node reversed match nodes len node users == node graph erase_node node find_all_gather_patterns graph torch fx Graph c d = torch ops _c d_functional make_zero_dim_all_gather_pattern shard CallFunction c d wait_tensor default CallFunction c d all_gather_into_tensor default shard Ignored KeywordArg group_name Matches funcol all_gather_tensor gather_dim == zero_dim_all_gather_pattern = make_zero_dim_all_gather_pattern KeywordArg shard make_all_gather_split_pattern shard CallFunction operator getitem CallFunction aten split Tensor make_zero_dim_all_gather_pattern shard Ignored _users=MULTIPLE Ignored make_cat_pattern splits CallFunction aten cat default ListOf splits KeywordArg gather_dim Matches funcol all_gather_tensor gather_dim non_zero_dim_all_gather_pattern = make_cat_pattern make_all_gather_split_pattern KeywordArg shard Match zero-dim all-gather which data transferred uint viewed back original dtype zero_dim_type_erased_all_gather_pattern = CallFunction aten view dtype make_zero_dim_all_gather_pattern KeywordArg shard Ignored Match non-zero dim all-gather which data transferred uint viewed back original dtype non_zero_dim_type_erased_all_gather_pattern = CallFunction aten view dtype make_cat_pattern CallFunction aten view dtype make_all_gather_split_pattern KeywordArg shard Ignored Ignored If two patterns same res_node_target have same suffix longer pattern should appear first list e g supposed we have A - B - C - D B - C - D should appear before list res_node_target_to_patterns = aten cat default non_zero_dim_all_gather_pattern aten view dtype non_zero_dim_type_erased_all_gather_pattern zero_dim_type_erased_all_gather_pattern c d wait_tensor default zero_dim_all_gather_pattern Match reverse ensure longer patterns prioritized all_gathers = visited_ag_nodes = OrderedSet torch fx Node node reversed graph nodes target patterns res_node_target_to_patterns items node target = target continue pattern ag_node_idx patterns match = pattern match node match continue assert isinstance match Match ag_node = match nodes ag_node_idx assert ag_node target == c d all_gather_into_tensor default ag_node visited_ag_nodes continue visited_ag_nodes add ag_node ag_match = _AllGatherMatch match=match shard_node=match kwargs shard ag_node=ag_node res_node=node gather_dim=match kwargs get gather_dim group_name=match kwargs group_name all_gathers append ag_match list reversed all_gathers dataclass _ReduceScatterMatch match Match input_node torch fx Node reduce_scatter_node torch fx Node wait_tensor_node torch fx Node reduce_op str scatter_dim int group_name str replace_with new_node torch fx Node - None Replace all uses result node wait_tensor fused node wait_tensor_node replace_all_uses_with new_node If reduce-scatter result saved backward save fused node backward instead _update_save_for_backward new_node _update_save_for_backward new_node torch fx Node - None If output node user reduce_scatter node indicating reduce_scatter result saved backward method will update output node use fused node instead output_node = None user reduce_scatter_node users user target == output output_node = user break output_node None output_node replace_input_with reduce_scatter_node new_node Assert now reduce scatter node has only one user wait_tensor s saved backward anymore assert len reduce_scatter_node users == Reduce scatter node has multiple users expected erase - None node reversed match nodes len node users == node graph erase_node node find_reduce_scatter_patterns graph torch fx Graph c d = torch ops _c d_functional reduce_scatter_template inp PatternExpr users int CallFunction c d wait_tensor default CallFunction c d reduce_scatter_tensor default inp KeywordArg reduce_op Ignored KeywordArg group_name _users=users Matches funcol reduce_scatter_tensor scatter_dim == zero_dim_reduce_scatter_pattern_single_user = reduce_scatter_template KeywordArg input users= Two users will occur when reduce-scatter result saved backward zero_dim_reduce_scatter_pattern_multi_user = reduce_scatter_template KeywordArg input users= Matches funcol reduce_scatter_tensor scatter_dim non_zero_dim_reduce_scatter_pattern_single_user = reduce_scatter_template CallFunction aten cat default ListOf CallFunction operator getitem CallFunction aten split Tensor KeywordArg input Ignored KeywordArg scatter_dim _users=MULTIPLE Ignored users= Two users will occur when reduce-scatter result saved backward non_zero_dim_reduce_scatter_pattern_multi_user = reduce_scatter_template CallFunction aten cat default ListOf CallFunction operator getitem CallFunction aten split Tensor KeywordArg input Ignored KeywordArg scatter_dim _users=MULTIPLE Ignored users= reduce_scatters = node reversed graph nodes node target == c d wait_tensor default match = non_zero_dim_reduce_scatter_pattern_single_user match node assert isinstance match Match reduce_scatters append _ReduceScatterMatch match=match input_node=match kwargs input reduce_scatter_node=match nodes - wait_tensor_node=node reduce_op=match kwargs reduce_op scatter_dim=match kwargs scatter_dim group_name=match kwargs group_name match = zero_dim_reduce_scatter_pattern_single_user match node assert isinstance match Match reduce_scatters append _ReduceScatterMatch match=match input_node=match kwargs input reduce_scatter_node=match nodes wait_tensor_node=node reduce_op=match kwargs reduce_op scatter_dim= group_name=match kwargs group_name match = non_zero_dim_reduce_scatter_pattern_multi_user match node assert isinstance match Match reduce_scatters append _ReduceScatterMatch match=match input_node=match kwargs input reduce_scatter_node=match nodes - wait_tensor_node=node reduce_op=match kwargs reduce_op scatter_dim=match kwargs scatter_dim group_name=match kwargs group_name match = zero_dim_reduce_scatter_pattern_multi_user match node assert isinstance match Match reduce_scatters append _ReduceScatterMatch match=match input_node=match kwargs input reduce_scatter_node=match nodes wait_tensor_node=node reduce_op=match kwargs reduce_op scatter_dim= group_name=match kwargs group_name list reversed reduce_scatters dataclass _Matmul nodes list torch fx Node arg_ancestor_nodes OrderedSet torch fx Node = field init=False A_node torch fx Node B_node torch fx Node pre_mm_reshape torch fx Node &#124; None post_mm_reshape torch fx Node &#124; None __post_init__ assert len nodes len nodes == assert nodes target aten mm default aten _scaled_mm default assert nodes target aten reshape default assert nodes target aten mm default aten _scaled_mm default assert nodes target aten reshape default arg_ancestor_nodes = _find_ancestors B_node replace_with new_node torch fx Node - None Replace matmul new node graph = new_node graph For D-matmuls we simply replace mm node ` new_node ` len nodes == mm_node = nodes assert mm_node target aten mm default aten _scaled_mm default mm_node replace_all_uses_with new_node graph erase_node mm_node An ND-matmul reshape - mm - reshape sequence We first replace second reshape node ` new_node ` Then we ensure original mm node sequence ends up zero users replacing reverse reshape ` new_node ` graph = new_node graph assert len nodes == mm_node = nodes output_reshape_node = nodes assert mm_node target aten mm default aten _scaled_mm default assert output_reshape_node target aten reshape default output_reshape_node replace_all_uses_with new_node len mm_node users graph inserting_after new_node new_mm_node = graph call_function aten reshape default args= new_node list _get_tensor mm_node shape mm_node replace_all_uses_with new_mm_node erase - None node reversed nodes len node users == node graph erase_node node classmethod from_match cls match list torch fx Node - _Matmul assert len match assert match target aten mm default aten reshape default mm_node = match len match == match _Matmul nodes=match A_node=cast torch fx Node match args B_node=cast torch fx Node mm_node args _Matmul handles reshapes via custom graph manipulation logic see ` replace_with ` method TODO explore unifying _Matmul _ScaledMatmul approaches handling reshapes pre_mm_reshape=None post_mm_reshape=None dataclass _ScaledMatmul _Matmul A_scale_node torch fx Node B_scale_node torch fx Node bias_node torch fx Node &#124; None result_scale_node torch fx Node &#124; None out_dtype torch dtype &#124; None use_fast_accum bool pre_mm_reshape torch fx Node &#124; None post_mm_reshape torch fx Node &#124; None __post_init__ super __post_init__ arg_ancestor_nodes &#124; = _find_ancestors A_scale_node arg_ancestor_nodes &#124; = _find_ancestors B_scale_node classmethod from_match cls match list torch fx Node - _ScaledMatmul assert len match assert match target aten _scaled_mm default aten reshape default get_arg node torch fx Node idx int default Any - Any idx = len node args default node args idx Use mm_node D args both A B even reshape - mm - reshape pattern We will store reshapes pre_mm_reshape post_mm_reshape referenced later produce correct output shapes reduce-scatter along correct dimensions etc is_reshape_mm_reshape_pattern = match target aten reshape default mm_node = match is_reshape_mm_reshape_pattern match pre_mm_reshape = match is_reshape_mm_reshape_pattern None post_mm_reshape = match - is_reshape_mm_reshape_pattern None A_node = cast torch fx Node mm_node args B_node = cast torch fx Node mm_node args A_scale_node = cast torch fx Node mm_node args B_scale_node = cast torch fx Node mm_node args _ScaledMatmul nodes=match A_node=A_node B_node=B_node A_scale_node=A_scale_node B_scale_node=B_scale_node bias_node=get_arg mm_node None result_scale_node=get_arg mm_node None out_dtype=get_arg mm_node None use_fast_accum=get_arg mm_node False pre_mm_reshape=pre_mm_reshape post_mm_reshape=post_mm_reshape _find_reshape_mm_reshape node torch fx Node - list _Matmul node target = aten reshape default matches = mm_node node users mm_node target aten mm default aten _scaled_mm default continue reshape_node mm_node users reshape_node target = aten reshape default continue Since reshape - mm - reshape pattern would subsumed into fused op we only match patterns where shape second reshape matches mm result produced fused op matmul_input_node = cast torch fx Node node args B_node = cast torch fx Node mm_node args matmul_out_shape = torch Size _get_tensor matmul_input_node shape - _get_tensor B_node shape - _get_tensor reshape_node shape = matmul_out_shape continue matches append node mm_node reshape_node If some rare reason mm_node being reshaped two different reshape nodes we only include mm_node once parsing result break matmuls = match matches mm_node = match mm_node target aten mm default matmul = _Matmul from_match match matmuls append matmul mm_node target aten _scaled_mm default matmul = _ScaledMatmul from_match match matmuls append matmul raise AssertionError Expect node s target either aten mm default f aten _scaled_mm default Got mm_node target matmuls _find_consumer_matmuls node torch fx Node - list _Matmul Find matmuls use ` node ` lhs argument matmuls = user node users ND matmuls user target aten reshape default matmuls extend _find_reshape_mm_reshape user D matmuls user target aten mm default matmul = _Matmul from_match match= user matmuls append matmul user target aten _scaled_mm default matmul = _ScaledMatmul from_match user matmuls append matmul matmuls _insert_fused_all_gather_matmul graph torch fx Graph matmuls list _Matmul shard_node torch fx Node gather_dim int group_name str - torch fx Node mm_types = OrderedSet map type matmuls assert len mm_types == mm_type = next iter mm_types mm_type == _Matmul B_nodes = matmul B_node matmul matmuls graph call_function torch ops symm_mem fused_all_gather_matmul default args= shard_node B_nodes gather_dim group_name kwargs= return_A True mm_type == _ScaledMatmul scaled_matmuls = cast list _ScaledMatmul matmuls graph call_function torch ops symm_mem fused_all_gather_scaled_matmul default args= shard_node matmul B_node matmul scaled_matmuls scaled_matmuls A_scale_node matmul B_scale_node matmul scaled_matmuls gather_dim group_name matmul bias_node matmul scaled_matmuls matmul result_scale_node matmul scaled_matmuls matmul out_dtype matmul scaled_matmuls matmul use_fast_accum matmul scaled_matmuls raise AssertionError f Unexpected matmul match type mm_type fuse_all_gather_matmul all_gather _AllGatherMatch - None Fused pattern A = all_gather_tensor A_shard gather_dim group_name C_ = torch matmul A B_ C_ = torch matmul A B_ C_ = torch matmul A B_ into A Cs = torch ops symm_mem fused_all_gather_matmul A_shard B_ B_ B_ gather_dim group_name torch distributed is_available torch distributed is_nccl_available torch distributed _symmetric_memory is_symm_mem_enabled_for_group restride_A_shard_for_fused_all_gather_matmul shard_node ag_node ag_res_node gather_dim group_name = all_gather shard_node all_gather ag_node all_gather res_node all_gather gather_dim all_gather group_name is_symm_mem_enabled_for_group group_name filter_matmul = None _is_last_dim _get_tensor shard_node gather_dim Decomposed mms should too small _get_tensor shard_node shape - scaled_mm supported yet last dim _filter_out_scaled_matmul matmul _Matmul isinstance matmul _ScaledMatmul filter_matmul = _filter_out_scaled_matmul Find consumer matmuls matmuls = _find_consumer_matmuls ag_res_node The matmuls only fusible non-A args don t depend all-gather result node matmuls = matmul matmul matmuls all_gather res_node matmul arg_ancestor_nodes len matmuls == len OrderedSet map type matmuls = _is_last_dim _get_tensor shard_node gather_dim len all_gather res_node users len matmuls The result ag-split-cat used only matmuls Then has materialized which can have overhead filter_matmul filter_matmul matmuls Fuse all_gather_tensor eligible matmuls graph = ag_node graph graph inserting_before ag_node _is_last_dim _get_tensor shard_node gather_dim val shard_node meta restrided = restride_A_shard_for_fused_all_gather_matmul _get_tensor shard_node gather_dim shard_node = graph call_function inductor_prims force_stride_order args= shard_node restrided stride fused_node = _insert_fused_all_gather_matmul graph matmuls shard_node gather_dim group_name new_ag_node = graph call_function operator getitem args= fused_node new_out_nodes = graph call_function operator getitem args= fused_node idx matmul enumerate matmuls new_out_node = graph call_function operator getitem args= new_out_nodes idx matmul replace_with new_out_node matmul erase all_gather replace_with new_ag_node all_gather erase If new_ag_node has no users we tell fused op This creates more optimization opportunities len new_ag_node users == graph erase_node new_ag_node kwargs = dict fused_node kwargs return_A kwargs kwargs return_A = False fused_node kwargs = kwargs Raise ancestors non-A args topologically ordered between ag_res_node matmul above fused_node order = node idx idx node enumerate graph nodes nodes_to_raise = sorted OrderedSet x matmul matmuls x matmul arg_ancestor_nodes key=lambda x order x node nodes_to_raise order node order fused_node fused_node prepend node _scatter_dim_after_reshape reshape_node torch fx Node orig_scatter_dim int - int Given reshape node original scatter dim target tensor returns new scatter dim reshaped tensor there no pre-mm reshape scatter dim will change reshape_node orig_scatter_dim reshape_op_output_tensor = _get_tensor reshape_node assert reshape_op_output_tensor ndim == reshape must produce D tensor scaled_mm assert len reshape_node args = reshape node must have least arg input_tensor_node = cast torch fx Node reshape_node args reshape_op_input_tensor = _get_tensor input_tensor_node assert reshape_op_input_tensor ndim reshape_op_output_tensor ndim reshape must D+ D Note N-D tensor reshaped into D either leading dims ending dims must collapsed single dim First determine which these happened input_shape = reshape_op_input_tensor shape output_shape = reshape_op_output_tensor shape leading_dims_collapsed = output_shape == prod input_shape - Case scatter dim always maps after any reshape D+ D regardless leading dims ending dims collapsed orig_scatter_dim == Case scatter dim ndim- always maps after any reshape D+ D regardless leading dims ending dims collapsed orig_scatter_dim == reshape_op_input_tensor ndim - Case scatter dim one middle dims between ndim- leading dims collapsed new scatter dim will ending dims collapsed new scatter dim will leading_dims_collapsed _find_producer_matmul node torch fx Node - _Matmul &#124; None Returns producer matmul node found otherwise returns None node target aten mm default _Matmul from_match match= node node target aten _scaled_mm default _ScaledMatmul from_match match= node node target aten reshape default reshape_node_ = node mm_node = reshape_node_ args assert isinstance mm_node torch fx Node mm_node target aten mm default aten _scaled_mm default None reshape_node_ = mm_node args assert isinstance reshape_node_ torch fx Node reshape_node_ target = aten reshape default None mm_node target aten mm default _Matmul from_match match= reshape_node_ mm_node reshape_node_ mm_node target aten _scaled_mm default _ScaledMatmul from_match match= reshape_node_ mm_node reshape_node_ None _insert_fused_matmul_reduce_scatter graph torch fx Graph matmul _Matmul reduce_op str orig_scatter_dim int group_name str scatter_dim_after_reshape int only used reshape - scaled_mm - reshape pattern output_shape list int only used reshape - scaled_mm - reshape pattern - torch fx Node type matmul _Matmul graph call_function torch ops symm_mem fused_matmul_reduce_scatter default args= matmul A_node matmul B_node reduce_op orig_scatter_dim group_name type matmul _ScaledMatmul graph call_function torch ops symm_mem fused_scaled_matmul_reduce_scatter default args= matmul A_node matmul B_node matmul A_scale_node matmul B_scale_node reduce_op orig_scatter_dim scatter_dim_after_reshape group_name output_shape matmul bias_node matmul result_scale_node matmul out_dtype matmul use_fast_accum raise AssertionError f Unexpected matmul match type type matmul fuse_matmul_reduce_scatter reduce_scatter _ReduceScatterMatch - None Fused pattern reduce_scatter_tensor A B scatter_dim group_name into torch ops symm_mem fused_matmul_reduce_scatter A B scatter_dim group_name Returns boolean indicating fusion successful torch distributed is_available torch distributed is_nccl_available torch distributed _symmetric_memory is_symm_mem_enabled_for_group restride_A_for_fused_matmul_reduce_scatter input_node _reduce_scatter_node rs_wait_tensor_node reduce_op orig_scatter_dim group_name = reduce_scatter input_node reduce_scatter reduce_scatter_node reduce_scatter wait_tensor_node reduce_scatter reduce_op reduce_scatter scatter_dim reduce_scatter group_name is_symm_mem_enabled_for_group group_name filter_matmul = None _is_last_dim _get_tensor input_node orig_scatter_dim scaled_mm supported yet last dim mm+rs _filter_out_scaled_matmul matmul _Matmul isinstance matmul _ScaledMatmul filter_matmul = _filter_out_scaled_matmul Currently fused_matmul_reduce_scatter doesn t matmul result so we can t apply fusion matmul result used multiple users This fundamental limitation fused op can addressed needed len input_node users = log warning matmul result has more than one user skipping fused_matmul_reduce_scatter fusion matmul = _find_producer_matmul input_node matmul None log warning no producer matmul found reduce scatter skipping fuse_matmul_reduce_scatter fusion filter_matmul filter_matmul matmul rs_wait_tensor_node matmul arg_ancestor_nodes log warning reduce-scatter result node ancestor matmul skipping fuse_matmul_reduce_scatter fusion We need track values fused scaled mm reduce scatter implementation The scatter dim before reshape which assigned using original b c c d = b d dims The scatter dim after reshape use when we doing D b c c d = b d scaled mm op Store expected potentially D+ mm output shape so we can reshape D mm output intended D+ shape before applying reduce-scatter prevent shape errors subsequent ops If A reshaped D+ - D mm we need determine new scattter dim after reshape fused matmul reduce scatter implementation use matmul pre_mm_reshape scatter_dim_after_maybe_reshape = _scatter_dim_after_reshape matmul pre_mm_reshape orig_scatter_dim scatter_dim_after_maybe_reshape = orig_scatter_dim If D mm output reshaped D - D+ we need store intended output shape fused matmul reduce scatter implementation use matmul post_mm_reshape output_shape = list _get_tensor matmul post_mm_reshape shape A_orig_shape = list _get_tensor matmul A_node shape B_shape = list _get_tensor matmul B_node shape output_shape = A_orig_shape - B_shape - graph = rs_wait_tensor_node graph graph inserting_before rs_wait_tensor_node Restride A tensor before fused op optimal perf fused matmul reduce scatter val matmul A_node meta restrided = restride_A_for_fused_matmul_reduce_scatter _get_tensor matmul A_node scatter_dim_after_maybe_reshape matmul A_node = graph call_function inductor_prims force_stride_order args= matmul A_node restrided stride Replace matched subgraph fused matmul reduce scatter node fused_node = _insert_fused_matmul_reduce_scatter graph matmul reduce_op orig_scatter_dim group_name scatter_dim_after_maybe_reshape output_shape reduce_scatter replace_with fused_node reduce_scatter erase matmul erase order = node idx idx node enumerate graph nodes nodes_to_raise = sorted matmul arg_ancestor_nodes key=lambda x order x node nodes_to_raise order node order fused_node fused_node prepend node log debug successfully fused matmul reduce scatter _get_node_to_ancestors graph torch fx Graph - dict torch fx Node OrderedSet torch fx Node Compute ancestors all nodes graph node_to_ancestors = defaultdict OrderedSet torch fx Node type ignore var-annotated node graph nodes node_to_ancestors node = OrderedSet node all_input_nodes dep node all_input_nodes node_to_ancestors node &#124; = node_to_ancestors dep node_to_ancestors _get_collective_to_overlappable_nodes graph torch fx Graph - dict torch fx Node list torch fx Node For each collective graph find nodes neither ancestors nor descendants collective is_collective node - bool Only consider all-gather reduce-scatter context micro-pipeline TP node target torch ops _c d_functional all_gather_into_tensor default torch ops _c d_functional reduce_scatter_tensor default node_to_ancestors = _get_node_to_ancestors graph collective_to_overlappable_nodes = defaultdict list node graph nodes is_collective node continue x graph nodes node node_to_ancestors x x node_to_ancestors node x op == call_function collective_to_overlappable_nodes node append x collective_to_overlappable_nodes _get_unexposed_collectives graph torch fx Graph - list torch fx Node Find all unexposed collectives graph Because we don t have runtime estimate function rough estimation using following strong hand-wavy assumptions - Only predefined set compute intensive operation can hide collective - Any compute intensive operation can hide exactly one collective _is_compute_intensive node torch fx Node - bool node target torch ops aten mm default collective_to_overlapping_candidates = defaultdict list available_nodes = OrderedSet torch fx Node collective_to_overlappable_nodes = _get_collective_to_overlappable_nodes graph collective overlappable_nodes collective_to_overlappable_nodes items candidates = x x overlappable_nodes _is_compute_intensive x collective_to_overlapping_candidates collective = candidates available_nodes update candidates unexposed_collectives = collective overlapping_candidates collective_to_overlapping_candidates items Each collective consumes exactly one overlapping candidate x overlapping_candidates x available_nodes unexposed_collectives append collective available_nodes remove x break unexposed_collectives micro_pipeline_tp_pass graph torch fx Graph all_gathers = find_all_gather_patterns graph reduce_scatters = find_reduce_scatter_patterns graph When collective can hidden through either simple overlapping micro-pipeline TP we prefer simple overlapping avoid overhead associated decomposition If reorder_for_compute_comm_overlap enabled we identify collectives can hidden through simple overlapping exclude them micro-pipeline TP candidates config reorder_for_compute_comm_overlap unexposed_collectives = _get_unexposed_collectives graph all_gathers = x x all_gathers x ag_node unexposed_collectives reduce_scatters = x x reduce_scatters x reduce_scatter_node unexposed_collectives all_gathers reduce_scatters log warning async TP found no matching all-gather reduce-scatter patterns fusion all_gather all_gathers fuse_all_gather_matmul all_gather reduce_scatter reduce_scatters fuse_matmul_reduce_scatter reduce_scatter