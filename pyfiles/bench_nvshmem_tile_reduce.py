usr bin env python Benchmark NVSHMEM tile reduce operations Usage python benchmarks distributed bench_nvshmem_tile_reduce py This benchmark measures performance tile reduce operations across different matrix sizes tile configurations time torch torch distributed dist torch distributed _symmetric_memory symm_mem torch testing _internal common_distributed MultiProcContinuousTest torch testing _internal common_utils requires_cuda_p p_access skip_but_pass_in_sandcastle_if skipIfRocm Decorator requires_nvshmem skip_but_pass_in_sandcastle_if symm_mem is_nvshmem_available bench_nvshmem_tile_reduce requires NVSHMEM skipping benchmark So benchmarks written device-agnostic way device_type = cuda device_module = torch get_device_module device_type requires_nvshmem requires_cuda_p p_access NVSHMEMTileReduceBenchmark MultiProcContinuousTest _init_device - None TODO relieve seems hang without device_module set_device device Set NVSHMEM SymmMem backend symm_mem set_backend NVSHMEM property device - torch device torch device device_type rank _benchmark_tile_reduce_single full_size int tile_size int warmup_iters int = bench_iters int = - dict Benchmark single configuration tile reduce Args full_size Size full matrix full_size x full_size warmup_iters Number warmup iterations bench_iters Number benchmark iterations Returns Dictionary benchmark results _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float Allocate full matrices full_inp = symm_mem empty full_size full_size dtype=dtype device=self device fill_ rank full_out = symm_mem empty full_size full_size dtype=dtype device=self device fill_ slice_ut = slice tile_size inp_tile = full_inp slice_ut slice_ut out_tile = full_out slice_ut slice_ut root = Warmup iterations _ range warmup_iters torch ops symm_mem tile_reduce inp_tile out_tile root group_name torch cuda synchronize device Benchmark iterations times = dist barrier torch cuda synchronize device start_time = time perf_counter _ range bench_iters torch ops symm_mem tile_reduce inp_tile out_tile root group_name torch cuda synchronize device end_time = time perf_counter times append end_time - start_time bench_iters Calculate statistics times = torch tensor times dtype=torch float tile_elements = tile_size tile_size tile_bytes = tile_elements dtype itemsize hasattr dtype itemsize tile_elements results = full_size full_size tile_size tile_size tile_elements tile_elements tile_bytes tile_bytes world_size world_size mean_time_ms times mean item std_time_ms times std item min_time_ms times min item max_time_ms times max item throughput_gb_s tile_bytes times mean item e elements_per_sec tile_elements times mean item results skipIfRocm test_benchmark_tile_reduce_various_sizes - None Benchmark tile reduce across various matrix sizes Test various matrix sizes tile_sizes = full_size = tile_sizes - warmup_iters = bench_iters = results = tile_size tile_sizes try result = _benchmark_tile_reduce_single full_size tile_size warmup_iters bench_iters results append result rank == print f Matrix Size full_size x full_size Tile Size tile_size x tile_size print f Mean Time result mean_time_ms f Â± result std_time_ms f ms print f Throughput result throughput_gb_s f GB s print f Bytes result tile_bytes f print except Exception e rank == print f Failed benchmark matrix size full_size e Print summary rank == results print === BENCHMARK SUMMARY === print f Matrix Size Tile Size Time ms Throughput GB s Bytes print - result results print f result full_size x result full_size f result tile_size x result tile_size f result mean_time_ms f f result throughput_gb_s f f result tile_bytes f __name__ == __main__ For standalone usage you d need set up distributed environment For now meant run via PyTorch test framework torch testing _internal common_utils run_tests run_tests