mypy ignore-errors itertools json logging math warnings warnings filterwarnings ignore message= The behavior DataFrame concatenation empty all-NA entries deprecated dataclasses dataclass numpy np pandas pd type ignore import-untyped ah_tree DecisionTree scipy stats gmean sklearn model_selection train_test_split sklearn tree DecisionTreeClassifier train AHTrain log = logging getLogger __name__ DEBUG = True DEBUG ch = logging StreamHandler ch setLevel logging DEBUG formatter = logging Formatter asctime s - message s datefmt= Y- m- d H M S ch setFormatter formatter log addHandler ch AHTrainDecisionTree AHTrain __init__ super __init__ debug_time row top_k_choices choices_feedback = json loads row choice time timings = sorted choices_feedback items key=lambda x x choice time timings result = f choice time choice top_k_choices result += TOPK print result is_unsafe_leaf row predicted_config choice time Can overridden subclasses define their own logic deciding when leaf unsafe Returns sample landed leaf choice predicted tree dictionary maps each choice execution time One can example decide mark leaf unsafe predicted choice x slower than fastest choice If leaf unsafe learned heuristic will always unsure input lands leaf False get_unsafe_leaves model df feature_columns Given trained decision tree dataframe containing training data returns list unsafe leaves X = df feature_columns leaf_ids = model apply X unique_leaves = np unique leaf_ids unsafe_leaves = Iterate over each leaf leaf unique_leaves leaf_mask = leaf_ids == leaf Get samples land leaf leaf_X = X leaf_mask predicted_config = model predict leaf_X iloc For each sample check we should mark leaf unsafe idx row leaf_X iterrows choice time = json loads df loc idx choice time is_unsafe_leaf row predicted_config choice time unsafe_leaves append leaf break unsafe_leaves get_allowed_wrong_prediction_pct This used determine threshold when learned heuristic returns unsure If function returns we will set probability required decision tree decision such most predictions will wrong validation set get_grid_search_values Standard values grid search Can overridden max_depth min_samples_leaf criterion gini entropy predict model df feature_columns Returns predictions probabilities leaf ids given dataframe predictions = model predict df feature_columns proba = model predict_proba df feature_columns leaf_ids = model apply df feature_columns predictions proba leaf_ids ranking_num_choices heuristic used ranking function returns number choices heuristic will args ranking None args ranking train_and_evaluate_models datasets max_depths min_samples_leafs criterion_list feature_columns ranking=False Does grid search over max_depths min_samples_leafs criterion_list returns best model results = best_model = None best_model_safe_proba = best_model_num_correct = best_model_unsafe_leaves = columns = set crit max_depth min_samples_leaf metrics_columns = max_depth min_samples_leaf criterion itertools product max_depths min_samples_leafs criterion_list print f max_depth= max_depth min_samples_leaf= min_samples_leaf criterion= criterion model = DecisionTreeClassifier max_depth=max_depth min_samples_leaf=min_samples_leaf criterion=criterion random_state= df_train = datasets train df_val = datasets val ranking model fit df_train feature_columns df_train winner sample_weight=df_train relative_performance model fit df_train feature_columns df_train winner model = DecisionTree model feature_columns ranking model prune df_train winner k=self ranking_num_choices unsafe_leaves = get_unsafe_leaves model df_train feature_columns predictions proba leaf_ids = predict model df_val feature_columns wrong_pct = get_allowed_wrong_prediction_pct evaluator = DecisionEvaluator model predictions df_val proba wrong_pct=wrong_pct unsafe_leaves=unsafe_leaves leaf_ids=leaf_ids k=self ranking_num_choices ranking=ranking safe_proba = evaluator get_safe_proba print f safe_proba= safe_proba eval name df ranking when ranking enabled we duplicate each input each choice almost good best choice we do want evaluate same input multiple times so we remove duplicates here df = df df winner == df actual_winner predictions proba leaf_ids = predict model df feature_columns evaluator = DecisionEvaluator model predictions df proba wrong_pct=wrong_pct threshold=safe_proba unsafe_leaves=unsafe_leaves leaf_ids=leaf_ids k=self ranking_num_choices ranking=ranking evaluator get_results dataset_name dataset datasets items eval_result EvalResults = eval dataset_name dataset eval_result_metrics = eval_result to_map dataset_name == val num_correct = eval_result accuracy num_correct num_wrong = eval_result accuracy num_wrong num_total = eval_result accuracy total num_wrong = num_total wrong_pct num_correct best_model_num_correct print f new best model num_correct correct num_wrong wrong best_model = model best_model_num_correct = num_correct best_model_safe_proba = safe_proba best_model_unsafe_leaves = unsafe_leaves result = dataset_name criterion max_depth min_samples_leaf result += tuple eval_result_metrics values results append result len metrics_columns == metrics_columns = list eval_result_metrics keys columns += metrics_columns pd DataFrame results columns=columns best_model best_model_safe_proba best_model_unsafe_leaves get_test_and_val_size Returns size test validation sets prepare_datasets df other_datasets cat_feature cats ranking=False Splits dataframe into train val test sets Also adds other datasets specified user train set test_size val_size = get_test_and_val_size Split into train+val test df_train_val df_test = train_test_split df test_size=test_size random_state= Split train+val inputs into train val train_val_size = - test_size df_train df_val = train_test_split df_train_val test_size=val_size train_val_size random_state= datasets = train df_train val df_val test df_test add_real_datasets datasets other_datasets cat_feature cats ranking datasets export_to_dot best_model df feature_columns Export learned decision tree dot file dot_str = best_model to_dot open best_model dot w f f write dot_str get_feature_columns df The dataframe contains columns features such winner speedup only used debugging purposes This function returns columns actually features exclude_columns = speedup winner target avail_choices choice time index actual_winner relative_performance feature_columns = col col df columns col exclude_columns feature_columns add_training_data df_train datasets datasets train main log_path other_datasets nrows heuristic_name save_dot=False ranking=False Main function trains decision tree generates heuristic TODO Enable apply_filters df choices cat_feature cats dummy_col_ _col_val metadata = get_df log_path nrows=nrows apply_filters=False add_near_best=ranking dummy_col_ _col_val = dummy_col_ _col_val datasets = prepare_datasets df other_datasets cat_feature cats ranking df_train = add_training_data datasets train datasets datasets train = df_train print datasets train winner value_counts to_string feature_columns = get_feature_columns df grid_search_values = get_grid_search_values max_depths = grid_search_values max_depth min_samples_leafs = grid_search_values min_samples_leaf criterion_list = grid_search_values criterion results_df best_model best_model_safe_proba unsafe_leaves = train_and_evaluate_models datasets max_depths min_samples_leafs criterion_list feature_columns ranking=ranking ranking columns_to_keep = set crit max_depth min_samples_leaf total top_k_correct top_k_wrong top_k_unsure wrong_max_speedup_k wrong_gmean_speedup_k results_df = results_df columns_to_keep prints results all models datasets print results_df to_string sort_metric = top_k_correct ranking correct prints results grouped dataset set_name results_df set unique dataset_results = results_df results_df set == set_name dataset_results = dataset_results sort_values by=sort_metric print dataset_results to_string + \n best_model None save_dot export_to_dot best_model df feature_columns codegen best_model metadata heuristic_name best_model_safe_proba dummy_col_ _col_val unsafe_leaves print All learned models have too many wrong predictions so no heuristic generated get_df log_path cat_feature cats=None nrows=None apply_filters=False add_near_best=False Parses log file processes data into dataframe can used training df metadata features categorical_features choices = parse_log log_path nrows calculate_stats group count = len group has_inf = np isinf group feedback any has_inf relative_std = np inf median = np inf mean = group feedback mean std = group feedback std relative_std = std mean mean = np inf median = group feedback median relative_std times = group feedback tolist times_str = join f t f t sorted times log debug High relative std f times= s relative_std times_str pd Series count count relative_std relative_std median_execution_time median feature_columns = features stats = df groupby feature_columns + choice as_index=False apply calculate_stats include_groups=False reset_index TODO We have careful removing certain choices because we e g remove winner heuristic will end up learning wrong things But execution times high variance also bad apply_filters Filter out inputs less than measurements high relative std valid_stats = stats stats count = stats relative_std = Group input features count how many valid choices we have each input valid_inputs = valid_stats groupby feature_columns filter lambda x len x = valid_inputs = stats Compute winner speedup each valid input get_winner_and_speedup group assert len group = Need least choices sorted_group = group sort_values median_execution_time winner = sorted_group iloc choice winning_time = sorted_group iloc median_execution_time second_best_time = sorted_group iloc median_execution_time speedup = second_best_time winning_time unique_choices = group choice unique choice time = row group itertuples choice time row choice = row median_execution_time assert len unique_choices == len group f len unique_choices = len group len unique_choices = len group pd Series winner winner speedup speedup avail_choices unique_choices choice time json dumps choice time results = valid_inputs groupby feature_columns as_index=False filter lambda x len x = groupby feature_columns as_index=False apply get_winner_and_speedup include_groups=False reset_index add_near_best_configs df new_rows = index row df iterrows dictionary = json loads row choice time min_value = min dictionary values key value dictionary items new_row = row copy relative_performance = min_value value new_row relative_performance = relative_performance relative_performance None relative_performance np inf breakpoint new_row actual_winner = row winner new_row winner = key relative_performance = new_rows append new_row pd DataFrame new_rows reset_index drop=True add_near_best results = add_near_best_configs results results added_categorical_features = add_new_features results categorical_features += added_categorical_features results cat_feature cats dummy_col_ _col_val = handle_categorical_features cat_feature cats categorical_features results results choices cat_feature cats dummy_col_ _col_val metadata ranking_always_included_choices gen_classes classes num_spaces If classes= choice choice choice then function returns following string choices append choice choices append choice choices append choice Used generated heuristic map index choice its name indent = num_spaces \n join f indent choices append c c classes get_default_config row Returns default config given sample The default config could example config chosen current handwritten heuristic This can example used get_unsafe_leaf compare predicted config default config None gen_predict_fn_def Generates definition predict function get_best_choices context AHContext - Optional list tuple float int codegen_boilerplate heuristic_name opt_name threshold shared_memory device_capa classes Generates boilerplate code generated heuristic This includes things like imports definition etc boiler_plate = f flake noqa B fmt off This file generated AutoHeuristic Do modify manually To regenerate file take look steps README md file inside torchgen _autoheuristic opt_name typing Optional torch _inductor autoheuristic autoheuristic_utils AHContext AHMetadata Choice torch _inductor autoheuristic learnedheuristic_interface LearnedHeuristicDecision heuristic_name LearnedHeuristicDecision __init__ - None choices list Choice = fill_choices gen_precondition opt_name shared_memory device_capa get_confidence_threshold - float threshold get_choice idx int - Optional str idx len choices choices idx None fill_choices - None gen_classes classes num_spaces= get_name - str opt_name boiler_plate add_real_datasets datasets other_datasets cat_feature cats ranking=False Adds datasets specified user datasets dictionary other_datasets name path other_datasets df_other choices _ _ _ = get_df path cat_feature cats=cat_feature cats apply_filters=False add_near_best=ranking datasets name = df_other codegen tree metadata heuristic_name threshold dummy_col_ _col_val unsafe_leaves lines = device_capa = metadata device_capa device_capa_str = f device_capa device_capa opt_name = metadata name lines append codegen_boilerplate heuristic_name opt_name threshold metadata shared_memory device_capa_str tree classes_ fn_def = f \n gen_predict_fn_def lines append fn_def tree codegen dummy_col_ _col_val lines unsafe_leaves write_heuristic_to_file lines heuristic_name dataclass AccuracyMetrics Number correct predictions num_correct int Number wrong predictions num_wrong int Number predictions where model unsure num_unsure int Total number predictions total int to_map correct num_correct wrong num_wrong unsure num_unsure total total dataclass WrongSpeedupMetrics If model predicted wrong choice maximum speedup best choice over predicted choice max_speedup float For all wrong predictions geometric mean speedups best choices over predicted choices gmean_speedup float to_map wrong_max_speedup max_speedup wrong_gmean_speedup gmean_speedup dataclass RankingMetrics Number predictions where best choice top k choices num_correct int Number predictions where best choice top k choices num_wrong int Maximum speedup best choice over best choice top k tells us how much better best choice which top k over best choice top k max_speedup float Geometric mean speedups best choice over best choice top k gmean_speedup float Number predictions where model unsure unsure int to_map top_k_correct num_correct top_k_wrong num_wrong wrong_max_speedup_k max_speedup wrong_gmean_speedup_k gmean_speedup top_k_unsure unsure dataclass DefaultComparisonMetrics Maximum speedup predicted choice over default choice max_speedup float Geometric mean speedups predicted choices over default choices gmean_speedup float Maximum speedup default choice over predicted choice max_slowdown float Number predictions where predicted choice default choice non_default_predictions int Number predictions where default choice better than predicted choice default_better bool to_map max_speedup_over_default max_speedup gmean_speedup_over_default gmean_speedup max_speedup_default_over_heuristic max_slowdown non_default_predictions non_default_predictions default_better default_better dataclass EvalResults accuracy AccuracyMetrics speedup WrongSpeedupMetrics ranking RankingMetrics default_comparison DefaultComparisonMetrics to_map accuracy to_map speedup to_map ranking to_map default_comparison to_map DecisionEvaluator __init__ train model predictions df probas wrong_pct= threshold= k= unsafe_leaves=None leaf_ids=None ranking=False - None train = train model = model predictions = predictions df = df probas = probas wrong_pct = wrong_pct threshold = threshold k = k unsafe_leaves = unsafe_leaves leaf_ids = leaf_ids ranking = ranking num_correct = num_wrong = num_unsure = wrong_probas = speedups_wrong = num_correct_top_k = num_wrong_top_k = wrong_speedups_top_k = top_k_unsure = num_non_default_predictions = speedups_over_default = num_default_better = compute_speedup_over_default default_config pred i predicted_time default_config None pred = default_config num_non_default_predictions += default_time = get_time df iloc i default_config TODO We should keep track how often happens default_time None math isinf default_time speedup_over_default = default_time predicted_time speedup_over_default num_default_better += speedups_over_default append speedup_over_default log debug cannot compute speedup over default because default_time= d default_time get_time row choice choices_feedback = json loads row choice time choices_feedback get choice None top_k_classes model probas k avail_choices Get classes their corresponding probabilities classes = model classes_ Sort probability descending filter out zero probabilities sorted_classes = c c p sorted zip classes probas key=lambda x x reverse=True p c avail_choices Return top k choices top_k_choices = sorted_classes k top_k_choices += train ranking_always_included_choices top_k_choices = list dict fromkeys top_k_choices top_k_choices eval_prediction avail_choices leaf_id pred true prob threshold default_config i predicted_time = get_time df iloc i pred max_prob = max prob leaf_id unsafe_leaves pred avail_choices max_prob = max_prob = threshold num_unsure += speedups_over_default append pred == true compute_speedup_over_default default_config pred i predicted_time num_correct += compute_speedup_over_default default_config pred i predicted_time num_wrong += wrong_probas append max_prob best_time = get_time df iloc i true wrong_speedup = predicted_time best_time speedups_wrong append wrong_speedup eval_ranking_prediction true top_k_choices i true top_k_choices num_correct_top_k += top_k_choices_times = choice top_k_choices time = get_time df iloc i choice time None top_k_choices_times append time best_time = get_time df iloc i true min_time = min top_k_choices_times default=None min_time None speedup = min_time best_time wrong_speedups_top_k append speedup num_wrong_top_k += top_k_unsure += TODO AlnisM print more info input choices log debug All top k choices have no time which means all top k unavailable get_safe_proba get_results return_safe_proba=True compute_safe_proba num_predictions wrong_probas wrong_pct wrong_probas sort num_wrong = len wrong_probas allowed_wrong = int num_predictions wrong_pct allowed_wrong = num_wrong too_many_wrong = num_wrong - allowed_wrong idx = min too_many_wrong len wrong_probas - wrong_probas idx get_results return_safe_proba=False - EvalResults Custom evaluation function evaluates learned decision tree y_true = df actual_winner ranking df winner i pred true prob leaf_id enumerate zip predictions y_true probas leaf_ids avail_choices = df avail_choices iloc i top_k_choices = top_k_classes model prob k=self k avail_choices=avail_choices assert true avail_choices f Best choice true available choices avail_choices default_config = train get_default_config df iloc i eval_prediction avail_choices leaf_id pred true prob threshold default_config i eval_ranking_prediction true top_k_choices i total = len predictions return_safe_proba compute_safe_proba total wrong_probas wrong_pct safe_gmean x gmean x x max_speedup = max speedups_wrong default= gmean_speedup = safe_gmean speedups_wrong max_speedup_top_k = max wrong_speedups_top_k default= gmean_speedup_top_k = safe_gmean wrong_speedups_top_k max_speedup_over_default = max speedups_over_default default= gmean_speedup_over_default = safe_gmean speedups_over_default max_slowdown_over_default = min speedups_over_default default= accuracyMetrics = AccuracyMetrics num_correct num_wrong num_unsure total wrongSpeedupMetrics = WrongSpeedupMetrics max_speedup gmean_speedup rankingMetrics = RankingMetrics num_correct_top_k num_wrong_top_k max_speedup_top_k gmean_speedup_top_k top_k_unsure defaultComparisonMetrics = DefaultComparisonMetrics max_speedup_over_default gmean_speedup_over_default max_slowdown_over_default num_non_default_predictions num_default_better EvalResults accuracyMetrics wrongSpeedupMetrics rankingMetrics defaultComparisonMetrics __name__ == __main__ train = AHTrainDecisionTree train generate_heuristic