mypy allow-untyped-defs r Implementation Stochastic Weight Averaging implementation itertools math warnings collections abc Callable Iterable copy deepcopy typing Any cast Literal Optional Union typing_extensions override torch torch Tensor torch nn Module torch optim lr_scheduler _format_param LRScheduler torch utils _foreach_utils _get_foreach_kernels_supported_devices optimizer Optimizer __all__ = AveragedModel update_bn SWALR get_ema_multi_avg_fn get_swa_multi_avg_fn get_ema_avg_fn get_swa_avg_fn torch utils _foreach_utils _group_tensors_by_device_and_dtype PARAM_LIST = Union tuple Tensor list Tensor get_ema_multi_avg_fn decay= Get function applying exponential moving average EMA across multiple params decay decay raise ValueError f Invalid decay value decay provided Please provide value range torch no_grad ema_update ema_param_list PARAM_LIST current_param_list PARAM_LIST _ foreach lerp only handles float complex torch is_floating_point ema_param_list torch is_complex ema_param_list torch _foreach_lerp_ ema_param_list current_param_list - decay p_ema p_model zip ema_param_list current_param_list strict=True p_ema copy_ p_ema decay + p_model - decay ema_update get_swa_multi_avg_fn Get function applying stochastic weight average SWA across multiple params torch no_grad swa_update averaged_param_list PARAM_LIST current_param_list PARAM_LIST num_averaged Union Tensor int foreach lerp only handles float complex torch is_floating_point averaged_param_list torch is_complex averaged_param_list torch _foreach_lerp_ averaged_param_list current_param_list cast float num_averaged + diffs = torch _foreach_sub current_param_list averaged_param_list isinstance num_averaged Tensor torch _foreach_addcdiv_ averaged_param_list diffs num_averaged + len averaged_param_list torch _foreach_add_ averaged_param_list diffs alpha= num_averaged + swa_update get_ema_avg_fn decay= Get function applying exponential moving average EMA across single param decay decay raise ValueError f Invalid decay value decay provided Please provide value range torch no_grad ema_update ema_param Tensor current_param Tensor num_averaged decay ema_param + - decay current_param ema_update get_swa_avg_fn Get function applying stochastic weight average SWA across single param torch no_grad swa_update averaged_param Tensor current_param Tensor num_averaged Union Tensor int averaged_param + current_param - averaged_param num_averaged + swa_update AveragedModel Module r Implements averaged model Stochastic Weight Averaging SWA Exponential Moving Average EMA Stochastic Weight Averaging proposed ` Averaging Weights Leads Wider Optima Better Generalization ` _ Pavel Izmailov Dmitrii Podoprikhin Timur Garipov Dmitry Vetrov Andrew Gordon Wilson UAI Exponential Moving Average variation ` Polyak averaging ` _ using exponential weights instead equal weights across iterations AveragedModel creates copy provided module attr ` model ` device attr ` device ` allows compute running averages parameters attr ` model ` Args model torch nn Module model use SWA EMA device torch device optional provided averaged model will stored attr ` device ` avg_fn function optional averaging function used update parameters function must take current value ` AveragedModel ` parameter current value attr ` model ` parameter number models already averaged None equally weighted average used default None multi_avg_fn function optional averaging function used update parameters inplace function must take current values ` AveragedModel ` parameters list current values attr ` model ` parameters list number models already averaged None equally weighted average used default None use_buffers bool ` ` True ` ` will compute running averages both parameters buffers model default ` ` False ` ` Example xdoctest +SKIP undefined variables loader optimizer model loss_fn = swa_model = torch optim swa_utils AveragedModel model scheduler = torch optim lr_scheduler CosineAnnealingLR optimizer T_max= swa_start = swa_scheduler = SWALR optimizer swa_lr= i range input target loader optimizer zero_grad loss_fn model input target backward optimizer step i swa_start swa_model update_parameters model swa_scheduler step scheduler step Update bn statistics swa_model end torch optim swa_utils update_bn loader swa_model You can also use custom averaging functions ` avg_fn ` ` multi_avg_fn ` parameters If no averaging function provided default compute equally-weighted average weights SWA Example xdoctest +SKIP undefined variables Compute exponential moving averages weights buffers ema_model = torch optim swa_utils AveragedModel model torch optim swa_utils get_ema_multi_avg_fn use_buffers=True note When using SWA EMA models containing Batch Normalization you may need update activation statistics Batch Normalization This can done either using meth ` torch optim swa_utils update_bn ` setting attr ` use_buffers ` ` True ` The first approach updates statistics post-training step passing data through model The second does during parameter update phase averaging all buffers Empirical evidence has shown updating statistics normalization layers increases accuracy you may wish empirically test which approach yields best results your problem note attr ` avg_fn ` ` multi_avg_fn ` saved meth ` state_dict ` model note When meth ` update_parameters ` called first time i e attr ` n_averaged ` ` ` parameters ` model ` copied parameters ` AveragedModel ` For every subsequent call meth ` update_parameters ` function ` avg_fn ` used update parameters _Averaging Weights Leads Wider Optima Better Generalization https arxiv org abs _There Are Many Consistent Explanations Unlabeled Data Why You Should Average https arxiv org abs _SWALP Stochastic Weight Averaging Low-Precision Training https arxiv org abs _Stochastic Weight Averaging Parallel Large-Batch Training That Generalizes Well https arxiv org abs _Polyak averaging https paperswithcode com method polyak-averaging n_averaged Tensor __init__ model Module device Optional Union int torch device = None avg_fn Optional Callable Tensor Tensor Union Tensor int Tensor = None multi_avg_fn Optional Callable PARAM_LIST PARAM_LIST Union Tensor int None = None use_buffers=False noqa D super __init__ avg_fn None multi_avg_fn None raise AssertionError Only one avg_fn multi_avg_fn should provided module = deepcopy model device None module = module device register_buffer n_averaged torch tensor dtype=torch long device=device avg_fn = avg_fn multi_avg_fn = multi_avg_fn use_buffers = use_buffers forward args kwargs Forward pass module args kwargs update_parameters model Module Update model parameters self_param = pyrefly ignore bad-argument-type itertools chain module parameters module buffers use_buffers parameters model_param = pyrefly ignore bad-argument-type itertools chain model parameters model buffers use_buffers model parameters self_param_detached list Optional Tensor = model_param_detached list Optional Tensor = copy_param = bool n_averaged == p_averaged p_model zip self_param model_param strict=False p_model_ = p_model detach p_averaged device self_param_detached append p_averaged detach model_param_detached append p_model_ copy_param p_averaged detach copy_ p_model_ n_averaged multi_avg_fn None avg_fn None grouped_tensors = _group_tensors_by_device_and_dtype self_param_detached model_param_detached device _ self_params model_params _ grouped_tensors items multi_avg_fn multi_avg_fn self_params type ignore arg-type model_params type ignore arg-type n_averaged device device None device type _get_foreach_kernels_supported_devices multi_avg_fn = get_swa_multi_avg_fn multi_avg_fn self_params model_params n_averaged device avg_fn = get_swa_avg_fn n_averaged = n_averaged device p_averaged p_model zip type ignore assignment self_params model_params strict=True pyrefly ignore missing-attribute p_averaged copy_ avg_fn p_averaged p_model n_averaged p_averaged p_model zip type ignore assignment self_param_detached model_param_detached strict=True pyrefly ignore missing-attribute n_averaged = n_averaged p_averaged device pyrefly ignore missing-attribute p_averaged detach copy_ pyrefly ignore missing-attribute bad-argument-type avg_fn p_averaged detach p_model n_averaged use_buffers If apply running averages buffers keep buffers sync source model b_swa b_model zip module buffers model buffers strict=True b_swa detach copy_ b_model detach b_swa device n_averaged += torch no_grad update_bn loader Iterable Any model Module device Optional Union int torch device = None r Update BatchNorm running_mean running_var buffers model It performs one pass over data ` loader ` estimate activation statistics BatchNorm layers model Args loader torch utils data DataLoader dataset loader compute activation statistics Each data batch should either tensor list tuple whose first element tensor containing data model torch nn Module model which we seek update BatchNorm statistics device torch device optional If set data will transferred attr ` device ` before being passed into attr ` model ` Example xdoctest +SKIP Undefined variables loader model = torch optim swa_utils update_bn loader model note The ` update_bn ` utility assumes each data batch attr ` loader ` either tensor list tuple tensors latter case assumed meth ` model forward ` should called first element list tuple corresponding data batch momenta = module model modules isinstance module torch nn modules batchnorm _BatchNorm module reset_running_stats momenta module = module momentum momenta was_training = model training model train module momenta keys module momentum = None input loader isinstance input list tuple input = input device None input = input device model input bn_module momenta keys bn_module momentum = momenta bn_module model train was_training SWALR LRScheduler r Anneals learning rate each parameter group fixed value This learning rate scheduler meant used Stochastic Weight Averaging SWA method see ` torch optim swa_utils AveragedModel ` Args optimizer torch optim Optimizer wrapped optimizer swa_lrs float list learning rate value all param groups together separately each group annealing_epochs int number epochs annealing phase default annealing_strategy str cos linear specifies annealing strategy cos cosine annealing linear linear annealing default cos last_epoch int index last epoch default - The ` SWALR ` scheduler can used together other schedulers switch constant learning rate late training example below Example xdoctest +SKIP Undefined variables loader optimizer model = lr_lambda = lambda epoch scheduler = torch optim lr_scheduler MultiplicativeLR optimizer lr_lambda=lr_lambda swa_scheduler = torch optim swa_utils SWALR optimizer anneal_strategy= linear anneal_epochs= swa_lr= swa_start = i range input target loader optimizer zero_grad loss_fn model input target backward optimizer step i swa_start swa_scheduler step scheduler step _Averaging Weights Leads Wider Optima Better Generalization https arxiv org abs __init__ optimizer Optimizer swa_lr float anneal_epochs= anneal_strategy Literal cos linear = cos last_epoch=- noqa D swa_lrs = _format_param swa_lr optimizer swa_lr swa_lr group zip swa_lrs optimizer param_groups strict=True group swa_lr = swa_lr anneal_strategy cos linear raise ValueError anneal_strategy must one cos linear f instead got anneal_strategy _set_anneal_func anneal_strategy isinstance anneal_epochs int anneal_epochs raise ValueError f anneal_epochs must equal greater than got anneal_epochs anneal_epochs = anneal_epochs super __init__ optimizer last_epoch staticmethod _linear_anneal t t staticmethod _cosine_anneal t - math cos math pi t staticmethod _get_initial_lr lr swa_lr alpha alpha == swa_lr lr - alpha swa_lr - alpha override get_lr r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Uses attr ` anneal_func ` interpolate between each group s ` ` group lr ` ` ` ` group swa_lr ` ` over attr ` anneal_epochs ` epochs Once attr ` anneal_epochs ` reached keeps learning rate fixed ` ` group swa_lr ` ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s ` _get_lr_called_within_step ` only available ` _enable_get_lr_call ` so we ignore type error here See ` LRScheduler step ` more details _get_lr_called_within_step warnings warn To get last learning rate computed scheduler please use ` get_last_lr ` UserWarning stacklevel= Set ` LRScheduler _initial_step ` step = _step_count - anneal_epochs == step = max step pyrefly ignore no-matching-overload prev_t = max min step - max anneal_epochs prev_alpha = anneal_func prev_t prev_lrs = _get_initial_lr group lr group swa_lr prev_alpha group optimizer param_groups pyrefly ignore no-matching-overload t = max min step max anneal_epochs alpha = anneal_func t group swa_lr alpha + lr - alpha group lr zip optimizer param_groups prev_lrs strict=True _set_anneal_func anneal_strategy Literal cos linear _anneal_strategy = anneal_strategy anneal_strategy == cos anneal_func = _cosine_anneal anneal_func = _linear_anneal override state_dict - dict str Any Return state scheduler ` dict ` It contains entry every variable __dict__ which optimizer anneal_func key value key value __dict__ items key optimizer anneal_func override load_state_dict state_dict dict str Any - None Load scheduler s state Args state_dict dict scheduler state Should object returned call meth ` state_dict ` __dict__ update state_dict _set_anneal_func _anneal_strategy