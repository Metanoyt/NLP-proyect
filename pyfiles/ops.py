mypy allow-untyped-defs functools math operator typing noqa F typing Optional torch torch nn functional F torch fx operator_schemas normalize_function torch nested _internal sdpa jagged_scaled_dot_product_attention nested_tensor NestedTensor __all__ list Any = JAGGED_OPS_TABLE Dict Any Any = _get_padding_value dtype padding_type dtype is_floating_point torch finfo dtype max padding_type == max torch finfo dtype min dtype == torch int Largest int value exactly representable float IEEE double precision Avoids overflow when padding_value passed double _jagged_to_padded_dense_forward int _safe_max = - int _safe_min = -int _safe_max int _safe_max padding_type == max int _safe_min torch iinfo dtype max padding_type == max torch iinfo dtype min _outer_to_inner_dim ndim dim ragged_dim canonicalize=False torch _prims_common canonicalize_dims isinstance dim tuple list output = type dim _outer_to_inner_dim ndim d ragged_dim d dim ensure no duplicates which can result both batch ragged mapping type output dict fromkeys output canonicalize dim = canonicalize_dims ndim dim assert dim = dim ndim pyrefly ignore unsupported-operation Map dim= AKA batch dim - packed dim i e outer ragged dim - For other dims subtract convert inner space pyrefly ignore unsupported-operation ragged_dim - dim == dim - _wrap_jagged_dim ndim dim ragged_dim op_name convert_to_inner_dim=True allow_ragged_dim=False allow_batch_dim=False torch _prims_common canonicalize_dims wrapped = canonicalize_dims ndim dim wrapped == ragged_dim allow_ragged_dim raise RuntimeError f op_name supported NestedTensor ragged dim wrapped == allow_batch_dim raise RuntimeError f op_name supported NestedTensor dim= ret = _outer_to_inner_dim ndim wrapped ragged_dim convert_to_inner_dim wrapped allow_batch_dim Need disambiguate whether we re operating batch dim Operating dim= - dim= after inner dim conversion operating_on_batch = wrapped == ret operating_on_batch ret _wrap_jagged_dims ndim dims op_name ragged_idx= For NestedTensor operators wraps dimensions non-negative values returns metadata related reduction dimension s torch _prims_common canonicalize_dims assert isinstance dims tuple list f _wrap_jagged_dims cannot iterate over dimensions type type dims wrapped_dims = canonicalize_dims ndim d d dims convert all indices non-negative values operate_on_batch = wrapped_dims operate_on_ragged = ragged_idx wrapped_dims operate_on_non_batch = any d = d = ragged_idx d wrapped_dims ensure no duplicates which can result both batch ragged mapping outer_to_inner_dim = tuple dict fromkeys _outer_to_inner_dim ndim d ragged_idx d wrapped_dims outer_to_inner_dim operate_on_batch operate_on_ragged operate_on_non_batch check_schema schema_str str func args kwargs - None named_arg_types = schema_str split num_optional_args = x endswith x named_arg_types count True min_args = len named_arg_types - num_optional_args special case ellipses allows any number unchecked args end named_arg_types - == named_arg_types = named_arg_types - len args = min_args len args = len named_arg_types raise ValueError f NestedTensor func __name__ schema_str expected least min_args f arguments most len named_arg_types arguments got f len args arguments arg_type_check_fns = t lambda x isinstance x torch Tensor isinstance x NestedTensor jt lambda x isinstance x NestedTensor x _lengths None x _ragged_idx == ops jt require contiguous JT only jt_all lambda x isinstance x NestedTensor ops jt_all can accept all kinds JT any lambda x True i named_arg_type enumerate named_arg_types name arg_type = named_arg_type split is_optional = arg_type endswith normalized_arg_type = arg_type - is_optional arg_type normalized_arg_type arg_type_check_fns keys raise AssertionError f Unknown arg type normalized_arg_type i = len args is_optional raise ValueError f NestedTensor func __name__ schema_str f missing required argument name continue _check_fn = arg_type_check_fns normalized_arg_type check_fn x is_optional=is_optional is_optional x None _check_fn x _check_fn x check_fn args i type_to_desc = t tensor t optional tensor jt contiguous jagged layout NestedTensor jt_all jagged layout NestedTensor any any type raise ValueError f NestedTensor func __name__ schema_str expected name f type_to_desc arg_type check_ragged_dim_same func NestedTensor a_name str b NestedTensor b_name str - None Calling into shape here _size _ragged_idx = b _size b _ragged_idx raise RuntimeError f NestedTensor func __name__ expected a_name b_name have same exact offsets tensor returns True raggedness-relevant portions NT shape match those specified size raggedness_matches nt size end = nt _ragged_idx + nt_ragged = nt _size end size_ragged = size end len nt_ragged == len size_ragged all ns == s s == - ns s zip nt_ragged size_ragged squeeze_leading_ones t Note Squeezing leading ones Squeeze leading ones t We want B j + - B j B j + - B j yet supported Squeeze extra ones grab values NT - sum - B j Do dense broadcasting sum + - sum Construct nested tensor sum - B j If unsqueezing th dim becomes supported we would unsqueeze step we would need update function record how many ones we unsqueezed while t dim t shape == t = t squeeze t register_func tables aten_ops schema_str isinstance aten_ops list aten_ops = aten_ops isinstance tables list tables = tables wrapper func aten_op aten_ops get_inner aten_op inner args kwargs check_schema schema_str func args kwargs func aten_op args kwargs inner table tables table aten_op = get_inner aten_op func wrapper register_jagged_func = functools partial register_func JAGGED_OPS_TABLE lookup_jagged func args kwargs - Optional Callable dispatch_func = JAGGED_OPS_TABLE get func None dispatch_func None dispatch_func Handle pointwise fallbacks torch Tag pointwise func tags torch fx experimental symbolic_shapes is_nested_int No pointwise ops legitimately accept nested int inputs Without check they will incorrectly interpreted tensors See https github com pytorch pytorch issues arg args is_nested_int arg raise RuntimeError f NestedTensor func __name__ invalid argument arg Assume there aren t additional tensors aren t unary binary args num_tensor_args = sum isinstance x torch Tensor x args num_tensor_args == Build up check schema string The first tensor arg assumed NJT other args sent through as-is schema_parts = arg func _schema arguments isinstance arg type torch TensorType schema_parts append f arg name jt_all break schema_parts append f arg name any schema_parts append check_schema_str = join schema_parts check_schema check_schema_str func args kwargs functools partial jagged_unary_pointwise func num_tensor_args == check_schema lhs any rhs any func args kwargs functools partial jagged_binary_pointwise func None extract_kwargs arg kwargs = offsets arg offsets lengths arg lengths _metadata_cache arg _metadata_cache _ragged_idx arg _ragged_idx kwargs jagged_unary_pointwise func args kwargs assume we get here there single NJT input args njt = next arg arg args isinstance arg NestedTensor NestedTensor func arg _values arg njt arg arg args kwargs extract_kwargs njt jagged_binary_pointwise func args kwargs b = args args assert isinstance NestedTensor isinstance b NestedTensor mismatch_error_msg = cannot call binary pointwise function inputs shapes NT b NT isinstance NestedTensor isinstance b NestedTensor ex B j D + B j D ex B j D + B j raggedness_matches b _size NestedTensor func _values b _values args kwargs extract_kwargs raise RuntimeError mismatch_error_msg format func __name__ _size b _size either NT b NT point a_is_nt = isinstance NestedTensor extracted_kwargs = extract_kwargs a_is_nt extract_kwargs b === Handle broadcasting across batch ragged dims === Easy case take advantage pre-existing broadcasting logic ex B j + - B j ex B j + - B j ex B j + - B j nt t = b a_is_nt b See Note Squeezing leading ones t dim nt dim raise NotImplementedError NYI broadcasting NT T larger dim t_squeezed = squeeze_leading_ones t nt dim = t_squeezed dim + lhs rhs = nt _values t_squeezed a_is_nt t_squeezed nt _values NestedTensor func lhs rhs args kwargs extracted_kwargs Harder case do manual broadcasting when NT dim == non-NT dim ex B j D_ D_ + B D_ D_ - B j D_ D_ dim == b dim ex B j D_ D_ + D_ D_ - should B j D_ D_ yet supported shape = b shape raise RuntimeError mismatch_error_msg format func __name__ shape b shape nested_tensor nested_from_padded handle broadcasting via padded dense - jagged conversion min_seqlen = nt _maybe_min_seqlen max_seqlen = nt _maybe_max_seqlen padded_max_S = max_seqlen total_L = nt _values shape nt _ragged_idx - padded_max_S None use upper bound max seqlen s present padded_max_S = total_L convert dense tensor - jagged t = t expand x i = nt _ragged_idx padded_max_S i x enumerate t shape t_as_nt = nested_from_padded t offsets=nt _offsets ragged_idx=nt _ragged_idx sum_S=total_L min_seqlen=min_seqlen max_seqlen=max_seqlen function call two NJTs lhs rhs = nt t_as_nt a_is_nt t_as_nt nt func lhs rhs args kwargs ex B j D_ D_ + A B D_ D_ - error because breaks invariant ragged dim wrt left-most batch dim raise RuntimeError mismatch_error_msg format func __name__ shape b shape jagged_torch_function func args kwargs SDPA has special kernels handle nested tensors Dispatch correct implementation here func torch _C _nn scaled_dot_product_attention jagged_scaled_dot_product_attention args kwargs func __name__ == apply_ func args _values args kwargs args Handle flatten here because s CompositeImplicit func __name__ == flatten _flatten_sig input start_dim= end_dim=- pass _ new_kwargs = normalize_function type ignore misc _flatten_sig args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input NB stay outer dim space because we re going redispatch NT input start_dim = _wrap_jagged_dim inp dim new_kwargs start_dim inp _ragged_idx flatten convert_to_inner_dim=False end_dim = _wrap_jagged_dim inp dim new_kwargs end_dim inp _ragged_idx flatten convert_to_inner_dim=False start_dim == end_dim inp product = functools reduce operator mul inp shape start_dim end_dim + new_shape = inp shape start_dim product inp shape end_dim + inp reshape new_shape Handle NestedTensor share_memory_ func __name__ == share_memory_ nt = args nt is_cuda nt names _ = nt __tensor_flatten__ torch _C DisableTorchFunctionSubclass name names component = getattr nt name None component None component share_memory_ nt Handle NestedTensor is_shared func __name__ == is_shared nt = args nt is_cuda False names _ = nt __tensor_flatten__ names False all getattr nt name None getattr nt name is_shared name names Handle nested-specific input validation CompositeImplicit rms_norm func __name__ == rms_norm _rms_norm_sig input normalized_shape weight=None eps=None pass _ new_kwargs = normalize_function type ignore misc _rms_norm_sig args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input normalized_shape = new_kwargs pop normalized_shape can t normalize over ragged dim yet max_normalizable = inp dim - inp _ragged_idx - len normalized_shape max_normalizable raise ValueError rms_norm Normalization over ragged dim supported nested tensors torch _C DisableTorchFunctionSubclass func args kwargs raise NotImplementedError func register_jagged_func torch ops aten is_non_overlapping_and_dense default torch ops aten sym_size default torch ops aten dim default torch ops aten numel default torch ops aten sym_numel default torch ops aten sym_stride default torch ops aten sym_storage_offset default jt_all tensor_attr_supported_getter func args kwargs func torch ops aten is_non_overlapping_and_dense default False func torch ops aten sym_size default args _size func torch ops aten dim default len args _size func torch ops aten sym_numel default torch ops aten numel default args _lengths None int sum args _lengths math prod args _size args _values numel func torch ops aten sym_stride default args _strides func torch ops aten sym_storage_offset default args _values storage_offset register_jagged_func torch ops prim layout default jt_all prim_layout_default func args kwargs torch jagged register_jagged_func torch ops aten size default jt_all tensor_attr_unsupported_getter func args kwargs func torch ops aten size default raise RuntimeError NestedTensor does support directly calling torch ops aten size please use ` nested_tensor size ` instead register_jagged_func torch ops aten is_contiguous default jt_all is_contiguous_general func args kwargs torch _prims_common is_contiguous_for_memory_format _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input If created narrow check lengths inp lengths None False new_kwargs memory_format = new_kwargs get memory_format torch contiguous_format new_kwargs memory_format == torch preserve_format True is_contiguous_for_memory_format inp _values new_kwargs register_jagged_func torch ops aten is_contiguous memory_format jt_all memory_format any is_contiguous_general register_jagged_func torch ops aten sym_is_contiguous default jt_all memory_format any sym_is_contiguous_general func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input If created narrow check lengths inp lengths None False new_kwargs memory_format = new_kwargs get memory_format torch contiguous_format new_kwargs memory_format == torch preserve_format True torch ops aten sym_is_contiguous default inp _values new_kwargs register_jagged_func torch ops aten clone default input jt_all memory_format any clone_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input new_meta = extract_kwargs inp inp _lengths None new_kwargs memory_format == torch contiguous_format need copy remove holes non-contiguity lengths metadata TODO write kernel nested_tensor jagged_from_list TODO We probably want output have same ragged structure nested int assert inp _ragged_idx == NJT ragged_idx = supported contiguous clone contig _ = jagged_from_list inp unbind offsets=None contig NestedTensor func inp _values new_kwargs new_meta register_jagged_func torch ops aten linear default input jt weight t bias t linear_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input NestedTensor func inp _values new_kwargs extract_kwargs inp register_jagged_func torch ops aten linear_backward default jt grad_output jt weight t output_mask any linear_backward_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input grad_output = new_kwargs pop grad_output weight = new_kwargs pop weight output_mask = new_kwargs pop output_mask ds dw db = None None None check_ragged_dim_same func inp grad_output grad_output output_mask ds = NestedTensor torch matmul grad_output _values weight extract_kwargs grad_output output_mask NB Fold dims values input grad_output treat them D This trick avoids materializing large intermediates immediately reducing over them via sum This equivalent computing torch matmul grad_output _values transpose - - inp _values then summing over leading dimensions get D weight grad grad_ d = grad_output _values reshape - weight size input_ d = inp _values reshape - weight size dw = torch matmul grad_ d t input_ d output_mask Sum over all last dim get D bias grad We cannot rely autograd engine reduce us because returning tensor aliasing input would violate aten signature annotation reduce_dims = tuple range grad_output _values ndim - reduce_dims == db = grad_output _values clone db = torch sum grad_output _values reduce_dims keepdim=False ds dw db register_jagged_func torch ops aten dtype input jt_all dtype any to_dtype func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input NestedTensor func inp _values new_kwargs extract_kwargs inp register_jagged_func torch ops aten _to_copy default jt_all to_copy_default func args kwargs nested_tensor _tensor_symint_registry _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input don t change layout new_kwargs pop layout new_values = func inp _values new_kwargs new_offsets = inp _offsets device=new_values device new_lengths = None inp _lengths None new_lengths = inp _lengths device=new_values device torch _subclasses fake_tensor FakeTensor torch _subclasses functional_tensor FunctionalTensor mb_unwrap_functional_tensor ragged_source = inp _offsets inp _lengths None inp _lengths new_thing = new_offsets new_lengths None new_lengths isinstance new_thing FakeTensor FunctionalTensor Temporary hack until we have union find tgt = mb_unwrap_functional_tensor new_thing src = mb_unwrap_functional_tensor ragged_source tgt nested_int_memo = src nested_int_memo _tensor_symint_registry new_thing = _tensor_symint_registry ragged_source inp_kwargs = extract_kwargs inp inp_kwargs offsets = new_offsets inp_kwargs lengths = new_lengths output = NestedTensor new_values inp_kwargs output register_jagged_func torch ops aten copy_ default jt_all src jt_all non_blocking any copy_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input src = new_kwargs pop src inp _size = src _size try recursively copy_ unbound components get around nested int mismatch TODO eventually do direct copy when possible inp_comps = inp unbind inp_comp_shapes = c shape c inp_comps src_comps = src unbind src_comp_shapes = c shape c src_comps inp_comp_shapes = src_comp_shapes raise RuntimeError copy_ expected compatible input src shapes got f inp shape src shape inp_comp src_comp zip inp_comps src_comps inp_comp copy_ src_comp AOTD allows mutations inputs only views inputs NJT values returns _values detach workaround some issues To keep mutation graph AOTD manually calls copy_ input NJT Here we directly mutate _values emit detach graph which would make non-compilable inp _values copy_ src _values inp register_jagged_func torch ops aten detach default jt_all jagged_unary_pointwise register_jagged_func torch ops aten empty_like default torch ops aten ones_like default torch ops aten zeros_like default torch ops aten rand_like default torch ops aten randn_like default jt_all like_factory_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input Default layout technically torch strided only jagged supported here Rather than force users specify layout assume jagged This should set strided redispatching values new_kwargs layout = torch strided new_values = func inp _values new_kwargs new_offsets = inp _offsets device=new_values device new_lengths = None inp _lengths None new_lengths = inp _lengths device=new_values device output_kwargs = extract_kwargs inp offsets output_kwargs output_kwargs offsets = new_offsets lengths output_kwargs output_kwargs lengths = new_lengths inp device = new_values device Update nested int registry indicate ragged structure same between two offsets lengths different devices torch _subclasses fake_tensor FakeTensor torch _subclasses functional_tensor FunctionalTensor mb_unwrap_functional_tensor nested_tensor _tensor_symint_registry ragged_source = inp _offsets inp _lengths None inp _lengths new_thing = new_offsets new_lengths None new_lengths isinstance new_thing FakeTensor FunctionalTensor Temporary hack until we have union find tgt = mb_unwrap_functional_tensor new_thing src = mb_unwrap_functional_tensor ragged_source tgt nested_int_memo = src nested_int_memo _tensor_symint_registry new_thing = _tensor_symint_registry ragged_source NestedTensor new_values output_kwargs register_jagged_func torch ops aten full_like default jt_all fill_value any like_factory_default register_jagged_func torch ops aten randint_like default jt_all high any like_factory_default register_jagged_func torch ops aten randint_like low_dtype jt_all low any high any like_factory_default register_jagged_func torch ops aten zero_ default jt_all zero__default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input func inp _values inp register_jagged_func torch ops aten _softmax default jt_all dim any half_to_float any _softmax_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True isinstance new_kwargs dim tuple raise RuntimeError softmax supported dimensions type tuple NestedTensor inp = new_kwargs pop input new_kwargs dim reduce_on_batch reduce_on_ragged _reduce_on_non_batch = _wrap_jagged_dims inp dim new_kwargs dim softmax inp _ragged_idx reduce_on_batch raise RuntimeError softmax supported when reducing across batch dimension NestedTensor reduce_on_ragged inp _ragged_idx raise RuntimeError softmax supported when reducing along ragged dimension ragged_idx NestedTensor reduce_on_ragged inp _lengths None raise RuntimeError softmax supported where lengths None + reducing across ragged dimension NestedTensor new_kwargs dim = new_kwargs dim torch softmax takes reduction dimension integer reduce_on_ragged padded_softmax_values = torch nn functional softmax torch ops aten _jagged_to_padded_dense_forward inp _values reshape inp _values shape - values required D tensors j pd inp _offsets max_lengths= inp _max_seqlen max length ragged dimension padding_value=float -inf e^-inf = dim=inp _ragged_idx softmax_values = torch ops aten _padded_dense_to_jagged_forward padded_softmax_values inp _offsets total_L=inp _values shape providing parameter helps avoid GPU CPU sync reshape - inp _values shape expand softmax_values back original shape inp _values shape NestedTensor softmax_values extract_kwargs inp NestedTensor func inp _values new_kwargs extract_kwargs inp register_jagged_func torch ops aten _log_softmax default jt_all dim any half_to_float any _log_softmax_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True isinstance new_kwargs dim tuple raise RuntimeError log_softmax supported dimensions type tuple NestedTensor inp = new_kwargs pop input new_kwargs dim reduce_on_batch reduce_on_ragged _reduce_on_non_batch = _wrap_jagged_dims inp dim new_kwargs dim log_softmax inp _ragged_idx reduce_on_batch raise RuntimeError log_softmax supported when reducing across batch dimension NestedTensor reduce_on_ragged raise RuntimeError log_softmax supported when reducing along ragged dimension NestedTensor torch log_softmax takes reduction dimension integer new_kwargs dim = new_kwargs dim NestedTensor func inp _values new_kwargs extract_kwargs inp register_jagged_func torch ops aten _softmax_backward_data default grad_output jt output jt dim any input_dtype any _softmax_backward func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True grad_out = new_kwargs pop grad_output output = new_kwargs pop output NestedTensor func grad_out _values output _values new_kwargs extract_kwargs grad_out register_jagged_func torch ops aten native_dropout default jt float any train any native_dropout_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input out out = func inp _values new_kwargs NestedTensor out extract_kwargs inp NestedTensor out extract_kwargs inp register_jagged_func torch ops aten native_dropout_backward default grad_output jt mask jt scale any native_dropout_backward_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True grad_output = new_kwargs pop grad_output mask = new_kwargs pop mask NestedTensor func grad_output _values mask _values new_kwargs extract_kwargs grad_output register_jagged_func torch ops aten prod dim_int jt_all dim any keepdim any dtype any prod_dim_int func args kwargs _apply_reduction func prod args kwargs register_jagged_func torch ops aten prod default jt_all dtype any prod_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input func inp _values new_kwargs register_jagged_func torch ops aten split Tensor jt split_size any dim any split_tensor func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input new_kwargs dim = _wrap_jagged_dim inp dim new_kwargs dim inp _ragged_idx split tuple NestedTensor values=x extract_kwargs inp x func inp _values new_kwargs register_jagged_func torch ops aten split_with_sizes default jt split_sizes any dim any split_with_sizes_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input new_kwargs dim = _wrap_jagged_dim inp dim new_kwargs dim inp _ragged_idx split_with_sizes NestedTensor values=x extract_kwargs inp x func inp _values new_kwargs register_jagged_func torch ops aten narrow default jt dim any start any length any narrow func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input dim = _wrap_jagged_dim inp dim new_kwargs dim inp _ragged_idx narrow values = func inp _values dim=dim start=new_kwargs start length=new_kwargs length NestedTensor values extract_kwargs inp register_jagged_func torch ops aten chunk default jt chunks any dim any chunk_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input new_kwargs dim operating_on_batch = _wrap_jagged_dim inp dim new_kwargs dim inp _ragged_idx chunk allow_batch_dim=True operating_on_batch chunks = new_kwargs chunks get _offsets chunks lengths = inp _offsets diff chunked_lengths = lengths chunk chunks chunked_offsets = torch cumsum x dim= x chunked_lengths chunked_offsets = F pad x value= x chunked_offsets type ignore arg-type nested_kwargs = offsets per_offsets _ragged_idx inp _ragged_idx per_offsets chunked_offsets get _values chunks split_sizes = x sum item x chunked_lengths chunk_values = inp _values split split_sizes Note actual number chunks returned necessarily same input number can counter-intuitive matches dense behavior NestedTensor values=chunk_values i nested_kwargs i i range len chunk_values NestedTensor values=x extract_kwargs inp x func inp _values new_kwargs register_jagged_func torch ops aten unbind int jt_all dim any unbind_int func args kwargs Note specializes length offsets _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True dim = new_kwargs dim dim = raise RuntimeError unbind only supported NestedTensor dim= inp = new_kwargs pop input values = inp values offsets = inp offsets lengths = inp lengths ragged_idx = inp _ragged_idx _torch_check _lengths list int _offsets Optional list int = None This torch _check needed torch compile symbolic shapes processing offsets lengths symbolic variables during compilation we guarantee correct offsets lengths correspondence sum lengths = total ragged_dim_size every length offset size-like variable allows sym shapes reason inf offset i + length i = ragged_dim_size unbind split dim correctness offsets i = ragged_dim_size lengths_sum = ragged_dim_size = values shape ragged_idx - i range len _lengths torch _check _lengths i = torch _check _lengths i = ragged_dim_size lengths_sum += _lengths i _offsets None torch _check _offsets i + _lengths i = ragged_dim_size lambda unbind nested tensor offsets lengths do match ragged_idx dimension torch _check lengths_sum = ragged_dim_size _offsets None i range len _offsets torch _check _offsets i = torch _check _offsets i = ragged_dim_size lengths None lengths_scalars = offsets diff tolist _torch_check lengths_scalars torch split values lengths_scalars dim= ragged_idx - ragged_idx = raise RuntimeError unbind nested tensor ragged_idx out bounds should = lengths_scalars = lengths tolist offsets_scalars = offsets tolist _torch_check lengths_scalars offsets_scalars torch narrow values dim= ragged_idx - start=offsets_scalars i length=lengths_scalars i i range lengths shape register_jagged_func torch ops aten squeeze dim jt dim any squeeze_dim func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input values = inp _values new_kwargs dim = _wrap_jagged_dim len inp _size new_kwargs dim inp _ragged_idx squeeze NestedTensor func values new_kwargs extract_kwargs inp register_jagged_func torch ops aten unsqueeze default jt_all dim any unsqueeze_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input values = inp _values Account collapsed jagged dim dim = new_kwargs dim new_kwargs dim = _wrap_jagged_dim len inp _size + dim inp _ragged_idx unsqueeze allow_ragged_dim=True ragged_idx changes dimension added before output_kwargs = extract_kwargs inp new_kwargs dim = inp _ragged_idx - output_kwargs _ragged_idx += NestedTensor func values new_kwargs output_kwargs register_jagged_func torch ops aten cat default tensors any dim any cat_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True tensors = new_kwargs pop tensors Convert any non-nested nested nested = t t tensors t is_nested assert len nested first = nested tensors = t t is_nested t expand_as first t tensors Account collapsed jagged dim dim = new_kwargs dim new_kwargs dim = _wrap_jagged_dim len first shape dim first _ragged_idx cat NestedTensor func t _values t tensors new_kwargs extract_kwargs tensors register_jagged_func torch ops aten matmul default any other any matmul_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input other = new_kwargs pop other _unbind_impl b func a_comp b_comp a_comp b_comp zip unbind b unbind _padded_impl b is_nested nt = nt = b nested_tensor nested_from_padded min_seqlen = nt _maybe_min_seqlen max_seqlen = nt _maybe_max_seqlen padded_max_S = max_seqlen total_L = nt _values shape nt _ragged_idx - padded_max_S None use upper bound max seqlen s present padded_max_S = total_L padded_shape = nt shape nt _ragged_idx padded_max_S nt shape nt _ragged_idx + padded_nt = nt to_padded_tensor output_size=padded_shape is_nested padded_t = func padded_nt b padded_t = func padded_nt nested_from_padded padded_t offsets=nt _offsets ragged_idx=nt _ragged_idx sum_S=total_L min_seqlen=min_seqlen max_seqlen=max_seqlen TODO Back these proper kernels e g grouped GEMM NJT x dense inp is_nested other is_nested B j D x B D E = B j E inp dim = inp dim == other dim inp _ragged_idx inp dim - convert padded _padded_impl inp other Support broadcasting dense B j D x D E = B j E B j D E x E F = B j D F etc other dim == inp dim other dim inp _ragged_idx inp dim - NestedTensor func inp _values other new_kwargs extract_kwargs inp Dense x NJT inp is_nested other is_nested B D E x B E j = B E j other dim = other dim == inp dim other _ragged_idx = convert padded _padded_impl inp other Support broadcasting dense D E x B E j = B D j D E x B E j F = B D j F etc inp dim == other dim inp dim other _ragged_idx = NestedTensor func inp other _values new_kwargs extract_kwargs other NJT x NJT inp is_nested other is_nested Support ragged batch dim B j D E x B j E F = B j D F etc inp dim other dim raggedness_matches inp other _size NestedTensor func inp _values other _values extract_kwargs inp Support reducing over ragged dense output B D j x B j E = B D E inp dim == other dim == inp _ragged_idx == other _ragged_idx == inp size inp _ragged_idx == other size other _ragged_idx do unbind can t use padded conversion due j last dim torch stack _unbind_impl inp other raise RuntimeError f matmul supported between inputs shapes inp _size other shape register_jagged_func torch ops aten bmm default jt_all mat any bmm_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input other = new_kwargs pop mat inp dim = raise ValueError bmm input must D other dim = raise ValueError bmm mat must D matmul_default torch ops aten matmul default inp other register_jagged_func torch ops aten expand default jt_all size any implicit any expand_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input size = new_kwargs size assert implicit new_kwargs new_kwargs pop implicit raggedness_matches inp size raise RuntimeError f expand cannot expand shape inp _size - size expand_arg = - d == inp _ragged_idx size d d range inp dim NestedTensor func inp _values expand_arg extract_kwargs inp register_jagged_func torch ops aten expand_as default t other jt expand_as_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input other = new_kwargs pop other NestedTensor func inp other _values extract_kwargs other register_jagged_func torch ops aten broadcast_to default jt_all size any broadcast_to func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input size = new_kwargs pop size len size = inp dim inp expand _ range inp dim - len size size raise ValueError broadcast_to broadcasting higher-dim shape currently supported nested tensors jagged layout register_jagged_func torch ops aten broadcast_tensors default tensors any broadcast_tensors func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True tensors = new_kwargs pop tensors len tensors == raise ValueError broadcast_tensors expected least one tensor input len tensors == tensors outs = broadcast_shape = torch broadcast_shapes t shape t tensors Pull out first NJT If broadcast_shapes worked nested ints compatible njt = next t t tensors isinstance t NestedTensor t tensors t is_nested outs append t broadcast_to broadcast_shape t dim len broadcast_shape outs append NestedTensor t broadcast_to njt _values shape extract_kwargs njt raise ValueError broadcast_tensors broadcasting nested tensors dense tensors equal higher dim currently supported tuple outs register_jagged_func torch ops aten where condition jt_all any other any where_self func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True condition = new_kwargs pop condition inp = new_kwargs pop input other = new_kwargs pop other tensors aren t compatible broadcast_tensors will let us know condition inp other = torch broadcast_tensors condition inp other NestedTensor func condition _values inp _values other _values new_kwargs extract_kwargs condition register_jagged_func torch ops aten _pin_memory default jt device any _pin_memory_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input NestedTensor func inp _values new_kwargs extract_kwargs inp register_jagged_func torch ops aten is_pinned default jt device any is_pinned_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input func inp _values new_kwargs register_jagged_func torch ops aten is_same_size default jt_all other jt_all is_same_size_default func args kwargs args _size == args _size _apply_reduction func func_name identity_element args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input some ops use dim=None indicate full reduction some use empty dim list full_reduction = new_kwargs dim None isinstance new_kwargs dim tuple list len new_kwargs dim == full_reduction out = func inp _values new_kwargs new_kwargs get keepdim False isinstance out tuple list some ops multiple things unsqueeze all them out = type out o unsqueeze inp _ragged_idx o out out = out unsqueeze inp _ragged_idx out some ops support lists dims some don t dim_to_convert = new_kwargs dim is_dimlist = isinstance new_kwargs dim tuple list is_dimlist dim_to_convert = dim_to_convert converted_dim reduce_on_batch reduce_on_ragged reduce_on_non_batch = _wrap_jagged_dims inp dim dim_to_convert f func_name inp _ragged_idx is_dimlist convert back list converted_dim = converted_dim new_kwargs dim = converted_dim reduce_on_ragged inp _lengths None raise RuntimeError f func_name reducing across ragged dimension supported non-contiguous nested tensors holes torch utils _pytree tree_map raggedness reduced away -- dense tensor reduce_on_ragged reduction cases batch ragged batch ragged non-batch etc reduce_on_batch no need read offsets -- apply sum directly values out = func inp _values new_kwargs new_kwargs get keepdim False some ops multiple things unsqueeze all them out = tree_map lambda o o unsqueeze out out invalid reduction cases ragged non-batch etc reduce_on_non_batch raise RuntimeError f func_name reducing along ragged non-batch dimension supported nested tensors reduction cases ragged convert padded dense reduce new_kwargs pop dim dim_to_pass = inp _ragged_idx is_dimlist inp _ragged_idx func inp to_padded_tensor identity_element dim=dim_to_pass new_kwargs raggedness preserved -- nested tensor invalid reduction cases batch batch non-batch etc reduce_on_batch raise RuntimeError f func_name reducing along batch dimension ragged dimension supported nested tensors reduction cases non-batch non-batch non-batch etc apply sum directly values out = func inp _values new_kwargs out_kwargs = extract_kwargs inp new_kwargs get keepdim False dims reduced away - ragged_idx output needs reevaluated dimlist = new_kwargs dim isinstance new_kwargs dim tuple list new_kwargs dim d dimlist adjust all dims reduced before ragged dim d inp _ragged_idx - out_kwargs _ragged_idx -= some ops multiple things wrap each them NJT tree_map lambda o NestedTensor o out_kwargs out register_jagged_func torch ops aten sum default jt_all dtype any sum_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input func inp _values new_kwargs register_jagged_func torch ops aten sum dim_IntList jt_all dim any keepdim any dtype any sum_dim_IntList func args kwargs _apply_reduction func sum args kwargs register_jagged_func torch ops aten transpose int jt_all dim any dim any transpose_int func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True torch _prims_common canonicalize_dims inp = new_kwargs pop input dim dim = canonicalize_dims inp dim new_kwargs dim new_kwargs dim To support SDPA API inputs need have ragged idx transposed dim instead although internal Flash mem-effn implementations will use inputs raggedness dim dim == inp _ragged_idx dim == inp _ragged_idx dim == dim == raise ValueError Transpose supported batch dimension jagged NT dim == inp _ragged_idx to_dim = dim to_dim = dim inp_kwargs = extract_kwargs inp inp_kwargs _ragged_idx = to_dim NestedTensor inp values transpose _outer_to_inner_dim len inp _size dim inp _ragged_idx _outer_to_inner_dim len inp _size dim inp _ragged_idx inp_kwargs new_kwargs dim = _wrap_jagged_dim inp dim new_kwargs dim inp _ragged_idx transpose new_kwargs dim = _wrap_jagged_dim inp dim new_kwargs dim inp _ragged_idx transpose NestedTensor func inp _values new_kwargs extract_kwargs inp register_jagged_func torch ops aten permute default jt_all dims any permute_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input dims = new_kwargs pop dims inp_kwargs = extract_kwargs inp inp_dim = len inp _size The first two checks same checks normal permute implementation inp_dim = len dims raise ValueError f permute number dimensions tensor input inp_dim + f does match length desired ordering dimensions len dims torch _prims_common canonicalize_dims canonicalized_dims = canonicalize_dims inp_dim dims len canonicalized_dims = len set canonicalized_dims raise ValueError permute duplicate dims allowed inp _lengths None raise ValueError permute supported jagged layout nested tensor holes canonicalized_dims = raise ValueError Permute supported batch dimension jagged NT inp_kwargs _ragged_idx = canonicalized_dims index inp _ragged_idx inner_dims = _outer_to_inner_dim inp_dim dim inp _ragged_idx dim canonicalized_dims new_kwargs dims = inner_dims NestedTensor func inp _values new_kwargs inp_kwargs register_jagged_func torch ops aten view default torch ops aten _unsafe_view default jt_all size any view_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input size = new_kwargs pop size inp _ragged_idx = tuple inp _size = tuple size raise RuntimeError f view does support ragged_idx = except when inp _size == size f inp _size inp _size size size Ensure specified size still includes batch ragged dims len size raggedness_matches inp size raise RuntimeError f view cannot view shape inp _size size outer size size NT e g j inner size size values e g e g offsets = function gets inner_size inner_idx given inner_idx example outer size b c j d e f assume j ragged other concrete integers ragged_idx= inner size will b c inp _values size ragged_idx d e f therefore inner_size = outer_size inner_size = outer_size inner_size = inp _values size ragged_idx - inner_size = outer_size inner_size = outer_size get_inner_size inner_idx nonlocal inp size inner_idx == inp _ragged_idx - inp _values size inner_idx size inner_idx + inner_size = get_inner_size i i range len size - Preserve inference-mode-ness input TODO Do all other views torch inference_mode inp is_inference NestedTensor func inp _values inner_size extract_kwargs inp register_jagged_func torch ops aten native_layer_norm default input jt_all normalized_shape any weight any bias any eps any native_layer_norm_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input inp dim = raise RuntimeError layer_norm supported NestedTensor objects fewer dimensions normalized_shape = new_kwargs normalized_shape ragged_size = inp shape inp _ragged_idx num_dims_not_normalized = inp dim - len normalized_shape num_dims_not_normalized == error trying normalize over batch dimension raise RuntimeError layer_norm supported when normalizing over batch dimension NestedTensor ragged_size normalized_shape inp _lengths None raise RuntimeError layer_norm supported where lengths None operating ragged dimension NestedTensor ragged_size normalized_shape special handling normalizing over ragged dimension padded_input = torch ops aten _jagged_to_padded_dense_forward inp _values flatten start_dim=inp _ragged_idx _jagged_to_padded_dense_forward requires values D tensor inp _offsets max_lengths= inp _max_seqlen max length ragged dimension padded_mask = torch ops aten _jagged_to_padded_dense_forward torch ones inp _values shape device=inp device dtype=inp dtype inp _offsets max_lengths= inp _max_seqlen max length ragged dimension expand padded_input shape mask elements outside ragged dimension expand same shape padded input D dense tensor ragged_lengths = inp _offsets diff unsqueeze unsqueeze padded_input shape ragged dim inner dim since we sum over dims layer which we normalize mean = torch sum padded_input dim= keepdim=True ragged_lengths sum over ensures layer norm whereas sum over would instance norm padded_normalized = padded_input - mean padded_mask mask elements outside ragged dimension size correct variance calculation variance = torch sum torch square padded_normalized dim= keepdim=True ragged_lengths sum over ensures layer norm whereas sum over would instance norm std = torch sqrt variance + new_kwargs eps padded_layer_norm = padded_normalized std jagged_layer_norm_values = torch ops aten _padded_dense_to_jagged_forward padded_layer_norm inp _offsets total_L=inp _values shape providing parameter helps avoid GPU CPU sync unflatten - inp shape inp _ragged_idx + unflatten last dimension back into original nested tensor shape e g B WH -- B W H NestedTensor jagged_layer_norm_values extract_kwargs inp mean std output mean std = func inp _values new_kwargs NestedTensor output extract_kwargs inp mean std register_jagged_func torch ops aten native_layer_norm_backward default grad_out jt input jt normalized_shape any mean any rstd any weight any bias any output_mask any native_layer_norm_backward_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True grad_out = new_kwargs pop grad_out inp = new_kwargs pop input d_input d_gamma d_beta = func grad_out _values inp _values new_kwargs d_input None None d_gamma d_beta NestedTensor d_input extract_kwargs inp d_gamma d_beta register_jagged_func torch ops aten select int jt_all dim any index any select_int func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input new_kwargs dim operating_on_batch = _wrap_jagged_dim inp dim new_kwargs dim inp _ragged_idx select allow_batch_dim=True handle batch dim slicing via unbind now TODO make more efficient operating_on_batch inp unbind new_kwargs index inp _lengths None raise ValueError select yet supported dim = non-contiguous nested tensor holes selecting before ragged dim adjust output ragged_idx out_kwargs = extract_kwargs inp new_kwargs dim inp _ragged_idx - out_kwargs _ragged_idx -= NestedTensor func inp _values new_kwargs out_kwargs register_jagged_func torch ops aten slice Tensor jt dim any start any end any step any slice_tensor func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input new_kwargs dim = _wrap_jagged_dim inp dim new_kwargs dim inp _ragged_idx slice NestedTensor func inp _values new_kwargs extract_kwargs inp register_jagged_func torch ops aten index_put default input jt_all indices any values t accumulate any register_jagged_func torch ops aten index_put_ default input jt_all indices any values t accumulate any index_put_ func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp NestedTensor = new_kwargs pop input For index_put_ work we add together indices ragged dimension batch dimension adding offsets each ragged dimension its indices indices = new_kwargs pop indices assert len indices = inp dim len indices inp _ragged_idx + inp is_contiguous raise RuntimeError index_put If ragged dimension part indices only works contiguous NJTs Ragged dim NOT part indices we need pad nested tensor apply func nested_tensor nested_from_padded min_seqlen = inp _maybe_min_seqlen max_seqlen = inp _maybe_max_seqlen padded_max_S = max_seqlen total_L = inp _values shape inp _ragged_idx - padded_max_S None use upper bound max seqlen s present padded_max_S = total_L padded_shape = inp shape inp _ragged_idx padded_max_S inp shape inp _ragged_idx + padded_inp = inp to_padded_tensor output_size=padded_shape new_njt = nested_from_padded func padded_inp indices new_kwargs offsets=inp _offsets ragged_idx=inp _ragged_idx sum_S=total_L min_seqlen=min_seqlen max_seqlen=max_seqlen func torch ops aten index_put_ default inp _values copy_ new_njt values inp new_njt We can run underlying values directly Validate indices inp lengths None lengths = inp offsets diff lengths = inp lengths torch _assert_async pyrefly ignore no-matching-overload torch all indices inp _ragged_idx lengths Some indices ragged dimension out bounds Recompute indices _values ragged_indices = inp offsets indices + indices inp _ragged_idx func_indices = before ragged dim indices inp _ragged_idx ragged dim combined batch + ragged_indices after ragged dim + indices inp _ragged_idx + func torch ops aten index_put_ default inp _values = func inp _values func_indices new_kwargs inp NestedTensor func inp _values func_indices new_kwargs extract_kwargs inp register_jagged_func torch ops aten convolution default input jt weight t bias t stride any padding any dilation any transposed any output_padding any groups any convolution_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input NestedTensor func inp _values new_kwargs extract_kwargs inp register_jagged_func torch ops aten mean dim jt_all dim any keepdim any dtype any mean_dim func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs input _ reduce_on_batch reduce_on_ragged reduce_on_non_batch = _wrap_jagged_dims inp dim new_kwargs dim mean inp _ragged_idx reduce_on_ragged reduce_on_batch assert reduce_on_non_batch calculate intermediate sum leave dim normalization purposes keepdim = new_kwargs keepdim new_kwargs keepdim = True intermediate_sum = _apply_reduction torch ops aten sum dim_IntList mean new_kwargs normalize sequence lengths lengths = inp _lengths inp _lengths None inp _offsets diff _ range intermediate_sum dim - lengths = lengths unsqueeze - out = intermediate_sum lengths keepdim out = out squeeze inp _ragged_idx out point we re just redispatching values buffer since we expect unused specify weird intermediate value hopefully make errors obvious intermediate_value = _apply_reduction func mean intermediate_value new_kwargs register_jagged_func torch ops aten mean default jt_all dtype any mean_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input func inp _values new_kwargs register_jagged_func torch ops aten any dims jt_all dim any keepdim any any_dims func args kwargs _apply_reduction func any False args kwargs register_jagged_func torch ops aten any dim jt_all dim any keepdim any any_dim func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True wrap dim list redispatch dims overload new_kwargs dim = new_kwargs dim any_dims torch ops aten any dims new_kwargs register_jagged_func torch ops aten all dims jt_all dim any keepdim any all_dims func args kwargs _apply_reduction func all True args kwargs register_jagged_func torch ops aten all dim jt_all dim any keepdim any all_dim func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True wrap dim list redispatch dims overload new_kwargs dim = new_kwargs dim all_dims torch ops aten all dims new_kwargs register_jagged_func torch ops aten all default torch ops aten any default torch ops aten max default torch ops aten min default jt_all all_any_max_min_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input func inp _values new_kwargs register_jagged_func torch ops aten _is_all_true default torch ops aten _is_any_true default jt_all _is_true_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input func inp _values register_jagged_func torch ops aten min dim jt_all dim any keepdim any min_dim func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True dtype = new_kwargs input dtype dtype_max = _get_padding_value dtype max _apply_reduction func min dtype_max args kwargs register_jagged_func torch ops aten max dim jt_all dim any keepdim any max_dim func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True dtype = new_kwargs input dtype dtype_min = _get_padding_value dtype min _apply_reduction func max dtype_min args kwargs register_jagged_func torch ops aten amin default jt_all dim any keepdim any amin_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True dtype = new_kwargs input dtype dtype_max = _get_padding_value dtype max _apply_reduction func amin dtype_max args kwargs register_jagged_func torch ops aten amax default jt_all dim any keepdim any amax_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True dtype = new_kwargs input dtype dtype_min = _get_padding_value dtype min _apply_reduction func amax dtype_min args kwargs register_jagged_func torch ops aten argmin default jt_all dim any keepdim any argmin_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True dtype = new_kwargs input dtype dtype_max = _get_padding_value dtype max _apply_reduction func argmin dtype_max args kwargs register_jagged_func torch ops aten argmax default jt_all dim any keepdim any argmax_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True dtype = new_kwargs input dtype dtype_min = _get_padding_value dtype min _apply_reduction func argmax dtype_min args kwargs register_jagged_func torch ops aten value_selecting_reduction_backward default grad jt_all dim any indices jt_all sizes any keepdim any value_selecting_reduction_backward_default func args kwargs torch fx experimental symbolic_shapes is_nested_int _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True grad = new_kwargs pop grad new_kwargs grad = grad _values indices = new_kwargs pop indices new_kwargs indices = indices _values should always succeed sizes should contain nested int ragged_idx = next i i s enumerate new_kwargs sizes is_nested_int s convert dim - values-space dim new_kwargs dim = _wrap_jagged_dim len new_kwargs sizes new_kwargs dim ragged_idx value_selecting_reduction_backward convert saved NJT sizes - values-space sizes sizes = new_kwargs pop sizes sizes ragged_idx = indices _values size indices _ragged_idx - sizes = sizes new_kwargs sizes = sizes output_kwargs = extract_kwargs indices output_kwargs _ragged_idx = ragged_idx NestedTensor func new_kwargs output_kwargs register_jagged_func torch ops aten stack default tensors any dim any stack_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True guaranteed non-empty we got here tensors = new_kwargs pop tensors t tensors isinstance t NestedTensor raise RuntimeError stack expected all nested tensors inputs t dim = tensors dim raise RuntimeError stack expected all nested tensors have same dim raggedness_matches t tensors shape raise RuntimeError stack expected all nested tensors have same nested structure new_kwargs dim = _wrap_jagged_dim tensors dim + new_kwargs dim tensors _ragged_idx stack NestedTensor func t _values t tensors new_kwargs extract_kwargs tensors register_jagged_func torch ops aten embedding default weight t indices jt padding_idx any scale_grad_by_freq any sparse any embedding_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True guaranteed non-empty we got here indices = new_kwargs pop indices weight = new_kwargs pop weight NestedTensor func weight indices _values new_kwargs extract_kwargs indices register_jagged_func torch ops aten embedding_dense_backward default grad_output jt indices jt num_weights any padding_idx any scale_grad_by_freq any embedding_dense_backward_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True indices = new_kwargs pop indices grad_output = new_kwargs pop grad_output func grad_output _values indices _values new_kwargs register_jagged_func torch ops aten values default torch ops aten _nested_get_values default jt_all values_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input TODO Handle inference mode properly See https github com pytorch pytorch issues #issuecomment- inp _values detach register_jagged_func torch ops aten all default jt_all all_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input func inp _values register_jagged_func torch ops aten to_padded_tensor default jt_all padding any output_size any to_padded_tensor_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input inp _lengths None raise RuntimeError to_padded_tensor supported nested tensors holes TODO Handle rest output_size output_size = new_kwargs output_size output_size None max_seq_len = output_size inp _ragged_idx max_seq_len = inp _max_seqlen inp _max_seqlen_tensor None inp _values size only D values ragged packed dim= supported underlying FBGEMM kernel so do shape gymnastics needed values = inp values inp _ragged_idx values = values transpose inp _ragged_idx - values_shape = values shape values dim values = values flatten start_dim= values dim == values = values unsqueeze - NB The CUDA kernel jagged - padded dense conversion does support integer bool types work around casting half is_bool = values dtype torch bool is_bool values is_cuda values = values torch half padded_out = torch ops aten _jagged_to_padded_dense_forward values inp _offsets max_seq_len new_kwargs padding is_bool padded_out is_cuda padded_out = padded_out torch bool shape gymnastics part len values_shape padded_out = padded_out unflatten - values_shape len values_shape == padded_out = padded_out squeeze - inp _ragged_idx padded_out = padded_out transpose inp _ragged_idx padded_out register_jagged_func torch ops aten _nested_from_padded_tensor default padded t offsets t dummy jt ragged_idx any min_seqlen any max_seqlen any sum_S any _nested_from_padded_tensor_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True padded offsets = new_kwargs padded new_kwargs offsets ragged_idx = new_kwargs get ragged_idx only D padded ragged packed dim= supported underlying FBGEMM kernel so do shape gymnastics ragged_idx padded = padded transpose ragged_idx padded_ragged_dim _shape = padded shape padded dim padded = padded flatten start_dim= padded dim padded = padded unsqueeze - NB The CUDA kernel padded dense - jagged conversion does support integer bool types work around casting half is_bool = padded dtype torch bool is_bool padded is_cuda padded = padded torch half values = torch ops aten _padded_dense_to_jagged_forward padded offsets new_kwargs sum_S is_bool values is_cuda values = values torch bool shape gymnastics part len padded_ragged_dim _shape values = values unflatten - padded_ragged_dim _shape len padded_ragged_dim _shape values = values squeeze - ragged_idx values = values transpose ragged_idx - min_seqlen = new_kwargs min_seqlen max_seqlen = new_kwargs max_seqlen metadata_cache = min_seqlen None metadata_cache min_seqlen = min_seqlen max_seqlen None metadata_cache max_seqlen = max_seqlen NestedTensor values offsets _ragged_idx=ragged_idx _metadata_cache=metadata_cache register_jagged_func torch ops aten _nested_view_from_jagged default values t offsets t dummy jt_all lengths t ragged_idx any min_seqlen t max_seqlen t _nested_view_from_jagged_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True values offsets lengths = new_kwargs input new_kwargs offsets new_kwargs lengths ragged_idx = new_kwargs ragged_idx min_seqlen = new_kwargs min_seqlen max_seqlen = new_kwargs max_seqlen metadata_cache = min_seqlen None metadata_cache min_seqlen = min_seqlen max_seqlen None metadata_cache max_seqlen = max_seqlen NestedTensor values offsets lengths=lengths _ragged_idx=ragged_idx _metadata_cache=metadata_cache register_jagged_func torch ops aten _nested_get_offsets default jt_all _nested_get_offsets func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input inp _offsets register_jagged_func torch ops aten _nested_get_lengths default jt_all _nested_get_lengths func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input inp _lengths register_jagged_func torch ops aten _nested_get_ragged_idx default jt_all _nested_get_ragged_idx func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input inp _ragged_idx register_jagged_func torch ops aten _nested_get_min_seqlen default jt_all _nested_get_min_seqlen func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input inp _metadata_cache get min_seqlen None register_jagged_func torch ops aten _nested_get_max_seqlen default jt_all _nested_get_max_seqlen func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input inp _metadata_cache get max_seqlen None If section Nested Tensor fully masked out we still retain section length register_jagged_func torch ops aten masked_select default jt mask any masked_select_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input mask = new_kwargs pop mask inp ndim raise RuntimeError masked_select only support -D selections currently inp shape = mask shape raise RuntimeError f Mask shape mask shape compatible input s shape inp shape res_values = inp _values masked_select mask values mask_cumsum = F pad mask values cumsum dim= type ignore arg-type args = extract_kwargs inp args offsets = mask_cumsum inp _offsets NestedTensor values=res_values args register_jagged_func torch ops aten _nested_select_backward default grad_output t jt_all dim any index any _nested_select_backward_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input grad_output = new_kwargs pop grad_output grad_input = torch zeros_like inp dtype=grad_output dtype grad_input select new_kwargs dim new_kwargs index copy_ grad_output grad_input register_jagged_func torch ops aten record_stream default jt_all s any record_stream_default func args kwargs inp = args stream = args ensure all components live until stream computation completes func inp _values stream func inp _offsets stream inp _lengths None func inp _lengths stream register_jagged_func torch ops aten new_empty default torch ops aten new_zeros default torch ops aten new_ones default jt_all size any dtype any layout any device any pin_memory any new_empty_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input len new_kwargs size == func inp _values new_kwargs raise RuntimeError new_empty supported NJT shape = register_jagged_func torch ops aten elu_backward default torch ops aten hardshrink_backward default torch ops aten hardsigmoid_backward default torch ops aten hardtanh_backward default torch ops aten softplus_backward default torch ops aten softshrink_backward default jt_all activation_backward func args kwargs first NJT arg expected grad_output grad_output = next arg arg args isinstance arg NestedTensor NestedTensor func arg _values isinstance arg NestedTensor arg arg args kwargs extract_kwargs grad_output register_jagged_func torch ops aten fill Scalar jt_all value any fill_Scalar func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input NestedTensor func inp _values new_kwargs extract_kwargs inp register_jagged_func torch ops aten fill_ Scalar jt_all value any fill__Scalar func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input func inp _values new_kwargs inp register_jagged_func torch ops aten frexp Tensor jt_all frexp_Tensor func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input output_kwargs = extract_kwargs inp mantissa exponent = func inp _values NestedTensor mantissa output_kwargs NestedTensor exponent output_kwargs register_jagged_func torch ops aten matmul_backward default grad any any other any mask any matmul_backward_default func args kwargs _ new_kwargs = normalize_function type ignore misc func args=args kwargs=kwargs normalize_to_only_use_kwargs=True grad = new_kwargs pop grad inp = new_kwargs pop input other = new_kwargs pop other grad_input_mask = new_kwargs pop mask grad None None None grad_self = None grad_input_mask grad_self = torch matmul grad other transpose - - grad_other = None grad_input_mask grad_other = torch matmul inp transpose - - grad grad_self grad_other Make dummy available C++ side register_jagged_func torch ops aten _nested_get_jagged_dummy default any _nested_get_jagged_dummy func args kwargs torch nested _internal nested_tensor _nt_view_dummy _nt_view_dummy torch library _scoped_library aten IMPL aten aten impl _nested_get_jagged_dummy _nested_get_jagged_dummy CPU aten impl _nested_get_jagged_dummy _nested_get_jagged_dummy CUDA aten impl _nested_get_jagged_dummy _nested_get_jagged_dummy Meta