Owner s module dynamo contextlib copy functools math unittest noqa F importlib import_module torch torch _dynamo config torch _dynamo test_case torch _functorch config torch distributed dist torch nn nn torch utils checkpoint functorch compile min_cut_rematerialization_partition torch _dynamo backends common aot_autograd torch _dynamo testing CompileCounterWithBackend torch _higher_order_ops wrap tag_activation_checkpoint torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_utils IS_WINDOWS skipIfHpu torch testing _internal inductor_utils HAS_CUDA_AND_TRITON torch testing _internal triton_utils requires_cuda_and_triton torch testing _internal two_tensor TwoTensor torch utils checkpoint checkpoint CheckpointPolicy create_selective_checkpoint_contexts HAS_CUDA_AND_TRITON triton triton language tl triton jit add_one_kernel in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask output = x + tl store out_ptr + offsets output mask=mask requires_distributed = functools partial unittest skipIf dist is_available requires distributed checkpoint_wrapper fn inner args torch utils checkpoint checkpoint fn args use_reentrant=True inner count_ops gm args freq=None freq_ge=None op=None freqs=None freqs_ge=None ops=None match_rng_op node op isinstance node target torch _ops HigherOrderOperator node name == run_and_save_rng_state node args == op node name == run_with_rng_state node args == op node name == graphsafe_run_with_rng_state node args == op False assert freq freq_ge op freqs freqs_ge ops op None assert isinstance op list ops = op freq None freqs = freq freq_ge None freqs_ge = freq_ge freqs op freq zip ops freqs actual_count = node gm graph nodes match_rng_op node op node target == op actual_count += err_msg = f In graph gm expected op have occurred freq times graph got actual_count assert actual_count == freq err_msg assert freqs_ge None op freq_ge zip ops freqs_ge actual_count = node gm graph nodes match_rng_op node op node target == op actual_count += assert actual_count = freq_ge f In graph gm expected op have occurred least freq_ge times graph got actual_count gm collect_fwd_graph_outputs graph torch fx Graph fwd_outputs set str torch _dynamo compiled_autograd in_compiled_autograd_region fwd graph return_node = list graph nodes - assert return_node target == output x return_node args fwd_outputs add str x _InvalidContext __init__ - None pass __enter__ __exit__ exc_type exc_val exc_tb pass _invalid_context_gen _InvalidContext _InvalidContext find_first_node gm func node gm graph nodes node target func node None op_count gm result = node gm graph nodes call node op result += result _get_custom_policy no_recompute_list=None must_recompute_list=None _custom_policy ctx func args kwargs no_recompute_list None func no_recompute_list CheckpointPolicy MUST_SAVE must_recompute_list None func must_recompute_list CheckpointPolicy MUST_RECOMPUTE CheckpointPolicy PREFER_RECOMPUTE _custom_policy ActivationCheckpointingViaTagsTests torch _dynamo test_case TestCaseWithNestedGraphBreaks _validate fn backend args skip_check=False fullgraph=True compiled_autograd=False cloned_args = arg args cloned_args append arg detach clone requires_grad_ arg requires_grad cloned_fn = copy deepcopy fn torch manual_seed expected = fn args expected sum backward torch manual_seed compiled_fn = torch compile cloned_fn fullgraph=fullgraph backend=backend ctx = contextlib nullcontext compiled_autograd ctx = torch _dynamo compiled_autograd _enable lambda gm torch compile gm fullgraph=fullgraph backend=backend ctx result = compiled_fn cloned_args result sum backward skip_check assertEqual result expected msg= Output mismatch between torch compile eager versions arg cloned_arg zip args cloned_args assertEqual arg grad cloned_arg grad msg= Gradient mismatch between torch compile eager versions _compare_orig_and_checkpointed_fns orig_fn checkpointed_fn args fullgraph=True The original version checkpointed version same function should produce same outputs same gradients under torch compile clone_args args cloned_args = arg args cloned_args append arg detach clone requires_grad_ arg requires_grad cloned_args run compiler Run original version cloned_args_orig_fn = clone_args args torch manual_seed compiled_orig_fn = compiler orig_fn result_orig_fn = compiled_orig_fn cloned_args_orig_fn result_orig_fn sum backward Run checkpointed version cloned_args_checkpointed_fn = clone_args args torch manual_seed compiled_checkpointed_fn = compiler copy deepcopy checkpointed_fn result_checkpointed_fn = compiled_checkpointed_fn cloned_args_checkpointed_fn result_checkpointed_fn sum backward Check outputs gradients equal assertEqual result_orig_fn result_checkpointed_fn msg= Output mismatch between original version checkpointed version same function cloned_arg_orig_fn cloned_arg_checkpointed_fn zip cloned_args_orig_fn cloned_args_checkpointed_fn assertEqual cloned_arg_orig_fn grad cloned_arg_checkpointed_fn grad msg= Gradient mismatch between original version checkpointed version same function run functools partial torch compile fullgraph=fullgraph fullgraph export_compiler fn WrapAsModule nn Module forward args kwargs fn args kwargs mod = WrapAsModule runtime_wrapper runtime_args torch export _trace gm = _trace _export_to_torch_ir f=mod args=tuple clone_args args kwargs= dynamic_shapes=None preserve_module_call_signature= restore_fqn=False prefer_deferred_runtime_asserts_over_guards=False _log_export_usage=False NOTE necessary rng added exported graph torch compile gm fullgraph=False runtime_args runtime_wrapper run export_compiler test_tags_function device gn x y torch sigmoid torch matmul x y fn x y torch utils checkpoint checkpoint gn torch sin x y use_reentrant=True x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops freq= op=torch ops aten mm default mm recomputed bwd backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler _validate fn backend x y requires_cuda_and_triton test_tags_function_via_global_checkpoint device gn x y torch sigmoid torch matmul x y fn x y This goes through VariableBuilder checkpoint gn torch sin x y use_reentrant=True x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops freq= op=torch ops aten mm default mm recomputed bwd backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler _validate fn backend x y requires_cuda_and_triton test_tags_function_with_kwargs device gn x y torch sigmoid torch matmul x y fn x y torch utils checkpoint checkpoint gn torch sin x y use_reentrant=True preserve_rng_state=False x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops freq= op=torch ops aten mm default mm recomputed bwd backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler _validate fn backend x y requires_cuda_and_triton test_tags_sequential_layers device gn x x = x cos _ range x = torch mm x x x = x cos x fn x x = torch utils checkpoint checkpoint gn x x = torch utils checkpoint checkpoint gn x x x = torch randn device=device requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops freqs= ops= torch ops aten cos default torch ops aten mm default mm recomputed bwd backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler _validate fn backend x requires_cuda_and_triton test_tags_multiple_checkpoints device gn x y torch sigmoid torch matmul x y fn x y x = torch sin x z = torch utils checkpoint checkpoint gn x y use_reentrant=True x = torch sin z z = torch utils checkpoint checkpoint gn x y use_reentrant=True z x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops freq= op=torch ops aten mm default mm recomputed bwd backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler _validate fn backend x y requires_cuda_and_triton test_tags_module device MockModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x torch sigmoid linear x mod = MockModule device fn x torch utils checkpoint checkpoint mod torch sin x use_reentrant=True x = torch randn device=device requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten sigmoid default bw_compiler = functools partial count_ops freq= op=torch ops aten sigmoid default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler _validate fn backend x requires_cuda_and_triton test_tags_decomps device Ensures tags passed through decompositions well MockModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x torch nn functional gelu linear x mod = MockModule device fn x torch utils checkpoint checkpoint mod torch sin x use_reentrant=True x = torch randn device=device requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten erf default bw_compiler = functools partial count_ops freq= op=torch ops aten erf default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler decompositions=lambda import_module torch _inductor compile_fx select_decomp_table _validate fn backend x requires_cuda_and_triton torch _inductor config patch fallback_random=True test_tags_recomputed_rand device gn x y torch sigmoid torch rand_like x y x fn x y x = torch sin x x = torch utils checkpoint checkpoint gn x y use_reentrant=True x = torch sin x z = torch utils checkpoint checkpoint gn x y use_reentrant=True z x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops freq= op=torch ops aten mm default mm recomputed bwd backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler backend = inductor _validate fn backend x y requires_cuda_and_triton torch _inductor config patch fallback_random=True test_tags_rand device gn x y x = torch mm x y x = torch mm x y x fn x y x = torch sin x x = torch utils checkpoint checkpoint gn x y use_reentrant=True x = torch sin x x = torch utils checkpoint checkpoint gn x y use_reentrant=True x x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops freq= op=torch ops aten mm default mm recomputed bwd backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler backend = aot_eager backend = inductor _validate fn backend x y requires_cuda_and_triton torch _inductor config patch fallback_random=True test_tags_dropout device Figure out way test number inductor_random calls MockModule torch nn Module __init__ - None super __init__ linear = torch nn Linear dropout = torch nn Dropout forward x dropout linear x mod = MockModule device fn x torch utils checkpoint checkpoint mod x use_reentrant=True x = torch randn device=device requires_grad=True backend = inductor rand decomps do have have numerical results eager _validate fn backend x skip_check=True skipIfHpu torch _functorch config patch recompute_views=True torch _inductor config patch fx_graph_cache=False test_tags_must_save_tensor_that_has_backward_hook my_post_forward_hook submod args output output register_hook my_backward_hook output my_backward_hook grad grad MySubmod torch nn Module __init__ super __init__ forward x y = torch matmul x x z = y y z MyMod torch nn Module __init__ super __init__ submod = MySubmod norm = torch nn LayerNorm forward x out = torch utils checkpoint checkpoint submod x use_reentrant=False norm_out = norm out norm_out _factory_fn mod = MyMod x = torch ones dtype=torch float requires_grad=True backend = inductor mod x backend mod_no_hook x backend = _factory_fn mod_no_hook_fwd_outputs = set torch _inductor config patch post_grad_custom_pre_pass=functools partial collect_fwd_graph_outputs fwd_outputs=mod_no_hook_fwd_outputs _validate mod_no_hook backend x fullgraph=True compiled_autograd=True torch _dynamo reset mod_with_hook x backend = _factory_fn mod_with_hook submod register_forward_hook my_post_forward_hook mod_with_hook_fwd_outputs = set torch _inductor config patch post_grad_custom_pre_pass=functools partial collect_fwd_graph_outputs fwd_outputs=mod_with_hook_fwd_outputs _validate mod_with_hook backend x fullgraph=True compiled_autograd=True If ` z ` has backward hook result ` z = y y ` should also saved addition usual saved tensors mod_no_hook_fwd_outputs_no_primal = x x mod_no_hook_fwd_outputs x startswith primals_ mod_with_hook_fwd_outputs_no_primal = x x mod_with_hook_fwd_outputs x startswith primals_ additional_saved_tensors = mod_with_hook_fwd_outputs_no_primal - mod_no_hook_fwd_outputs_no_primal expected_additional_saved_tensors = mul assertEqual additional_saved_tensors expected_additional_saved_tensors f Expected additional saved tensors expected_additional_saved_tensors got additional_saved_tensors Non-primal fwd outputs model w backward hook mod_with_hook_fwd_outputs_no_primal Non-primal fwd outputs model w o backward hook mod_no_hook_fwd_outputs_no_primal requires_cuda_and_triton test_fallback device gn x y torch _dynamo graph_break = torch sigmoid torch matmul x y torch _dynamo graph_break torch cos fn x y torch cos checkpoint gn torch sin x y use_reentrant=False x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device args = x y backend = aot_eager cnt = CompileCounterWithBackend backend expected = fn args result = torch compile fn backend=cnt args assertEqual result expected One graph torch sin input other torch cos assertEqual cnt frame_count assertEqual cnt op_count assertEqual len cnt graphs requires_cuda_and_triton test_kwargs device gn x y z=None = torch matmul x y z None torch matmul z fn x y z torch cos checkpoint gn x y use_reentrant=False z=z x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device z = torch randn requires_grad=True device=device args = x y z backend = aot_eager cnt = CompileCounterWithBackend backend expected = fn args result = torch compile fn backend=cnt args assertEqual result expected assertEqual cnt frame_count assertEqual len cnt graphs wrap_node = find_first_node cnt graphs tag_activation_checkpoint one checkpoint x y z assertEqual len wrap_node args body_function = getattr cnt graphs wrap_node args name assertEqual op_count body_function requires_cuda_and_triton test_symints_location device gn x y torch matmul x torch nn functional dropout y fn x y torch utils checkpoint checkpoint gn x y use_reentrant=True backend = aot_eager cnt = CompileCounterWithBackend backend opt_fn = torch compile fn backend=cnt x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device args = x y expected = fn args result = opt_fn args x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device args = x y expected = fn args result = opt_fn args assertEqual result shape expected shape assertEqual cnt frame_count assertEqual len cnt graphs wrap_node = find_first_node cnt graphs tag_activation_checkpoint assertEqual len wrap_node args requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows test_compile_selective_checkpoint_must_recompute device context_fn_must_recompute_mm must_recompute_list = torch ops aten mm default create_selective_checkpoint_contexts _get_custom_policy must_recompute_list=must_recompute_list context_fn_no_recompute_mm no_recompute_list = torch ops aten mm default create_selective_checkpoint_contexts _get_custom_policy no_recompute_list=no_recompute_list _test context_fn bw_compiler gn x torch sigmoid torch matmul x x fn x torch utils checkpoint checkpoint gn x use_reentrant=False context_fn=context_fn x = torch randn requires_grad=True device=device fw_compiler = functools partial count_ops freq= op=torch ops aten mm default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition _validate fn backend x _test context_fn=context_fn_must_recompute_mm bw_compiler=functools partial count_ops freq= matmul recompute bwd mm ops per fwd matmul so + = op=torch ops aten mm default _test context_fn=context_fn_no_recompute_mm bw_compiler=functools partial count_ops freq= bwd mm ops per fwd matmul op=torch ops aten mm default test_sac_with_partial_context_fn CustomPolicy __init__ super __init__ __call__ ctx out func args kwargs CheckpointPolicy MUST_SAVE f x y torch sigmoid torch matmul torch matmul x y y y context_fn = functools partial create_selective_checkpoint_contexts CustomPolicy fn x y torch utils checkpoint checkpoint f x y use_reentrant=False context_fn=context_fn opt_fn = torch compile fn backend= aot_eager_decomp_partition fullgraph=True = torch randn requires_grad=True device= cpu b = torch randn requires_grad=True device= cpu expected = fn b result = opt_fn b assertEqual result expected requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows test_compile_selective_checkpoint_must_not_recompute_gemm device selective_checkpointing_context_fn no_recompute_list = torch ops aten mm default create_selective_checkpoint_contexts _get_custom_policy no_recompute_list=no_recompute_list gn x y torch sigmoid torch matmul torch matmul x y y y fn x y torch utils checkpoint checkpoint gn x y use_reentrant=False context_fn=selective_checkpointing_context_fn x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops We would ve expected here matmul recompute mm ops per fwd matmul so + = we didn t enable selective checkpointing freq= op=torch ops aten mm default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition _validate fn backend x y _compare_orig_and_checkpointed_fns gn fn x y requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows test_compile_selective_checkpoint_must_not_recompute_gemm_no_functionalization device selective_checkpointing_context_fn no_recompute_list = torch ops aten mm default create_selective_checkpoint_contexts _get_custom_policy no_recompute_list=no_recompute_list gn x y torch sigmoid torch matmul torch matmul x y y y fn x y torch utils checkpoint checkpoint gn x y use_reentrant=False context_fn=selective_checkpointing_context_fn x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device fw_compiler = functools partial count_ops freq= op=torch ops aten sigmoid default bw_compiler = functools partial count_ops Main check here just sigmoid properly recomputed we will see sigmoid sigmoid_backward bw graph freq= op=torch ops aten sigmoid default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition disable_functionalization=True _validate fn backend x y _compare_orig_and_checkpointed_fns gn fn x y requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows test_compile_selective_checkpoint_triton_kernel device Copy above test make sure having triton kernel region does error add_one x out = torch empty_like x n_elements = x numel add_one_kernel n_elements x out n_elements BLOCK_SIZE= out AddOne torch autograd Function staticmethod forward ctx x add_one x staticmethod backward ctx x x selective_checkpointing_context_fn no_recompute_list = torch ops aten mm default create_selective_checkpoint_contexts _get_custom_policy no_recompute_list=no_recompute_list gn x y torch sigmoid torch matmul torch matmul AddOne apply x sin y y y fn x y torch utils checkpoint checkpoint gn x y use_reentrant=False context_fn=selective_checkpointing_context_fn x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops We would ve expected here matmul recompute mm ops per fwd matmul so + = we didn t enable selective checkpointing freq= op=torch ops aten mm default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition _validate fn backend x y _compare_orig_and_checkpointed_fns gn fn x y requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows test_compile_selective_checkpoint_tensor_subclass device selective_checkpointing_context_fn no_recompute_list = torch ops aten mm default create_selective_checkpoint_contexts _get_custom_policy no_recompute_list=no_recompute_list gn x y torch sigmoid torch matmul torch matmul x y y y fn x y torch utils checkpoint checkpoint gn x y use_reentrant=False context_fn=selective_checkpointing_context_fn rand_tensor = torch randn requires_grad=True device=device tensor subclasses inputs x = TwoTensor rand_tensor rand_tensor clone y = TwoTensor rand_tensor clone rand_tensor clone fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops We would ve expected here matmul recompute mm ops per fwd matmul so + = we didn t enable selective checkpointing freq= op=torch ops aten mm default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition _validate fn backend x y _compare_orig_and_checkpointed_fns gn fn x y requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows test_compile_selective_checkpoint_custom_rule device _get_custom_policy meta no_recompute_list = torch ops aten mm default _custom_policy mode func args kwargs mm_count_key = f mode _mm_count mm_count_key meta meta mm_count_key = func == torch ops aten mm default meta mm_count_key += Saves output all compute ops except second mm i e we will hint partitioner recompute second mm backward pass func no_recompute_list func == torch ops aten mm default meta mm_count_key == _custom_policy selective_checkpointing_context_fn meta = create_selective_checkpoint_contexts _get_custom_policy meta gn x y torch sigmoid torch sigmoid torch matmul torch matmul x y y y y fn x y torch utils checkpoint checkpoint gn x y use_reentrant=False context_fn=selective_checkpointing_context_fn x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops Q How do we come number A We have matmuls forward pass each matmul contributes ` mm ` ops backward pass so we have least ` mm ` ops backward pass It s least because whether second matmul forward pass recomputed backward pass up partitioner decide freq_ge= op=torch ops aten mm default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition _validate fn backend x y _compare_orig_and_checkpointed_fns gn fn x y requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows test_compile_selective_checkpoint_partial_ctx_fn device selective_checkpointing_context_fn no_recompute_list create_selective_checkpoint_contexts _get_custom_policy no_recompute_list=no_recompute_list gn x y torch sigmoid torch matmul torch matmul x y y y fn x y torch utils checkpoint checkpoint gn x y use_reentrant=False context_fn=functools partial selective_checkpointing_context_fn torch ops aten mm default x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops We would ve expected here matmul recompute mm ops per fwd matmul so + = we didn t enable selective checkpointing freq= op=torch ops aten mm default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition _validate fn backend x y _compare_orig_and_checkpointed_fns gn fn x y requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows test_compile_selective_checkpoint_outplace_op device selective_checkpointing_context_fn no_recompute_list = torch ops aten mm default torch ops aten sigmoid default create_selective_checkpoint_contexts _get_custom_policy no_recompute_list=no_recompute_list gn x y torch sigmoid torch selu torch matmul torch matmul x y y relu fn x y torch utils checkpoint checkpoint gn x y use_reentrant=False context_fn=selective_checkpointing_context_fn x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device fw_compiler = functools partial count_ops freqs= ops= torch ops aten mm default torch ops aten sigmoid default bw_compiler = functools partial count_ops freqs= ops= torch ops aten mm default torch ops aten sigmoid default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition _validate fn backend x y _compare_orig_and_checkpointed_fns gn fn x y requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows test_compile_selective_checkpoint_list_ops device selective_checkpointing_context_fn recompute everything no_recompute_list = create_selective_checkpoint_contexts _get_custom_policy no_recompute_list=no_recompute_list gn x y torch cat x y sin fn x y torch utils checkpoint checkpoint gn x y use_reentrant=False context_fn=selective_checkpointing_context_fn x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device fw_compiler = functools partial count_ops freqs= ops= torch ops aten cat default bw_compiler = functools partial count_ops freqs= ops= torch ops aten cat default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition _validate fn backend x y _compare_orig_and_checkpointed_fns gn fn x y unittest skipIf IS_WINDOWS torch compile doesn t work windows unittest skip In-place op support selective checkpointing + torch compile requires TorchDispatchMode + torch compile work complete requires_cuda_and_triton test_compile_selective_checkpoint_inplace_op device selective_checkpointing_context_fn no_recompute_list = torch ops aten mm default torch ops aten sigmoid default create_selective_checkpoint_contexts _get_custom_policy no_recompute_list=no_recompute_list gn x y torch sigmoid torch selu_ torch matmul torch matmul x y y relu_ fn x y torch utils checkpoint checkpoint gn x y use_reentrant=False context_fn=selective_checkpointing_context_fn x = torch randn requires_grad=True device=device y = torch randn requires_grad=True device=device fw_compiler = functools partial count_ops freqs= ops= torch ops aten mm default torch ops aten sigmoid default bw_compiler = functools partial count_ops freqs= ops= torch ops aten mm default torch ops aten sigmoid default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition _validate fn backend x y _compare_orig_and_checkpointed_fns gn fn x y requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows torch _inductor config patch fallback_random=True test_compile_selective_checkpoint_random_op device preserve_rng_state True False selective_checkpointing_context_fn no_recompute_list = torch ops aten sigmoid default create_selective_checkpoint_contexts _get_custom_policy no_recompute_list=no_recompute_list gn x torch sigmoid torch dropout torch sigmoid x p= train=True fn x torch utils checkpoint checkpoint gn x use_reentrant=False Regardless whether ` preserve_rng_state ` True False we will always preserve RNG state when using ` torch compile ` preserve_rng_state=preserve_rng_state context_fn=selective_checkpointing_context_fn x = torch randn requires_grad=True device=device fw_compiler = functools partial count_ops freqs= ops= torch ops aten sigmoid default torch ops aten native_dropout default bw_compiler = functools partial count_ops NOTE This unit test expects ` dropout ` recomputed notice count ` native_dropout ` freqs= ops= torch ops aten sigmoid default torch ops aten native_dropout default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition NOTE when ` preserve_rng_state ` False gradient will mismatch between torch compile eager because eager version doesn t preserve RNG state while torch compile still does Hence when ` preserve_rng_state ` False we skip output gradient comparison between torch compile eager _validate fn backend x skip_check=not preserve_rng_state _compare_orig_and_checkpointed_fns gn fn x requires_cuda_and_triton unittest skipIf IS_WINDOWS torch compile doesn t work windows test_compile_selective_checkpoint_invalid_context gn x y torch sigmoid torch matmul x y y fn x y torch utils checkpoint checkpoint gn x y use_reentrant=False context_fn=_invalid_context_gen x = torch randn requires_grad=True y = torch randn requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops freq_ge= op=torch ops aten mm default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition assertRaisesRegex Exception must generate tuple two ` TorchDispatchMode ` s _validate fn backend x y requires_cuda_and_triton torch _dynamo config patch inline_inbuilt_nn_modules=True test_compile_selective_checkpoint_parametrization sac_policy _recomp_policy _custom_policy ctx func args kwargs to_recompute = func torch ops aten mul Tensor torch ops aten sigmoid default CheckpointPolicy MUST_RECOMPUTE to_recompute CheckpointPolicy MUST_SAVE _custom_policy create_selective_checkpoint_contexts _recomp_policy Parametrization torch nn Module __init__ - None super __init__ parametrization x torch sigmoid torch mul x x forward x checkpoint parametrization x use_reentrant=False context_fn=sac_policy apply_parametrization model modules = list model modules mod modules params_dict = dict mod named_parameters recurse=False p_name p params_dict items mod register_parameter p_name nn Parameter p nn utils parametrize register_parametrization mod p_name Parametrization unsafe=True model MLPModule nn Module __init__ - None super __init__ torch manual_seed net = nn Linear bias=False forward x net x reset_parameters net reset_parameters fw_compiler = functools partial count_ops freqs= ops= torch ops aten mul Tensor torch ops aten sigmoid default bw_compiler = functools partial count_ops freqs= mul recompute mul backward ops= torch ops aten mul Tensor torch ops aten sigmoid default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler partition_fn=min_cut_rematerialization_partition model = MLPModule model = apply_parametrization model model_compiled = torch compile copy deepcopy model backend=backend fullgraph=True input = torch randn requires_grad=True input_compiled = copy deepcopy input out = model input out sum backward out_compiled = model_compiled input_compiled out_compiled sum backward assertEqual out out_compiled assertEqual input grad input_compiled grad requires_cuda_and_triton test_autocast_flash_attention device fn primals_ primals_ primals_ torch ops aten _scaled_dot_product_efficient_attention default primals_ primals_ primals_ None True scale= gn args torch utils checkpoint checkpoint fn args use_reentrant=True torch autocast device_type=device x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True z = torch randn device=device requires_grad=True args = x y z torch manual_seed ref = gn args opt_gn = torch compile gn torch manual_seed res = opt_gn args assertEqual ref res requires_cuda_and_triton test_error_msg device MockModule torch nn Module __init__ - None super __init__ forward x x = torch sin x torch _dynamo graph_break x = torch cos x x mod = MockModule device fn x torch utils checkpoint checkpoint mod x use_reentrant=True x = torch randn device opt_fn = torch compile fn fullgraph=True assertRaisesRegex torch _dynamo exc Unsupported User-inserted graph break opt_fn x requires_cuda_and_triton test_list_inputs device MockModule torch nn Module __init__ - None super __init__ forward x ys = torch sin x noqa F b = torch cos ys c = torch cos ys x b c mod = MockModule device fn x ys torch utils checkpoint checkpoint mod x ys use_reentrant=True x = torch randn device y = torch randn device z = torch randn device ref = fn x y z opt_fn = torch compile fn backend= eager fullgraph=True res = opt_fn x y z assertEqual ref res requires_cuda_and_triton test_pattern_matcher device Check sdpa op recomputed backward graph tests percolate_tags checkpoint_wrapper dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor torch matmul query key transpose - - mul math sqrt key shape - softmax dim=- matmul value fn query key value Checks sin recomputed backward graph dot_prod_attention query sin key value tensor_shape = dtype = torch float args = torch randn tensor_shape device=device dtype=dtype requires_grad=True torch randn tensor_shape device=device dtype=dtype requires_grad=True torch randn tensor_shape device=device dtype=dtype requires_grad=True Save AOT graphs aot_graphs = torch _inductor compile_fx debug_compile_fx_inner graph example_inputs args kwargs aot_graphs append graph compile_fx compile_fx_inner graph example_inputs args kwargs backend = functools partial compile_fx compile_fx inner_compile=debug_compile_fx_inner opt_fn = torch compile fn backend=backend fullgraph=True opt_fn args sum backward fwd_graph = aot_graphs op = torch ops aten _scaled_dot_product_flash_attention default op = torch ops aten _scaled_dot_product_cudnn_attention default assertTrue count_ops fwd_graph freq= op=op count_ops fwd_graph freq= op=op bwd_graph = aot_graphs Check sin recomputed backward graph - checks percolate tags assertTrue count_ops bwd_graph freq= op=torch ops aten sin default Check sdpa op recomputed backward graph assertTrue count_ops bwd_graph freq= op=op count_ops bwd_graph freq= op=op requires_distributed requires_cuda_and_triton test_distributed_utils_checkpoint_wrapper torch distributed algorithms _checkpoint checkpoint_wrapper checkpoint_wrapper dist_checkpoint_wrapper MockModule torch nn Module __init__ - None super __init__ linear = torch nn Linear c = forward x x = torch sin x x = linear x x = torch cos x x c mod = dist_checkpoint_wrapper MockModule x = torch randn ref = mod x opt_mod = torch compile mod backend= eager fullgraph=True res = opt_mod x assertEqual ref res requires_distributed requires_cuda_and_triton torch _dynamo config patch inline_inbuilt_nn_modules=True test_dynamo_does_not_trace_getattr_as_top_frame inline_inbuilt_nn_modules proxy emulate what FSDP tests do torch distributed algorithms _checkpoint checkpoint_wrapper CheckpointWrapper cnt = CompileCounterWithBackend eager lin = torch nn Linear mod = torch nn Sequential lin lin mod = CheckpointWrapper mod mod _checkpoint_wrapped_module = torch ones fn x mod x mod opt_fn = torch compile fn backend=cnt fullgraph=True x = torch randn assertEqual opt_fn x fn x torch _dynamo config patch skip_fwd_side_effects_in_bwd_under_checkpoint=True test_nonlocal_mutation counter = gn x nonlocal counter counter += torch sin x fn x torch utils checkpoint checkpoint gn x use_reentrant=True x = torch randn requires_grad=True fn x sum backward The mutation reapplied backward well assertEqual counter counter = opt_fn = torch compile fn backend= eager fullgraph=True opt_fn x sum backward The mutation reapplied backward because flag assertEqual counter devices = cuda hpu instantiate_device_type_tests ActivationCheckpointingViaTagsTests globals only_for=devices __name__ == __main__ torch _dynamo test_case run_tests run_tests