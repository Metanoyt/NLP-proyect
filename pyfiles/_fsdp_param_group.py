mypy allow-untyped-defs contextlib logging collections abc Callable typing Any cast NamedTuple Optional torch torch distributed dist torch nn nn torch distributed device_mesh _get_device_handle torch distributed fsdp _common_utils _named_parameters_with_duplicates torch distributed tensor Shard torch profiler record_function torch utils _pytree tree_flatten tree_unflatten torch utils hooks RemovableHandle _fsdp_api CPUOffloadPolicy MixedPrecisionPolicy OffloadPolicy _fsdp_collectives AllGather AllGatherResult DefaultAllGather DefaultReduceScatter foreach_all_gather foreach_all_gather_copy_out foreach_reduce ProcessGroupAllocAllGather ProcessGroupAllocReduceScatter ReduceScatter _fsdp_common compiled_autograd_enabled FSDPMeshInfo HSDPMeshInfo is_bw TrainingState _fsdp_param alloc_storage FSDPParam ParamModuleInfo ShardedState logger = logging getLogger torch distributed fsdp fully_shard _ModuleToHandleDict = dict nn Module RemovableHandle state dict Note Overlapping all-gather copy-in all-gather For implicit forward prefetching we want overlap next copy-in current all-gather We do so using separate copy-in stream However since we have all-gather input view into output we must make sure copy into different memory current all-gather s output Thus we keep reference current all-gather s output have next FSDP parameter group free after its copy-in Finally we have last FSDP state flush reference avoid holding onto memory after forward FSDPCommContext This has communication state shared across FSDP states parameter groups lazy_init device torch device device_handle = _get_device_handle device type Setting all-gather reduce-scatter streams higher priority can help avoid some issues where their copies out delayed block computation different high-pri NCCL streams high_priority = - All-gather state copy-in stream allow overlapping next copy-in current all-gather forward copy-in overlaps reduce-scatter backward without separate copy-in stream all_gather_copy_in_stream = device_handle Stream priority=high_priority All-gather stream allows overlapping next all-gather current forward compute all_gather_stream = device_handle Stream priority=high_priority Reduce-scatter stream gives separate execution thread post- backward logic like pre post-gradient division reduce-scatter reduce_scatter_stream = device_handle Stream priority=high_priority Run HSDP all-reduces concurrently all-gather reduce-scatter since collectives use different network resources can overlap typical intra-node sharding inter-node replication case all_reduce_stream = device_handle Stream All-gather reduce-scatter states keep references collective tensors produced one stream used another accompanying CUDA events synchronization all_gather_state Optional AllGatherState = None reduce_scatter_state Optional ReduceScatterState = None Post-forward order explicit backward prefetching post_forward_order list FSDPParamGroup = will cause ref cycles get_all_gather_streams async_op bool training_state TrainingState - tuple torch Stream torch Stream async_op training_state TrainingState FORWARD TrainingState PRE_BACKWARD Use separate streams implicit prefetching all_gather_copy_in_stream all_gather_stream current_stream = device_handle current_stream current_stream current_stream See Note Overlapping all-gather copy-in all-gather AllGatherState NamedTuple all_gather_result AllGatherResult event Optional torch Event all-gather copy-out ReduceScatterState NamedTuple reduce_scatter_input torch Tensor event Optional torch Event reduce-scatter event AllReduceState NamedTuple all_reduce_input torch Tensor event Optional torch Event all-reduce event FSDPParamGroup This represents parameter group communicate together _orig_dtype Optional torch dtype _reduce_dtype Optional torch dtype __init__ params list nn Parameter modules tuple nn Module mesh_info FSDPMeshInfo post_forward_mesh_info Optional FSDPMeshInfo device torch device shard_placement_fn Optional Callable nn Parameter Optional Shard mp_policy MixedPrecisionPolicy offload_policy OffloadPolicy modules = modules permit ref cycle because lifetime param_module_infos = _get_param_module_infos params modules fsdp_params = FSDPParam param module_info mesh_info post_forward_mesh_info device shard_placement_fn mp_policy offload_policy param module_info zip params param_module_infos mesh_info = mesh_info post_forward_mesh_info = post_forward_mesh_info pyrefly ignore read-only device = device device_handle = _get_device_handle device type mp_policy = mp_policy offload_policy = offload_policy _training_state = TrainingState IDLE Group s sharded state always matches its parameters sharded states _sharded_state = ShardedState SHARDED _module_fqn Optional str = None prefixed root module Only consider resetting sharded parameters once lazy init since can incur nontrivial overhead reset them _reset_sharded_params bool = False - Hook state _module_to_pre_save_state_dict_hook_handle _ModuleToHandleDict = _module_to_pre_load_state_dict_hook_handle _ModuleToHandleDict = _all_reduce_hook Optional Callable torch Tensor None = None _all_gather_comm AllGather = DefaultAllGather _all_gather_output = torch empty device=self device _reduce_scatter_comm ReduceScatter = DefaultReduceScatter Optional stream run user-defined all-reduce hook Saved here comm context because we allow user specify possibly construction time before lazy init _all_reduce_hook_stream Optional torch cuda Stream = None - Communication communication computation overlap comm_ctx = FSDPCommContext Group s indices shared post-forward order _post_forward_indices list int = Whether reduce gradients all whether FSDP HSDP reduce_grads bool = True Whether all-reduce gradients HSDP only used ` reduce_grads ` true which case setting false means reduce-scatter no all-reduce all_reduce_grads bool = True Whether reshard parameters after backward only useful gradient accumulation reshard_after_backward bool = True Optional custom factor gradient reduction op e g divide factor other than world size gradient_divide_factor Optional float = None Whether reduce-scatter all-reduce should issued using only summations potentially separate pre- post-scaling force_sum_reduction_for_comms bool = False ` async_op ` arg used pre-forward pre-backward unshard can overridden only do explicit prefetching avoid inter-stream fragmentation using separate unshard streams unshard_async_op bool = False Whether unshard backward can overridden user parameters group needed backward e g embedding unshard_in_backward bool = True - CUDA events stream synchronization Holds all-gather output buffer sync objects metadata _all_gather_result Optional AllGatherResult = None Holds reduce-scatter all-reduce view-out CUDA event marks end group s post-backward e g reduce-scatter all-reduce div which should waited end backward _post_reduce_event Optional torch Event = None Holds reshard-after-forward CUDA event when resharding different world size which should waited next unshard _reshard_after_forward_event Optional torch Event = None Only HSDP accumulating gradients without all-reduce save partial reduce output only reduce-scattered all-reduced _partial_reduce_output Optional torch Tensor = None Holds all-reduce input all-reduce event keep alive until end backward critical when doing bf reduction fp parameters since all-reduce input allocated RS stream will have no refs after being upcast fp _all_reduce_state Optional AllReduceState = None Initialization _init_mp_dtypes - None fsdp_param fsdp_params fsdp_param init_dtype_attrs mp_policy trainable_params list FSDPParam = p p fsdp_params p sharded_param requires_grad orig_dtypes = p orig_dtype p trainable_params reduce_dtypes = p reduce_dtype p trainable_params len trainable_params len orig_dtypes = Models may have no grad params raise AssertionError f FSDP expects uniform original parameter dtype got orig_dtypes _orig_dtype = next iter orig_dtypes trainable_params None len trainable_params len reduce_dtypes = This can relaxed we issue one reduce-scatter per reduce dtype we would need way users specify multiple reduce dtypes raise AssertionError f FSDP expects uniform reduce dtype got reduce_dtypes _reduce_dtype = next iter reduce_dtypes trainable_params None lazy_init Lazy init should idempotent Users may change register parameters after construction time For example DoRA https arxiv org abs initializes linear magnitudes based other parameters e g loaded state dict hasattr comm_ctx device_handle comm_ctx device_handle = _get_device_handle device type is_sharded _reset_sharded_params fsdp_param fsdp_params fsdp_param reset_sharded_param fsdp_param _init_extensions allow monkey patch after init _reset_sharded_params = True _validate_no_meta_params _validate_cpu_offload_params Initialize mixed precision attributes lazily case user changes parameter dtypes after construction time before forward _init_mp_dtypes _register_state_dict_hooks set_allocate_memory_from_process_group enable bool - None Whether try use ProcessGroup s allocate_tensor method staging buffers collective comms isinstance _all_gather_comm DefaultAllGather &#124; ProcessGroupAllocAllGather raise AssertionError cannot call set_allocate_memory_from_process_group f when all gather comm custom _all_gather_comm __class__ __name__ _all_gather_comm = ProcessGroupAllocAllGather _all_gather_process_group enable DefaultAllGather isinstance _reduce_scatter_comm DefaultReduceScatter &#124; ProcessGroupAllocReduceScatter raise AssertionError cannot call set_allocate_memory_from_process_group f when reduce scatter comm custom _reduce_scatter_comm __class__ __name__ _reduce_scatter_comm = ProcessGroupAllocReduceScatter _reduce_scatter_process_group enable DefaultReduceScatter Runtime unshard async_op bool = False _all_gather_result None already called pending wait is_unsharded no-op unshard_in_backward _training_state == TrainingState PRE_BACKWARD _reshard_after_forward_event None Resharded parameter data allocated default stream used all-gather streams _wait_all_gather_streams_on_event _reshard_after_forward_event _reshard_after_forward_event = None world_size = _all_gather_process_group size world_size == can t skip due early wait_for_unshard no _all_gather_result _all_gather_result = AllGatherResult all_gather_output=self _all_gather_output all_gather_event=self device_handle Event record all_gather_work=None param_all_gather_input_dtypes= param_all_gather_input_numels= all_gather_input_split_sizes= record_function _with_fqn FSDP all_gather _all_gather_result = foreach_all_gather fsdp_params _all_gather_process_group async_op comm_ctx get_all_gather_streams async_op _training_state device _all_gather_comm wait_for_unshard In forward implicit prefetching overlap current copy-out next all-gather we save reference current all-gather result free after next copy-out Otherwise explicit prefetching backward we free all-gather result immediately after current copy-out since we can already overlap current copy-out previous reduce-scatter _all_gather_result no preceding unshard async_op = _all_gather_result all_gather_work None _training_state == TrainingState FORWARD implicit prefetch prev_all_gather_state = comm_ctx all_gather_state _wait_all_gather_streams_on_event prev_all_gather_state event comm_ctx all_gather_state = None free all-gather result world_size = _all_gather_process_group size world_size == directly initialize unsharded parameters sharded parameters fsdp_param fsdp_params Use all_gather_inputs which already handles conversion param_dtype This consistent world_size path all_gather_input = fsdp_param all_gather_inputs Make sure all_gather_outputs has proper storage size before using First ensure we have least one tensor all_gather_outputs fsdp_param init_all_gather_outputs all_gather_input numel all_gather_input dtype world_size device force_recreate=False tensor = fsdp_param all_gather_outputs alloc_storage tensor find alternative way check tensor is_inference torch autograd _unsafe_preserve_version_counter tensor tensor copy_ all_gather_input record_function _with_fqn FSDP all_gather_copy_out foreach_all_gather_copy_out _all_gather_result fsdp_params _all_gather_process_group fsdp_param fsdp_params fsdp_param init_unsharded_param _to_unsharded all_gather_copy_out_event = device_handle Event all_gather_copy_out_event record async_op _training_state == TrainingState FORWARD world_size Defer free allow overlap copy-out next all-gather collective comm_ctx all_gather_state = AllGatherState _all_gather_result all_gather_copy_out_event _wait_all_gather_streams_on_event all_gather_copy_out_event _all_gather_result = None free unless saved ` all_gather_state ` _wait_all_gather_streams_on_event event Optional torch Event Calling ` unshard ` before lazy init means streams initialized hasattr comm_ctx all_gather_copy_in_stream event None comm_ctx all_gather_copy_in_stream wait_event event hasattr comm_ctx all_gather_stream event None comm_ctx all_gather_stream wait_event event reshard _training_state == TrainingState FORWARD _reshard_after_forward _use_post_forward_mesh _to_sharded_post_forward _reshard_after_forward_event = device_handle Event _reshard_after_forward_event None _reshard_after_forward_event record _to_sharded pre_forward module nn Module args tuple Any kwargs dict str Any - tuple tuple Any dict str Any compiled_autograd_enabled logger debug s _with_fqn FSDP pre_forward record_function _with_fqn FSDP pre_forward _training_state = TrainingState FORWARD unshard unshard_async_op wait_for_unshard args kwargs = _register_post_backward_hook args kwargs args kwargs post_forward module nn Module input Any output Any compiled_autograd_enabled logger debug s _with_fqn FSDP post_forward record_function _with_fqn FSDP post_forward compiled_autograd_enabled AC fully_shard model AC runs fsdp s _pre_forward shouldn t change post_forward_order is_bw reshard _record_post_forward reshard _record_post_forward _training_state = TrainingState IDLE output _record_post_forward - None Since group has one pre-backward unshard each forward call before backward we record each usage multiplicity post_forward_index = len comm_ctx post_forward_order comm_ctx post_forward_order append _post_forward_indices append post_forward_index pre_backward default_prefetch bool unused Any compiled_autograd_enabled _training_state == TrainingState PRE_BACKWARD Traceable FSDP cannot trigger param group s ` post_backward ` immediately after param usage instead relies trigger previously unexecuted ` post_backward ` post_backward _training_state == TrainingState PRE_BACKWARD compiled_autograd_enabled logger debug s _with_fqn FSDP pre_backward record_function _with_fqn FSDP pre_backward _training_state = TrainingState PRE_BACKWARD unshard unshard_async_op no-op prefetched wait_for_unshard default_prefetch compiled_autograd_enabled _backward_prefetch post_backward unused Any This method should idempotent safe call even when FSDP parameter group used backward should no-op compiled_autograd_enabled logger debug s _with_fqn FSDP post_backward _training_state = TrainingState POST_BACKWARD record_function _with_fqn FSDP post_backward_accumulate fsdp_param fsdp_params fsdp_param accumulate_unsharded_grad_if_needed record_function _with_fqn FSDP post_backward_reshard reduce_grads reshard_after_backward reshard fsdp_param fsdp_params fsdp_param to_accumulated_grad_if_needed Save autograd-computed gradients before resharding only access unsharded parameters when their data present fsdp_params_with_grad list FSDPParam = unsharded_grads list torch Tensor = fsdp_param fsdp_params hasattr fsdp_param _unsharded_param continue May have accumulated gradient reduce dtype previous backward did reduce-scatter fsdp_param unsharded_accumulated_grad None fsdp_params_with_grad append fsdp_param unsharded_grads append fsdp_param unsharded_accumulated_grad_data fsdp_param unsharded_accumulated_grad = None fsdp_param unsharded_param grad None fsdp_params_with_grad append fsdp_param unsharded_grads append fsdp_param unsharded_grad_data fsdp_param unsharded_param grad = None reshard_after_backward reshard len fsdp_params_with_grad == record_function _with_fqn FSDP post_backward_reduce comm_ctx reduce_scatter_state None comm_ctx reduce_scatter_state event None device_handle current_stream wait_event comm_ctx reduce_scatter_state event comm_ctx reduce_scatter_state = None all_reduce_pg = _all_reduce_process_group _is_hsdp None all_reduce_stream torch cuda Stream all_reduce_pg None _all_reduce_hook_stream None means native HSDP enabled user may want have custom HSDP setup _all_reduce_hook None raise AssertionError all reduce hook stream specified hook itself missing all_reduce_stream = _all_reduce_hook_stream all_reduce_stream = comm_ctx all_reduce_stream _wait_for_post_backward reduce_scatter_input reduce_scatter_event _post_reduce_event all_reduce_input all_reduce_event _partial_reduce_output = foreach_reduce fsdp_params_with_grad unsharded_grads _reduce_scatter_process_group comm_ctx reduce_scatter_stream _reduce_scatter_comm _orig_dtype _reduce_dtype device gradient_divide_factor _all_reduce_process_group _is_hsdp None all_reduce_stream all_reduce_grads _partial_reduce_output _all_reduce_hook force_sum_reduction_for_comms comm_ctx reduce_scatter_state = ReduceScatterState reduce_scatter_input reduce_scatter_event all_reduce_input None device type = cpu all_reduce_event None raise AssertionError Expected all_reduce_event set non-CPU device _all_reduce_state = AllReduceState all_reduce_input all_reduce_event finalize_backward _wait_for_post_backward fsdp_param fsdp_params fsdp_param grad_offload_event None fsdp_param grad_offload_event synchronize fsdp_param grad_offload_event = None _all_gather_result None If there mistargeted unshard without corresponding wait then we wait here clear unshard event = _all_gather_result all_gather_event None torch accelerator current_stream wait_event event work = _all_gather_result all_gather_work isinstance work dist distributed_c d Work work wait _all_gather_result = None _post_forward_indices clear _wait_for_post_backward _post_reduce_event None device_handle current_stream wait_event _post_reduce_event _post_reduce_event = None _all_reduce_state None _all_reduce_state event None device_handle current_stream wait_event _all_reduce_state event _all_reduce_state = None _backward_prefetch - None _training_state == TrainingState PRE_BACKWARD _post_forward_indices Can cleared running multiple ` backward ` s curr_index = _post_forward_indices pop target_index = curr_index - Prefetch naively using reverse post-forward order which may have mistargeted prefetches all modules used forward used backward pyrefly ignore unbound-name target_fsdp_param_group = comm_ctx post_forward_order target_index _prefetch_unshard target_fsdp_param_group backward staticmethod _prefetch_unshard target_fsdp_param_group FSDPParamGroup pass_type str - None pass_type == backward training_state = TrainingState PRE_BACKWARD pass_type == forward training_state = TrainingState FORWARD raise ValueError f Unknown pass type pass_type target_fqn = target_fsdp_param_group _module_fqn record_function f FSDP pass_type _prefetch target_fqn target_fsdp_param_group use_training_state training_state async_op = target_fsdp_param_group unshard_async_op target_fsdp_param_group unshard async_op Utilities _to_sharded is_sharded fsdp_param fsdp_params fsdp_param to_sharded _sharded_state = ShardedState SHARDED _to_sharded_post_forward is_sharded_post_forward fsdp_param fsdp_params fsdp_param to_sharded_post_forward _sharded_state = ShardedState SHARDED_POST_FORWARD _to_unsharded is_unsharded fsdp_param fsdp_params fsdp_param to_unsharded _sharded_state = ShardedState UNSHARDED property is_sharded - bool _sharded_state == ShardedState SHARDED property is_sharded_post_forward - bool _sharded_state == ShardedState SHARDED_POST_FORWARD property is_unsharded - bool _sharded_state == ShardedState UNSHARDED contextlib contextmanager use_training_state training_state TrainingState old_training_state = _training_state _training_state = training_state try yield finally _training_state = old_training_state Hook Registration _register_post_backward_hook args tuple Any kwargs dict str Any - tuple tuple Any dict str Any Traceable FSDP relies ` root_post_backward_callback ` call each ` FSDPParamGroup post_backward ` torch _dynamo config skip_fsdp_hooks compiled_autograd_enabled args kwargs torch is_grad_enabled args kwargs args_list args_spec = tree_flatten args kwargs_list kwargs_spec = tree_flatten kwargs args_kwargs_list = list args_list + list kwargs_list inp_tensor_indices list int = inp_tensors list torch Tensor = i obj enumerate args_kwargs_list torch is_tensor obj obj requires_grad inp_tensor_indices append i inp_tensors append obj len inp_tensors == args kwargs no tensors require gradients inp_tensors = RegisterPostBackwardFunction apply inp_tensors inp_tensor_idx inp_tensor zip inp_tensor_indices inp_tensors args_kwargs_list inp_tensor_idx = inp_tensor args_list = args_kwargs_list len args_list kwargs_list = args_kwargs_list len args_list args = tree_unflatten args_list args_spec kwargs = tree_unflatten kwargs_list kwargs_spec args kwargs _register_state_dict_hooks - None num_pre_save_hooks = len _module_to_pre_save_state_dict_hook_handle num_pre_load_hooks = len _module_to_pre_load_state_dict_hook_handle num_pre_save_hooks = num_pre_load_hooks raise AssertionError f Pre-save num_pre_save_hooks pre-load num_pre_load_hooks num_pre_save_hooks already registered modules_with_fsdp_params set nn Module = fsdp_param _module_info module fsdp_param fsdp_params to_sharded_hook args Any kwargs Any - None _to_sharded module modules_with_fsdp_params _module_to_pre_save_state_dict_hook_handle module = module register_state_dict_pre_hook to_sharded_hook _module_to_pre_load_state_dict_hook_handle module = module _register_load_state_dict_pre_hook to_sharded_hook Properties property _reshard_after_forward - bool post_forward_mesh_info None property _use_post_forward_mesh - bool _reshard_after_forward mesh_info = post_forward_mesh_info property _is_hsdp - bool isinstance mesh_info HSDPMeshInfo property _all_gather_process_group - dist ProcessGroup mesh_info = cast FSDPMeshInfo post_forward_mesh_info is_sharded_post_forward mesh_info isinstance mesh_info FSDPMeshInfo raise AssertionError f Expected mesh_info FSDPMeshInfo got type mesh_info mesh_info shard_process_group property _reduce_scatter_process_group - dist ProcessGroup isinstance mesh_info FSDPMeshInfo raise AssertionError f Expected mesh_info FSDPMeshInfo got type mesh_info mesh_info shard_process_group property _all_reduce_process_group - dist ProcessGroup isinstance mesh_info HSDPMeshInfo raise AssertionError f Expected mesh_info HSDPMeshInfo got type mesh_info mesh_info replicate_process_group _with_fqn label str - str _module_fqn f label _module_fqn label __repr__ f FSDPParamGroup fqn= _module_fqn _validate_no_meta_params param_names_on_meta = fsdp_param _param_fqn fsdp_param fsdp_params fsdp_param sharded_param device type == meta param_names_on_meta raise RuntimeError FSDP parameters should materialized meta device before training f following still meta device param_names_on_meta \n For example call module to_empty device materialize device call module reset_parameters each module initialize values _validate_cpu_offload_params isinstance offload_policy CPUOffloadPolicy fsdp_params_not_on_cpu = fsdp_param fsdp_param fsdp_params fsdp_param sharded_param device type = cpu fsdp_params_not_on_cpu raise RuntimeError FSDP parameters should materialized CPU when enabling CPU offloading For example load CPU state dict call module to_empty device= cpu Found following parameters non-CPU device f fsdp_param _param_fqn fsdp_param sharded_param device fsdp_param fsdp_params_not_on_cpu \n _get_param_module_infos params list nn Parameter modules tuple nn Module - list ParamModuleInfo Shared parameter lin weight = lin weight Shared module mlp lin = mlp lin We do remove duplicates when traversing both modules parameters find shared modules parameters shared parameters within module params_set = set params param_to_module_info dict nn Parameter ParamModuleInfo = module modules _ submodule module named_modules remove_duplicate=False param_name param _named_parameters_with_duplicates submodule recurse=False param params_set param param_to_module_info param_to_module_info param = ParamModuleInfo submodule param_name param_to_module_info param shared_modules append submodule param_to_module_info param shared_param_names append param_name len param_to_module_info = len params raise AssertionError f Some parameters module tree modules param_to_module_info param param params RegisterPostBackwardFunction torch autograd Function staticmethod _assert_not_tracing_fsdp compiled_autograd_enabled TODO Find way print offending FSDP module msg = \ When Traceable FSDP enabled we should calling into ` RegisterPostBackwardFunction ` Instead we rely param group s next ` pre_backward ` hook trigger its previously unexecuted ` post_backward ` we rely FSDPState s ` root_post_backward_callback ` trigger resharding any leftover unsharded param groups If you here means forward part FSDP instance compiled you must also compile forward part you want use Traceable FSDP torch _dynamo comptime comptime print msg raise RuntimeError msg staticmethod pyrefly ignore bad-override forward ctx param_group FSDPParamGroup inputs torch Tensor All tensors ` inputs ` should require gradient RegisterPostBackwardFunction _assert_not_tracing_fsdp ctx param_group = param_group inputs staticmethod backward ctx grads torch Tensor RegisterPostBackwardFunction _assert_not_tracing_fsdp ctx param_group post_backward None + grads