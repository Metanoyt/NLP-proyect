This module implements CUDA graphs support TorchDynamo backends CUDA graphs allow capturing replaying GPU operations which can significantly reduce CPU overhead GPU-accelerated PyTorch models This module provides - CUDA graph creation management both forward backward passes - Input mutation detection handling - Device compatibility checking - Stack trace management debugging - Integration TorchInductor s cudagraph trees The backend supports two main modes cudagraphs Full CUDA graph support both forward backward pass optimization cudagraphs_inner Lower-level CUDA graph implementation used benchmarking Key components - CudagraphsBackend Main backend CUDA graph integration - Mutation detection utilities ensure graph safety - Device mapping compatibility checks - Stack trace collection debugging functools collections defaultdict collections abc Callable Sequence typing Any Optional torch torch fx torch _dynamo config torch _dynamo backends common aot_autograd torch _dynamo backends debugging boxed_nop torch _inductor cudagraph_utils BoxedDeviceIndex check_multiple_devices_or_any_cpu_nodes format_default_skip_message get_mutation_stack_trace get_placeholder_info log_cudagraph_skip_and_bump_counter torch _inductor utils BoxedBool count_tangents get_first_incompatible_cudagraph_node num_fw_fixed_arguments output_node torch multiprocessing reductions StorageWeakRef registry register_backend find_input_mutations g torch fx Graph - set int meta_fk meta dict str Any - Any meta val val meta meta fake_result inputs = defaultdict set input_idx = mutated_inputs = set n g nodes n op == placeholder isinstance meta_fk n meta torch Tensor inputs StorageWeakRef meta_fk n meta _typed_storage add input_idx input_idx += n op == call_function hasattr n target _schema continue schema = n target _schema i arg enumerate schema arguments i len n args argument = n args i arg name n kwargs continue argument = n kwargs arg name mut_arg = False arg alias_info arg alias_info is_write mut_arg = True mut_arg TODO correct args contain tensors struct like list mutated_inputs &#124; = inputs StorageWeakRef meta_fk argument meta _typed_storage TODO error unrecognized nodes mutated_inputs get_device_node_mapping gm torch fx GraphModule - dict torch device torch fx Node device_node_mapping dict torch device torch fx Node = n gm graph nodes t = n meta get val None isinstance t torch Tensor t device device_node_mapping device_node_mapping t device = n device_node_mapping check_for_mutation_ignore_cuda_graph_managed_tensor aot_model torch fx GraphModule num_fixed int - Optional str mutation_indices = find_input_mutations aot_model graph - set range num_fixed mutation_indices None placeholders = get_placeholder_info aot_model graph get_mutation_stack_trace placeholders mutation_indices check_for_skip aot_model torch fx GraphModule num_fixed int - Optional str config cudagraph_backend_support_input_mutation mut_skip = check_for_mutation_ignore_cuda_graph_managed_tensor aot_model num_fixed mut_skip skip = check_multiple_devices_or_any_cpu_nodes get_device_node_mapping aot_model skip node = get_first_incompatible_cudagraph_node aot_model format_default_skip_message f incompatible op node name None get_device_index gm torch fx GraphModule - int device = next iter get_device_node_mapping gm assert device type == cuda device index get_stack_traces gm torch fx GraphModule - list Optional str output = output_node gm assert len output args == args = output args hasattr args __iter__ arg stack_trace isinstance arg torch fx node Node None arg args type ignore union-attr cudagraphs dynamo_model torch fx GraphModule dynamo_inputs Sequence Any - Any torch _inductor cudagraph_trees cudagraphify_impl do_cudagraphs = BoxedBool True boxed_device_index = BoxedDeviceIndex None forward_cudagraphs aot_model torch fx GraphModule aot_inputs list Any is_inference bool = False - Any interp = boxed_nop aot_model aot_inputs fixed = num_fw_fixed_arguments len dynamo_inputs len aot_inputs skip_msg = check_for_skip aot_model fixed BoxedBool disable do_cudagraphs log_cudagraph_skip_and_bump_counter f skipping cudagraphs due skip_msg interp boxed_device_index set get_device_index aot_model out = cudagraphify_impl interp aot_inputs range fixed device_index=boxed_device_index value is_backward=False is_inference=False Q should forward is_inference here stack_traces=get_stack_traces aot_model placeholders=get_placeholder_info aot_model graph mutated_input_idxs=find_input_mutations aot_model graph out _boxed_call = True type ignore attr-defined out backward_cudagraphs aot_model torch fx GraphModule aot_inputs list Any - Any interp = boxed_nop aot_model aot_inputs do_cudagraphs aot_model fixed = count_tangents aot_model skip_msg = check_for_skip aot_model fixed log_cudagraph_skip_and_bump_counter f skipping cudagraphs due skip_msg See Backward Generation Handling device_idx = boxed_device_index value device_idx None device_idx = Default device set manager = torch _inductor cudagraph_trees get_manager device_idx create_if_none_exists=False assert manager None fn inputs list Any - Any pyrefly ignore missing-attribute manager set_to_running_backward aot_model inputs fn _boxed_call = True type ignore attr-defined fn out = cudagraphify_impl interp aot_inputs range fixed device_index=get_device_index aot_model is_backward=True is_inference=False stack_traces=get_stack_traces aot_model placeholders=get_placeholder_info aot_model graph mutated_input_idxs=find_input_mutations aot_model graph out _boxed_call = True type ignore attr-defined out aot_cudagraphs = aot_autograd fw_compiler=forward_cudagraphs bw_compiler=backward_cudagraphs inference_compiler=functools partial forward_cudagraphs is_inference=True keep_inference_input_mutations=torch _dynamo config cudagraph_backend_keep_input_mutation aot_cudagraphs dynamo_model dynamo_inputs CudagraphsBackend compiler_name = cudagraphs staticmethod reset - None torch _inductor cudagraph_trees reset_cudagraph_trees reset_cudagraph_trees staticmethod __call__ model torch fx GraphModule inputs Sequence Any - Any cudagraphs model inputs aot_cudagraphs only applies CUDA graphs graph It also helpful debugging can serve perf baseline register_backend name= cudagraphs compiler_fn=CudagraphsBackend cudagraphs_inner model Callable Any inputs Sequence Any copy_outputs bool = True copy_inputs bool = True - Callable Sequence Any This isn t registered backend used some benchmarks assert isinstance inputs list tuple copy_inputs static_inputs = torch zeros_like x x inputs static_inputs = list inputs warmup torch cuda synchronize stream = torch cuda Stream stream wait_stream torch cuda current_stream torch cuda stream stream model inputs stream synchronize torch cuda current_stream wait_stream stream torch cuda synchronize record graph = torch cuda CUDAGraph torch cuda graph graph stream=stream static_outputs = model static_inputs isinstance static_outputs list tuple static_outputs = static_outputs run new_inputs Any - Sequence Any assert len static_inputs == len new_inputs copy_inputs dst src zip static_inputs new_inputs dst copy_ src graph replay copy_outputs x clone x static_outputs static_outputs run