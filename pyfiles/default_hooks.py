mypy allow-untyped-defs functools typing Optional torch torch distributed dist DefaultState r Stores state needed perform default communication algorithm within communication hook Args process_group ProcessGroup The process group used __slots__ = process_group world_size gradient_predivide_factor gradient_postdivide_factor __init__ process_group dist ProcessGroup process_group None raise ValueError f Expected pass explicit ProcessGroup process_group = process_group world_size = dist get_world_size process_group Setting two factors ` gradient_predivide_factor ` ` gradient_postdivide_factor ` avoid underflow overflow gradient_predivide_factor = _get_gradient_predivide_factor world_size gradient_postdivide_factor = world_size gradient_predivide_factor staticmethod _get_gradient_predivide_factor world_size int - float factor int = while world_size factor == world_size factor factor factor = float factor LowPrecisionState DefaultState r Stores state needed perform gradient communication lower precision within communication hook Communication hook will cast gradients back original parameter precision specified ` ` parameter_type ` ` default torch float Builds top ` DefaultState ` Args parameter_type torch dtype The precision model s parameters Required hook cast gradients back parameter s precision __slots__ = parameter_type __init__ process_group parameter_type=torch float super __init__ process_group parameter_type = parameter_type _decompress state LowPrecisionState grad torch Tensor Casts gradients back full parameter precision so further computation happens full precision orig_grad_data = grad data grad data = grad data state parameter_type device_type = try grad device type == privateuse device_type = torch _C _get_privateuse _backend_name device_type = grad device type backend = getattr torch device_type except AttributeError e raise AttributeError f Device grad device does have \ corresponding backend registered torch device_type e Don t let memory get reused until after transfer orig_grad_data record_stream backend current_stream type ignore arg-type allreduce_hook state DefaultState grad torch Tensor r Implement FSDP communication hook ` ` all_reduce ` ` algorithm necessary pre- post-division gradients Args state DefaultState State information configures pre- post-division factors grad torch Tensor A gradient local batch needs communicated across ranks Average grad pre-division factor Together pre- post-division factors lead overall averaging world_size required consistency PyTorch DDP This two-step process avoid potential underflow overflow state gradient_predivide_factor grad div_ state gradient_predivide_factor dist all_reduce grad group=state process_group Average grad post-division factor state gradient_postdivide_factor grad div_ state gradient_postdivide_factor reduce_scatter_hook state DefaultState grad torch Tensor output torch Tensor r Implement FSDP communication hook ` ` reduce_scatter ` ` algorithm For sharded FSDP strategies necessary pre- post-division gradients Args state DefaultState State information configures pre- post-division factors grad torch Tensor An unsharded gradient local batch needs communicated across ranks output torch Tensor Stores single shard gradient after ` ` reduce_scatter ` ` Average grad pre-division factor state gradient_predivide_factor grad div_ state gradient_predivide_factor dist reduce_scatter_tensor output grad group=state process_group Average grad s shard post-division factor state gradient_postdivide_factor output div_ state gradient_postdivide_factor _low_precision_hook prec torch dtype state LowPrecisionState grad torch Tensor output Optional torch Tensor grad dtype = prec grad data = grad data prec output None output dtype = prec output data = output data prec reduce_scatter_hook state grad output _decompress state output allreduce_hook state grad _decompress state grad fp _compress_hook state LowPrecisionState grad torch Tensor output Optional torch Tensor = None r Implement FSDP communication hook simple gradient compression approach Casts ` ` grad ` ` half-precision floating-point format ` ` torch float ` ` It also averages gradients ` ` world_size ` ` two steps first pre-divides gradients ` ` state gradient_predivide_factor ` ` after communication step ` ` all_reduce ` ` ` ` reduce_scatter ` ` gradients averaged ` ` state gradient_postdivide_factor ` ` Once post-division done compressed gradients casted back parameters precision Args state LowPrecisionState State information configures pre- post-division factors parameters precision grad torch Tensor A gradient local batch needs communicated across ranks lower precision output torch Tensor Stores single shard gradient after ` ` reduce_scatter ` ` fp _hook = functools partial _low_precision_hook torch float fp _hook state grad output bf _compress_hook state LowPrecisionState grad torch Tensor output Optional torch Tensor = None r Implement FSDP communication hook simple gradient compression approach Casts ` ` grad ` ` half-precision floating-point format It also averages gradients ` ` world_size ` ` two steps first pre-divides gradients ` ` state gradient_predivide_factor ` ` after communication step ` ` all_reduce ` ` ` ` reduce_scatter ` ` gradients averaged ` ` state gradient_postdivide_factor ` ` Once post-division done compressed gradients casted back parameters precision Args state LowPrecisionState State information configures pre- post-division factors parameters precision grad torch Tensor A gradient local batch needs communicated across ranks lower precision output torch Tensor Stores single shard gradient after ` ` reduce_scatter ` ` bf _hook = functools partial _low_precision_hook torch bfloat bf _hook state grad output