Owner s oncall distributed To run python test distributed test_nvshmem py torch torch distributed dist torch distributed _symmetric_memory symm_mem torch distributed device_mesh init_device_mesh torch testing _internal common_distributed MultiProcContinuousTest skip_if_lt_x_gpu torch testing _internal common_utils instantiate_parametrized_tests parametrize requires_cuda_p p_access run_tests skip_but_pass_in_sandcastle_if skipIfRocm Decorator requires_nvshmem skip_but_pass_in_sandcastle_if symm_mem is_nvshmem_available test_nvshmem requires NVSHMEM skipping tests So tests written device-agnostic way device_type = cuda device_module = torch get_device_module device_type requires_nvshmem requires_cuda_p p_access NVSHMEMSymmetricMemoryTest MultiProcContinuousTest _init_device - None TODO relieve seems hang without device_module set_device device Set NVSHMEM SymmMem backend symm_mem set_backend NVSHMEM property device - torch device torch device device_type rank skipIfRocm test_alloc - None _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float numel = foo inp = symm_mem empty numel dtype=dtype device=self device symm_mem rendezvous inp group=group_name foo out = symm_mem empty numel dtype=dtype device=self device symm_mem rendezvous out group=group_name skipIfRocm test_alloc_without_device_context - None Set NVSHMEM SymmMem backend symm_mem set_backend NVSHMEM group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float numel = out = symm_mem empty numel dtype=dtype device=self device assertEqual out device device symm_mem rendezvous out group=group_name skipIfRocm test_mempool_tensor_factory - None Test effectiveness MemPool tensor factory ops _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float numel = src_rank = allocator = symm_mem get_mempool_allocator device mempool = torch cuda MemPool allocator torch cuda use_mem_pool mempool rank == src_rank tensor = torch arange numel dtype=dtype device=self device tensor = torch zeros numel dtype=dtype device=self device symm_mem rendezvous tensor group=group_name torch ops symm_mem nvshmem_broadcast tensor src_rank group_name assertEqual tensor torch arange numel dtype=dtype device=self device skipIfRocm test_mempool_tensor_w_collective - None Test effectiveness MemPool tensor factory ops _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float numel = allocator = symm_mem get_mempool_allocator device mempool = torch cuda MemPool allocator torch cuda use_mem_pool mempool tensor = torch ones numel dtype=dtype device=self device symm_mem rendezvous tensor group=group_name dist all_reduce tensor assertEqual tensor torch ones numel dtype=dtype device=self device world_size skipIfRocm test_mempool_compute_ops - None Apply MemPool context compute op creates input collective _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float dim = w = torch ones dim dim dtype=dtype device=self device x = torch ones dim dtype=dtype device=self device allocator = symm_mem get_mempool_allocator device mempool = torch cuda MemPool allocator torch cuda use_mem_pool mempool x = x + rank y = torch mm x w y should symm tensor torch ops symm_mem nvshmem_broadcast y group_name expected = torch mm x w assertEqual y expected skipIfRocm test_handle_offset - None Test handle offset correctly set _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float numel = allocator = symm_mem get_mempool_allocator device mempool = torch cuda MemPool allocator torch cuda use_mem_pool mempool x = torch empty numel dtype=dtype device=self device x = torch empty_like x hdl = symm_mem rendezvous x group=group_name hdl = symm_mem rendezvous x group=group_name assertEqual hdl offset assertEqual hdl offset x untyped_storage nbytes test_get_remote_tensor - None Get remote tensor use regular aten ops write _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float numel = allocator = symm_mem get_mempool_allocator device mempool = torch cuda MemPool allocator torch cuda use_mem_pool mempool src data stores my rank x = torch empty numel dtype=dtype device=self device fill_ rank y = torch empty_like x hdl_y = symm_mem rendezvous y group=group_name peer = rank + world_size Shifting pattern y_remote = hdl_y get_remote_tensor peer y size y dtype y_remote copy_ x dist barrier Expecting data - rank expected = torch empty numel dtype=dtype device=self device fill_ rank - world_size assertEqual y expected skipIfRocm test_nvshmem_put - None _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float numel = tensor = symm_mem empty numel dtype=dtype device=self device fill_ rank hdl = symm_mem rendezvous tensor group=group_name signal_pad = hdl get_signal_pad rank signal_val = rank == torch ops symm_mem nvshmem_put_with_signal tensor signal_pad signal_val rank == torch ops symm_mem nvshmem_wait_for_signal signal_pad signal_val torch testing assert_close tensor torch zeros numel dtype=dtype device=self device skipIfRocm test_nvshmem_get - None _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float numel = tensor = symm_mem empty numel dtype=dtype device=self device fill_ rank symm_mem rendezvous tensor group=group_name rank == torch ops symm_mem nvshmem_get tensor TODO remove after we have wait_signal dist barrier torch testing assert_close tensor torch ones numel dtype=dtype device=self device handle wait_signal src_rank= TODO remove after we have wait_signal dist barrier instantiate_parametrized_tests requires_nvshmem requires_cuda_p p_access NVSHMEMAll AllTest MultiProcContinuousTest _init_device - None TODO relieve seems hang without device_module set_device device Set NVSHMEM SymmMem backend symm_mem set_backend NVSHMEM property device - torch device torch device device_type rank skipIfRocm test_nvshmem_all_to_all - None _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float numel_per_peer = numel = world_size numel_per_peer inp = symm_mem empty numel dtype=dtype device=self device fill_ rank out = symm_mem empty numel dtype=dtype device=self device fill_ - symm_mem rendezvous inp group=group_name symm_mem rendezvous out group=group_name torch ops symm_mem nvshmem_all_to_all inp out group_name expected = torch cat torch empty numel_per_peer dtype=dtype device=self device fill_ i i range world_size torch testing assert_close out expected skipIfRocm test_all_to_all_vdev - None _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float Number elements peer random between k k = inp_splits = torch randint k world_size device=self device inp_numel = inp_splits sum item Exchange input splits get output splits out_splits = torch zeros_like inp_splits dist all_to_all_single out_splits inp_splits out_numel = out_splits sum item Max number input elements must constant across ranks symmetric memory allocation max_inp_numel = k world_size Max number output elements must constant across ranks symmetric memory allocation overflow_factor = world_size worst case one rank receives all data max_out_numel = max_inp_numel overflow_factor inp = symm_mem empty max_inp_numel dtype=dtype device=self device copy_ torch randn max_inp_numel dtype=dtype device=self device out = symm_mem empty max_out_numel dtype=dtype device=self device fill_ - in_splits = symm_mem empty world_size dtype=torch int device=self device out_splits_offsets = symm_mem empty world_size dtype=torch int device=self device Row input splits in_splits copy_ inp_splits Sync all ranks ensure remote tensors allocated dist barrier torch ops symm_mem all_to_all_vdev inp out in_splits out_splits_offsets group_name Check input splits row -- should change torch testing assert_close in_splits inp_splits Check output splits row torch testing assert_close out_splits_offsets out_splits Check output offsets row out_offsets = torch cumsum out_splits dim= inclusive scan output offsets ` all_to_all_vdev ` exclusive scan assertEqual out_splits_offsets torch testing assert_close out_splits_offsets out_offsets - Check data expected = torch empty out_numel dtype=dtype device=self device dist all_to_all_single expected inp inp_numel out_splits tolist inp_splits tolist torch testing assert_close out out_numel expected skipIfRocm parametrize align ` major_align ` output test_all_to_all_vdev_ d align int - None torch manual_seed + rank _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float Number experts per rank ne = nsplits = ne world_size Number elements expert random between k k = inp_splits = torch randint k nsplits dtype=torch int device=self device Exchange input splits get output splits out_splits = torch zeros_like inp_splits dist all_to_all_single out_splits inp_splits We do t here because there rank-major expert-major shuffle out_splits_t = out_splits reshape world_size ne t Actual number input elements inp_numel = inp_splits sum item Actual number output elements out_numel = out_splits sum item Max number input elements must constant across ranks symmetric memory allocation max_inp_numel = k nsplits Max number output elements must constant across ranks symmetric memory allocation overflow_factor = world_size worst case one rank receives all data max_out_numel = max_inp_numel overflow_factor inp = symm_mem empty max_inp_numel dtype=dtype device=self device copy_ torch randn max_inp_numel dtype=dtype device=self device out = symm_mem empty max_out_numel dtype=dtype device=self device fill_ - in_splits = symm_mem empty nsplits dtype=torch int device=self device copy_ inp_splits rows output splits output offsets Initializing all values - check they updated out_splits_offsets = symm_mem empty nsplits dtype=torch int device=self device fill_ - Sync all ranks ensure remote tensors allocated dist barrier torch ops symm_mem all_to_all_vdev_ d inp out in_splits out_splits_offsets group_name major_align=align received_out_splits = out_splits_offsets received_out_offsets = out_splits_offsets Check input splits row -- should change torch testing assert_close in_splits inp_splits Check output splits row torch testing assert_close received_out_splits out_splits_t reshape - Check output offsets row out_split_list = out_splits_t tolist i range ne expert_sum = j range world_size expert_sum += out_split_list i j Align up expert_sum expert_sum_aligned = expert_sum + align - align align If make least ` align ` bc cutlass currently does support empty bins expert_sum_aligned = max expert_sum_aligned align last element absorbs padding out_split_list i - += expert_sum_aligned - expert_sum out_splits_padded = torch tensor out_split_list device=self device reshape - out_offsets = torch cumsum out_splits_padded dim= inclusive scan Make exclusive scan because s what ` all_to_all_vdev_ d ` returns out_offsets = torch cat torch zeros device=self device out_offsets - torch int torch testing assert_close received_out_offsets out_offsets Check data expected = torch empty out_numel dtype=dtype device=self device inp_splits_rank = inp_splits reshape world_size ne sum out_splits_rank = out_splits reshape world_size ne sum dist all_to_all_single expected inp inp_numel out_splits_rank tolist inp_splits_rank tolist We still need shuffle ` expected ` out_offsets = torch cumsum out_splits dim= inclusive scan result_list = j range ne i range world_size chunk_id = i ne + j offset = out_offsets chunk_id chunk = expected offset - out_splits chunk_id offset result_list append chunk Do chunk-wise comparison c chunk enumerate result_list start = received_out_offsets c item split = received_out_splits c item received_chunk = out start start + split torch testing assert_close received_chunk chunk skipIfRocm test_all_to_all_vdev_ d_offset - None torch manual_seed + rank _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float Number experts per rank ne = nsplits = ne world_size Number elements expert random between k k = inp_splits = torch randint k nsplits dtype=torch int device=self device Each split up align k offset i e k k k inp_offsets = torch arange k nsplits k dtype=torch int device=self device Max number input elements must constant across ranks symmetric memory allocation Remember we up-align each input split k max_inp_numel = k nsplits Max number output elements must constant across ranks symmetric memory allocation overflow_factor = world_size worst case one rank receives all data max_out_numel = max_inp_numel overflow_factor inp = symm_mem empty max_inp_numel dtype=dtype device=self device copy_ torch randn max_inp_numel dtype=dtype device=self device out = symm_mem empty max_out_numel dtype=dtype device=self device fill_ - rows input splits input offsets in_splits_offsets = symm_mem empty nsplits dtype=torch int device=self device rows output splits output offsets Initializing all values - check they updated out_splits_offsets = symm_mem empty nsplits dtype=torch int device=self device fill_ - Row input splits in_splits_offsets copy_ inp_splits Row input offsets in_splits_offsets copy_ inp_offsets Sync all ranks ensure remote tensors allocated dist barrier torch ops symm_mem all_to_all_vdev_ d_offset inp out in_splits_offsets out_splits_offsets group_name received_out_splits = out_splits_offsets received_out_offsets = out_splits_offsets Check input splits offsets -- should change torch testing assert_close in_splits_offsets inp_splits torch testing assert_close in_splits_offsets inp_offsets Check output splits row Exchange input splits get output splits out_splits = torch zeros_like inp_splits First need transpose input splits inp_splits_t = inp_splits reshape ne world_size t contiguous dist all_to_all_single out_splits inp_splits_t torch testing assert_close received_out_splits out_splits Check output offsets row out_offsets = torch cumsum out_splits dim= inclusive scan output offsets ` all_to_all_vdev_ d_offset ` exclusive scan assertEqual received_out_offsets torch testing assert_close received_out_offsets out_offsets - Check data Let s squeeze padding out input data first inp_chunks = ne nranks i range ne inp_chunks_e = nranks j range world_size chunk_id = i world_size + j offset = in_splits_offsets chunk_id chunk = inp offset offset + inp_splits chunk_id inp_chunks_e append chunk inp_chunks append inp_chunks_e Transpose D input chunks inp_chunks_t = list zip inp_chunks Now nranks ne concatenate e s inp_chunks_t = torch cat row row inp_chunks_t Create empty output tensors -- each tensor data received peer out_splits = out_splits reshape world_size ne Sum split sizes all experts per peer receive_size_per_peer = out_splits sum out_chunks = nranks i range world_size out_chunks append torch empty receive_size_per_peer i item dtype=dtype device=self device All-to-all dist all_to_all out_chunks inp_chunks_t Concatenate output chunks received all peers out_expected = torch cat out_chunks Actual number output elements out_numel = out_splits sum item assertEqual out_expected shape out_numel Check data torch testing assert_close out_expected out out_numel Help function used multiple tests dispatch_then_combine device align int group - None Shuffle tokens then combine them check combined data exactly same original input data group_name = group group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float Number experts per rank ne = nsplits = ne group size Number elements expert random between k k = inp_splits = torch randint k nsplits dtype=torch int device=device Actual number input elements inp_numel = inp_splits sum item Max number input elements must constant across ranks symmetric memory allocation max_inp_numel = k nsplits Max number output elements must constant across ranks symmetric memory allocation overflow_factor = group size worst case one rank receives all data max_out_numel = max_inp_numel overflow_factor Buffers shuffle inp = symm_mem empty max_inp_numel dtype=dtype device=device copy_ torch randn max_inp_numel dtype=dtype device=device out = symm_mem empty max_out_numel dtype=dtype device=device fill_ - in_splits = symm_mem empty nsplits dtype=torch int device=device copy_ inp_splits rows output splits output offsets Initializing all values - check they updated out_splits_offsets = symm_mem empty nsplits dtype=torch int device=device fill_ - Buffers combine combine_out = symm_mem empty max_out_numel dtype=dtype device=device fill_ - rows output splits output offsets Initializing all values - check they updated combine_out_splits_offsets = symm_mem empty nsplits dtype=torch int device=device fill_ - Wait all ranks finish tensor allocation before accessing them torch cuda synchronize device dist barrier group=group Shuffle tokens torch ops symm_mem all_to_all_vdev_ d inp out in_splits out_splits_offsets group_name major_align=align Combine tokens ` out_splits_offsets ` shuffle exactly ` input_splits_offsets ` combine ` out ` data shuffle exactly ` input ` data combine torch ops symm_mem all_to_all_vdev_ d_offset out combine_out out_splits_offsets combine_out_splits_offsets group_name Assert combined data exactly same original input data torch testing assert_close combine_out inp_numel inp inp_numel Assert combined out splits exactly same original input splits torch testing assert_close combine_out_splits_offsets inp_splits Assert combined out offsets exactly same original input offsets inp_offsets = torch cumsum inp_splits dim= inclusive scan Make exclusive scan because s what ` all_to_all_vdev_ d_offset ` returns inp_offsets = torch cat torch zeros device=device inp_offsets - torch int torch testing assert_close combine_out_splits_offsets inp_offsets Wait all ranks finish accessing tensors before freeing them dist barrier group=group torch cuda synchronize device instantiate_parametrized_tests requires_nvshmem requires_cuda_p p_access DispatchCombineTest MultiProcContinuousTest _init_device - None TODO relieve seems hang without device_module set_device device Set NVSHMEM SymmMem backend symm_mem set_backend NVSHMEM property device - torch device torch device device_type rank skipIfRocm parametrize align ` major_align ` output test_dispatch_combine align int - None Test dispatch-and-combine over World group torch manual_seed + rank _init_device dispatch_then_combine device align dist group WORLD instantiate_parametrized_tests requires_nvshmem requires_cuda_p p_access DispatchCombineInSubgroups MultiProcContinuousTest _init_device - None TODO relieve seems hang without device_module set_device device Set NVSHMEM SymmMem backend symm_mem set_backend NVSHMEM property device - torch device torch device device_type rank skipIfRocm TODO FIXIT Currently ` MultiProcContinuousTest ` treats skip code failure skip_if_lt_x_gpu test_dispatch_combine_subgroup - None Test dispatch-and-combine over concurrent subgroups torch manual_seed + rank _init_device symm_mem enable_symm_mem_for_group dist group WORLD group_name Test two concurrent subgroups ngroups = subgroup_size = world_size ngroups dm = init_device_mesh device_type ngroups subgroup_size mesh_dim_names= dp ep subgroup = dm get_group ep dispatch_then_combine device align= group=subgroup instantiate_parametrized_tests requires_nvshmem requires_cuda_p p_access NVSHMEMTileCommTest MultiProcContinuousTest _init_device - None TODO relieve seems hang without device_module set_device device Set NVSHMEM SymmMem backend symm_mem set_backend NVSHMEM property device - torch device torch device device_type rank skipIfRocm parametrize tile_size parametrize dtype torch float torch half torch bfloat test_tile_reduce tile_size int dtype torch dtype - None full_size = assert tile_size = full_size _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name full_inp = symm_mem empty full_size full_size dtype=dtype device=self device fill_ rank full_out = symm_mem empty full_size full_size dtype=dtype device=self device fill_ slice_ut = slice tile_size tile_size inp_tile = full_inp slice_ut slice_ut out_tile = full_out slice_ut slice_ut Reduce tile root = torch ops symm_mem tile_reduce inp_tile out_tile root group_name Check data expected = torch zeros_like full_out expected_tile = expected slice_ut slice_ut rank == root expected_tile fill_ world_size world_size - torch testing assert_close full_out expected skipIfRocm parametrize tile_size parametrize root_ratio all ranks roots half ranks roots parametrize dtype torch float torch half torch bfloat test_multi_root_tile_reduce tile_size int root_ratio int dtype torch dtype - None full_size = num_slices_col = number tiles column dimension num_slices_row = world_size num_slices_col number tiles row dimension assert tile_size num_slices_col = full_size assert tile_size num_slices_row = full_size _init_device group_name = dist group WORLD group_name symm_mem enable_symm_mem_for_group group_name full_inp = symm_mem empty full_size full_size dtype=dtype device=self device fill_ rank full_out = symm_mem empty full_size full_size dtype=dtype device=self device fill_ Get range each slice terms element indices slices_row = slice s tile_size s + tile_size s range num_slices_row slices_col = slice s tile_size s + tile_size s range num_slices_col Active roots can subset all ranks num_active_roots = world_size root_ratio active_roots = list range num_active_roots Map rank slice indices e g rank - rank - rank - rank - map_rank_to_slices = lambda r noqa E slices_row r num_slices_col slices_col r num_slices_col Populate input tiles input_tiles_ij = map_rank_to_slices r r active_roots input_tiles = full_inp slice_i slice_j slice_i slice_j input_tiles_ij My output tile i e one I will reduce out_tile_ij = map_rank_to_slices rank out_tile = full_out out_tile_ij out_tile_ij Reduce tiles torch ops symm_mem multi_root_tile_reduce input_tiles out_tile active_roots group_name Check data expected = torch zeros_like full_out expected_tile = expected out_tile_ij out_tile_ij rank active_roots expected_tile fill_ world_size world_size - torch testing assert_close full_out expected __name__ == __main__ run_tests