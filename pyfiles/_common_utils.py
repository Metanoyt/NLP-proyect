mypy allow-untyped-defs This file includes private common utilities FSDP logging traceback warnings weakref collections abc Callable Generator Iterable enum auto Enum functools partial itertools chain typing Any cast no_type_check Optional TYPE_CHECKING torch torch distributed dist torch distributed fsdp _flat_param flat_param_file torch nn nn torch distributed _composable_state _get_module_state _State torch distributed algorithms _checkpoint checkpoint_wrapper _CHECKPOINT_PREFIX torch distributed utils _apply_to_tensors torch utils _mode_utils no_dispatch api FullOptimStateDictConfig FullStateDictConfig OptimStateDictConfig ShardingStrategy StateDictConfig StateDictType TYPE_CHECKING torch distributed device_mesh DeviceMesh torch distributed fsdp _fsdp_extensions FSDPExtensions _flat_param FlatParamHandle FSDP_WRAPPED_MODULE = _fsdp_wrapped_module FSDP_PREFIX = FSDP_WRAPPED_MODULE + FSDP_FLATTENED = _fsdp_flattened Save global mapping module its input tensor dtype populated during forward pre-hook consumed forward post-hook when overriding module s mixed precision NOTE We currently take last input tensor s dtype case multiple floating-point input tensors which may incorrect However since there correspondence between input output tensors we must use some heuristic like predict desired output dtype _MODULE_TO_INP_DTYPE weakref WeakKeyDictionary = weakref WeakKeyDictionary _FSDPDeviceHandle This simple abstraction FSDP computing devices which enables custom backends implement CUDA-like semantics integrated FSDP __init__ device torch device backend Any = None backend None try __backend = getattr torch device type pyrefly ignore read-only __device = device except AttributeError exc raise AttributeError f Device device does have corresponding backend registered torch device type exc __backend = backend classmethod from_device cls device torch device - _FSDPDeviceHandle Return device handle corresponding device through handle operations same semantics CUDA can performed device Just torch cuda device cuda make attribute-access faster Custom backend must first register module same name device type torch device type == cuda cast _FSDPDeviceHandle torch cuda device type == mtia cast _FSDPDeviceHandle torch mtia cls device __getattr__ name str - Any try getattr __backend name except AttributeError exc raise AttributeError f Custom backend __device type implement torch __device type name exc _UninitializedDeviceHandle _FSDPDeviceHandle __init__ - None pass __getattribute__ name str - Any raise RuntimeError Trying use uninitialized device handle _FSDPState _State __init__ - None TODO Move all attributes enable typing FSDP fully_shard _ignored_modules set nn Module = set _ignored_params set nn Parameter = set Buffer names cleaned without wrapper prefixes _ignored_buffer_names set str = set process_group Optional dist ProcessGroup = None rank int = - world_size int = - _device_mesh Optional DeviceMesh = None sharding_strategy = ShardingStrategy FULL_SHARD _use_orig_params bool = False training_state = TrainingState IDLE _unshard_params_ctx dict nn Module Generator = _state_dict_type StateDictType = StateDictType FULL_STATE_DICT _state_dict_config StateDictConfig = FullStateDictConfig _optim_state_dict_config OptimStateDictConfig = FullOptimStateDictConfig _is_root Optional bool = None _handle Optional flat_param_file FlatParamHandle = None _fully_sharded_module_to_handle dict nn Module Optional flat_param_file FlatParamHandle = compute_device Optional torch device = None _gradient_predivide_factor int = _gradient_postdivide_factor int = _comm_hook Optional Callable = None _comm_hook_state Optional Any = None _unshard_event Optional torch Event = None Abstract device handle fsdp compute device For now compute device must implement cuda semantics used fsdp _device_handle _FSDPDeviceHandle = _UninitializedDeviceHandle All following attributes should only used root states Save these static lists avoid repeated tree traversals _all_fsdp_states list _FSDPState = _all_handles list flat_param_file FlatParamHandle = _fsdp_extension Optional FSDPExtensions = None _get_module_fsdp_state module nn Module - Optional _FSDPState state = _get_module_state module state None isinstance state _FSDPState None state _get_module_fsdp_state_if_fully_sharded_module module nn Module - Optional _FSDPState state = _get_module_fsdp_state module state None None state == module FullyShardedDataParallel module case state module state _fully_sharded_module_to_handle fully_shard case state None TrainingState Enum An enum indicates state ` ` FullyShardedDataParallel ` instance IDLE = auto FORWARD_BACKWARD = auto SUMMON_FULL_PARAMS = auto HandleTrainingState Enum An enum indicates state ` ` FlatParamHandle ` IDLE = auto FORWARD = auto BACKWARD_PRE = auto BACKWARD_POST = auto SUMMON_FULL_PARAMS = auto _is_composable state _FSDPState TODO This temporary hack differentiate between code paths isinstance state nn Module no_type_check _module_handle state _FSDPState module nn Module - Optional FlatParamHandle Returns ` ` FlatParamHandle ` ` s corresponding ` ` module ` ` This handle contains some parameter ` ` module ` ` _is_composable state A valid FSDP state may have no managed parameters hence no handles meaning no entry ` _fully_sharded_module_to_handles ` state _handle None None module state _fully_sharded_module_to_handle raise AssertionError f Expects fully sharded module got module rank state rank state _fully_sharded_module_to_handle module NOTE This assumes ` module ` ` FullyShardedDataParallel ` instance module _handle no_type_check _has_fsdp_params state _FSDPState module nn Module - bool Returns ` ` module ` ` has parameters managed FSDP _module_handle state module None _get_sharding_strategy handle Returns sharding strategy handle handle _sharding_strategy handle None clean_tensor_name tensor_name str - str Cleans parameter buffer name removing any module wrapper prefixes tensor_name = tensor_name replace FSDP_PREFIX TODO Explicitly replacing checkpoint wrapper prefix ideal couples ` CheckpointWrapper ` FSDP also does scale more module wrappers tensor_name = tensor_name replace _CHECKPOINT_PREFIX tensor_name _set_fsdp_flattened tensor torch Tensor - None Sets attribute ` ` tensor ` ` mark flattened FSDP This avoid re-flattening during nested construction setattr tensor FSDP_FLATTENED True _is_fsdp_flattened tensor torch Tensor - bool Returns ` ` tensor ` ` has been marked flattened FSDP getattr tensor FSDP_FLATTENED False _named_parameters_with_duplicates module nn Module kwargs Any - list tuple str nn Parameter This API required some modules overwrite ` named_parameters ` do support ` remove_duplicate ` remove_duplicate kwargs raise AssertionError _named_parameters_with_duplicates cannot used ` remove_duplicate ` argument kwargs remove_duplicate = False try ret = list module named_parameters kwargs except AssertionError kwargs pop remove_duplicate ret = list module named_parameters kwargs ret _get_param_to_fqns model torch nn Module dedup_shared_params bool = True - dict nn Parameter list str Constructs mapping parameter list its \ canonical\ FQNs Here we use canonical mean fully-qualified name assigned parameter based its position original nn Module hierarchy before any wrapper parallelism has been applied This contrast FQNs may generated after parallelisms wrappers have been applied model Each normal parameter maps singleton list containing its FQN while each ` ` FlatParameter ` ` maps list its original parameter FQNs which may have length greater than one All FQNs prefixed starting ` ` model ` ` In case where FSDP applied ` ` use_orig_params=True ` ` there should no ` ` FlatParameter ` ` s registered model s modules mapping will only contain mappings ` ` nn Parameter ` ` s singleton FQN lists It only case where FSDP applied ` ` use_orig_params=False ` ` where ` ` FlatParameter ` ` will registered place original parameters there will mappings each ` ` FlatParameter ` ` lists FQNs corresponding original parameters Args model torch nn Module Root module which may may ` FullyShardedDataParallel ` instance dedup_shared_params bool For shared parameters ` ` True ` ` only includes FQNs corresponding first encounter shared parameter module traversal ` ` False ` ` then includes FQNs across all encounters Default ` ` True ` ` module_fn module prefix tree_level param_to_fqns param_name param _named_parameters_with_duplicates module recurse=False local_fqns = param _fqns isinstance param flat_param_file FlatParameter param_name prefixed ` module ` global_fqns = clean_tensor_name prefix + name name local_fqns prefixed top level ` model ` i e including ` prefix ` is_shared_param = param param_to_fqns is_shared_param param_to_fqns param = global_fqns isinstance param flat_param_file FlatParameter DMP overwrites ` named_parameters ` skip advance next child module wrapped_module e g _dmp_wrapped_module _fsdp_wrapped_module When user calls ` named_child ` traverse module recursively calls ` named_parameters ` ` recurse=False ` parameters will traversed more than once This hack specified designed DMP + FSDP We overwrite flat_parameters traversal result only obtain last one which happens correct one TODO Remove hack once DMP + FSDP supported warnings warn FlatParameter being traversed more than once This case should only happen when using DistributedModelParallel FullyShardedDataParallel stacklevel= param_to_fqns param = global_fqns dedup_shared_params param_to_fqns param extend global_fqns return_fn param_to_fqns param_to_fqns param_to_unflat_param_names dict torch nn Parameter list str = _apply_to_modules model module_fn return_fn key key _ _named_parameters_with_duplicates model param_to_unflat_param_names no_type_check _log_post_backward_hook state _FSDPState handle FlatParamHandle logger logging Logger - None Under TORCH_DISTRIBUTED_DEBUG=INFO log module names hook fires Below logging module names post-bwd hook fires can help debug certain cases where hooks don t fire such under certain activation checkpoint configs state _use_orig_params handle _debug_level == dist DebugLevel INFO param_fqns = _get_handle_fqns_from_root state handle logger warning FSDP firing post-backward hooks parameters s param_fqns no_type_check _get_handle_fqns_from_root state _FSDPState handle FlatParamHandle - Optional list str handle None None param_to_fqn = state _exec_order_data param_to_fqn handle_params = handle flat_param _params only populated use_orig_params param_fqns = chain from_iterable param_to_fqn p p handle_params param_fqns _apply_to_modules root_module torch nn Module module_fn Callable return_fn Callable filter_fqns Optional list str = None args kwargs Performs pre-order traversal modules hierarchy rooted ` ` root_module ` ` applying ` ` module_fn ` ` each module finally returning value using ` ` return_fn ` ` The traversal constructs full module prefix name e g module submodule just like model state dict makes available ` ` module_fn ` ` ` ` filter_fqns ` ` used because some module may have its own prefix similar ` ` FullyShardedDataParallel ` ` ` ` named_parameters ` ` overwritten remove prefix f module torch nn Module prefix str tree_level int args kwargs Call module function before recursing over children pre-order module_fn module prefix tree_level args kwargs submodule_name submodule module named_children submodule None continue new_prefix = prefix + submodule_name + new_tree_level = tree_level + filter_fqns None fqn filter_fqns fqn startswith new_prefix break DMP s named_parameter will mess up traversal ` ` named_children ` ` + ` named_parameter recurse=False ` ` This hack must make traversal work TODO Remove hack once DMP + FSDP supported It turns out recursive wrapping may trigger well submodule_name == _fsdp_wrapped_module submodule_name == _dmp_wrapped_module new_prefix = prefix submodule_name == module new_prefix = prefix f submodule new_prefix new_tree_level args kwargs f root_module args kwargs return_fn args kwargs no_type_check _assert_in_training_states state _FSDPState training_states list TrainingState - None Asserts FSDP states ` ` _training_states ` ` Raise ` ValueError ` instead using ` assert ` ensure these logical assertions run even ` assert ` s disabled state training_state training_states msg = f expected states training_states current state f state training_state Print error rank case called backward pass state rank == isinstance state nn Module print f Asserting FSDP instance state print f ERROR msg traceback print_stack raise ValueError msg _get_root_modules modules set nn Module - set nn Module Returns Set nn Module The subset ` ` modules ` ` root modules i e parent-less respect modules set itself In other words these modules ` ` modules ` ` child any other module ` ` modules ` ` root_modules set nn Module = set module_to_submodules = module set module modules module modules candidate_module modules is_root_module = True module submodules module_to_submodules items is_child_module = candidate_module module candidate_module submodules is_child_module is_root_module = False break is_root_module root_modules add candidate_module root_modules _override_module_mixed_precision root torch nn Module module_classes_to_override Iterable type nn Module wrap_override_dict dict str Any = mixed_precision None noqa B - set type nn Module module_classes_to_override = tuple set module_classes_to_override Return set actually overridden module classes overridden_module_classes set type nn Module = set mod root modules isinstance mod module_classes_to_override overridden_module_classes add type mod mod _wrap_overrides = wrap_override_dict type ignore assignment TODO We need run mixed precision ignored module fp ensure subsequent modules may possibly running mixed precision still receive appropriate precision inputs without user having adjust mixed precision config too much As result we attach pre post forward hooks up down cast We should revisit design cast_fn dtype torch dtype module nn Module x torch Tensor - torch Tensor torch is_floating_point x x dtype == dtype x _MODULE_TO_INP_DTYPE module = x dtype x dtype forward_pre_hook module args _apply_to_tensors partial cast_fn torch float module args forward_post_hook module args output NOTE If forward did have any floating-point tensors then dtype will set module we do upcast dtype module _MODULE_TO_INP_DTYPE old_dtype = _MODULE_TO_INP_DTYPE module _apply_to_tensors partial cast_fn old_dtype module output We intentionally append both these hooks so they run after all other hooks mod register_forward_pre_hook forward_pre_hook prepend=False mod register_forward_hook forward_post_hook prepend=False overridden_module_classes _no_dispatch_record_stream tensor torch Tensor stream torch Stream - None FIXME record_stream doesn t work non-cuda mtia xpu tensors tensor device type cuda mtia xpu torch _C _get_privateuse _backend_name torch distributed _functional_collectives is_torchdynamo_compiling ezyang The no_dispatch added https github com pytorch pytorch pull cc fegin Looking over PR looks like because we don t actually support Stream arguments torch dispatch so just chokes If Dynamo able answer there any torch dispatch modes active should answer False better version would just check there any modes before disabling dispatch TODO voz Extend dynamo util answer above unify codepaths here tensor record_stream stream no_dispatch tensor record_stream stream