mypy allow-untyped-defs logging torch torch utils _pytree pytree torch _inductor utils is_symbolic torch utils _ordered_set OrderedSet config ir virtualized V log = logging getLogger __name__ NOTE lowering-time collective optimization In collective communication libraries such NCCL every rank maintains communication buffers remotely accessible some peers Depending underlying transport remote accessibility may established via mechanisms such ib_reg_mr CUDA P P CUDA multicast Typically these buffers private communication library default communication ops copy user data out these buffers To prevent these copies optimization commonly known user buffer registration can employed This allows direct establishment remote accessibility user buffers eliminating need copying However optimization introduces stringent usage requirements which typically hard satisfy without being intrusive user code - Establishing remote accessibility expensive often done ahead time In such implementations all ranks must agree set allocations used every collective op Failing meet requirement can lead runtime errors even silent correctness issues - Even collective communication library supports gracefully falling back unregistered implementations fallback mechanism would nullify optimization - Some communication mechanisms impose stricter requirements than others For example CUDA s multicast + multi-mem instructions require all ranks agree only allocations used every collective also offsets within these allocations To support all different mechanisms optimal results we aim satisfy strictest requirement family optimizations - we ensures every collective op invocation guaranteed operate same allocation same offset every iteration For eligible collective ops we identify communication buffers lowering time optionally choose lower op different kernel ommunication libraries like NCCL handle both registered non-registered buffers transparently within same op though some may require different ops different cases Later codegen will perform persistent allocation satisfy aforementioned constraints optionally perform buffer planning optimize overall memory usage can_realize_as_comm_buffer x ir TensorBox comm_buffer_type ir CommBufferType - bool Check input can realized comm buffer specified ` comm_buffer_type ` data = _get_data x isinstance data ir Loops True layout = data get_output_spec isinstance layout ir CommBufferLayout True isinstance layout ir FlexibleLayout is_symbolic data get_numel True False realize_as_comm_buffer x ir TensorBox comm_buffer_type ir CommBufferType group_name str - None Realize input comm buffer specified ` comm_buffer_type ` Specifically realizes underlying buffer s still unrealized changes layout buffer ` ir CommBufferLayout ` x realize buffer = _get_data x assert isinstance buffer ir Buffer layout = buffer get_output_spec isinstance layout ir CommBufferLayout isinstance layout ir FlexibleLayout raise AssertionError A buffer can only realized comm buffer f has ` FlexibleLayout ` got layout is_symbolic buffer get_numel raise AssertionError A buffer symbolic shape cannot converted f comm buffer got layout buffer layout = ir CommBufferLayout layout=layout comm_buffer_type=comm_buffer_type group_name=group_name _get_data x ir TensorBox - ir IRNode isinstance x data ir BaseView TensorBox - View - StorageBox - IRNode node = x data unwrap_view assert isinstance node ir BaseView ir MutableBox node data isinstance x data ir StorageBox TensorBox - StorageBox - IRNode x data data raise AssertionError Expect data attr ` TensorBox ` either f ` ir BaseView ` ` ir StorageBox ` got x data _bufs_to_skip_wait = OrderedSet tuple int str mark_as_skip_wait x ir IRNode - None If non-blocking collective lowered blocking collective wait node original graph becomes useless we can skip lowering _bufs_to_skip_wait add id V graph x get_name should_skip_wait x ir IRNode - bool id V graph x get_name _bufs_to_skip_wait _should_lower_as_one_shot_all_reduce inp ir TensorBox reduce_op str group_name str torch distributed _symmetric_memory is_symm_mem_enabled_for_group inp_size = inp get_numel inp get_dtype itemsize config _collective auto_select is_symm_mem_enabled_for_group group_name can_realize_as_comm_buffer inp ir CommBufferType SYMM_MEM reduce_op == sum inp_size = config _collective one_shot_all_reduce_threshold_bytes _one_shot_all_reduce inp ir TensorBox reduce_op group_name realize_as_comm_buffer inp ir CommBufferType SYMM_MEM group_name pytree tree_map ir TensorBox create ir FallbackKernel create torch ops symm_mem one_shot_all_reduce default inp reduce_op group_name register_comm_lowerings try torch ops _c d_functional all_reduce except AttributeError log info Inductor support distributed collectives depends building torch distributed lowering add_layout_constraint clone constrain_to_fx_strides copy_ register_lowering register_comm_lowering fn add_layout_constraint fn constrain_to_fx_strides register_lowering fn c d = torch ops _c d_functional register_comm_lowering c d all_reduce type ignore misc _all_reduce inp ir TensorBox reduce_op str group_name str - ir TensorBox _should_lower_as_one_shot_all_reduce inp reduce_op group_name _one_shot_all_reduce inp reduce_op group_name Lower c d all_reduce_ inp = clone inp config reorder_for_compute_comm_overlap The horizontal fusion clone often severely delays scheduling all_reduce_ node Horizontally fusing clone can almost never out-perform scheduling all_reduce_ earlier Also most cases clone eliminated via in-place reuse Therefore we tell scheduler fuse inp realize V graph no_fuse_buffer_names add inp get_name pyrefly ignore bad-assignment inp = ir ExternKernel require_contiguous inp Because we lowering inplace c d all_reduce_ we should generate _AllReduce_Kernel instead _AllReduceKernel ir _AllReduce_Kernel create_inplace c d all_reduce_ default inp type ignore arg-type reduce_op group_name type ignore arg-type inp type ignore return-value register_comm_lowering c d all_reduce_ type ignore misc _all_reduce_ inp ir TensorBox reduce_op str group_name str - ir TensorBox _should_lower_as_one_shot_all_reduce inp reduce_op group_name ret = copy_ inp _one_shot_all_reduce inp reduce_op group_name mark_as_skip_wait ret inp Lower c d all_reduce_ pyrefly ignore bad-assignment inp = ir ExternKernel require_contiguous inp ir _AllReduce_Kernel create_inplace c d all_reduce_ default inp type ignore arg-type reduce_op group_name type ignore arg-type inp type ignore return-value register_comm_lowering c d all_reduce_coalesced _all_reduce_coalesced inputs reduce_op group_name inputs = clone inp inp inputs ir _CollectiveKernel create_inplace c d all_reduce_coalesced_ default inputs reduce_op group_name inputs register_comm_lowering c d all_reduce_coalesced_ _all_reduce_coalesced_ inputs reduce_op group_name ir _CollectiveKernel create_inplace c d all_reduce_coalesced_ default inputs reduce_op group_name inputs _create_out_of_place kernel inputs args - ir IRNode node = ir _CollectiveKernel create_out_of_place kernel inputs args assert isinstance node ir IRNode ir TensorBox create node register_comm_lowering c d all_gather_into_tensor _all_gather_into_tensor inp group_size group_name _create_out_of_place c d all_gather_into_tensor default inp group_size group_name register_comm_lowering c d all_gather_into_tensor_coalesced _all_gather_into_tensor_coalesced inputs group_size group_name pytree tree_map ir TensorBox create ir _CollectiveKernel create_out_of_place c d all_gather_into_tensor_coalesced default inputs group_size group_name register_comm_lowering c d all_gather_into_tensor_out _all_gather_into_tensor_out inp group_size group_name out ir _CollectiveKernel create_inplace c d all_gather_into_tensor_out default inp group_size group_name out=out out register_comm_lowering c d reduce_scatter_tensor _reduce_scatter_tensor inp reduce_op group_size group_name _create_out_of_place c d reduce_scatter_tensor default inp reduce_op group_size group_name register_comm_lowering c d reduce_scatter_tensor_coalesced _reduce_scatter_tensor_coalesced inputs reduce_op group_size group_name pytree tree_map ir TensorBox create ir _CollectiveKernel create_out_of_place c d reduce_scatter_tensor_coalesced default inputs reduce_op group_size group_name register_comm_lowering c d all_to_all_single _all_to_all_single inp output_split_sizes input_split_sizes group_name _create_out_of_place c d all_to_all_single default inp output_split_sizes input_split_sizes group_name register_comm_lowering c d broadcast _broadcast inp src group_name inp = clone inp ir _CollectiveKernel create_inplace c d broadcast_ default inp src group_name inp register_comm_lowering c d broadcast_ _broadcast_ inp src group_name ir _CollectiveKernel create_inplace c d broadcast_ default inp src group_name inp register_comm_lowering torch ops _dtensor shard_dim_alltoall _shard_dim_alltoall inp gather_dim shard_dim group_name _create_out_of_place torch ops _dtensor shard_dim_alltoall default inp gather_dim shard_dim group_name register_comm_lowering c d wait_tensor _wait_tensor inp should_skip_wait inp inp ir _WaitKernel create_wait c d wait_tensor default inp inp