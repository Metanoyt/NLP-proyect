mypy ignore-errors functools itertools math operator sys collections abc Callable functools reduce typing Optional Union torch torch _custom_op torch _logging torch _prims_common utils torch _dispatch python no_python_dispatcher torch _ops OpOverload torch _prims_common canonicalize_dim elementwise_dtypes ELEMENTWISE_TYPE_PROMOTION_KIND is_boolean_dtype is_contiguous is_contiguous_for_memory_format_or_false is_contiguous_or_false is_float_dtype is_integer_dtype make_contiguous_strides_for torch _subclasses fake_tensor DataDependentOutputException DynamicOutputShapeException FakeTensor in_kernel_invocation_manager run_fallback_kernel UnsupportedOperatorException torch fx operator_schemas normalize_function torch utils _stats count_label pytree = torch utils _pytree __all__ = op_implementations_checks get_fast_op_impls stride_incorrect_op has_meta op_implementations_dict = op_implementations_checks = aten = torch _ops ops aten ordered_set items dict fromkeys items True This function indicates backend device supports non-contiguous tensors is_noncontiguous_supported device device type = hpu _like_tensor_constructors = ordered_set aten empty_like default aten empty_like out aten full_like default aten full_like out aten ones_like default aten ones_like out aten rand_like default aten rand_like out aten randn_like default aten randn_like out aten randint_like default aten randint_like Tensor aten randint_like Tensor_out aten randint_like out aten randint_like low_dtype aten randint_like low_dtype_out aten zeros_like default aten zeros_like out aten new_empty default aten new_empty out aten new_empty_strided default aten new_empty_strided out aten new_full default aten new_full out aten new_zeros default aten new_zeros out aten new_ones default aten new_ones out _device_not_kwarg_ops = ordered_set aten _resize_output_ default aten _nested_tensor_from_tensor_list default aten _nested_tensor_from_tensor_list out aten pin_memory default aten device aten prim_Device aten is_pinned default aten _pin_memory default aten _pin_memory out aten _resize_output default aten _resize_output out op never actually used _non_kwarg_device_constructors = aten _list_to_tensor contains_tensor_types type tensor_type = torch _C TensorType get type isSubtypeOf tensor_type any contains_tensor_types e e type containedTypes functools cache _is_tensor_constructor func OpOverload assert isinstance func OpOverload schema = func _schema any contains_tensor_types arg type arg schema arguments False TODO no real reason restrict multiple outputs len schema returns == schema returns type torch _C TensorType get register_op_impl run_impl_check Union Callable OpOverload bool OpOverload impl_decorator op_impl isinstance run_impl_check OpOverload assert run_impl_check op_implementations_dict f duplicate registration run_impl_check op_implementations_dict run_impl_check = op_impl isinstance run_impl_check list tuple op run_impl_check register_op_impl op op_impl assert callable run_impl_check op_implementations_checks append run_impl_check op_impl op_impl impl_decorator _is_op_registered_to_fake_rule op op op_implementations_dict _deregister_op_impl op op_implementations_dict pop op None check impl op_implementations_checks check op op_implementations_checks remove check impl break register_op_impl op_implementations_dict __contains__ dispatch_to_op_implementations_dict fake_mode func args kwargs op_implementations_dict func fake_mode func args kwargs register_op_impl _is_tensor_constructor register_op_impl _like_tensor_constructors constructors fake_mode func args kwargs assert func _non_kwarg_device_constructors _ new_kwargs = normalize_function func args=args kwargs=kwargs normalize_to_only_use_kwargs=True names kwargs raise UnsupportedOperatorException torch compile doesn t support named tensors func _like_tensor_constructors default_device = new_kwargs input device TODO file issue args = new_kwargs pop input cpu default device none specified default_device = torch device cpu args = out_device = new_kwargs pop device None out_device = out_device out_device None default_device new_kwargs device = torch device meta _like constructors have fake tensor inputs maybe causes non-like fail hmmm in_kernel_invocation_manager fake_mode r = func args new_kwargs FakeTensor fake_mode r out_device register_op_impl aten is_pinned default non_kwarg_is_pinned fake_mode func args kwargs _ new_kwargs = normalize_function func args kwargs normalize_to_only_use_kwargs=True inp = new_kwargs pop input we ll ignore device argument because deprecated actually used is_pinned in_kernel_invocation_manager fake_mode r = func inp r register_op_impl aten prim_Device register_op_impl aten device non_kwarg_to fake_mode func args kwargs _ new_kwargs = normalize_function func args kwargs normalize_to_only_use_kwargs=True input_device = new_kwargs device out_device = input_device input_device new_kwargs input device new_kwargs device = torch device meta inp = new_kwargs pop input in_kernel_invocation_manager fake_mode r = func inp new_kwargs TODO I think does wrong thing r inp fake_mode fake_tensor_converter from_meta_and_device fake_mode r out_device stride_incorrect_op op False These operators have meta implementations incorrect strides register_op_impl stride_incorrect_op wordaround_stride_incorrect_op fake_mode func args kwargs This workaround meta implementations incorrect strides is_symbolic x isinstance x FakeTensor x _has_symbolic_sizes_strides isinstance x torch SymInt torch SymFloat torch SymBool True False For static shapes we can fall back eager real strides fake_mode allow_fallback_kernels require_dynamic = any is_symbolic x x itertools chain args kwargs values require_dynamic flat_args args_spec = pytree tree_flatten args kwargs run_fallback_kernel fake_mode func flat_args args_spec None raise UnsupportedOperatorException func Dont default default device handling since device ` the_template ` ignored register_op_impl aten resize_as_ default resize_as_ fake_mode func args kwargs in_kernel_invocation_manager fake_mode func args kwargs register_op_impl aten _sparse_coo_tensor_with_dims_and_tensors default _sparse_coo_tensor_with_dims_and_tensors fake_mode func args kwargs TODO remove me constructors fake_mode func args kwargs index Tensor data-dependent only some conditions register_op_impl lambda func torch Tag dynamic_output_shape func tags func aten index Tensor aten nonzero default aten repeat_interleave Tensor dyn_shape fake_mode func args kwargs raise DynamicOutputShapeException func _unique fake_mode func arg dim sorted=True return_inverse=False return_counts=False unique_consecutive=False fake_mode shape_env None fake_mode shape_env allow_dynamic_output_shape_ops Without symints symfloats cannot handle raise DynamicOutputShapeException func nnz = arg unique_consecutive_memo unique_consecutive arg unique_memo Do use memo unique_dim dim None nnz None Avoid importing sympy module level torch fx experimental symbolic_shapes _constrain_range_for_size has_free_symbols has_free_symbols arg numel arg numel == If numel zero then output size must zero In case we must allocate unbacked SymInt because we do will immediately get refined zero will inconsistent size oblivious tests which will continue claim unbacked symint cannot equal zero We could also unconditionally allocate unbacked SymInt refine its range seems more precise nnz = nnz = fake_mode shape_env create_unbacked_symint maxval = sys maxsize - numel = arg numel dim None arg size dim has_free_symbols numel maxval = int numel _constrain_range_for_size nnz max=maxval dim None unique_consecutive arg unique_consecutive_memo = nnz arg unique_memo = nnz dim None ret = arg new_empty nnz ret = arg new_empty arg shape dim nnz arg shape dim + return_if_dim_and_cpu = dim None arg fake_device == torch device cpu return_inverse return_if_dim_and_cpu inverse = arg new_empty arg shape dim None arg shape dim inverse = arg new_empty ret append inverse return_counts return_if_dim_and_cpu counts = arg new_empty ret shape dim None ret shape dim counts = arg new_empty ret append counts tuple ret register_op_impl aten _unique default unique fake_mode func arg sorted=True return_inverse=False return_counts=False _unique fake_mode func arg None sorted return_inverse return_counts register_op_impl aten select int meta_select fake_mode func dim index torch fx experimental symbolic_shapes guard_or_false is_sparse NotImplemented ndim = dim torch _check_index ndim = lambda select cannot applied -dim tensor dim = dim dim = dim + ndim size = size dim new_size = list size new_stride = list stride new_storage_offset = None guard_or_false index = new_storage_offset = storage_offset + index new_stride dim guard_or_false index new_storage_offset = storage_offset + index + size new_stride dim new_storage_offset None fake_mode shape_env None fake_mode shape_env allow_scalar_outputs fake_mode allow_scalar_outputs raise DataDependentOutputException func index data-dependent we do know which index we accessing could index index+size we assign new data-dependent symbol storage offset new_storage_offset = fake_mode shape_env create_unbacked_symint del new_size dim del new_stride dim assert new_storage_offset None as_strided new_size new_stride new_storage_offset register_op_impl aten unique_dim default unique_dim fake_mode func arg dim sorted=True return_inverse=False return_counts=False _unique fake_mode func arg normalize dim non-negative dim dim = dim max arg ndim sorted return_inverse return_counts register_op_impl aten unique_consecutive default _ fake_mode func arg return_inverse=False return_counts=False dim=None _unique fake_mode func arg dim False return_inverse return_counts unique_consecutive=True This function python match computeStride_impl TensorUtils cpp _compute_stride old_shape old_stride new_shape size_oblivious=False torch fx experimental symbolic_shapes guard_or_false guard_or_true sym_eq maybe_guard_or_false x size_oblivious guard_or_false x x maybe_guard_or_true x size_oblivious guard_or_true x x len old_shape == len new_shape numel = reduce operator mul old_shape zero_numel = maybe_guard_or_false numel == zero_numel maybe_guard_or_false sym_eq old_shape new_shape old_stride new_stride = len new_shape zero_numel view_d range len new_shape - - - view_d == len new_shape - new_stride view_d = new_stride view_d = max new_shape view_d + new_stride view_d + new_stride view_d = len new_shape - chunk_base_stride = old_stride - tensor_numel = view_numel = tensor_d range len old_shape - - - tensor_numel = old_shape tensor_d tensor_d == maybe_guard_or_true old_shape tensor_d - = maybe_guard_or_true old_stride tensor_d - = tensor_numel chunk_base_stride while view_d = maybe_guard_or_true view_numel tensor_numel maybe_guard_or_false new_shape view_d == new_stride view_d = view_numel chunk_base_stride view_numel = new_shape view_d view_d -= maybe_guard_or_true view_numel = tensor_numel None tensor_d chunk_base_stride = old_stride tensor_d - tensor_numel = view_numel = view_d = - None new_stride _view_has_unbacked_input shape torch fx experimental symbolic_shapes has_hint shape = utils extract_shape_from_varargs shape validate=False any has_hint s s size any has_hint s s stride any has_hint s s shape _view_unbacked_meta shape size_oblivious_enabled=True torch _prims view_of torch fx experimental symbolic_shapes guard_or_false sym_eq Creates valid shape shape = utils extract_shape_from_varargs shape validate=False Reshape may given shape - length This indicates dimension s length should inferred shape = utils infer_size shape numel Special-cases reshaping zero dim tensors ndim == _a = length shape torch _check length == _a = torch _refs unsqueeze _a - _a view_of _a Special-cases reshaping zero dim tensors len shape == _a = length shape torch _check length == _a = torch _refs squeeze _a - _a view_of _a shape_numel = reduce operator mul shape torch _check numel == shape_numel lambda f Could reshape tensor shape shape tensor shape shape len shape == len shape guard_or_false sym_eq shape shape view_of is_contiguous_or_false size_oblivious_enabled is_contiguous strides = make_contiguous_strides_for shape as_strided shape strides new_strides = _compute_stride size stride shape size_oblivious=size_oblivious_enabled new_strides None as_strided shape new_strides If we fail do size oblivious view backed_size_oblivious then we redo everything looking hints guarding instead failing Also expression has unbacked symbols then we run again size_oblivious_enabled=False throw data dependent error size_oblivious_enabled torch fx experimental _config backed_size_oblivious _view_has_unbacked_input shape _view_unbacked_meta shape size_oblivious_enabled=False msg = f Cannot view tensor shape shape strides stride tensor shape shape raise ValueError msg register_op_impl aten _reshape_copy default _reshape_copy fake_mode func shape is_sparse is_mkldnn NotImplemented shape = utils infer_size shape numel is_contiguous_or_false view = _view_meta fake_mode func shape view clone memory_format=torch contiguous_format _view_meta fake_mode func clone memory_format=torch contiguous_format shape register_op_impl aten view default register_op_impl aten _unsafe_view default _view_meta fake_mode func shape torch fx experimental _config backed_size_oblivious _view_has_unbacked_input shape _view_unbacked_meta shape torch _refs _reshape_view_helper shape allow_copy=False register_op_impl aten view_copy default _view_meta_copy fake_mode func shape out=None result = _view_meta fake_mode func shape out None result pytree tree_map lambda x x clone memory_format=torch contiguous_format result register_op_impl aten repeat_interleave Tensor repeat_interleave_tensor fake_mode func repeats output_size=None output_size None fake_mode shape_env None fake_mode shape_env allow_dynamic_output_shape_ops raise DynamicOutputShapeException func output_size = fake_mode shape_env create_unbacked_symint Avoid importing sympy module level torch fx experimental symbolic_shapes _constrain_range_for_size _constrain_range_for_size output_size TODO consider memo repeats new_empty output_size register_op_impl torch ops aten item default register_op_impl torch ops aten _local_scalar_dense default local_scalar_dense fake_mode func arg r = arg item_memo None r fake_mode shape_env None fake_mode shape_env allow_scalar_outputs fake_mode allow_scalar_outputs Without symints symfloats cannot handle raise DataDependentOutputException func is_float_dtype arg dtype r = fake_mode shape_env create_unbacked_symfloat is_integer_dtype arg dtype r = fake_mode shape_env create_unbacked_symint is_boolean_dtype arg dtype r = fake_mode shape_env create_unbacked_symbool raise NotImplementedError f local_scalar_dense item NYI arg dtype arg item_memo = r r register_op_impl torch ops aten nonzero_numpy default nonzero_numpy fake_mode func arg torch ops aten nonzero default arg unbind register_op_impl torch ops aten nonzero default nonzero fake_mode func arg fake_mode shape_env None fake_mode shape_env allow_dynamic_output_shape_ops Without symints symfloats cannot handle raise DynamicOutputShapeException func nnz = arg nonzero_memo None Avoid importing sympy module level torch fx experimental symbolic_shapes _constrain_range_for_size has_free_symbols torch utils _sympy numbers IntInfinity torch utils _sympy value_ranges bound_sympy has_free_symbols arg numel arg numel == If numel zero then output size must zero In case we must allocate unbacked SymInt because we do will immediately get refined zero will inconsistent size oblivious tests which will continue claim unbacked symint cannot equal zero We could also unconditionally allocate unbacked SymInt refine its range seems more precise nnz = nnz = fake_mode shape_env create_unbacked_symint maxval = sys maxsize - has_free_symbols arg numel maxval = int arg numel prod_node = math prod arg shape node prod_range = bound_sympy prod_node expr prod_node shape_env var_to_range isinstance prod_range upper IntInfinity maxval = sys maxsize - maxval = prod_range upper _constrain_range_for_size nnz max=maxval arg nonzero_memo = nnz arg new_empty_strided nnz arg dim nnz dtype=torch int register_op_impl torch ops aten _padded_dense_to_jagged_forward default _padded_dense_to_jagged_forward fake_mode func padded offsets total_L=None only one jagged dim supported now assert len offsets == total_L fake_mode shape_env None fake_mode shape_env allow_dynamic_output_shape_ops Without symints symfloats cannot handle raise DynamicOutputShapeException func total_L = fake_mode shape_env create_unbacked_symint maxval = sys maxsize - Avoid importing sympy module level torch fx experimental symbolic_shapes _constrain_range_for_size has_free_symbols has_free_symbols padded numel maxval = int padded numel _constrain_range_for_size total_L min= max=maxval output_shape = total_L padded shape padded new_empty output_shape _compute_slice_index size index torch fx experimental symbolic_shapes guard_or_false sym_and guard_or_false sym_and index = index = size index guard_or_false sym_and index index = -size index + size guard_or_false index -size guard_or_false index size size None register_op_impl torch ops aten slice Tensor slice_forward fake_mode func dim int = start Optional int = None end Optional int = None step int = torch fx experimental symbolic_shapes guard_or_false statically_known_true shape_env = fake_mode shape_env ndim = dim ndim == raise RuntimeError slice cannot applied -dim tensor dim = canonicalize_dim dim dim sizes = list size strides = list stride step = raise RuntimeError slice step must positive start end start_index = start None _compute_slice_index sizes dim start end_index = sizes dim statically_known_true end == sys maxsize end None _compute_slice_index sizes dim end size new_size = None start_index None end_index None guard_or_false end_index = start_index new_size = end_index - start_index + step - step guard_or_false start_index = end_index new_size = create unbacked case unknown new_size None new_size = shape_env create_unbacked_symint torch _check new_size = torch _check new_size = sizes dim stride new_stride = strides dim step storage offset start_index None storage_offset = storage_offset + start_index strides dim storage_offset = shape_env create_unbacked_symint torch _check storage_offset = sizes dim = new_size strides dim = new_stride is_quantized raise NotImplementedError Slice decomposition quantized tensors aren t implemented as_strided sizes strides storage_offset register_op_impl torch ops aten masked_select default masked_select fake_mode func mask fake_mode shape_env None fake_mode shape_env allow_dynamic_output_shape_ops Without symints symfloats cannot handle raise DynamicOutputShapeException func nnz = fake_mode shape_env create_unbacked_symint see nonzero commentary maxval = sys maxsize - Avoid importing sympy module level torch fx experimental symbolic_shapes _constrain_range_for_size has_free_symbols torch utils _sympy numbers IntInfinity torch utils _sympy value_ranges bound_sympy If num elements expressed symbolically calculate concrete value based upper bounds Otherwise we can set max val directly has_free_symbols numel num_elements = int numel prod_node = math prod shape node prod_range = bound_sympy prod_node expr prod_node shape_env var_to_range isinstance prod_range upper IntInfinity num_elements = sys maxsize - num_elements = prod_range upper num_elements maxval = num_elements _constrain_range_for_size nnz max=maxval new_empty nnz register_op_impl torch ops aten _assert_tensor_metadata default assert_tensor_metadata fake_mode func t sizes=None strides=None dtype=None device=None layout=None - None sizes None assert t size == sizes f Tensor sizes mismatch Expected sizes Got t size strides None assert t stride == strides f Tensor strides mismatch Expected strides Got t stride dtype None assert t dtype == dtype f Tensor dtype mismatch Expected dtype Got t dtype layout None assert t layout == layout f Tensor layout mismatch Expected layout Got t layout device None assert t device == device f Tensor device mismatch Expected device Got t device NB must ordered after local_scalar_dense register_op_impl lambda func torch Tag data_dependent_output func tags data_dep fake_mode func args kwargs raise DataDependentOutputException func Bool Indices get Expanded Masks See IndexingUtils h expandTensors check_no_bool_index_tensors func indices index indices index None index dtype torch bool torch uint raise DynamicOutputShapeException func run_and_return_new_tensor_of_input_device fake_mode func args kwargs _ new_kwargs = normalize_function func args=args kwargs=kwargs normalize_to_only_use_kwargs=True out_device = new_kwargs input device in_kernel_invocation_manager fake_mode out = func args kwargs is_noncontiguous_supported out_device out = out new_empty out shape out new_kwargs input out copy_ FakeTensor fake_mode out out_device _is_builtin_namespaces = ordered_set aten prims prim is_builtin op op namespace _is_builtin_namespaces has_meta func torch _C _dispatch_has_computed_kernel_for_dispatch_key func name Meta These ` torch _foreach_ ` ops like ` torch _foreach_add ` register_op_impl lambda func is_builtin func func name startswith aten _foreach_ has_meta func foreach_run_and_map_input_device fake_mode func args kwargs tensor_lists = arg arg itertools chain args kwargs values isinstance arg list tuple len arg isinstance arg torch Tensor try in_kernel_invocation_manager fake_mode out_meta = func args kwargs except NotImplementedError NotImplemented out_meta out_meta assert tensor_lists out_fake = i meta_t enumerate out_meta device _ = FakeTensor _find_common_device func tl i tl tensor_lists out_fake append fake_mode fake_tensor_converter from_meta_and_device fake_mode meta_t device out_fake Dont default default device handling Since op can take non-zero sized cpu index tensors cuda register_op_impl aten index Tensor index_tensor fake_mode func args kwargs torch _meta_registrations meta_index_Tensor _ new_kwargs = normalize_function func args=args kwargs=kwargs normalize_to_only_use_kwargs=True out_device = new_kwargs input device ensure nonzero call goes fake tensor fake_mode out = meta_index_Tensor args kwargs out out_device Can take mixed meta non-meta arguments meta registration will roughly do right thing even when given real devices register_op_impl aten _embedding_bag default embedding_bag fake_mode func args kwargs torch _meta_registrations meta_embedding_bag fake_mode meta_embedding_bag args kwargs takes multiple-devices dont default default device handling register_op_impl aten _unsafe_index_put default register_op_impl aten copy default register_op_impl aten copy_ default register_op_impl aten slice_scatter default multi_device_op_default fake_mode func args kwargs run_and_return_new_tensor_of_input_device fake_mode func args kwargs same multi_device_op_default input register_op_impl aten copy out register_op_impl aten slice_scatter out multi_device_op_out fake_mode func args kwargs in_kernel_invocation_manager fake_mode func args kwargs _ new_kwargs = normalize_function func args=args kwargs=kwargs normalize_to_only_use_kwargs=True new_kwargs input register_op_impl aten index_put default register_op_impl aten index_put_ default index_put_impl fake_mode func args kwargs _ new_kwargs = normalize_function func args=args kwargs=kwargs normalize_to_only_use_kwargs=True values = new_kwargs values self_device = new_kwargs input fake_device torch _check self_device == values fake_device values ndim == values numel == lambda f Mismatching func device between self_device values values device out = run_and_return_new_tensor_of_input_device fake_mode func args kwargs func aten index_put_ default new_kwargs input out register_op_impl aten _nested_tensor_from_tensor_list default register_op_impl aten _nested_tensor_from_tensor_list out register_op_impl aten _nested_view_from_buffer default register_op_impl aten _nested_view_from_buffer_copy default nested_tensors_unsupported fake_mode func args kwargs raise UnsupportedOperatorException torch compile does support strided NestedTensor register_op_impl x x _device_not_kwarg_ops x these already registered elsewhere aten is_pinned default aten device aten prim_Device aten _nested_tensor_from_tensor_list default aten _nested_tensor_from_tensor_list out nyi fake_mode func args kwargs assert func _device_not_kwarg_ops f NYI func register_op_impl aten convolution default aten convolution_backward default conv fake_mode func args kwargs _ kwargs = normalize_function func args=args kwargs=kwargs normalize_to_only_use_kwargs=True device = kwargs input fake_device need re-enable mode so tensors report fake device fake_mode input unsqueezed done Convolution cpp we get segfault k = kwargs weight ndim batch = kwargs input shape Avoid importing sympy module level torch fx experimental symbolic_shapes has_hint has_hint batch TODO We can make little more faithful best effort channels last detection only s statically obvious mem_fmt = None func aten convolution default conv_backend = torch _C _select_conv_backend kwargs conv_backend = torch _C _select_conv_backend kwargs input kwargs weight bias=None stride=kwargs stride padding=kwargs padding dilation=kwargs dilation transposed=kwargs transposed output_padding=kwargs output_padding groups=kwargs groups bias_sizes=kwargs bias_sizes Expand d - d Note Avoid expanding before calling _select_conv_backend function handles D expansion internally k == kwargs input is_mkldnn kwargs input is_xpu Note Using input memory_format=contiguous does work kwargs input = kwargs input contiguous unsqueeze kwargs weight = kwargs weight unsqueeze len kwargs stride == kwargs stride insert kwargs padding insert kwargs dilation insert kwargs output_padding insert mem_fmt = torch _C _conv_determine_backend_memory_format kwargs input kwargs weight conv_backend revert d - d k == kwargs input is_mkldnn kwargs input is_xpu kwargs input = kwargs input squeeze kwargs weight = kwargs weight squeeze len kwargs stride == kwargs stride pop kwargs padding pop kwargs dilation pop kwargs output_padding pop convert t mem_fmt t None t mem_fmt None channels last only support d try expand dim then convert back later t dim == mem_fmt == torch channels_last t = t unsqueeze memory_format=mem_fmt squeeze t = t memory_format=mem_fmt FakeTensor fake_mode t device in_kernel_invocation_manager fake_mode out = func kwargs func aten convolution default convert out mem_fmt convert out mem_fmt convert out mem_fmt convert out None register_op_impl torch ops aten bincount default bincount fake_mode func inputs weights=None minlength= fake_mode shape_env None fake_mode shape_env allow_dynamic_output_shape_ops Without symints symfloats cannot handle raise DynamicOutputShapeException func new_size = fake_mode shape_env create_unbacked_symint torch fx experimental symbolic_shapes _constrain_range_for_size _constrain_range_for_size new_size torch _check new_size = minlength inputs new_empty new_size register_op_impl torch ops aten _pack_padded_sequence default _pack_padded_sequence fake_mode func inputs lengths batch_first fake_mode shape_env None fake_mode shape_env allow_dynamic_output_shape_ops Without symints symfloats cannot handle raise DynamicOutputShapeException func new_batch_size = fake_mode shape_env create_unbacked_symint torch fx experimental symbolic_shapes _constrain_range_for_size _constrain_range_for_size new_batch_size batch_first Inputs should have shape batch_size seq_len inputs = inputs transpose res_size = inputs shape packed_data = inputs new_empty res_size batch_size = inputs new_empty new_batch_size packed_data batch_size FAST_OP_IMPLEMENTATIONS = Unlike register_op_impl these don t do slow iteration run_impl_check these run BEFORE decompositions register_fast_op_impl func OpOverload impl_decorator op_impl FAST_OP_IMPLEMENTATIONS func = op_impl op_impl impl_decorator infer_size_impl ExpandUtils infer_size b torch fx experimental symbolic_shapes guard_or_false dimsA = len dimsB = len b ndim = max dimsA dimsB expandedSizes = ndim i range ndim - - - offset = ndim - - i dimA = dimsA - - offset dimB = dimsB - - offset sizeA = dimA dimA = sizeB = b dimB dimB = NB It very important test broadcasting before testing sizeA == sizeB This because broadcasting tests likely statically known particular sizeA sizeB unbacked size-like we will unsoundly assume they never equal sizeA == sizeB test may statically known However once we have established no broadcasting happening sizeA == sizeB now expect_true we can defer runtime assert works because Python will terminal expression statement as-is without bool ing case we d need write using torch sym_or something like torch _check guard_or_false sizeA == guard_or_false sizeB == sizeA == sizeB lambda f The size tensor sizeA f must match size tensor b sizeB f non-singleton dimension i expandedSizes i = sizeB guard_or_false sizeA == sizeA tuple expandedSizes make_fast_binary_impl slow_ref type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT fast_binary_impl mode args kwargs slow msg count_label f slow msg mode slow_ref args kwargs count_label attempt fast Fast path based off TensorIterator fast path Unfortunately there no way easily deduplicate either TensorIterator C++ implementation which we don t want SymIntify also algorithm here slightly different TensorIterator allow broadcasting nor PrimTorch implementation which does actually implement fast path operands = args compute_shape final_shape = None op operands shape = op shape isinstance op torch Tensor final_shape None final_shape = shape TODO Minor optimization track shapes equal so you can skip equality check below unnecessary final_shape = infer_size final_shape shape assert final_shape None torch fx experimental symbolic_shapes guard_or_false sym_eq Do some extra safety checks see output stride obvious op operands isinstance op torch Tensor len op shape == len final_shape take slow path result determined guard_or_false sym_eq op shape final_shape break we never break loop above we take slow path slow both tensors nontrivially broadcast compute_types cpu = torch device cpu common_device = cpu common_dtype = None has_different_input_dtypes = False op operands isinstance op torch Tensor Use elementwise_dtypes tricky case has_different_input_dtypes = True continue common_device == cpu op device type = cpu common_device = op device Slightly simplified here target_dtype cannot vary common_dtype None common_dtype = op dtype common_dtype = op dtype has_different_input_dtypes = True has_different_input_dtypes compute promotion TODO we don t need compute type _ common_dtype = elementwise_dtypes operands type_promotion_kind=type_promotion_kind check all tensors same device cpu scalars assumed allow current_cpu_scalars_on_non_cpu = max_cpu_scalars_on_non_cpu = hard coded atm op operands isinstance op torch Tensor continue common_device = cpu op dim == op device == cpu current_cpu_scalars_on_non_cpu = max_cpu_scalars_on_non_cpu slow error current_cpu_scalars_on_non_cpu += op device = common_device slow error compute_fast_setup_type definitely_contiguous = True definitely_channels_last = True TODO is_non-overlapping_and_dense bound Python no inplace no out everything defined is_noncontiguous_supported common_device op operands isinstance op torch Tensor continue definitely_contiguous = definitely_contiguous is_contiguous_for_memory_format_or_false op memory_format=torch contiguous_format definitely_channels_last = definitely_channels_last is_contiguous_for_memory_format_or_false op memory_format=torch channels_last definitely_contiguous do contiguous count_label fast is_contiguous FakeTensor mode torch empty final_shape dtype=common_dtype device= meta memory_format=torch contiguous_format device=common_device definitely_channels_last count_label fast channels_last do channels last FakeTensor mode torch empty final_shape dtype=common_dtype device= meta memory_format=torch channels_last device=common_device slow no contiguity match fast_binary_impl disable python dispatcher avoid decomposing detach further proxy_mode should still decompose detach though fast_detach fake_mode x include_real=False no_python_dispatcher in_kernel_invocation_manager fake_mode out = torch ops aten detach default x include_real FakeTensor fake_mode out x device real_tensor=x real_tensor FakeTensor fake_mode out x device functools cache get_fast_op_impls torch _refs register_fast_op_impl torch ops aten add Tensor make_fast_binary_impl torch _refs add register_fast_op_impl torch ops aten sub Tensor make_fast_binary_impl torch _refs sub register_fast_op_impl torch ops aten mul Tensor make_fast_binary_impl torch _refs mul type ignore has-type register_fast_op_impl torch ops aten div Tensor make_fast_binary_impl torch _refs div type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT register_fast_op_impl torch ops aten detach default fast_detach FAST_OP_IMPLEMENTATIONS