mypy allow-untyped-defs collections functools warnings collections abc Callable Iterable itertools product typing Optional Union typing_extensions deprecated torch torch testing pyrefly ignore deprecated torch _vmap_internals _vmap vmap torch overrides is_tensor_like torch types _TensorOrTensors Note ` get_ _jacobian ` functions added here even though we didn t intend make them public since they have been exposed before we added ` __all__ ` we already maintain BC them We should eventually deprecate them remove them ` __all__ ` __all__ = gradcheck gradgradcheck GradcheckError get_numerical_jacobian get_analytical_jacobian get_numerical_jacobian_wrt_specific_input GradcheckError RuntimeError r Error raised func ` gradcheck ` func ` gradgradcheck ` _is_sparse_compressed_tensor obj torch Tensor obj layout torch sparse_csr torch sparse_csc torch sparse_bsr torch sparse_bsc _is_sparse_any_tensor obj torch Tensor _is_sparse_compressed_tensor obj obj layout torch sparse_coo _is_float_or_complex_tensor obj is_tensor_like obj obj is_floating_point obj is_complex _allocate_jacobians_with_inputs input_tensors tuple numel_output - tuple torch Tensor Makes zero-filled tensors inputs If ` numel_output ` None each tensor ` input_tensors ` returns new zero-filled tensor height ` t numel ` width ` numel_output ` Otherwise each tensor returns -d tensor size ` t numel ` Each new tensor will strided have same dtype device those corresponding input out list torch Tensor = t new_zeros t numel numel_output layout=torch strided t input_tensors _is_float_or_complex_tensor t t requires_grad tuple out _allocate_jacobians_with_outputs output_tensors tuple numel_input dtype=None device=None - tuple torch Tensor Makes zero-filled tensors outputs If ` dim ` None each tensor ` output_tensors ` returns new zero-filled tensor height ` dim ` width ` t numel ` Otherwise each tensor returns -d tensor size t numel options = dtype dtype device device layout torch strided out list torch Tensor = t new_zeros numel_input t numel options t output_tensors _is_float_or_complex_tensor t tuple out _iter_tensors x Union torch Tensor Iterable torch Tensor only_requiring_grad bool = False - Iterable torch Tensor is_tensor_like x mypy doesn t narrow type ` x ` torch Tensor x requires_grad only_requiring_grad type ignore union-attr yield x type ignore misc isinstance x collections abc Iterable isinstance x str elem x yield _iter_tensors elem only_requiring_grad _densify x copy sparse x all unspecified elements replaced zero-valued elements isinstance x list tuple type x map _densify x is_tensor_like x x layout torch strided torch _mkldnn type ignore attr-defined no attr _mkldnn x x layout torch sparse_coo device = x device indices_dtype = x _indices dtype tmp = torch ones x shape x sparse_dim dtype=torch int device=device indices = tmp nonzero t dtype=indices_dtype values = torch zeros tmp numel x shape x sparse_dim dtype=x dtype device=device x_coalesced = x detach coalesce x_coalesced numel stride = tmp stride flat_indices = x_coalesced indices mul torch tensor stride dtype=indices_dtype device=device unsqueeze sum values flat_indices = x_coalesced values torch sparse_coo_tensor indices values x shape _coalesced_ True requires_grad_ x requires_grad _is_sparse_compressed_tensor x blocksize = x values shape x layout torch sparse_bsr torch sparse_bsc None compressed_indices = x crow_indices x layout torch sparse_csr torch sparse_bsr x ccol_indices We ll use intermediate sparse COO simplicity r = _densify x detach to_sparse layout=torch sparse_coo to_sparse layout=x layout blocksize=blocksize Check all elements specified also after ` to_sparse ` op dense_numel = r values numel max r values shape batch_numel = compressed_indices numel compressed_indices shape - sparse_numel = r numel max dense_numel batch_numel sparse_numel = r _nnz raise AssertionError f x layout densify failed expected nnz= sparse_numel got r _nnz r requires_grad_ x requires_grad _is_sparse_any_tensor x raise NotImplementedError x layout x _iter_tensor x_tensor Only used slow gradcheck Returns generator yields following elements each iteration tensor same tensor returned across all iterations The tensor same original x_tensor given input - prepared so can modified in-place Depending whether input tensor strided sparse dense returned tensor may may share storage x_tensor tuple indices can used advanced indexing yielded dictionary order flattened index will used index into Jacobian tensor For tensor t size _iter_tensor yields ` x ` ` x ` ` x ` ` x ` where x t data original tensor Perturbing entry x index yields rd column overall Jacobian matrix _is_sparse_any_tensor x_tensor get_stride size dim = len size tmp = stride = dim i reversed range dim stride i = tmp tmp = size i stride x_nnz = x_tensor _nnz x_size = list x_tensor size x_tensor layout torch sparse_coo x_indices = x_tensor _indices t x_values = x_tensor _values x_tensor layout torch sparse_csr x_indices = torch _convert_indices_from_csr_to_coo x_tensor crow_indices x_tensor col_indices t x_values = x_tensor values x_tensor layout torch sparse_csc x_indices = torch _convert_indices_from_csr_to_coo x_tensor ccol_indices x_tensor row_indices transpose=True t x_values = x_tensor values x_tensor layout torch sparse_bsr x_block_values = x_tensor values x_blocksize = x_block_values size x_indices = torch _convert_indices_from_csr_to_coo x_tensor crow_indices x_tensor col_indices repeat_interleave x_blocksize x_blocksize mul_ torch tensor x_blocksize device=x_tensor device reshape add_ torch stack torch where torch ones x_blocksize device=x_tensor device repeat x_nnz t x_values = x_block_values flatten x_nnz = x_values size x_tensor layout torch sparse_bsc x_block_values = x_tensor values x_blocksize = x_block_values size x_indices = torch _convert_indices_from_csr_to_coo x_tensor ccol_indices x_tensor row_indices transpose=True repeat_interleave x_blocksize x_blocksize mul_ torch tensor x_blocksize device=x_tensor device reshape add_ torch stack torch where torch ones x_blocksize device=x_tensor device repeat x_nnz t x_values = x_block_values flatten x_nnz = x_values size raise NotImplementedError f _iter_tensor x_tensor layout input x_stride = get_stride x_size Use data here get around version check x_values = x_values data i range x_nnz x_value = x_values i x_idx product range m m x_values size indices = x_indices i tolist + list x_idx d_idx = sum indices k x_stride k k range len x_size yield x_value x_idx d_idx x_tensor layout == torch _mkldnn type ignore attr-defined d_idx x_idx enumerate product range m m x_tensor size really inefficient without indexing implemented there s really better way than converting back forth x_tensor_dense = x_tensor to_dense yield x_tensor_dense x_idx d_idx Use data here get around version check x_tensor = x_tensor data d_idx x_idx enumerate product range m m x_tensor size yield x_tensor x_idx d_idx _get_numerical_jacobian fn inputs outputs=None target=None eps= e- is_forward_ad=False - list tuple torch Tensor Compute numerical Jacobian ` fn inputs ` respect ` target ` If specified targets input Returns M N Jacobians where N number tensors target require grad M number non-integral outputs Args fn function compute jacobian inputs inputs ` fn ` outputs provide precomputed outputs avoid one extra invocation fn target Tensors wrt whom Jacobians calculated default= ` inputs ` eps magnitude perturbation during finite differencing default= ` e- ` is_forward_ad numerical jacobian computed checked wrt forward AD gradients used error checking only Returns A list M N-tuples tensors Note ` target ` may even part ` input ` ` fn ` so please very careful clone ` target ` jacobians list tuple torch Tensor = outputs None outputs = _as_tuple fn _as_tuple inputs is_forward_ad any o is_complex o outputs raise ValueError Expected output non-complex get_numerical_jacobian no longer supports functions complex outputs target None target = inputs inp_indices = i i enumerate target is_tensor_like requires_grad inp inp_idx zip _iter_tensors target True inp_indices jacobians += get_numerical_jacobian_wrt_specific_input fn inp_idx inputs outputs eps input=inp is_forward_ad=is_forward_ad jacobians deprecated ` get_numerical_jacobian ` part PyTorch s private API meant exposed We deprecating will removed future version PyTorch If you have specific use feature request stable API please file us issue https github com pytorch pytorch issues new category=FutureWarning get_numerical_jacobian fn inputs target=None eps= e- grad_out= Compute numerical Jacobian given fn its inputs This Deprecated API Args fn function compute Jacobian must take inputs tuple inputs input ` fn ` target Tensors wrt whom Jacobians calculated default= ` input ` eps magnitude perturbation during finite differencing default= ` e- ` grad_out defaults Returns A list Jacobians ` fn ` restricted its first output respect each input target provided Note ` target ` may even part ` input ` ` fn ` so please very careful clone ` target ` grad_out = grad_out param only kept backward compatibility reasons raise ValueError Expected grad_out get_numerical_jacobian no longer supports values grad_out = fn_pack_inps inps fn inps jacobians = _get_numerical_jacobian fn_pack_inps inputs None target eps tuple jacobian_for_each_output jacobian_for_each_output jacobians _compute_numerical_gradient fn entry v norm_v nbhd_checks_fn Computes numerical directional derivative finite difference function ` fn ` input ` entry ` perturbed vector ` v ` _is_sparse_compressed_tensor entry sparse compressed tensors don t implement sub add copy_ yet However non-masked semantics context entry v have same sparse indices entry layout = v layout raise AssertionError f Expected entry v have same layout got entry layout v layout entry _nnz = v _nnz raise AssertionError f Expected entry v have same nnz got entry _nnz v _nnz f entry shape entry shape finite differencing can performed values only entry = entry values v = v values we ll detach avoid backward computations sparse tensors have limited support entry = entry detach orig = entry clone entry copy_ orig - v outa = fn entry copy_ orig + v outb = fn entry copy_ orig compute b nbhd_checks_fn b ret = b - norm_v use central difference approx ret detach reshape - tuple compute b b zip outa outb _compute_numerical_jvps_wrt_specific_input jvp_fn delta input_is_complex is_forward_ad=False - list torch Tensor Computing jacobian only works real delta For details algorithm used here refer Section https arxiv org pdf pdf s = fn z where z = x real valued input z = x + yj complex valued input jvps list torch Tensor = ds_dx_tup = jvp_fn delta isinstance delta tuple delta input_is_complex C - R ds_dy_tup = jvp_fn delta j isinstance delta tuple jvp_fn delta j ds_dx ds_dy zip ds_dx_tup ds_dy_tup ds_dx is_complex raise AssertionError Expected ds_dx real-valued complex conjugate wirtinger derivative conj_w_d = ds_dx + ds_dy j jvps append conj_w_d ds_dx ds_dx_tup R - R R - C forward AD case is_forward_ad ds_dx is_complex raise AssertionError Expected ds_dx real-valued complex jvps append ds_dx jvps _combine_jacobian_cols jacobians_cols dict int list torch Tensor outputs input numel - tuple torch Tensor jacobian_cols maps column_idx - output_idx - single column jacobian Tensor we list maps output_idx - full jacobian Tensor jacobians = _allocate_jacobians_with_outputs outputs numel dtype=input dtype input dtype is_complex None i jacobian enumerate jacobians k v jacobians_cols items jacobian k = v i jacobians _prepare_input input torch Tensor maybe_perturbed_input Optional torch Tensor fast_mode=False - torch Tensor Prepares inputs passed into function while including new modified input input layout == torch _mkldnn type ignore attr-defined no attr _mkldnn Convert back mkldnn maybe_perturbed_input None maybe_perturbed_input to_mkldnn input _is_sparse_any_tensor input fast_mode maybe_perturbed_input None entry already cloned version original tensor thus changes entry reflected input maybe_perturbed_input input We cannot use entry input data we want gradgrad work because fn gradgrad case needs compute grad wrt input input _check_outputs_same_dtype_and_shape output output eps idx=None - None Check returned outputs don t have different dtype shape when you perturb input on_index = f index idx idx None output shape = output shape raise AssertionError f Expected ` func ` outputs same shape f when inputs perturbed on_index eps got f shapes output shape output shape output dtype = output dtype raise AssertionError f Expected ` func ` outputs same dtype f when inputs perturbed on_index eps got f dtypes output dtype output dtype get_numerical_jacobian_wrt_specific_input fn input_idx inputs outputs eps input=None is_forward_ad=False - tuple torch Tensor Computes numerical jacobians wrt single input Returns N jacobian tensors where N number outputs We use dictionary jacobian_cols because indices aren t necessarily consecutive sparse inputs When we perturb only single element input tensor time jvp equivalent single col Jacobian matrix fn jacobian_cols dict int list torch Tensor = input = inputs input_idx input None input input requires_grad raise AssertionError Expected input have requires_grad=True x idx d_idx _iter_tensor input wrapped_fn = _with_prepare_inputs fn inputs input_idx x input_to_perturb = x idx nbhd_checks_fn = functools partial _check_outputs_same_dtype_and_shape idx=idx eps=eps jvp_fn = _get_numerical_jvp_fn wrapped_fn input_to_perturb eps nbhd_checks_fn jacobian_cols d_idx = _compute_numerical_jvps_wrt_specific_input jvp_fn eps x is_complex is_forward_ad _combine_jacobian_cols jacobian_cols outputs input input numel _get_analytical_jacobian_forward_ad fn inputs outputs check_grad_dtypes=False all_u=None - tuple tuple torch Tensor Compute analytical Jacobian using forward mode AD ` fn inputs ` using forward mode AD respect ` target ` Return N M Jacobians where N number tensors target require grad M number non-integral outputs Contrary other functions here function requires inputs actually used function The computed value expected wrong function captures inputs side effect instead using passed ones many torch nn tests do Args fn function compute jacobian inputs inputs ` fn ` outputs provide precomputed outputs avoid one extra invocation fn check_grad_dtypes True will check gradient dtype valid all_u optional provided Jacobian will right multiplied vector Returns A tuple M N-tuples tensors To avoid early issues fwAD = torch autograd forward_ad tensor_inputs = tuple i i inputs is_tensor_like i i requires_grad any i is_complex i tensor_inputs raise ValueError Expected inputs non-complex _get_analytical_jacobian_forward_ad all_u jacobians = tuple _allocate_jacobians_with_outputs outputs i tensor_inputs jacobians = tuple _allocate_jacobians_with_outputs outputs i numel i tensor_inputs fwAD dual_level fw_grads = dual_inputs = inp inputs is_tensor_like inp inp requires_grad inp layout == torch _mkldnn type ignore attr-defined raise ValueError MKLDNN inputs support forward AD gradcheck inp = fwAD make_dual inp detach torch zeros_like inp If inp differentiable view dual might tangent given make_dual so read explicitly dual tensor fw_grads append fwAD unpack_dual inp dual_inputs append inp all_u Do full reduction one pass To consistent numerical evaluation we actually compute one reduction per input i fw_grad u enumerate zip fw_grads all_u fw_grad copy_ u view_as fw_grad raw_outputs = _as_tuple fn dual_inputs dual_outputs = filter _is_float_or_complex_tensor raw_outputs index_o d_o enumerate dual_outputs val res = fwAD unpack_dual d_o check_grad_dtypes res None val is_complex = res is_complex raise GradcheckError Forward AD gradient has dtype mismatch Remove extra dimension size corresponding reduced input jacobians i index_o squeeze_ res None jacobians i index_o zero_ jacobians i index_o copy_ res reshape - fw_grad zero_ Reconstruct full Jacobian column column i fw_grad enumerate fw_grads lin_idx grad_idx enumerate product range m m fw_grad size fw_grad grad_idx = raw_outputs = _as_tuple fn dual_inputs dual_outputs = filter _is_float_or_complex_tensor raw_outputs index_o d_o enumerate dual_outputs val res = fwAD unpack_dual d_o check_grad_dtypes res None val is_complex = res is_complex raise GradcheckError Forward AD gradient has dtype mismatch res None jacobians i index_o lin_idx zero_ jacobians i index_o lin_idx copy_ res reshape - fw_grad grad_idx = jacobians _get_input_to_perturb input Prepare input so can modified in-place do certain operations require tensor have strides If fast_mode=False _iter_tensor would handle below cases input layout == torch _mkldnn type ignore attr-defined no attr _mkldnn Convert dense so we can perform operations require strided tensors input_to_perturb = input to_dense _is_sparse_any_tensor input Clone because input may require grad copy_ calls resize_ which allowed data input_to_perturb = input clone input_to_perturb = input data input_to_perturb _with_prepare_inputs fn inputs input_idx input_to_perturb fast_mode=False Wraps ` fn ` so its inputs already supplied wrapped_fn inp = tuple _prepare_input input_to_perturb i == input_idx None fast_mode is_tensor_like i enumerate _as_tuple inputs tuple clone _as_tuple fn inp wrapped_fn _get_numerical_jvp_fn wrapped_fn input_to_perturb eps nbhd_checks_fn Wraps jvp_fn so certain arguments already supplied jvp_fn delta _compute_numerical_gradient wrapped_fn input_to_perturb delta eps nbhd_checks_fn jvp_fn _reshape_tensor_or_tuple u shape We don t need reshape when input corresponding u sparse isinstance u tuple _is_sparse_any_tensor u u reshape shape u reshape shape _is_sparse_any_tensor u u reshape shape u _mul_tensor_or_tuple u k isinstance u tuple k u k u k u _get_numerical_jvp_wrt_specific_input fn input_idx inputs u eps is_forward_ad=False - list torch Tensor input = inputs input_idx input_to_perturb = _get_input_to_perturb input wrapped_fn = _with_prepare_inputs fn inputs input_idx input_to_perturb True nbhd_checks_fn = functools partial _check_outputs_same_dtype_and_shape eps=eps jvp_fn = _get_numerical_jvp_fn wrapped_fn input_to_perturb eps nbhd_checks_fn u = _reshape_tensor_or_tuple u input_to_perturb shape u = _mul_tensor_or_tuple u eps _compute_numerical_jvps_wrt_specific_input jvp_fn u input is_complex is_forward_ad _get_numerical_vJu fn inputs inp_indices func_out all_u all_v eps is_forward_ad Note all_v can also None case function only computes Ju reduced_jacobians list list torch Tensor = inp_idx u zip inp_indices all_u all_Ju = _get_numerical_jvp_wrt_specific_input fn inp_idx inputs u eps is_forward_ad Filter out Ju non floating point outputs filtered_Ju = func_out = _as_tuple func_out len all_Ju = len func_out raise AssertionError f Expected all_Ju func_out have same length f got len all_Ju len func_out Ju output zip all_Ju func_out _is_float_or_complex_tensor output filtered_Ju append Ju TODO handle other Ju pass all_v None jacobian_scalars list torch Tensor = v Ju zip all_v filtered_Ju jacobian_scalars append _dot_with_type_promotion v Ju reduced_jacobians append jacobian_scalars reduced_jacobians append filtered_Ju reduced_jacobians _check_jacobians_equal j j atol Check whether max difference between two Jacobian tensors within some tolerance ` atol ` j _x j _x zip j j j _x numel = j _x - j _x abs max atol False True _stack_and_check_tensors list_of_list_of_tensors inputs numel_outputs - tuple tuple torch Tensor bool bool For ith tensor inner list checks whether has same size dtype ith differentiable input out_jacobians = _allocate_jacobians_with_inputs inputs numel_outputs diff_input_list = list _iter_tensors inputs True correct_grad_sizes = True correct_grad_types = True i tensor_list enumerate list_of_list_of_tensors inp = diff_input_list i out_jacobian = out_jacobians i j tensor enumerate tensor_list tensor None tensor size = inp size correct_grad_sizes = False tensor None tensor dtype = inp dtype correct_grad_types = False tensor None out_jacobian j zero_ dense = tensor to_dense tensor layout = torch strided tensor out_jacobian j numel = dense numel raise AssertionError f Expected out_jacobian column have dense numel elements f got out_jacobian j numel out_jacobian j = dense reshape - out_jacobians correct_grad_sizes correct_grad_types FAILED_NONDET_MSG = \n NOTE If your op relies non-deterministic operations i e listed here https pytorch org docs stable generated torch use_deterministic_algorithms html failure might expected If you adding new operator please file issue then use one workarounds The workaround depends how your test invokes gradcheck gradgradcheck If test - manually invokes gradcheck gradgradcheck then call gradcheck gradgradcheck ` nondet_tol= tol ` keyword argument - OpInfo-based e g test_ops_gradients py then modify OpInfo test have ` gradcheck_nondet_tol= tol ` - Module test e g common_nn py then modify corresponding module_test entry have ` gradcheck_nondet_tol= tol ` _check_analytical_jacobian_attributes inputs output nondet_tol check_grad_dtypes fast_mode=False v=None - tuple torch Tensor This used both fast slow mode - For slow mode vjps i j jth row Jacobian wrt ith input - For fast mode vjps i linear combination rows Jacobian wrt ith input diff_input_list = list _iter_tensors inputs True vjp_fn grad_output torch autograd grad output diff_input_list grad_output retain_graph=True allow_unused=True Compute everything twice check nondeterminism which we call reentrancy fast_mode vjps = _get_analytical_vjps_wrt_specific_output vjp_fn output clone v vjps = _get_analytical_vjps_wrt_specific_output vjp_fn output clone v vjps = _compute_analytical_jacobian_rows vjp_fn output clone vjps = _compute_analytical_jacobian_rows vjp_fn output clone output_numel = output numel fast_mode jacobians types_ok sizes_ok = _stack_and_check_tensors vjps inputs output_numel jacobians _ _ = _stack_and_check_tensors vjps inputs output_numel reentrant = _check_jacobians_equal jacobians jacobians nondet_tol types_ok check_grad_dtypes raise GradcheckError Gradient has dtype mismatch sizes_ok raise GradcheckError Analytical gradient has incorrect size reentrant raise GradcheckError Backward reentrant i e running backward same input grad_output multiple times gives different values although analytical gradient matches numerical gradient f The tolerance nondeterminism nondet_tol + FAILED_NONDET_MSG jacobians _get_analytical_vJu_backward_mode inputs outputs nondet_tol check_grad_dtypes all_v all_u reduced_jacobians list list torch Tensor = output v zip outputs all_v all_vJ = _check_analytical_jacobian_attributes inputs output nondet_tol check_grad_dtypes fast_mode=True v=v jacobian_scalars list torch Tensor = vJ u zip all_vJ all_u Why do we need squeeze here vJ -d tensor so we can reuse error checking logic slow mode vJ = vJ T squeeze vJ is_complex C - R tv = torch view_as_real vJ resolve_conj tr = tv select - ti = tv select - jacobian_scalars append tr dot u + j ti dot u R - R jacobian_scalars append vJ dot u reduced_jacobians append jacobian_scalars reduced_jacobians deprecated ` get_analytical_jacobian ` part PyTorch s private API meant exposed We deprecating will removed future version PyTorch If you have specific use feature request stable API please file us issue https github com pytorch pytorch issues new category=FutureWarning get_analytical_jacobian inputs output nondet_tol= grad_out= Replicates behavior old get_analytical_jacobian before refactor This shares much its code _check_analytical_jacobian_attributes grad_out = grad_out param only kept backward compatibility reasons raise ValueError Expected grad_out get_analytical_jacobian no longer supports values grad_out = output is_complex raise ValueError Expected output non-complex get_analytical_jacobian no longer supports functions complex outputs diff_input_list = list _iter_tensors inputs True vjp_fn grad_output torch autograd grad output diff_input_list grad_output retain_graph=True allow_unused=True Compute everything twice check nondeterminism which we call reentrancy vjps = _compute_analytical_jacobian_rows vjp_fn output clone vjps = _compute_analytical_jacobian_rows vjp_fn output clone output_numel = output numel jacobians types_ok sizes_ok = _stack_and_check_tensors vjps inputs output_numel jacobians _ _ = _stack_and_check_tensors vjps inputs output_numel reentrant = _check_jacobians_equal jacobians jacobians nondet_tol jacobians reentrant sizes_ok types_ok _get_analytical_jacobian inputs outputs input_idx output_idx Computes analytical Jacobian slow mode single input-output pair Forgoes performing checks dtype shape reentrancy jacobians = _check_analytical_jacobian_attributes inputs outputs output_idx nondet_tol=float inf check_grad_dtypes=False jacobians input_idx _compute_analytical_jacobian_rows vjp_fn sample_output - list list Optional torch Tensor Computes Jacobian row-by-row projecting ` vjp_fn ` = v^T J standard basis vectors vjp_fn e = e^T J corresponding row Jacobian NB function does assume vjp_fn v tensors same number elements different v This checked when we later combine rows into single tensor grad_out_base = torch zeros_like sample_output memory_format=torch legacy_contiguous_format flat_grad_out = grad_out_base view - jacobians_rows i j Jacobian jth row ith input jacobians_rows list list Optional torch Tensor = j range flat_grad_out numel flat_grad_out zero_ flat_grad_out j = projection jth row Jacobian grad_inputs = vjp_fn grad_out_base i d_x enumerate grad_inputs j == jacobians_rows append jacobians_rows i += d_x clone isinstance d_x torch Tensor None jacobians_rows _get_analytical_vjps_wrt_specific_output vjp_fn sample_output v - list list Optional torch Tensor grad_inputs = vjp_fn v reshape sample_output shape vjps list list Optional torch Tensor = vjp clone isinstance vjp torch Tensor None vjp grad_inputs vjps _check_inputs tupled_inputs - bool Make sure gradients saved least one input any_input_requiring_grad = False idx inp enumerate tupled_inputs is_tensor_like inp inp requires_grad inp dtype == torch float inp dtype == torch complex warnings warn f Input idx requires gradient double precision floating point complex This check will likely fail all inputs double precision floating point complex stacklevel= inp is_sparse content = inp _values _is_sparse_compressed_tensor inp content = inp values content = inp TODO To cover more problematic cases replace stride = check any overlap memory once we have proper function check content layout torch _mkldnn type ignore attr-defined all st sz = st sz zip content stride content size raise RuntimeError f The idx th input has dimension stride gradcheck only supports inputs non-overlapping able compute numerical gradients correctly You should call contiguous input before passing gradcheck any_input_requiring_grad = True any_input_requiring_grad raise ValueError gradcheck expects least one input tensor require gradient none them have requires_grad=True True _check_outputs outputs - None any _is_sparse_any_tensor t t outputs isinstance t torch Tensor easier call to_dense sparse output than modify analytical jacobian raise ValueError Sparse output supported gradcheck yet Please call to_dense masked_grad= output fn gradcheck any t layout == torch _mkldnn t outputs isinstance t torch Tensor type ignore attr-defined raise ValueError MKLDNN output supported gradcheck yet Please call to_dense masked_grad= output fn gradcheck _check_no_differentiable_outputs func inputs func_out eps is_forward_ad - bool When there no differentiable outputs numerical gradient function expected zero jacobians_all_inputs_outputs = _get_numerical_jacobian func inputs func_out eps=eps is_forward_ad=is_forward_ad jacobians_all_outputs_and_fixed_input jacobians_all_inputs_outputs jacobian jacobians_all_outputs_and_fixed_input torch ne jacobian sum raise GradcheckError Numerical gradient function expected zero True _check_no_differentiable_outputs_fast func func_out all_inputs inputs_indices all_u eps nondet_tol inp_idx u zip inputs_indices all_u jvps = _get_numerical_jvp_wrt_specific_input func inp_idx all_inputs u eps jvp jvps jvp numel == continue jvp - torch zeros_like jvp abs max nondet_tol raise GradcheckError Numerical gradient function expected zero True FAILED_BATCHED_GRAD_MSG = gradcheck gradgradcheck failed while testing batched gradient computation This could have been invoked number ways via test calls gradcheck gradgradcheck directly via autogenerated test If you adding new operator please file issue then use one workarounds The workaround depends how your test invokes gradcheck gradgradcheck If test - manually invokes gradcheck gradgradcheck then call gradcheck gradgradcheck ` check_batched_grad=False ` keyword argument - OpInfo-based e g test_ops_gradients py then modify OpInfo test have ` check_batched_grad=False ` ` check_batched_gradgrad=False ` If you re modifying existing operator supports batched grad computation wish make new operator work batched grad computation please read following To compute batched grads e g jacobians hessians we vmap over backward computation The most common failure case there vmap-incompatible operation backward pass Please see NOTE How write vmap-compatible backward formulas codebase explanation how fix strip FAILED_BATCHED_GRAD_MSG_FWD_AD = gradcheck failed while testing batched gradient computation forward-mode AD This test enabled automatically when both ` check_batched_grad=True ` ` check_forward_ad=True ` can disabled following ways dependong how test invoked via test calls gradcheck directly via autogenerated test If you adding new operator please file issue then use one workarounds The workaround depends how your test invokes gradcheck gradgradcheck If test - manually invokes gradcheck gradgradcheck then call gradcheck gradgradcheck ` check_batched_forward_grad=False ` keyword argument - OpInfo-based e g test_ops_gradients py then modify OpInfo test have ` check_batched_forward_grad=False ` _get_failed_batched_grad_test_msg output_idx input_idx res exp is_forward_ad=False f For output output_idx input input_idx FAILED_BATCHED_GRAD_MSG_FWD_AD is_forward_ad FAILED_BATCHED_GRAD_MSG Got res Expected exp strip _test_batched_grad_forward_ad func inputs - bool fwAD = torch autograd forward_ad To avoid early issues do we need isinstance inputs tuple raise AssertionError Expected inputs tuple input_idx current_input enumerate inputs is_tensor_like current_input current_input requires_grad continue jvp tangent torch Tensor fwAD dual_level dual = fwAD make_dual current_input detach tangent inputs_with_dual = tuple dual idx == input_idx inp detach is_tensor_like inp inp idx inp enumerate inputs dual_outputs = _as_tuple func inputs_with_dual ret = dual_output dual_outputs dual_output None continue primal_out tangent_out = fwAD unpack_dual dual_output tangent_out None ret append tangent_out ret append torch zeros dtype=primal_out dtype device=primal_out device expand primal_out shape tuple ret _is_float_or_complex_tensor current_input continue tangents = torch randn_like current_input _ range expected = jvp t t tangents expected = torch stack shards shards zip expected try result = _vmap jvp torch stack tangents except RuntimeError ex Rethrow provide better error message raise GradcheckError f While computing batched gradients got ex \n\n FAILED_BATCHED_GRAD_MSG_FWD_AD ex input_idx res exp enumerate zip result expected torch allclose res exp continue raise GradcheckError _get_failed_batched_grad_test_msg input_idx input_idx res exp is_forward_ad=True True _test_batched_grad input output output_idx - bool NB _test_batched_grad compares two autograd grad invocations single vmap autograd grad invocation It s exactly gradcheck sense we re comparing analytical jacobian numeric one morally similar we could have computed full analytic jac via vmap potentially slow diff_input_list = list _iter_tensors input True grad = functools partial torch autograd grad output diff_input_list retain_graph=True allow_unused=True vjp v results = grad v results = tuple grad grad None torch zeros dtype=inp dtype device=inp device expand inp shape grad inp zip results diff_input_list results grad_outputs = torch randn_like output _ range expected = vjp gO gO grad_outputs expected = torch stack shards shards zip expected Squash warnings since these expected happen most cases NB doesn t work CUDA tests https github com pytorch pytorch issues warnings catch_warnings warnings filterwarnings ignore message= There performance drop warnings filterwarnings ignore message= Please use ` torch vmap ` try result = vmap vjp torch stack grad_outputs except RuntimeError ex It s OK we re raising error correct callsite That s because callsite always going inside Python autograd grad instead C++ traceback what line backward formula raise GradcheckError f While computing batched gradients got ex \n\n FAILED_BATCHED_GRAD_MSG ex input_idx res exp enumerate zip result expected torch allclose res exp continue raise GradcheckError _get_failed_batched_grad_test_msg output_idx input_idx res exp True _test_backward_mul_by_grad_output outputs inputs masked - bool Tests backward multiplied grad_output diff_input_list list torch Tensor = list _iter_tensors inputs True diff_input_list raise GradcheckError no Tensors requiring grad found input grads_input = torch autograd grad outputs diff_input_list torch zeros_like o memory_format=torch legacy_contiguous_format o outputs allow_unused=True gi di zip grads_input diff_input_list gi None continue isinstance gi torch Tensor gi layout = torch strided gi layout = di layout raise GradcheckError grad incorrect layout + str gi layout + + str di layout + _is_sparse_any_tensor gi sparse_kind = str gi layout replace torch replace _coo gi sparse_dim = di sparse_dim raise GradcheckError f grad sparse_kind tensor has incorrect sparse_dim f gi sparse_dim expected di sparse_dim gi dense_dim = di dense_dim raise GradcheckError f grad sparse_kind tensor has incorrect dense_dim f gi dense_dim expected di dense_dim gi = gi to_dense di = di to_dense masked torch allclose gi torch zeros_like gi raise GradcheckError backward multiplied grad_output gi eq all raise GradcheckError backward multiplied grad_output gi dtype = di dtype raise GradcheckError grad incorrect type gi device = di device raise GradcheckError grad incorrect device gi size = di size raise GradcheckError grad incorrect size True _test_undefined_forward_mode func outputs inputs fwAD = torch autograd forward_ad _inp_tensors_idx inp_tensors = _get_inp_tensors inputs _all_v all_u _all_u_dense = _make_vectors inp_tensors outputs use_forward_ad=True fwAD dual_level fw_grads = dual_inputs = tensor_indices = set i inp enumerate inputs is_tensor_like inp inp requires_grad inp layout == torch _mkldnn type ignore attr-defined raise ValueError MKLDNN inputs support forward AD gradcheck inp = fwAD make_dual inp detach torch zeros_like inp If inp differentiable view dual might tangent given make_dual so read explicitly dual tensor fw_grads append fwAD unpack_dual inp tensor_indices add i dual_inputs append inp fw_grad u zip fw_grads all_u fw_grad copy_ u view_as fw_grad idx inp enumerate inputs idx tensor_indices continue dual_inp_obj = dual_inputs idx case Materialized Zero Tensor Tangent dual_inputs idx = fwAD make_dual inp detach torch zeros_like inp raw_outputs = _as_tuple func dual_inputs dual_outputs = filter _is_float_or_complex_tensor raw_outputs case Efficient Zero Tensor Tangent since we don t make dual object pass regular tensor dual_inputs idx = inp detach raw_outputs = _as_tuple func dual_inputs dual_outputs = filter _is_float_or_complex_tensor raw_outputs reset dual_inputs idx = dual_inp_obj index_o d_o d_o enumerate zip dual_outputs dual_outputs _val res = fwAD unpack_dual d_o _val res = fwAD unpack_dual d_o res None res None torch allclose res res raise GradcheckError Mismatch tangent values output index index_o when input inp has undefined tangent value Got res expected res True _test_undefined_backward_mode func outputs inputs - bool diff_input_list list torch Tensor = list _iter_tensors inputs True diff_input_list raise GradcheckError no Tensors requiring grad found input warn_bc_breaking warnings warn Backwards compatibility New undefined gradient support checking feature enabled default may break existing callers function If true you you can call function check_undefined_grad=False disable feature stacklevel= check_undefined_grad_support output_to_check grads_output = torch zeros_like o memory_format=torch legacy_contiguous_format o output_to_check try grads_input = torch autograd grad output_to_check diff_input_list grads_output allow_unused=True except RuntimeError e warn_bc_breaking raise GradcheckError Expected backward function handle undefined output grads Please look Notes about undefined output gradients tools autograd derivatives yaml e gi grads_input gi None gi eq all warn_bc_breaking raise GradcheckError Expected all input grads undefined zero when all output grads undefined zero Please look Notes about undefined output gradients tools autograd derivatives yaml True All backward functions must work properly all output grads undefined outputs_to_check = torch _C _functions UndefinedGrad o o _differentiable_outputs func inputs This check filters out Tensor-likes aren t instances Tensor isinstance o torch Tensor If there multiple output grads we should able undef one time without error len outputs_to_check undef_grad_idx range len outputs output_to_check = _differentiable_outputs func inputs outputs_to_check append torch _C _functions UndefinedGrad o idx == undef_grad_idx o idx o enumerate output_to_check all check_undefined_grad_support output output outputs_to_check _as_tuple x isinstance x tuple x isinstance x list tuple x x _differentiable_outputs x tuple o o _as_tuple x o requires_grad _get_notallclose_msg analytical numerical output_idx input_idx complex_indices test_imag=False is_forward_ad=False - str out_is_complex = is_forward_ad complex_indices output_idx complex_indices inp_is_complex = is_forward_ad complex_indices input_idx complex_indices part = imaginary test_imag real element = inputs is_forward_ad outputs prefix = out_is_complex inp_is_complex f While considering part part complex element only mode = computed forward mode is_forward_ad prefix + f Jacobian mode mismatch output output_idx d respect input input_idx d \n f numerical numerical \nanalytical analytical \n _transpose matrix_of_tensors returns list tuples list zip matrix_of_tensors _real_and_imag_output fn returns new functions real fn imag fn where real fn imag fn behave same original fn except torch real torch imag applied complex outputs apply_to_c_outs fn fn_to_apply wrapped_fn inputs outs = _as_tuple fn inputs tuple fn_to_apply o o is_complex o o outs wrapped_fn apply_to_c_outs fn torch real apply_to_c_outs fn torch imag _real_and_imag_input fn complex_inp_indices tupled_inputs returns new functions take real inputs instead complex inputs x y - fn x + y j And computes inp - fn inp + y j inp - fn x + inp j In each case other part considered constant We do use constant here make sure we always call user function valid input apply_to_c_inps fn fn_to_apply wrapped_fn inputs new_inputs = list inputs should_be_complex complex_inp_indices new_inputs should_be_complex = fn_to_apply new_inputs should_be_complex tupled_inputs should_be_complex _as_tuple fn new_inputs wrapped_fn real_fn = apply_to_c_inps fn lambda inp orig inp + orig imag j imag_fn = apply_to_c_inps fn lambda inp orig orig real + inp j real_fn imag_fn _gradcheck_real_imag gradcheck_fn func func_out tupled_inputs outputs eps rtol atol check_grad_dtypes check_forward_ad check_backward_ad nondet_tol check_undefined_grad complex_out_indices = i i o enumerate outputs o is_complex has_any_complex_output = any o is_complex o _as_tuple func_out check_backward_ad has_any_complex_output real_fn imag_fn = _real_and_imag_output func imag_func_out = imag_fn tupled_inputs imag_outputs = _differentiable_outputs imag_func_out gradcheck_fn imag_fn imag_func_out tupled_inputs imag_outputs eps rtol atol check_grad_dtypes nondet_tol complex_indices=complex_out_indices test_imag=True real_func_out = real_fn tupled_inputs real_outputs = _differentiable_outputs real_func_out gradcheck_fn real_fn real_func_out tupled_inputs real_outputs eps rtol atol check_grad_dtypes nondet_tol complex_indices=complex_out_indices gradcheck_fn func func_out tupled_inputs outputs eps rtol atol check_grad_dtypes nondet_tol check_forward_ad complex_inp_indices = i i inp enumerate tupled_inputs is_tensor_like inp inp is_complex complex_inp_indices real_fn imag_fn = _real_and_imag_input func complex_inp_indices tupled_inputs imag_inputs = inp imag is_tensor_like inp inp is_complex inp inp tupled_inputs imag_func_out = imag_fn imag_inputs diff_imag_func_out = _differentiable_outputs imag_func_out gradcheck_fn imag_fn imag_func_out imag_inputs diff_imag_func_out eps rtol atol check_grad_dtypes nondet_tol complex_indices=complex_inp_indices test_imag=True use_forward_ad=True real_inputs = inp real is_tensor_like inp inp is_complex inp inp tupled_inputs real_func_out = real_fn real_inputs diff_real_func_out = _differentiable_outputs real_func_out gradcheck_fn real_fn real_func_out real_inputs diff_real_func_out eps rtol atol check_grad_dtypes nondet_tol complex_indices=complex_inp_indices use_forward_ad=True check_undefined_grad _test_undefined_forward_mode imag_fn imag_func_out imag_inputs _test_undefined_forward_mode real_fn real_func_out real_inputs gradcheck_fn func func_out tupled_inputs outputs eps rtol atol check_grad_dtypes nondet_tol use_forward_ad=True check_undefined_grad _test_undefined_forward_mode func outputs tupled_inputs _slow_gradcheck func func_out tupled_inputs outputs eps rtol atol check_grad_dtypes nondet_tol use_forward_ad=False complex_indices=None test_imag=False masked=False func_out = _as_tuple func_out outputs _check_no_differentiable_outputs func tupled_inputs func_out eps=eps is_forward_ad=use_forward_ad tupled_inputs_numerical = tupled_inputs masked _densify tupled_inputs numerical = _transpose _get_numerical_jacobian func tupled_inputs_numerical func_out eps=eps is_forward_ad=use_forward_ad Note numerical vs analytical output length The numerical path returns jacobian quantity all outputs even requires_grad output False This behavior necessary _check_no_differentiable_outputs work numerical = nj o nj zip func_out numerical o requires_grad use_forward_ad analytical_forward = _get_analytical_jacobian_forward_ad func tupled_inputs func_out check_grad_dtypes=check_grad_dtypes i n_per_out enumerate numerical j n enumerate n_per_out = analytical_forward j i _allclose_with_type_promotion n device rtol atol raise GradcheckError _get_notallclose_msg n i j complex_indices test_imag is_forward_ad=True i o enumerate outputs analytical = _check_analytical_jacobian_attributes tupled_inputs o nondet_tol check_grad_dtypes j n enumerate zip analytical numerical i _allclose_with_type_promotion n device rtol atol raise GradcheckError _get_notallclose_msg n i j complex_indices test_imag True _dot_with_type_promotion u v u dim = v dim = raise AssertionError f Expected u v D tensors got dims u dim v dim u v sum _allclose_with_type_promotion b rtol atol promoted_type = torch promote_types dtype b dtype = dtype=promoted_type b = b dtype=promoted_type torch allclose b rtol atol _to_real_dtype dtype dtype == torch complex torch float dtype == torch complex torch float dtype _vec_from_tensor x generator downcast_complex=False Create random vector same number elements x same dtype device If x complex downcast_complex False we create complex tensor only real component x layout == torch sparse_coo For sparse create random sparse vec random values same indices Make sure size set so isn t inferred smaller x_values = x _values dtype = _to_real_dtype x dtype downcast_complex x dtype values = torch rand x_values numel generator=generator dtype=dtype device=x device view x_values shape values = values norm vec = torch sparse_coo_tensor x _indices values x size device=x device _is_sparse_compressed_tensor x x layout torch sparse_csr torch sparse_bsr compressed_indices plain_indices = x crow_indices x col_indices compressed_indices plain_indices = x ccol_indices x row_indices x_values = x values dtype = _to_real_dtype x dtype downcast_complex x dtype values = torch rand x_values numel generator=generator dtype=dtype device=x device view x_values shape values = values norm vec = torch sparse_compressed_tensor compressed_indices plain_indices values x size layout=x layout device=x device dtype = _to_real_dtype x dtype downcast_complex x dtype vec = torch rand x numel generator=generator dtype=dtype device=x device vec = vec norm vec _get_inp_tensors tupled_inputs inp_idx_tup = i t i t enumerate tupled_inputs is_tensor_like t t requires_grad tup tup inp_idx_tup tup tup inp_idx_tup _adjusted_atol atol u v In slow gradcheck we compare A B element-wise i e some b we allow &#124; - b &#124; atol + rtol b But since we now compare q = v^T A u q = v^T B u we must allow &#124; q - q &#124; v^T E u + rtol v^T B u where E correctly sized matrix which each entry atol We see atol needs scaled v^T M u where M all-ones M x N matrix v^T M u = \sum_ i \sum_ j u_i v_j = \sum_ i u_i \sum_ i v_i TODO properly handle case when u tuple instead only taking first element u = u isinstance u tuple u sum_u = u sum sum_v = v None v sum atol float sum_u float sum_v FAST_FAIL_SLOW_OK_MSG = Fast gradcheck failed element-wise differences small This means test might ve passed slow_mode If you adding new operator please file issue then use one workarounds The workaround depends how your test invokes gradcheck gradgradcheck If test - manually invokes gradcheck gradgradcheck then call gradcheck gradgradcheck ` fast_mode=False ` keyword argument - OpInfo-based e g test_ops_gradients py then modify OpInfo test have ` gradcheck_fast_mode=False ` - Module test e g common_nn py then modify corresponding module_test entry have ` gradcheck_fast_mode=False ` strip _run_slow_mode_and_get_error func tupled_inputs outputs input_idx output_idx rtol atol eps is_forward_ad Compute jacobians slow mode better error message slow_numerical = _get_numerical_jacobian func tupled_inputs outputs eps=eps is_forward_ad=is_forward_ad input_idx output_idx is_forward_ad new_fn inp new_inputs = list tupled_inputs new_inputs input_idx = inp _as_tuple func new_inputs output_idx slow_analytical = _get_analytical_jacobian_forward_ad new_fn tupled_inputs input_idx outputs output_idx slow_analytical = _get_analytical_jacobian tupled_inputs outputs input_idx output_idx Assume jacobians non-empty have same shape slow_max_diff = slow_numerical - slow_analytical abs max slow_allclose = torch allclose slow_analytical slow_numerical rtol atol msg = \nThe above quantities relating numerical analytical jacobians computed \n fast mode See https github com pytorch pytorch issues more background \n about fast mode Below we recompute numerical analytical jacobians slow mode \n\n f Numerical \n slow_numerical \n f Analytical \n slow_analytical \n\n f The max per-element difference slow mode slow_max_diff \n slow_allclose Slow gradcheck would ve passed msg += FAST_FAIL_SLOW_OK_MSG msg _to_flat_dense_if_sparse tensor _is_sparse_any_tensor tensor tensor to_dense reshape - tensor _make_vectors inp_tensors outputs use_forward_ad Use our own generator avoid messing user s RNG state g_cpu = torch Generator _vec_from_tensor_cpu args Default allocate all tensors CPU so they same device generator even user specified default device torch device cpu _vec_from_tensor args all_u = all_u_dense = inp inp_tensors ur = _vec_from_tensor_cpu inp g_cpu True ur_dense = _to_flat_dense_if_sparse ur inp is_complex ui = _vec_from_tensor_cpu inp g_cpu True all_u append ur ui ui_dense = _to_flat_dense_if_sparse ui all_u_dense append ur_dense ui_dense all_u append ur all_u_dense append ur_dense all_v = None use_forward_ad _vec_from_tensor_cpu out g_cpu out outputs all_v all_u all_u_dense _check_analytical_numerical_equal all_analytical all_numerical complex_indices tupled_inputs outputs func all_v all_u rtol atol eps test_imag is_forward_ad=False i all_numerical_for_input_i enumerate all_numerical j n enumerate all_numerical_for_input_i Forward AD generates transpose what function expects is_forward_ad = all_analytical i j = all_analytical j i n = n device=a device updated_atol = _adjusted_atol atol all_u i all_v j all_v None _allclose_with_type_promotion n device rtol updated_atol jacobians_str = _run_slow_mode_and_get_error func tupled_inputs outputs i j rtol atol eps is_forward_ad raise GradcheckError _get_notallclose_msg n j i complex_indices test_imag is_forward_ad + jacobians_str _fast_gradcheck func func_out inputs outputs eps rtol atol check_grad_dtypes nondet_tol use_forward_ad=False complex_indices=None test_imag=False masked=False See https github com pytorch pytorch issues details inp_tensors_idx inp_tensors = _get_inp_tensors inputs Backward mode computes v^T J VJP Since we computed J u JVP through finite difference method we perform equality check between VJP u v JVP ---- Forward mode computes J u JVP Since we already compute JVP through finite difference method we don t need v correctness check here asserted below all_v all_u all_u_dense = _make_vectors inp_tensors outputs use_forward_ad=use_forward_ad inputs_numerical all_u_numerical all_v_numerical = inputs all_u all_v masked _densify inputs all_u all_v numerical_vJu = _get_numerical_vJu func inputs_numerical inp_tensors_idx func_out all_u_numerical all_v_numerical eps is_forward_ad=use_forward_ad TODO replicate https github com pytorch pytorch pull fast gradcheck well use_forward_ad all_v None raise AssertionError Expected all_v None analytical_vJu = _get_analytical_jacobian_forward_ad func inputs _as_tuple func_out all_u=all_u check_grad_dtypes=check_grad_dtypes outputs _check_no_differentiable_outputs_fast func func_out inputs inp_tensors_idx all_u eps nondet_tol analytical_vJu = _get_analytical_vJu_backward_mode inputs outputs nondet_tol check_grad_dtypes all_v all_u_dense _check_analytical_numerical_equal analytical_vJu numerical_vJu complex_indices inputs outputs func all_v all_u rtol atol eps test_imag is_forward_ad=use_forward_ad True Note VarArg Tensors ~~~~~~~~~~~~~~~~~~~~~~~~ func accepts vararg tensors which isn t expressible type system moment If https mypy readthedocs io en latest additional_features html highlight=callable#extended-callable-types accepted first argument Callable can replaced VarArg Tensor For now we permit any input gradcheck func Callable Union _TensorOrTensors See Note VarArg Tensors inputs _TensorOrTensors eps float = e- atol float = e- rtol float = e- raise_exception bool = True nondet_tol float = check_undefined_grad bool = True check_grad_dtypes bool = False check_batched_grad bool = False check_batched_forward_grad bool = False check_forward_ad bool = False check_backward_ad bool = True fast_mode bool = False masked Optional bool = None - bool noqa D D r Check gradients computed via small finite differences against analytical gradients wrt tensors attr ` inputs ` floating point complex type ` ` requires_grad=True ` ` The check between numerical analytical gradients uses func ` ~torch allclose ` For most complex functions we consider optimization purposes no notion Jacobian exists Instead gradcheck verifies numerical analytical values Wirtinger Conjugate Wirtinger derivatives consistent Because gradient computation done under assumption overall function has real-valued output we treat functions complex output special way For these functions gradcheck applied two real-valued functions corresponding taking real components complex outputs first taking imaginary components complex outputs second For more details check out ref ` complex_autograd-doc ` note The default values designed attr ` input ` double precision This check will likely fail attr ` input ` less precision e g ` ` FloatTensor ` ` note Gradcheck may fail when evaluated non-differentiable points because numerically computed gradients via finite differencing may differ those computed analytically necessarily because either incorrect For more context see ref ` non-differentiable-func-grad ` warning If any checked tensor attr ` input ` has overlapping memory i e different indices pointing same memory address e g func ` torch Tensor expand ` check will likely fail because numerical gradients computed point perturbation such indices will change values all other indices share same memory address Args func function Python function takes Tensor inputs returns Tensor tuple Tensors inputs tuple Tensor Tensor inputs function eps float optional perturbation finite differences atol float optional absolute tolerance rtol float optional relative tolerance raise_exception bool optional indicating whether raise exception check fails The exception gives more information about exact nature failure This helpful when debugging gradchecks nondet_tol float optional tolerance non-determinism When running identical inputs through differentiation results must either match exactly default within tolerance check_undefined_grad bool optional ` ` True ` ` check undefined output grads supported treated zeros ` ` Tensor ` ` outputs check_batched_grad bool optional ` ` True ` ` check we can compute batched gradients using prototype vmap support Defaults False check_batched_forward_grad bool optional ` ` True ` ` checks we can compute batched forward gradients using forward ad prototype vmap support Defaults ` ` False ` ` check_forward_ad bool optional ` ` True ` ` check gradients computed forward mode AD match numerical ones Defaults ` ` False ` ` check_backward_ad bool optional ` ` False ` ` do perform any checks rely backward mode AD implemented Defaults ` ` True ` ` fast_mode bool optional Fast mode gradcheck gradgradcheck currently only implemented R R functions If none inputs outputs complex faster implementation gradcheck no longer computes entire jacobian run otherwise we fall back slow implementation masked bool optional ` ` True ` ` gradients unspecified elements sparse tensors ignored Defaults ` ` False ` ` Returns ` ` True ` ` all differences satisfy allclose condition check_forward_ad check_backward_ad raise AssertionError Expected least one check_forward_ad check_backward_ad True check_batched_grad check_backward_ad raise AssertionError Setting check_batched_grad=True requires check_backward_ad True check_batched_forward_grad check_forward_ad raise AssertionError Setting check_batched_forward_grad=True requires check_forward_ad True args = locals copy args pop raise_exception raise_exception try _gradcheck_helper args except GradcheckError False _gradcheck_helper args _gradcheck_helper func inputs eps atol rtol nondet_tol check_undefined_grad check_grad_dtypes check_batched_grad check_batched_forward_grad check_forward_ad check_backward_ad fast_mode masked tupled_inputs = _as_tuple inputs _check_inputs tupled_inputs func_out = func tupled_inputs outputs = _differentiable_outputs func_out _check_outputs outputs gradcheck_fn = functools partial _fast_gradcheck fast_mode _slow_gradcheck masked=masked _gradcheck_real_imag gradcheck_fn func func_out tupled_inputs outputs eps rtol atol check_grad_dtypes check_forward_ad=check_forward_ad check_backward_ad=check_backward_ad nondet_tol=nondet_tol check_undefined_grad=check_undefined_grad check_batched_forward_grad _test_batched_grad_forward_ad func tupled_inputs Short circuit because remaining tests rely backward AD implemented check_backward_ad True i o enumerate outputs check_batched_grad _test_batched_grad tupled_inputs o i _test_backward_mul_by_grad_output outputs tupled_inputs masked check_undefined_grad check_backward_ad _test_undefined_backward_mode func outputs tupled_inputs True gradgradcheck func Callable _TensorOrTensors See Note VarArg Tensors inputs _TensorOrTensors grad_outputs Optional _TensorOrTensors = None eps float = e- atol float = e- rtol float = e- gen_non_contig_grad_outputs bool = False raise_exception bool = True nondet_tol float = check_undefined_grad bool = True check_grad_dtypes bool = False check_batched_grad bool = False check_fwd_over_rev bool = False check_rev_over_rev bool = True fast_mode bool = False masked bool = False - bool noqa D D r Check gradients gradients computed via small finite differences against analytical gradients wrt tensors attr ` inputs ` attr ` grad_outputs ` floating point complex type ` ` requires_grad=True ` ` This function checks backpropagating through gradients computed given attr ` grad_outputs ` correct The check between numerical analytical gradients uses func ` ~torch allclose ` note The default values designed attr ` input ` attr ` grad_outputs ` double precision This check will likely fail they less precision e g ` ` FloatTensor ` ` warning If any checked tensor attr ` input ` attr ` grad_outputs ` has overlapping memory i e different indices pointing same memory address e g func ` torch Tensor expand ` check will likely fail because numerical gradients computed point perturbation such indices will change values all other indices share same memory address Args func function Python function takes Tensor inputs returns Tensor tuple Tensors inputs tuple Tensor Tensor inputs function grad_outputs tuple Tensor Tensor optional The gradients respect function s outputs eps float optional perturbation finite differences atol float optional absolute tolerance rtol float optional relative tolerance gen_non_contig_grad_outputs bool optional attr ` grad_outputs ` ` ` None ` ` attr ` gen_non_contig_grad_outputs ` ` ` True ` ` randomly generated gradient outputs made noncontiguous raise_exception bool optional indicating whether raise exception check fails The exception gives more information about exact nature failure This helpful when debugging gradchecks nondet_tol float optional tolerance non-determinism When running identical inputs through differentiation results must either match exactly default within tolerance Note small amount nondeterminism gradient will lead larger inaccuracies second derivative check_undefined_grad bool optional True check undefined output grads supported treated zeros check_batched_grad bool optional True check we can compute batched gradients using prototype vmap support Defaults False fast_mode bool optional True run faster implementation gradgradcheck no longer computes entire jacobian masked bool optional True gradients unspecified elements sparse tensors ignored default False Returns True all differences satisfy allclose condition check_fwd_over_rev check_rev_over_rev raise AssertionError Expected least one check_fwd_over_rev check_rev_over_rev True check_undefined_grad check_rev_over_rev raise AssertionError Setting check_undefined_grad=True requires check_rev_over_rev True check_batched_grad check_rev_over_rev raise AssertionError Setting check_batched_grad=True requires check_rev_over_rev True TODO do we want test too assert check_batched_forward_grad check_fwd_over_rev Setting check_batched_forward_grad=True requires check_fwd_over_rev True tupled_inputs = _as_tuple inputs grad_outputs None If grad_outputs specified create random Tensors same shape type device outputs outputs = _differentiable_outputs func tupled_inputs tupled_grad_outputs = tuple torch testing make_tensor x shape dtype=x dtype x is_floating_point x is_complex torch double device=x device low=- high= requires_grad=True noncontiguous=gen_non_contig_grad_outputs x outputs tupled_grad_outputs = _as_tuple grad_outputs num_outputs = len tupled_grad_outputs NB We need save requires_grad information about inputs here because gradcheck detaches inputs before running forward mode AD diff_input_args_indices = i i x enumerate tupled_inputs is_tensor_like x x requires_grad diff_grad_output_indices = i i x enumerate tupled_grad_outputs x requires_grad new_func args Restore requires_grad information input_args = tuple x requires_grad_ i diff_input_args_indices x i x enumerate args -num_outputs outputs = _differentiable_outputs func input_args grad_outputs = tuple x requires_grad_ i diff_grad_output_indices x i x enumerate args -num_outputs diff_input_args = tuple x i x enumerate input_args i diff_input_args_indices grad_inputs = torch autograd grad outputs diff_input_args grad_outputs create_graph=True allow_unused=True grad_inputs = tuple g g grad_inputs g None grad_inputs gradcheck new_func tupled_inputs + tupled_grad_outputs eps=eps atol=atol rtol=rtol raise_exception=raise_exception nondet_tol=nondet_tol check_undefined_grad=check_undefined_grad check_grad_dtypes=check_grad_dtypes check_batched_grad=check_batched_grad fast_mode=fast_mode check_forward_ad=check_fwd_over_rev check_backward_ad=check_rev_over_rev masked=masked