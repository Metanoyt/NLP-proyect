Owner s oncall distributed sys unittest mock patch torch torch nn nn torch distributed dist torch distributed fsdp BackwardPrefetch FullyShardedDataParallel FSDP torch distributed fsdp _common_utils _get_handle_fqns_from_root torch distributed fsdp _flat_param HandleTrainingState torch distributed fsdp _runtime_utils _get_handle_to_prefetch _get_training_state torch distributed fsdp wrap ModuleWrapPolicy torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_devtype torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN device_type = torch device get_devtype NUM_ITERS = DECODER_PARAM_FQNS = decoder layers index self_attn in_proj_weight decoder layers index self_attn in_proj_bias decoder layers index self_attn out_proj weight decoder layers index self_attn out_proj bias decoder layers index multihead_attn in_proj_weight decoder layers index multihead_attn in_proj_bias decoder layers index multihead_attn out_proj weight decoder layers index multihead_attn out_proj bias decoder layers index linear weight decoder layers index linear bias decoder layers index linear weight decoder layers index linear bias decoder layers index norm weight decoder layers index norm bias decoder layers index norm weight decoder layers index norm bias decoder layers index norm weight decoder layers index norm bias ENCODER_PARAM_FQNS = encoder layers index self_attn in_proj_weight encoder layers index self_attn in_proj_bias encoder layers index self_attn out_proj weight encoder layers index self_attn out_proj bias encoder layers index linear weight encoder layers index linear bias encoder layers index linear weight encoder layers index linear bias encoder layers index norm weight encoder layers index norm bias encoder layers index norm weight encoder layers index norm bias TOTAL_NUM_PREFETCH_FOR_PRE = TOTAL_NUM_PREFETCH_FOR_POST = ENCODER_BEGIN_INDEX_FOR_PRE = ENCODER_BEGIN_INDEX_FOR_POST = ENCODER_PREFETCH_NUM = dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TestBackwardPrefetch FSDPTest property world_size _dist_train backward_prefetch=BackwardPrefetch BACKWARD_PRE rank = rank orig_get_handle_to_prefetch = _get_handle_to_prefetch torch manual_seed policy = ModuleWrapPolicy nn TransformerEncoderLayer nn TransformerDecoderLayer model = FSDP nn Transformer d_model= nhead= device=device_type device_id=device_type type auto_wrap_policy=policy use_orig_params=True backward_prefetch=backward_prefetch optim = torch optim SGD model parameters lr= e- prepare input torch manual_seed rank + src = torch randn device=device_type tgt = torch randn device=device_type monkey patch all_handle_fqns list list str = patched_get_handle_to_prefetch args kwargs handle = orig_get_handle_to_prefetch args kwargs assertEqual len args expect _get_handle_to_prefetch state current_handle state = args current_handle = args training_state = _get_training_state current_handle training_state == HandleTrainingState BACKWARD_PRE state backward_prefetch == BackwardPrefetch BACKWARD_PRE training_state == HandleTrainingState BACKWARD_POST state backward_prefetch == BackwardPrefetch BACKWARD_POST nonlocal all_handle_fqns FQNs prefixed root module state _exec_order_data param_to_fqn fqns = _get_handle_fqns_from_root state handle all_handle_fqns append fqns handle flat params prefetch handle should match DECODER_PARAM_FQNS ENCODER_PARAM_FQNS patch torch distributed fsdp _runtime_utils _get_handle_to_prefetch patched_get_handle_to_prefetch _ range NUM_ITERS optim zero_grad loss = model src tgt sum loss backward optim step backward_prefetch None assertEqual len all_handle_fqns continue backward_prefetch == BackwardPrefetch BACKWARD_PRE state _exec_order_data handles_post_forward_order equals forward order encoder - decoder - root pre-backward hook order root - decoder - encoder prefetch order decoder - encoder - None None when current_handle=encoder _get_handle_to_prefetch returns None + above None encoder_begin_index = ENCODER_BEGIN_INDEX_FOR_PRE assertEqual len all_handle_fqns TOTAL_NUM_PREFETCH_FOR_PRE + backward_prefetch == BackwardPrefetch BACKWARD_POST state _exec_order_data handles_post_forward_order equals forward order same BACKWARD_PRE encoder - decoder - root post-backward hook AccumulateGrad order decoder - encoder - root prefetch order decoder - encoder - None - None st None when current_handle=encoder _get_handle_to_prefetch returns None nd None when current_handle=root get decoder inside _get_handle_to_prefetch needed since decoder computed already + above Nones encoder_begin_index = ENCODER_BEGIN_INDEX_FOR_POST assertEqual len all_handle_fqns TOTAL_NUM_PREFETCH_FOR_POST + ith_prefetch st nd rd th ith prefetch ith_prefetch fqns enumerate all_handle_fqns ith_prefetch = ith_prefetch encoder_begin_index layer_index = encoder_begin_index - - ith_prefetch assertEqual fqns x format index=layer_index x DECODER_PARAM_FQNS ith_prefetch = encoder_begin_index ith_prefetch = encoder_begin_index + ENCODER_PREFETCH_NUM layer_index = encoder_begin_index + ENCODER_PREFETCH_NUM - ith_prefetch assertEqual fqns x format index=layer_index x ENCODER_PARAM_FQNS assertTrue fqns None all_handle_fqns = skip_if_lt_x_gpu test_backward_prefetch subtest reuse process group shorten test time run_subtests backward_prefetch None BackwardPrefetch BACKWARD_PRE BackwardPrefetch BACKWARD_POST _test_backward_prefetch _test_backward_prefetch backward_prefetch BackwardPrefetch _dist_train backward_prefetch __name__ == __main__ run_tests