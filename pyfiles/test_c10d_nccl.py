Owner s oncall distributed copy json os pickle random re signal sys tempfile threading time warnings contextlib contextmanager datetime datetime timedelta enum auto Enum itertools chain product unittest mock SkipTest torch torch distributed c d torch distributed _functional_collectives _functional_collectives c d is_available c d is_nccl_available print c d NCCL available skipping tests file=sys stderr sys exit test_c d_common test_c d_common ConvNet DoubleGpuNet FFTModel gpus_for_rank ModuleForDdpCommHook torch distributed dist torch distributed algorithms ddp_comm_hooks default_hooks default torch distributed algorithms ddp_comm_hooks powerSGD_hook powerSGD torch nn functional F torch testing _internal common_utils common torch nn torch _C _distributed_c d ErrorType OpType WorkResult torch nn parallel DistributedDataParallel torch testing _internal common_cuda _get_torch_rocm_version TEST_MULTIGPU torch testing _internal common_distributed get_timeout init_multigpu_helper MultiProcessTestCase requires_multicast_support requires_nccl requires_nccl_version skip_if_lt_x_gpu skip_if_rocm_multiprocess sm_is_or_higher_than TEST_SKIPS with_dist_debug_levels with_nccl_blocking_wait torch testing _internal common_utils instantiate_parametrized_tests IS_SANDCASTLE MI _ARCH parametrize retry_on_connect_failures run_tests runOnRocmArch skip_but_pass_in_sandcastle skip_but_pass_in_sandcastle_if TEST_CUDA TEST_WITH_DEV_DBG_ASAN TEST_WITH_ROCM TestCase TEST_WITH_DEV_DBG_ASAN print Skip ASAN torch + multiprocessing spawn have known issues file=sys stderr sys exit BFLOAT _AVAILABLE = torch cuda is_available torch version cuda None torch version hip None RendezvousEnvTest TestCase retry_on_connect_failures requires_nccl skip_but_pass_in_sandcastle_if TEST_CUDA No GPUs available skipping test test_common_errors vars = WORLD_SIZE RANK MASTER_ADDR MASTER_PORT str common find_free_port Env __init__ vars env_patcher = mock patch dict os environ vars clear=True __enter__ env_patcher start __exit__ type value traceback env_patcher stop without d key d = d copy d pop key d withouts d keys d = d copy key keys d pop key d Env without vars WORLD_SIZE assertEqual None os environ get WORLD_SIZE assertRaisesRegex ValueError WORLD_SIZE expected gen = c d rendezvous env next gen c d init_process_group backend= nccl world_size= assertEqual c d get_rank assertEqual c d get_world_size c d destroy_process_group Env without vars RANK assertEqual None os environ get RANK assertRaisesRegex ValueError RANK expected gen = c d rendezvous env next gen c d init_process_group backend= nccl rank= assertEqual c d get_rank assertEqual c d get_world_size c d destroy_process_group Env withouts vars RANK WORLD_SIZE assertEqual None os environ get RANK assertEqual None os environ get WORLD_SIZE c d init_process_group backend= nccl rank= world_size= assertEqual c d get_rank assertEqual c d get_world_size c d destroy_process_group Env vars c d init_process_group backend= nccl assertEqual c d get_rank assertEqual c d get_world_size c d destroy_process_group Env without vars MASTER_ADDR assertEqual None os environ get MASTER_ADDR assertRaisesRegex ValueError MASTER_ADDR expected gen = c d rendezvous env next gen Env without vars MASTER_PORT assertEqual None os environ get MASTER_PORT assertRaisesRegex ValueError MASTER_PORT expected gen = c d rendezvous env next gen Env without vars WORLD_SIZE assertEqual None os environ get WORLD_SIZE gen = c d rendezvous f env world_size= _ _ size = next gen assertEqual size Env without vars RANK assertEqual None os environ get RANK gen = c d rendezvous f env rank= _ rank _ = next gen assertEqual rank Env withouts vars RANK WORLD_SIZE assertEqual None os environ get RANK assertEqual None os environ get WORLD_SIZE gen = c d rendezvous f env rank= world_size= _ rank size = next gen assertEqual rank assertEqual size TimeoutTest test_c d_common AbstractTimeoutTest TestCase requires_nccl retry_on_connect_failures skip_but_pass_in_sandcastle_if TEST_CUDA No GPUs available skipping test test_default_store_timeout_nccl _test_default_store_timeout nccl ProcessGroupNCCLNoGPUTest TestCase MAIN_PROCESS_RANK = setUp rank = MAIN_PROCESS_RANK world_size = file = tempfile NamedTemporaryFile delete=False tearDown pass requires_nccl skip_but_pass_in_sandcastle_if TEST_CUDA GPUs available skipping test test_init_no_gpus store = c d FileStore file name world_size assertRaisesRegex ValueError ProcessGroupNCCL only supported GPUs no GPUs found c d ProcessGroupNCCL store rank world_size ProcessGroupNCCLInitTest MultiProcessTestCase device_type = cuda setUp super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass property world_size dm = torch get_device_module device_type dm device_count property device torch device device_type rank world_size A helper must-needed init args test infra kwargs can filled individual init tests _init_process_group kwargs store = c d FileStore file_name world_size c d init_process_group rank=self rank world_size=self world_size store=store kwargs requires_nccl skip_if_lt_x_gpu test_init_wo_backend_str _init_process_group device_id=self device x = torch empty device=self device c d all_reduce x requires_nccl skip_if_lt_x_gpu test_scalable_init os environ TORCH_NCCL_RANKS_PER_ROOT = _init_process_group device_id=self device x = torch empty device=self device c d all_reduce x os environ TORCH_NCCL_RANKS_PER_ROOT = ProcessGroupNCCLGroupTest MultiProcessTestCase _create_process_group_nccl store opts device_id=None create nccl processgroup opts c d init_process_group nccl world_size=self world_size rank=self rank store=store pg_options=opts device_id=device_id pg = c d distributed_c d _get_default_group pg opts high_priority_stream=False opts = c d ProcessGroupNCCL Options opts is_high_priority_stream = high_priority_stream opts setUp super setUp These tests expected throw SIGABRT But we Sandcastle ` skip_but_pass_in_sandcastle ` would TEST_NAN_ASSERT_RETURN = IS_SANDCASTLE signal SIGABRT special_return_code_checks = test_nan_assert_float __wrapped__ TEST_NAN_ASSERT_RETURN test_nan_assert_float __wrapped__ TEST_NAN_ASSERT_RETURN test_nan_assert_float __wrapped__ TEST_NAN_ASSERT_RETURN test_nan_assert_bfloat __wrapped__ TEST_NAN_ASSERT_RETURN test_nan_assert_float _e m fn __wrapped__ TEST_NAN_ASSERT_RETURN test_nan_assert_float _e m __wrapped__ TEST_NAN_ASSERT_RETURN TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests use TORCH_NCCL_BLOCKING_WAIT will test expected os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = num_gpus = torch cuda device_count _spawn_processes tearDown super tearDown try os remove file_name except OSError pass property world_size property rank_to_GPU rank GPU map init_multigpu_helper world_size nccl property destroy_pg_upon_exit - bool This TestCase focuses creation destroy abort PG s So does need auto-destroy upon exit False requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires GPU skip_if_lt_x_gpu test_nccl_dist_backend_error store = c d FileStore file_name world_size _create_process_group_nccl store opts Both rank will use same CUDA device resulting ncclInvalidUsage assertRaises dist DistBackendError cm dist broadcast torch tensor cuda assertTrue isinstance cm exception dist DistError assertIsInstance cm exception RuntimeError requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_abort_pg Disable ASYNC_ERROR_HANDLING test ensure we can programmatically abort process group os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = store = c d FileStore file_name world_size _create_process_group_nccl store opts device = rank_to_GPU rank t = torch rand device=device First allreduce initialize state dist all_reduce t abortpg c d distributed_c d _get_default_group _get_backend torch device device abort Initialize DDP ensure destroy_process_group will call ProcessGroupNCCL destructor since DDP holds reference process group Run single iteration DDP initialize state model = DistributedDataParallel torch nn Linear device device_ids= device model t sum backward Now simulate collective getting stuck abort gets us unstuck rank == dist all_reduce t Schedule thread before we get stuck abort pg thread = threading Thread target=abortpg thread start We would get stuck here due d h we didn t abort t cpu thread join requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize eager_init True False test_close_pg eager_init bool Disable ASYNC_ERROR_HANDLING test ensure we can programmatically abort process group os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = store = c d FileStore file_name world_size device = torch device f cuda rank torch cuda device_count c d init_process_group nccl world_size=self world_size rank=self rank store=store device_id=device eager_init None t = torch rand device=device First allreduce initialize state dist all_reduce t Destroy pg validate pg no longer valid dist destroy_process_group assertRaises ValueError dist all_reduce t requires_nccl skip_if_rocm_multiprocess skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_restart_pg Note restart test passes steadily only blocking mode now TODO expand test non-blocking mode store = c d FileStore file_name world_size device = torch device f cuda rank torch cuda device_count initialize pg first time c d init_process_group nccl world_size=self world_size rank=self rank store=store t = torch rand device=device First allreduce lazy initialize default pg dist all_reduce t torch cuda synchronize Destroy pg dist destroy_process_group we need new Store new PG achieving adding prefix new_store = c d PrefixStore nd store re-initialize pg c d init_process_group nccl world_size=self world_size rank=self rank store=new_store t = torch rand device=device dist all_reduce t torch cuda synchronize dist destroy_process_group validate default pg no longer valid assertRaises ValueError dist all_reduce t requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_cuda_event_cache_mthd_race This unit test test case when collective launched side thread thread dies before cache has been fully recycled More details can found issue https github com pytorch pytorch issues initiate collectives here init_collective_task t dist all_reduce t dist all_reduce t dist all_reduce t os environ TORCH_NCCL_CUDA_EVENT_CACHE = store = c d FileStore file_name world_size _create_process_group_nccl store opts device = rank_to_GPU rank t = torch rand device=device First allreduce initialize state dist all_reduce t dist all_reduce t dist all_reduce t side_thread = threading Thread target=init_collective_task args= t side_thread start side_thread join torch cuda synchronize reset ENV os environ TORCH_NCCL_CUDA_EVENT_CACHE = CUDA_ _AND_ABOVE = torch cuda is_available torch version cuda None int torch version cuda split = requires_nccl skip_but_pass_in_sandcastle_if skip cu well due https github com pytorch pytorch issues TEST_MULTIGPU CUDA_ _AND_ABOVE NCCL test requires + GPUs Device side assert could cause unexpected errors lower versions CUDA parametrize type torch float torch float torch float torch bfloat torch float _e m fn torch float _e m skip_if_rocm_multiprocess test_nan_assert type Expecting device-side error when NaN detected os environ TORCH_NCCL_NAN_CHECK = store = c d FileStore file_name world_size pg = _create_process_group_nccl store opts backend = pg _get_backend torch device cuda device = rank_to_GPU rank Cover different buffer sizes type == torch float size = K elements type == torch float size = M elements type == torch float size = G elements size = element Note currently we cannot fill values into FP tensor thus we create NaN tensor float type cast FP type == torch float _e m fn type == torch float _e m init_type = torch float init_type = type nan_tensor = torch zeros size dtype=init_type device=device randomly pick nan element index = tuple random randrange size i i range len size nan_tensor index = float nan init_type = type Now cast targeted dtype nan_tensor = nan_tensor type output = torch empty world_size size dtype=type device=device confirm enable disable flag works backend _set_enable_nan_check False Note using all-gather here bc some NCCL SM version does support FP reduction temporarily skip due https github com pytorch pytorch issues pg _allgather_base output nan_tensor backend _set_enable_nan_check True try pg _allgather_base output nan_tensor except Exception sys exit signal SIGABRT dist destroy_process_group reset env os environ TORCH_NCCL_NAN_CHECK = requires_nccl skip_if_lt_x_gpu test_nan_rank_filter Putting NaN recv buffer program should fail NaN checker should check receive buffer os environ TORCH_NCCL_NAN_CHECK = store = c d FileStore file_name world_size device = torch device f cuda rank d c d init_process_group backend= nccl store=store rank=self rank world_size=self world_size t = torch ones dtype=torch bfloat device=device rank = Putting NaN recv buffer t = float nan Against broadcast c d broadcast t Against P P rank == c d send t rank == c d recv t c d destroy_process_group reset env os environ TORCH_NCCL_NAN_CHECK = requires_nccl skip_if_lt_x_gpu test_nan_check Not expecting error NaN check should make legit code fail device = torch device f cuda rank d os environ TORCH_NCCL_NAN_CHECK = store = c d FileStore file_name world_size c d init_process_group backend= nccl store=store rank=self rank world_size=self world_size x = torch ones device=device rank t = torch ones device=device c d broadcast x src= c d all_reduce t c d barrier c d destroy_process_group reset env os environ TORCH_NCCL_NAN_CHECK = _helper_test_extra_cuda_context_by_nvml A helper ` test_extra_cuda_context ` pynvml available pynvml provides python bindings NVIDIA NVML functionalities Here we interested nvmlDeviceGetComputeRunningProcesses pynvml pynvml nvmlInit device = torch device f cuda rank d x = torch empty device=device work = c d all_reduce x async_op=True Wait non- ranks garbage collect Work -- latest point where extra CUDA context can created rank == time sleep del work handle = pynvml nvmlDeviceGetHandleByIndex rank processes = pynvml nvmlDeviceGetComputeRunningProcesses handle nprocs = len processes A barrier non- ranks c d all_reduce x torch cuda synchronize device c d destroy_process_group assertLessEqual nprocs f Found nprocs processes creating contexts device expecting most _helper_test_extra_cuda_context_by_memory A helper ` test_extra_cuda_context ` pynvml NOT available If extra context created would manifest into device s memory usage device = torch device f cuda rank d x = torch empty device=device Rank takes snapshot before collective -- snapshot should have included rank s own context rank == free total = torch cuda mem_get_info device used_before = float total - free work = c d all_reduce x async_op=True Wait non- ranks garbage collect Work -- latest point where extra CUDA context can created rank == time sleep free total = torch cuda mem_get_info device used_after = float total - free del work A barrier non- ranks c d all_reduce x torch cuda synchronize device c d destroy_process_group rank == If non- rank creates context device assert would fail because one context takes about GB -- much more than tensor size created test assertTrue Bump heuristic due https github com pytorch pytorch issues used_after used_before f device used used_after bytes after collective f more than status before used_before bytes f Extra CUDA context may have been created requires_nccl skip_if_lt_x_gpu test_extra_cuda_context Check non- ranks would create extra CUDA context device store = c d FileStore file_name world_size device = torch device f cuda rank d c d init_process_group backend= nccl store=store rank=self rank world_size=self world_size device_id=device try _helper_test_extra_cuda_context_by_nvml except ModuleNotFoundError _helper_test_extra_cuda_context_by_memory requires_nccl skip_if_lt_x_gpu test_extra_cuda_context_sync_ops Loop bunch sync ops see any them creates extra context Requires nvml check number processes resident device try pynvml pynvml nvmlInit except Exception skipTest pynvml available Check non- ranks would create extra CUDA context device store = c d FileStore file_name world_size device = torch device f cuda rank d c d init_process_group backend= nccl store=store rank=self rank world_size=self world_size device_id=device x = torch empty device=device y = torch empty world_size device=device c d all_reduce x c d reduce x dst= c d broadcast x src= c d all_gather_into_tensor y x c d reduce_scatter_tensor x y c d barrier Wait bit remote processes touch my device rank == time sleep handle = pynvml nvmlDeviceGetHandleByIndex rank processes = pynvml nvmlDeviceGetComputeRunningProcesses handle nprocs = len processes Don t exit till rank done nvml detection c d barrier c d destroy_process_group assertLessEqual nprocs f Found nprocs processes creating contexts device expecting most requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_destruct_before_terminate_pg Disable ASYNC_ERROR_HANDLING test ensure we can programmatically abort process group os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = store = c d FileStore file_name world_size pg = _create_process_group_nccl store opts device = rank_to_GPU rank t = torch rand device=device First allreduce initialize state pg allreduce t force destruction before terminating comms destructor would terminate comms del pg requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_abort_in_destroy_pg Disable ASYNC_ERROR_HANDLING test ensure we can programmatically abort process group os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = store = c d FileStore file_name world_size pg = _create_process_group_nccl store opts device = rank_to_GPU rank t = torch rand device=device First allreduce initialize state pg allreduce t Destroy pg validate pg NOT working condition since we have shutdown comms dist destroy_process_group assertRaises dist DistBackendError pg allreduce t requires_nccl skip_but_pass_in_sandcastle_if torch cuda device_count NCCL test requires + GPUs test_abort_in_destroy_multi_pgs store = c d FileStore file_name world_size pg = _create_process_group_nccl store opts device = rank_to_GPU rank t = torch rand device=device First allreduce initialize default PG s communicator pg allreduce t wait new_pg = c d new_group new_pg = c d new_group t = torch rand device=device t = torch rand device=device new_pg allreduce t wait new_pg allreduce t wait backend = pg _get_backend torch device device default PG s backend should have split count because s eager initialized assertEqual backend comm_split_count shutdown all NCCL PGs one shot dist destroy_process_group requires_nccl skip_but_pass_in_sandcastle_if torch cuda device_count NCCL test requires + GPUs test_abort_in_destroy_mixed_empty_pgs store = c d FileStore file_name world_size pg = _create_process_group_nccl store opts device = rank_to_GPU rank t = torch rand device=device First allreduce initialize default PG s communicator pg allreduce t wait PG PG without comms initialized since we don t call collective new_pg = c d new_group noqa F new_pg = c d new_group t = torch rand device=device new_pg allreduce t wait backend = pg _get_backend torch device device default PG s backend should have split count assertEqual backend comm_split_count shutdown all NCCL PGs one shot dist destroy_process_group requires_nccl skip_but_pass_in_sandcastle_if torch cuda device_count NCCL test requires + GPUs test_file_store_check os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = os environ TORCH_NCCL_ENABLE_MONITORING = FileStore check would executed os environ TORCH_NCCL_DUMP_ON_TIMEOUT = os environ TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC = file_name created using delete=False e g file_name = tempfile NamedTemporaryFile delete=False name store = dist FileStore file_name world_size dist init_process_group backend= nccl rank=self rank world_size=self world_size store=store pg = dist distributed_c d _get_default_group assertEqual pg rank rank assertEqual pg size world_size give enough time check executed multiple times time sleep dist destroy_process_group _check_nccl_timeout expected_timeout pg = dist distributed_c d _get_default_group options = pg _get_backend torch device f cuda rank options assertEqual options _timeout expected_timeout requires_nccl skip_but_pass_in_sandcastle_if TEST_CUDA No GPUs available skipping test test_init_process_group_nccl_timeout nccl handled specially inside init_process_group its options different options used other PG s There specific edge cases nccl need tested store = c d FileStore file_name world_size base_opts = dict backend= nccl store=store rank=self rank world_size=self world_size test default value coming ` init_process_group ` kwarg default dist init_process_group base_opts _check_nccl_timeout torch distributed constants default_pg_nccl_timeout dist destroy_process_group test ` kwarg ` timeout takes effect new_timeout = timedelta seconds= dist init_process_group base_opts timeout=new_timeout _check_nccl_timeout new_timeout dist destroy_process_group test timeout value provided via ` pg_options ` kwarg ignored issues warning timeout kwarg its kwdefault taking precedence opts = dist ProcessGroupNCCL Options opts _timeout = timedelta seconds= warnings catch_warnings record=True dist init_process_group base_opts pg_options=opts TODO whc i verified we indeed emitting warning i can t figure out why i can t catch assertEqual len w assertTrue pg_options _timeout specified str w - message _check_nccl_timeout torch distributed constants default_pg_nccl_timeout dist destroy_process_group test timeout value provided via ` pg_options ` kwarg ignored issues warning timeout kwarg taking precedence opts = dist ProcessGroupNCCL Options opts _timeout = timedelta seconds= dist init_process_group base_opts pg_options=opts timeout=timedelta seconds= _check_nccl_timeout timedelta seconds= dist destroy_process_group requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize backend None nccl test_set_nccl_pg_timeout backend store = c d FileStore file_name world_size opts = dict backend=backend store=store rank=self rank world_size=self world_size timeout=timedelta seconds= dist init_process_group opts pg = dist distributed_c d _get_default_group pg allreduce torch rand cuda rank _check_nccl_timeout timedelta seconds= pg _get_backend torch device f cuda rank _set_default_timeout timedelta seconds= _check_nccl_timeout timedelta seconds= pg allreduce torch rand cuda rank c d distributed_c d _set_pg_timeout timedelta seconds= pg _check_nccl_timeout timedelta seconds= requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize backend None nccl test_extend_nccl_pg_timeout backend torch cuda set_device rank store = c d FileStore file_name world_size opts = dict backend=backend store=store rank=self rank world_size=self world_size timeout=timedelta seconds= dist init_process_group opts pg = dist distributed_c d _get_default_group bankend = pg _get_backend torch device f cuda rank w = pg allreduce torch rand cuda rank assertTrue bankend _verify_work_timeout w timedelta seconds= w wait bankend _set_default_timeout timedelta seconds= rank == Ideally we want sleep very long time feasible unit test So only very tiny case time sleep pg allreduce torch rand cuda rank time sleep pg allreduce torch rand cuda rank w = pg allreduce torch rand cuda rank assertTrue bankend _verify_work_timeout w timedelta seconds= w wait dist distributed_c d _add_ephemeral_timeout_for_all_pgs timedelta seconds= w = pg allreduce torch rand cuda rank w = pg allreduce torch rand cuda rank assertTrue bankend _verify_work_timeout w timedelta seconds= assertTrue bankend _verify_work_timeout w timedelta seconds= w wait dist distributed_c d _add_ephemeral_timeout_for_all_pgs timedelta seconds= Since we block wait so use sync here leave enough time watchdog reset first timeout extension torch cuda synchronize torch device f cuda rank w = pg allreduce torch rand cuda rank assertTrue bankend _verify_work_timeout w timedelta seconds= w wait requires_nccl_version Need NCCL + ncclCommSplit skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize eager_init True False test_new_group eager_init bool Test optimization new groups contain all world ranks use transparent ` ncclCommSplit ` optimization store = c d FileStore file_name world_size device = torch device f cuda rank torch cuda device_count c d init_process_group nccl world_size=self world_size rank=self rank store=store device_id=device eager_init None ng = c d new_group tensor = torch tensor rank device=device dist broadcast tensor dist broadcast tensor group=ng dist destroy_process_group requires_nccl_version Need NCCL + ncclCommSplit skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs skip_but_pass_in_sandcastle_if torch cuda nccl version - == x NCCL test NCCLX test_comm_split_subgroup Test ` ncclCommSplit ` smaller subgroups world when we ve passed specific device_id init_process_group store = c d FileStore file_name world_size device = torch device f cuda rank pg = _create_process_group_nccl store opts device_id=device backend = pg _get_backend torch device device tensor = torch full rank cuda device original_tensor = tensor clone ng = c d new_group comm split happens eagerly since device_id passed init_process_group assertEqual backend comm_split_count rank == dist broadcast tensor group=ng no additional comm split happens after collective assertEqual backend comm_split_count assertEqual tensor original_tensor dist destroy_process_group requires_nccl_version Need NCCL + ncclCommSplit skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_comm_eager_init_subgroup Test ` ncclCommSplit ` smaller subgroups world when we ve passed specific device_id init_process_group store = c d FileStore file_name world_size device = torch device f cuda rank default PG comm initialized yet pg = _create_process_group_nccl store opts backend = pg _get_backend torch device device assertEqual backend _is_initialized False create subgroup eagerly new_group = c d new_group device_id=device tensor = torch full rank cuda device dist broadcast tensor group=new_group default group should stay lazy assertEqual backend _is_initialized False torch cuda synchronize dist destroy_process_group requires_nccl_version Need NCCL + ncclCommSplit skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_comm_split_group Test ` ncclCommSplit ` smaller subgroups world when we ve passed specific device_id init_process_group store = c d FileStore file_name world_size device = torch device f cuda rank pg = _create_process_group_nccl store opts device_id=device backend = pg _get_backend torch device device tensor = torch full rank cuda device Create subgroup between ranks subg_ranks = ng = c d split_group pg subg_ranks backend = ng _get_backend torch device device check basic options same between parent child assertEqual backend options _timeout backend options _timeout assertEqual backend options is_high_priority_stream backend options is_high_priority_stream assertEqual ng group_desc default_pg split comm split happens eagerly since device_id passed init_process_group assertEqual backend comm_split_count dist get_process_group_ranks returns global ranks subgroup assertEqual dist get_process_group_ranks ng subg_ranks rank subg_ranks part ng otherwise - dist get_rank ng = dist broadcast tensor dist get_global_rank ng group=ng assertEqual tensor torch full ng = c d split_group pg subg_ranks assertEqual ng group_desc default_pg split assertEqual backend comm_split_count dist destroy_process_group requires_nccl_version Need NCCL + ncclCommSplit skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_comm_split_group_mixed_backend Test ` ncclCommSplit ` smaller subgroups world when we ve passed specific device_id init_process_group store = c d FileStore file_name world_size device = torch device f cuda rank pg = _create_process_group_nccl store opts device_id=device create nccl processgroup opts c d init_process_group cpu gloo cuda nccl world_size=self world_size rank=self rank store=store pg_options=self opts device_id=device pg = c d distributed_c d _get_default_group backend = pg _get_backend torch device device cuda_tensor = torch full rank cuda device cpu_tensor = torch full rank Create subgroup between ranks subg_ranks = ng = c d split_group pg subg_ranks backend = ng _get_backend torch device device check basic options same between parent child assertEqual backend options _timeout backend options _timeout assertEqual backend options is_high_priority_stream backend options is_high_priority_stream assertEqual ng group_desc default_pg split comm split happens eagerly since device_id passed init_process_group assertEqual backend comm_split_count dist get_process_group_ranks returns global ranks subgroup assertEqual dist get_process_group_ranks ng subg_ranks rank subg_ranks part ng otherwise - dist get_rank ng = dist broadcast cuda_tensor dist get_global_rank ng group=ng assertEqual cuda_tensor torch full dist broadcast cpu_tensor dist get_global_rank ng group=ng assertEqual cpu_tensor torch full ng = c d split_group pg subg_ranks assertEqual ng group_desc default_pg split assertEqual backend comm_split_count dist destroy_process_group requires_nccl_version Need NCCL + ncclCommSplit skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_non_blocking_init Test creating pg using nonblocking mode eagerly os environ TORCH_NCCL_USE_COMM_NONBLOCKING = os environ TORCH_NCCL_NONBLOCKING_TIMEOUT = store = c d FileStore file_name world_size device = rank_to_GPU rank pg = _create_process_group_nccl store opts backend = pg _get_backend torch device device assertEqual backend comm_split_count reduce_tensor = torch rand device=device Run allreduce which should trigger comm init pg pg allreduce reduce_tensor wait new_pg = c d new_group even after pg s collective call new pg s comm initialized until its own collectcive calls assertEqual backend comm_split_count broadcast_tensor = torch tensor rank cuda device new_pg broadcast broadcast_tensor wait assertEqual backend comm_split_count dist destroy_process_group requires_nccl_version Need NCCL + ncclCommSplit skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_non_blocking_with_eager_init Test creating pg eagerly nonblocking mode when we ve passed specific device_id init_process_group os environ TORCH_NCCL_USE_COMM_NONBLOCKING = os environ TORCH_NCCL_NONBLOCKING_TIMEOUT = store = c d FileStore file_name world_size device = torch device f cuda rank bound device trigger eager init mode pg = _create_process_group_nccl store opts device_id=device backend = pg _get_backend torch device device assertEqual backend comm_split_count reduce_tensor = torch rand device=device Run allreduce comm should have already started initilizaing allreduce issued CUDA STREAM only after initialization success pg allreduce reduce_tensor wait new_pg = c d new_group new pg s comm initialized eagerly assertEqual backend comm_split_count broadcast_tensor = torch tensor rank cuda device new_pg broadcast broadcast_tensor wait assertEqual backend comm_split_count dist destroy_process_group skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_non_blocking_p p Test creating pg using nonblocking mode eagerly os environ TORCH_NCCL_USE_COMM_NONBLOCKING = os environ TORCH_NCCL_NONBLOCKING_TIMEOUT = store = c d FileStore file_name world_size device = rank_to_GPU rank _create_process_group_nccl store opts Generate same tensor send_tensor = torch ones device=device rank == dist send send_tensor rank == recv_tensor = torch rand device=device dist recv recv_tensor assertEqual send_tensor recv_tensor dist destroy_process_group skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize eager_init True False test_subgroup_p p eager_init bool store = c d FileStore file_name world_size device = torch device f cuda rank torch cuda device_count c d init_process_group nccl world_size=self world_size rank=self rank store=store device_id=device eager_init None send_tensor = torch ones device=device group = dist new_group rank == dist send send_tensor group=group rank == recv_tensor = torch rand device=device dist recv recv_tensor group=group assertEqual send_tensor recv_tensor dist destroy_process_group requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_get_uid store = c d FileStore file_name world_size device = torch device f cuda rank pg = _create_process_group_nccl store opts device_id=device torch distributed distributed_c d _get_process_group_uid assertEqual _get_process_group_uid pg pg_ = c d new_group assertEqual _get_process_group_uid pg_ requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_set_process_group_desc store = c d FileStore file_name world_size device = torch device f cuda rank pg_default = _create_process_group_nccl store opts device_id=device assertEqual pg_default group_desc default_pg pg_ = c d new_group group_desc= test_purpose assertEqual pg_ group_desc test_purpose pg_ = c d new_group assertEqual pg_ group_desc undefined requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_deterministic_mode_no_break torch use_deterministic_algorithms True store = c d FileStore file_name world_size device = torch device f cuda rank _create_process_group_nccl store opts device_id=device tensor = torch empty device=device dist all_reduce tensor requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_init_with_idx store = c d FileStore file_name world_size device_idx = rank dist init_process_group world_size=self world_size rank=self rank store=store device_id=device_idx dist all_reduce torch empty device=torch device cuda device_idx requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_block_current_stream store = c d FileStore file_name world_size device = torch device f cuda rank pg = _create_process_group_nccl store opts device_id=device t = torch rand device=device work = pg allreduce t work block_current_stream torch cuda current_stream synchronize work wait torch cuda synchronize DistributedDataParallelTest test_c d_common CommonDistributedDataParallelTest MultiProcessTestCase setUp super setUp TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests use TORCH_NCCL_BLOCKING_WAIT will test expected os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = _spawn_processes _get_process_group store = _get_store c d init_process_group nccl store=store rank=self rank world_size=self world_size c d distributed_c d _get_default_group _test_nccl_backend devices device_ids multi_device=False gradient_as_bucket_view=False process_group = _get_process_group _test_ddp_with_process_group process_group devices device_ids multi_device gradient_as_bucket_view requires_nccl skip_if_lt_x_gpu test_nccl_propagate_error_reason Need use TORCH_NCCL_BLOCKING_WAIT ASYNC_ERROR_HANDLING otherwise process will taken down we can t check errors os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = os environ TORCH_NCCL_BLOCKING_WAIT = Need disable TORCH_NCCL_DUMP_ON_TIMEOUT otherwise test times out os environ TORCH_NCCL_DUMP_ON_TIMEOUT = store = c d FileStore file_name world_size provide sufficient timeout initialize NCCL comm pg = c d ProcessGroupNCCL store rank world_size timeout=timedelta seconds= pg_gloo = c d ProcessGroupGloo store rank world_size pg barrier wait timedelta seconds= Simulate stuckness rank rank == pg_gloo barrier wait inp = torch ones cuda rank rank = Time out due rank calling into allreduce assertRaises dist DistBackendError pg allreduce inp wait timedelta seconds= Now when nonzero rank attempts use communicator original failure reason should logged try pg allreduce torch ones cuda rank wait except dist DistBackendError e assertTrue aborted str e fail Expected error raised Unblock rank pg_gloo barrier wait TODO We can also test rank attempts use communicator then we should error out info aborted due timeout another rank Although would only case after watchdog has run rank there no reliable way confirm has run requires_nccl skip_if_lt_x_gpu test_nccl_backend_multi_device_ids_not_allowed int_devices = list range torch cuda device_count devices = torch device cuda + str i i int_devices assertRaisesRegex ValueError device_ids can only None contain single element _test_nccl_backend devices int_devices requires_nccl skip_if_lt_x_gpu test_nccl_backend_single_device_module_device_ids_None _test_nccl_backend None None requires_nccl skip_if_lt_x_gpu test_nccl_backend_single_device_module_empty_device_ids This tests backward compatibility accepting empty list ` device_ids ` although we no longer document favor default value ` None ` which consistent multi-device modules CPU modules _test_nccl_backend None requires_nccl skip_if_lt_x_gpu test_nccl_backend_multi_device_module_device_ids_None int_devices = gpus_for_rank world_size rank devices = torch device cuda + str i i int_devices _test_nccl_backend devices None multi_device=True requires_nccl skip_if_lt_x_gpu test_nccl_backend_ gpu_module_device_ids_integer_list int_devices = gpus_for_rank world_size rank devices = torch device cuda + str i i int_devices _test_nccl_backend devices int_devices requires_nccl skip_if_lt_x_gpu test_nccl_backend_ gpu_module_device_ids_torch_device_list int_devices = gpus_for_rank world_size rank devices = torch device cuda + str i i int_devices _test_nccl_backend devices devices requires_nccl skip_if_lt_x_gpu test_nccl_backend_ gpu_module int_devices = gpus_for_rank world_size rank devices = torch device cuda + str i i int_devices _test_nccl_backend devices None multi_device=True requires_nccl skip_if_lt_x_gpu test_nccl_backend_ gpu_module int_devices = gpus_for_rank world_size rank devices = torch device cuda + str i i int_devices _test_nccl_backend devices None multi_device=True requires_nccl skip_if_lt_x_gpu test_ddp_multi_device_module_config gpus = gpus_for_rank world_size rank assertTrue len gpus = expecting least gpus per process process_group = _get_process_group gpus = gpus model = DoubleGpuNet gpus assertRaisesRegex ValueError DistributedDataParallel device_ids output_device arguments only work single-device multiple-device GPU modules CPU modules DistributedDataParallel model output_device=gpus process_group=process_group assertRaisesRegex ValueError device_ids can only None contain single element DistributedDataParallel model device_ids=gpus process_group=process_group assertRaisesRegex ValueError input module must same type devices model fc = model fc cpu DistributedDataParallel model process_group=process_group model = model cpu assertRaisesRegex ValueError device_ids can only None contain single element DistributedDataParallel model device_ids=gpus process_group=process_group _test_fp gradient_as_bucket_view=False process_group = _get_process_group gpus = gpus_for_rank world_size rank model = nn Linear bias=False cuda gpus half nn init constant_ model weight ddp_model = DistributedDataParallel model device_ids= gpus process_group=process_group bucket_cap_mb= gradient_as_bucket_view=gradient_as_bucket_view Input so gradients will overflow world_size unless we normalize gradient world_size before reduction input = torch tensor cuda gpus half Step model ddp_model train output = ddp_model input loss = output sum loss backward assertFalse any torch isinf p grad any p ddp_model parameters requires_nccl skip_if_lt_x_gpu test_fp _test_fp requires_nccl skip_if_lt_x_gpu test_fp _grad_is_view _test_fp gradient_as_bucket_view=True _test_arbitrary_forward_return_value gradient_as_bucket_view=False Note test can sped up only running CPU module once DistributedDataParallel supports them process_group = _get_process_group ForwardReturnValueModule nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False fc = nn Linear bias=False relu = nn ReLU forward x fn x = relu fc x x = relu fc x The first softmax does NOT include fc its autograd graph whereas second softmax DOES If we pass only first tensor we see output reducer marks gradient fc ready because doesn t show up If downstream uses value choose differentiate against second output tensor would still receive gradient callback tensor resulting crash fn F softmax x dim= F softmax fc x dim= device_id = gpus_for_rank world_size rank model = DistributedDataParallel ForwardReturnValueModule float device_id device_ids= device_id process_group=process_group gradient_as_bucket_view=gradient_as_bucket_view batch_size = criterion = nn CrossEntropyLoss input = torch rand batch_size dtype=torch float target = torch LongTensor random randrange _ range batch_size device_id Always run backward ensure reducer called autograd If we don t correctly capture output tensors value reducer won t see hook unused parameter throw error The correct capture what we re testing function test box unbox output = model input fn=box loss = criterion unbox output target loss backward Test identity value test box=lambda x y x y unbox=lambda obj obj Test list value test box=lambda x y foo x bar y unbox=lambda obj obj Test tuple value test box=lambda x y foo x bar y unbox=lambda obj obj Test dict value test box=lambda x y foo bar x b y unbox=lambda obj obj b Test list dict value test box=lambda x y foo bar x b y unbox=lambda obj obj b Test dict list value test box=lambda x y foo bar list x y unbox=lambda obj obj list requires_nccl skip_if_lt_x_gpu test_arbitrary_forward_return_value _test_arbitrary_forward_return_value requires_nccl skip_if_lt_x_gpu test_arbitrary_forward_return_value_grad_is_view _test_arbitrary_forward_return_value gradient_as_bucket_view=True requires_nccl skip_if_lt_x_gpu test_ddp_with_lazy_parameters process_group = _get_process_group assertRaisesRegex RuntimeError Modules uninitialized parameters DistributedDataParallel torch nn LazyLinear process_group=process_group _test_find_unused_parameters_kwarg gradient_as_bucket_view=False Note test can sped up only running CPU module once DistributedDataParallel supports them torch cuda set_device rank dist init_process_group backend= nccl world_size=self world_size rank=self rank init_method=f file file_name process_group = c d distributed_c d _get_default_group FindUnusedParametersModule nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False fc = nn Linear bias=False relu = nn ReLU forward x x = relu fc x x = relu fc x Return fc module so caller can invoke outside forward function While bad practice we can use trigger reducer error F softmax x dim= fc device_id = gpus_for_rank world_size rank batch_size = criterion = nn CrossEntropyLoss input = torch rand batch_size dtype=torch float target = torch LongTensor random randrange _ range batch_size device_id ddp_model = None test_find_unused_parameters find_unused_parameters test_default=False gradient_as_bucket_view=False test_default model = DistributedDataParallel FindUnusedParametersModule float device_id device_ids= device_id process_group=process_group gradient_as_bucket_view=gradient_as_bucket_view model = DistributedDataParallel FindUnusedParametersModule float device_id device_ids= device_id process_group=process_group find_unused_parameters=find_unused_parameters gradient_as_bucket_view=gradient_as_bucket_view nonlocal ddp_model ddp_model = model output fc = model input output = fc output loss = criterion output target loss backward First test finding unused params under these conditions trigger error when ` backward ` called because fc unused parameter will therefore marked ready twice try test_find_unused_parameters True gradient_as_bucket_view=gradient_as_bucket_view except Exception ex assertTrue str ex startswith Expected mark variable ready only once unused_index = unused_index_str = f Parameter index unused_index model = ddp_model module module_name module model named_modules module == model fc parameter_name _ module named_parameters recurse=False unused_fqn = f module_name parameter_name Only one such parameter model fc since bias=False break dist get_debug_level = dist DebugLevel OFF unused_index_str += f name unused_fqn assertTrue unused_index_str str ex fail Expected exception dist barrier process_group Then test default behavior can overridden setting ` find_unused_parameters=False ` try test_find_unused_parameters False gradient_as_bucket_view=gradient_as_bucket_view except Exception ex fail f Unexpected exception ex Test find_unused_parameters defaults False try test_find_unused_parameters True test_default=True gradient_as_bucket_view=gradient_as_bucket_view except Exception ex fail f Unexpected exception ex TODO Combine following tests once https github com pytorch pytorch issues resolved requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= DETAIL test_find_unused_parameters_kwarg_debug_detail _test_find_unused_parameters_kwarg requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= INFO test_find_unused_parameters_kwarg_debug_info _test_find_unused_parameters_kwarg requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= OFF test_find_unused_parameters_kwarg_debug_off _test_find_unused_parameters_kwarg requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= DETAIL test_find_unused_parameters_kwarg_grad_is_view_debug_detail _test_find_unused_parameters_kwarg gradient_as_bucket_view=True requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= INFO test_find_unused_parameters_kwarg_grad_is_view_debug_info _test_find_unused_parameters_kwarg gradient_as_bucket_view=True requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= OFF test_find_unused_parameters_kwarg_grad_is_view_debug_off _test_find_unused_parameters_kwarg gradient_as_bucket_view=True _test_multiple_outputs_multiple_backward gradient_as_bucket_view=False Note test can sped up only running CPU module once DistributedDataParallel supports them process_group = _get_process_group MultipleOutputModule nn Module __init__ - None super __init__ define_module nn Sequential nn Linear bias=False nn ReLU nn Linear bias=False nn ReLU module = define_module module = define_module forward x F softmax module x dim= F softmax module x dim= device_id = gpus_for_rank world_size rank model = DistributedDataParallel MultipleOutputModule float device_id device_ids= device_id process_group=process_group gradient_as_bucket_view=gradient_as_bucket_view batch_size = criterion = nn CrossEntropyLoss input = torch rand batch_size dtype=torch float target = torch LongTensor random randrange _ range batch_size device_id Compute loss gradients both outputs output output = model input loss = criterion output target loss backward loss = criterion output target loss backward requires_nccl skip_if_lt_x_gpu test_multiple_outputs_multiple_backward _test_multiple_outputs_multiple_backward requires_nccl skip_if_lt_x_gpu test_multiple_outputs_multiple_backward_grad_is_view _test_multiple_outputs_multiple_backward gradient_as_bucket_view=True requires_nccl skip_if_lt_x_gpu test_no_grad Note test can sped up only running CPU module once DistributedDataParallel supports them process_group = _get_process_group NoGradModule nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False relu = nn ReLU forward x x = relu fc x x = relu fc x F softmax x dim= device_id = gpus_for_rank world_size rank model = DistributedDataParallel NoGradModule float device_id device_ids= device_id process_group=process_group batch_size = input = torch rand batch_size dtype=torch float check_no_grads p model parameters assertTrue p requires_grad assertIsNone p grad After initialization no parameter has their gradient set check_no_grads Run ` forward ` function torch no_grad torch no_grad output = model input assertTrue isinstance output torch Tensor No parameter should have their gradient set check_no_grads _test_accumulate_gradients_module gradient_as_bucket_view=False This NOT recommended way implement accumulating grads we would like make sure DDP does mess up underlying module int_devices = gpus_for_rank world_size rank devices = torch device cuda + str i i int_devices process_group = _get_process_group global_batch_size = world_size model ddp_model input target = _prepare_single_device_module process_group devices devices global_batch_size gradient_as_bucket_view step_model model input target model train output = model input loss = F mse_loss output target output device loss backward ensure accumulate grads works no_grad torch no_grad ddp_model train ddp_model module input Check two model parameters over iterations Use iterations because we alternate between reducing reducing want make sure we switch both ways iteration range step_model model input target iteration == Skip gradients sync without calling prepare_for_backward step_model ddp_model module input rank rank + target rank rank + i j zip model parameters ddp_model parameters assertNotEqual i grad j grad step_model ddp_model input rank rank + target rank rank + i j zip model parameters ddp_model parameters assertEqual i grad j grad rtol= e- atol= e- Shuffle input so DDP input different torch manual_seed + iteration input = input torch randperm global_batch_size requires_nccl skip_if_lt_x_gpu test_accumulate_gradients_module _test_accumulate_gradients_module requires_nccl skip_if_lt_x_gpu test_accumulate_gradients_module_with_grad_is_view _test_accumulate_gradients_module gradient_as_bucket_view=True requires_nccl skip_if_lt_x_gpu test_failure_recovery process_group = _get_process_group need create separate file recovered FileStore because original one will deleted when destructing first FileStore recovery_filename = file_name + _recovery rank == file will deleted recovered FileStore open recovery_filename w close necessary run barrier here DDP will synchronize TestModel nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False relu = nn ReLU forward x x = relu fc x x = relu fc x F softmax x dim= device_id = gpus_for_rank world_size rank model = TestModel float device_id ddp = DistributedDataParallel model device_ids= device_id process_group=process_group batch_size = criterion = nn CrossEntropyLoss input = torch rand batch_size dtype=torch float target = torch LongTensor random randrange _ range batch_size device_id _ range output = ddp input loss = criterion output target loss backward del ddp c d destroy_process_group process_group store = c d FileStore recovery_filename world_size c d init_process_group nccl store=store rank=self rank world_size=self world_size process_group = c d distributed_c d _get_default_group ddp = DistributedDataParallel model device_ids= device_id process_group=process_group input = torch rand batch_size dtype=torch float target = torch LongTensor random randrange _ range batch_size device_id _ range output = ddp input loss = criterion output target loss backward requires_nccl skip_if_lt_x_gpu test_pass_default_pg dist init_process_group nccl init_method=f file file_name world_size=self world_size rank=self rank default_pg = c d distributed_c d _get_default_group dist destroy_process_group default_pg assertFalse dist is_initialized _test_grad_layout replica_devices layer_devs local_batch_size process_group = _get_process_group global_batch_size = local_batch_size world_size Carry out some trials small buckets some big buckets bucketsizes = Tuples lists Each list describes per-layer characteristics one trial layer_formats = torch contiguous_format torch channels_last + torch contiguous_format torch channels_last layer_dtypes = torch float torch float + torch half torch half input_dev = layer_devs isinstance layer_devs list layer_devs target_dev = layer_devs - isinstance layer_devs list layer_devs input = torch randn global_batch_size device=input_dev dtype=torch float target = torch randn global_batch_size device=target_dev dtype=torch float local_batch_start = rank local_batch_size local_batch_end = rank + local_batch_size Reducer cpp sneakily creates one initial bucket ignores bucket_cap_mb argument The following makes sure initial bucket also complies contextmanager first_bucket_size ddp_bucket_mb old_DEFAULT_FIRST_BUCKET_BYTES = dist _DEFAULT_FIRST_BUCKET_BYTES dist _DEFAULT_FIRST_BUCKET_BYTES = int ddp_bucket_mb e try yield finally dist _DEFAULT_FIRST_BUCKET_BYTES = old_DEFAULT_FIRST_BUCKET_BYTES torch backends cudnn flags enabled=True deterministic=True benchmark=False formats dtypes bucketsize product layer_formats layer_dtypes bucketsizes first_bucket_size bucketsize model_msg = f rank = rank formats = formats dtypes = dtypes bucketsize = bucketsize try m = ConvNet layer_devs formats dtypes m_ddp = DistributedDataParallel copy deepcopy m device_ids=replica_devices process_group=process_group bucket_cap_mb=bucketsize opt = torch optim SGD m parameters lr= opt_ddp = torch optim SGD m_ddp parameters lr= has_half = any p dtype torch half p m parameters tol = e- has_half e- except BaseException Prints case-specific debugging info narrow down failing case print Caught exception during model creation + model_msg flush=True raise iters First iter creates grads second iter retests after rebucketing third iter tries zeroed grads range iter_msg = f iter = + model_msg named_msg = iter_msg try F mse_loss m input float target backward F mse_loss m_ddp input local_batch_start local_batch_end float target local_batch_start local_batch_end backward i layer_name m_child m_ddp_child enumerate zip m named_children m_ddp module children named_msg = layer_name + weight + + iter_msg assertTrue m_child weight grad is_contiguous memory_format=formats i named_msg assertTrue m_ddp_child weight grad is_contiguous memory_format=formats i named_msg param_name p p_ddp zip m_child named_parameters m_ddp_child parameters named_msg = layer_name + + param_name + + iter_msg assertEqual p grad p_ddp grad rtol=tol atol=tol opt step opt_ddp step == p p_ddp zip m parameters m_ddp parameters p grad = None p_ddp grad = None m zero_grad m_ddp zero_grad except BaseException Makes sure we still get info error occurred somewhere other than asserts print Caught exception during iterations + named_msg flush=True raise requires_nccl skip_if_lt_x_gpu test_grad_layout_ devicemodule_ replicaperprocess dev = torch device cuda + str gpus_for_rank world_size rank Tells DDP use just one device replica_devices = dev Tells _test_grad_layout construct ConvNet all layers process s first assigned device layer_devs = dev local_batch_size = _test_grad_layout replica_devices layer_devs local_batch_size requires_nccl skip_if_lt_x_gpu skip_if_rocm_multiprocess test_grad_layout_ devicemodule int_devices = gpus_for_rank world_size rank dev = torch device cuda + str int_devices dev = torch device cuda + str int_devices DDP s default behavior multi-device module don t replicate replica_devices = None Tells _test_grad_layout constructs process s ConvNet devices layers each device layer_devs = dev + dev local_batch_size = _test_grad_layout replica_devices layer_devs local_batch_size requires_nccl skip_if_lt_x_gpu test_param_layout_mismatch_error process_group = _get_process_group dev = torch device cuda + str gpus_for_rank world_size rank layer_devs = dev layer_formats = torch contiguous_format rank == torch channels_last layer_dtypes = torch float m = ConvNet layer_devs layer_formats layer_dtypes rank == DistributedDataParallel m device_ids= dev process_group=process_group assertRaisesRegex RuntimeError appears match strides same param process DistributedDataParallel m device_ids= dev process_group=process_group _gpu_model_with_ddp_comm_hook process_group hook=None gradient_as_bucket_view=False state=None static_graph=False device_id = gpus_for_rank world_size rank gpu_model = DistributedDataParallel ModuleForDdpCommHook device_id device_ids= device_id process_group=process_group gradient_as_bucket_view=gradient_as_bucket_view static_graph=static_graph Register DDP communication hook any hook None gpu_model register_comm_hook state hook gpu_model requires_nccl skip_if_lt_x_gpu test_ddp_comm_hook_future_passing_gpu_nccl This unit test verifies whether Future object passed properly using nccl backend The hook callback function creates Future object sets value process_group = _get_process_group Get GPU model simple_hook registered gpu_model = _gpu_model_with_ddp_comm_hook process_group _simple_hook check whether grads equal what simple_hook s then callback returns without comm_hook result would torch ones _run_and_verify_hook gpu_model torch ones _test_ddp_comm_hook_allreduce_hook_nccl gradient_as_bucket_view=False static_graph=False This unit test verifies whether DDP communication hook just calls allreduce gives same result case no hook registered Without then callback future_value reducer no longer PyObject unit test verifies future_value properly checked process_group = _get_process_group allreduce_hook state object bucket dist GradBucket - torch futures Future torch Tensor tensors = bucket buffer world_size process_group allreduce tensors get_future then lambda fut fut value Get GPU model allreduce_hook registered gpu_model = _gpu_model_with_ddp_comm_hook process_group allreduce_hook gradient_as_bucket_view static_graph check whether grads equal what DDP without hook would _run_and_verify_hook gpu_model torch ones _test_default_ddp_comm_hooks_nccl gradient_as_bucket_view=False This unit test verifies whether default Python DDP communication hooks ALLREDUCE FP _COMPRESS BF _COMPRESS can give same result case no hook registered process_group = _get_process_group For these default DDP comm hooks only state process group state = process_group hook_options = default allreduce_hook default fp _compress_hook TEST_WITH_ROCM BFLOAT _AVAILABLE c d is_nccl_available torch cuda nccl version = hook_options append default bf _compress_hook hook hook_options Get GPU model hook registered The first arg process_group used initializing test environment so cannot replaced state although they have same value gpu_model = _gpu_model_with_ddp_comm_hook process_group hook gradient_as_bucket_view state check whether grads equal what DDP without hook would _run_and_verify_hook gpu_model torch ones _test_fp _compress_wrapper gradient_as_bucket_view=False This unit test verifies whether wrapping ALLREDUCE POWER_SGD hooks FP _WRAPPER can give same result when there no hook registered process_group = _get_process_group powerSGD_state = powerSGD PowerSGDState process_group=process_group hook_args = powerSGD powerSGD_hook powerSGD_state default allreduce_hook process_group hook state hook_args gpu_model = _gpu_model_with_ddp_comm_hook process_group default fp _compress_wrapper hook gradient_as_bucket_view state check whether grads equal what DDP without hook would _run_and_verify_hook gpu_model torch ones _test_bf _compress_wrapper gradient_as_bucket_view=False This unit test verifies whether wrapping ALLREDUCE POWER_SGD hooks BF _WRAPPER can give same result when there no hook registered process_group = _get_process_group powerSGD_state = powerSGD PowerSGDState process_group=process_group hook_args = powerSGD powerSGD_hook powerSGD_state default allreduce_hook process_group hook state hook_args gpu_model = _gpu_model_with_ddp_comm_hook process_group default bf _compress_wrapper hook gradient_as_bucket_view state check whether grads equal what DDP without hook would _run_and_verify_hook gpu_model torch ones _test_powerSGD_ddp_comm_hook_nccl gradient_as_bucket_view=False This unit test verifies whether Python DDP communication hook POWER_SGD can give same result case no hook registered process_group = _get_process_group Get GPU model hook registered Test hook different algorithmic configs use_error_feedback warm_start batch_tensors_with_same_shape product True False True False True False state = powerSGD PowerSGDState process_group=process_group matrix_approximation_rank= use_error_feedback=use_error_feedback warm_start=warm_start batch_tensors_with_same_shape=batch_tensors_with_same_shape hook powerSGD powerSGD_hook powerSGD batched_powerSGD_hook gpu_model = _gpu_model_with_ddp_comm_hook process_group hook gradient_as_bucket_view state check whether grads equal what DDP without hook would _run_and_verify_hook gpu_model torch ones _test_builtin_ddp_comm_hooks_nccl gradient_as_bucket_view=False This unit test verifies whether built-in C++ DDP communication hooks ALLREDUCE FP _COMPRESS can give same result case no hook registered process_group = _get_process_group comm_hook_type dist BuiltinCommHookType ALLREDUCE dist BuiltinCommHookType FP _COMPRESS Get GPU model built-in communication hook gpu_model = _gpu_model_with_builtin_ddp_comm_hook process_group comm_hook_type gradient_as_bucket_view check whether grads equal what DDP without hook would _run_and_verify_hook gpu_model torch ones requires_nccl skip_if_lt_x_gpu test_ddp_comm_hook_allreduce_hook_nccl _test_ddp_comm_hook_allreduce_hook_nccl requires_nccl skip_if_lt_x_gpu test_default_ddp_comm_hooks_nccl _test_default_ddp_comm_hooks_nccl requires_nccl skip_if_lt_x_gpu test_fp _compress_wrapper_nccl _test_fp _compress_wrapper requires_nccl requires_nccl_version Need NCCL + BF _COMPRESS skip_but_pass_in_sandcastle_if BFLOAT _AVAILABLE BFloat only supported CUDA + skip_if_lt_x_gpu test_bf _compress_wrapper_nccl _test_bf _compress_wrapper requires_nccl skip_if_lt_x_gpu test_builtin_ddp_comm_hooks_nccl _test_builtin_ddp_comm_hooks_nccl requires_nccl skip_if_lt_x_gpu test_powerSGD_ddp_comm_hook_nccl _test_powerSGD_ddp_comm_hook_nccl requires_nccl skip_if_lt_x_gpu test_ddp_comm_hook_allreduce_hook_nccl_grad_is_view _test_ddp_comm_hook_allreduce_hook_nccl gradient_as_bucket_view=True requires_nccl skip_if_lt_x_gpu test_ddp_comm_hook_allreduce_hook_nccl_static_graph _test_ddp_comm_hook_allreduce_hook_nccl static_graph=True requires_nccl skip_if_lt_x_gpu test_default_ddp_comm_hooks_nccl_is_view _test_default_ddp_comm_hooks_nccl gradient_as_bucket_view=True requires_nccl skip_if_lt_x_gpu test_fp _compress_wrapper_is_view _test_fp _compress_wrapper gradient_as_bucket_view=True requires_nccl requires_nccl_version Need NCCL + BF _COMPRESS skip_but_pass_in_sandcastle_if BFLOAT _AVAILABLE BFloat only supported CUDA + skip_if_lt_x_gpu test_bf _compress_wrapper_is_view _test_bf _compress_wrapper gradient_as_bucket_view=True requires_nccl skip_if_lt_x_gpu test_builtin_ddp_comm_hooks_nccl_grad_is_view _test_builtin_ddp_comm_hooks_nccl gradient_as_bucket_view=True requires_nccl skip_if_lt_x_gpu test_powerSGD_ddp_comm_hook_nccl_grad_is_view _test_powerSGD_ddp_comm_hook_nccl gradient_as_bucket_view=True requires_nccl skip_if_lt_x_gpu test_ddp_comm_hook_allreduce_with_then_hook_nccl This unit test verifies whether DDP communication hook calls allreduce then multiplies result ten divides two gives expected result process_group = _get_process_group allreduce_with_then_hook state object bucket dist GradBucket - torch futures Future torch Tensor tensors = bucket buffer world_size fut = process_group allreduce tensors get_future mult fut Multiply result fut value div fut Divide result fut value fut then mult then div Get GPU model allreduce_with_then_hook registered gpu_model = _gpu_model_with_ddp_comm_hook process_group allreduce_with_then_hook check whether grads equal what allreduce returns multiplied without comm_hook result would still torch ones _run_and_verify_hook gpu_model torch ones AcceptsParam torch nn Module __init__ p factor super __init__ = p f = factor forward input input + f requires_nccl skip_if_lt_x_gpu test_ddp_weight_sharing process_group = _get_process_group size = dev = rank world = world_size p = torch nn Parameter torch randn size requires_grad=True try_set_to_none use_bucket_view product False True False True m = torch nn Sequential AcceptsParam p dev + AcceptsParam p dev + cuda dev m = torch nn parallel DistributedDataParallel m bucket_cap_mb= gradient_as_bucket_view=use_bucket_view device_ids= dev process_group=process_group _ range m zero_grad set_to_none=try_set_to_none m sum backward Each param value multiplied rank + twice forward so grad values produced particular rank should rank + Summing these over ranks dividing world size gives expected result analytic = torch full_like p world world + world device=dev name p m named_parameters assertEqual p grad analytic mismatch + name + grad + f set_to_none = try_set_to_none use_bucket_view = use_bucket_view requires_nccl skip_if_lt_x_gpu test_ddp_packed_sequence Tests DDP ` ` device_ids ` ` specified can run forward backward pass ` ` PackedSequence ` ` s parity compared local version model store = c d FileStore file_name world_size process_group = dist init_process_group nccl world_size=self world_size rank=self rank store=store seqs = sequence_sequence seq sequence vocab = pad + sorted ch seq seqs ch seq vectorized_seqs = vocab index tok tok seq seq seqs Set seed make embedding LSTM deterministic even across ranks since DDP broadcasts parameters rank torch manual_seed embed = nn Embedding len vocab keep CPU lstm = nn LSTM input_size= hidden_size= batch_first=True rank lstm_ddp = DistributedDataParallel copy deepcopy lstm device_ids= rank process_group=process_group p p zip lstm parameters lstm_ddp module parameters assertEqual p p seq_lengths = torch LongTensor list map len vectorized_seqs seq_tensor = torch Tensor torch zeros len vectorized_seqs seq_lengths max long i seq seq_len enumerate zip vectorized_seqs seq_lengths seq_tensor i seq_len = torch LongTensor seq seq_lengths permutation_idx = seq_lengths sort descending=True seq_tensor = seq_tensor permutation_idx embedded_seq_tensor = embed seq_tensor packed_input = torch nn utils rnn pack_padded_sequence embedded_seq_tensor seq_lengths batch_first=True packed_input_ddp = torch nn utils rnn pack_padded_sequence embedded_seq_tensor detach clone seq_lengths batch_first=True Move input GPU explicitly local model packed_output ht ct = lstm packed_input rank Let DDP move input GPU internally packed_output_ddp ht_ddp ct_ddp = lstm_ddp packed_input_ddp assertEqual packed_output data packed_output_ddp data assertEqual ht ht_ddp assertEqual ct ct_ddp packed_output data sum backward packed_output_ddp data sum backward p p zip lstm parameters lstm_ddp parameters assertEqual p grad p grad requires_nccl skip_if_lt_x_gpu test_channels_last_contig process_group = _get_process_group device = torch device f cuda rank tensor = torch ones dtype=torch float device=device memory_format=torch channels_last process_group broadcast tensor wait requires_nccl skip_if_lt_x_gpu test_ddp_complex_params process_group = _get_process_group device_id = gpus_for_rank world_size rank N C H W = ddp_model = DistributedDataParallel FFTModel hin=H win=W n_features=C device_id device_ids= device_id process_group=process_group optimizer = torch optim Adam ddp_model parameters lr= inp = torch ones N C H W dtype=torch float train step out = ddp_model inp loss = torch sum out loss backward optimizer step torch cuda synchronize device=device_id WorkHookTest MultiProcessTestCase property world_size setUp super setUp set TORCH_NCCL_ENABLE_TIMING enable timing CUDAEvents ProcessGroup Work os environ TORCH_NCCL_ENABLE_TIMING = _spawn_processes tearDown super tearDown del os environ TORCH_NCCL_ENABLE_TIMING try os remove file_name except OSError pass _get_store dist FileStore file_name world_size _get_process_group store = _get_store c d init_process_group nccl store=store rank=self rank world_size=self world_size c d distributed_c d _get_default_group requires_nccl skip_if_lt_x_gpu test_on_completion_hook_broadcast pg = _get_process_group num_hook_fired = durations list float = hook work_info torch _C _distributed_c d WorkInfo nonlocal num_hook_fired durations num_hook_fired += durations append work_info active_duration total_seconds pg _register_on_completion_hook hook tensor = torch ones cuda rank rank pg broadcast tensor wait pg broadcast tensor wait N B destroy_process_group necessary wait all pending works finish c d destroy_process_group pg assertEqual num_hook_fired assertEqual len durations duration durations assertTrue duration assertEqual tensor torch zeros cuda rank requires_nccl skip_if_lt_x_gpu test_on_completion_hook_mixed_ops pg = _get_process_group num_hook_fired = durations list float = hook work_info torch _C _distributed_c d WorkInfo nonlocal num_hook_fired durations num_hook_fired += durations append work_info active_duration total_seconds pg _register_on_completion_hook hook tensor = torch ones cuda rank tensor_list = torch empty_like tensor _ range world_size intentionally using async ops pg allreduce tensor pg allgather tensor_list tensor pg allreduce tensor N B destroy_process_group necessary wait all pending works finish c d destroy_process_group pg assertEqual num_hook_fired assertEqual len durations duration durations assertTrue duration assertEqual tensor torch ones cuda rank world_size world_size assertEqual tensor_list torch ones cuda rank world_size _ range world_size requires_nccl skip_if_lt_x_gpu test_on_completion_hook_with_ddp pg = _get_process_group num_hook_fired dict int int = durations dict OpType list float = hook work_info torch _C _distributed_c d WorkInfo nonlocal num_hook_fired durations op_type = work_info op_type op_type num_hook_fired num_hook_fired op_type = durations op_type = num_hook_fired op_type += durations op_type append work_info active_duration total_seconds pg _register_on_completion_hook hook nlayers = net = nn Sequential nn Linear bias=False _ range nlayers rank ddp = DistributedDataParallel net device_ids= rank process_group=pg bucket_cap_mb= pg _wait_for_pending_works DDP expected synchronize model parameter broadcasting rank other ranks However DDP s internal implementation which subject change future versions assertTrue num_hook_fired OpType BROADCAST ctor_allreduce = num_hook_fired get OpType ALLREDUCE x = torch zeros cuda rank ddp x sum backward c d destroy_process_group pg assertTrue OpType ALLREDUCE num_hook_fired The number allreduce ops depend DDP internal implementation there should least one allreduce assertTrue num_hook_fired OpType ALLREDUCE - ctor_allreduce assertTrue all duration duration chain durations values Not testing FSDP due https github com pytorch pytorch issues We cannot disable workCleanupLoop hooks fired thread requires_nccl skip_if_lt_x_gpu test_on_completion_hook_all_gather_object torch cuda set_device rank pg = _get_process_group num_hook_fired dict int int = durations dict OpType list float = hook work_info torch _C _distributed_c d WorkInfo nonlocal num_hook_fired durations op_type = work_info op_type op_type num_hook_fired num_hook_fired op_type = durations op_type = num_hook_fired op_type += durations op_type append work_info active_duration total_seconds pg _register_on_completion_hook hook obj = rank rank world_size world_size obj_list = None _ range world_size c d all_gather_object obj_list obj group=pg r o enumerate obj_list assertTrue isinstance o dict assertTrue set o keys rank world_size assertEqual o rank r assertEqual o world_size world_size c d destroy_process_group pg assertTrue OpType ALLGATHER num_hook_fired assertEqual len num_hook_fired two allgathers one size another values assertEqual num_hook_fired OpType ALLGATHER assertTrue all duration duration durations OpType ALLGATHER requires_nccl skip_if_lt_x_gpu test_on_completion_hook_seq pg = _get_process_group num_hook_fired = seq int = - work int = hook work_info torch _C _distributed_c d WorkInfo nonlocal num_hook_fired seq num_hook_fired += seq = work_info seq pg _register_on_completion_hook hook tensor = torch ones cuda rank rank work_count = _ range work_count work += pg broadcast tensor wait N B destroy_process_group necessary wait all pending works finish c d destroy_process_group pg assertEqual num_hook_fired work_count assertEqual work seq NcclErrorHandlingTest MultiProcessTestCase setUp super setUp TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests use TORCH_NCCL_BLOCKING_WAIT will test expected os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = _spawn_processes tearDown super tearDown try os remove file_name except OSError pass property op_timeout_sec property world_size property blocking_wait_error_msg timeout _run_all_reduce pg pg allreduce torch rand cuda rank _reduce_timeout set heartbeat timeout small value so we don t wait too long things shutdown os environ TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC = os environ TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC = requires_nccl skip_if_lt_x_gpu skip_if_rocm_multiprocess test_send_recv_non_dense_tensor store = c d FileStore file_name world_size device = torch device cuda rank torch cuda device_count dist init_process_group rank=self rank world_size=self world_size store=store device_id=device full = torch empty device=device fill_ rank Take slice col dimension making non-dense block = full rank == assertRaises ValueError dist send block dst= rank == assertRaises ValueError dist recv block src= requires_nccl requires_nccl_version Need NCCL + error checking skip_if_lt_x_gpu skip_if_rocm_multiprocess skip_but_pass_in_sandcastle Test does pass when run locally test_nccl_errors_nonblocking _reduce_timeout Note we unset restore TORCH_NCCL_ASYNC_ERROR_HANDLING test since test_c d_common runs async error handling default tests behavior when enabled prev_nccl_async_error_handling = os environ get TORCH_NCCL_ASYNC_ERROR_HANDLING None os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = store = c d FileStore file_name world_size process_group = c d ProcessGroupNCCL store rank world_size process_group allreduce torch rand cuda rank rank == This allreduce does block Python thread allreduce enqueues cuda operation then wait only blocks current cuda stream work = process_group allreduce torch rand cuda rank work wait Now work scheduled next should hang forever since previous allreduce will never complete t = threading Thread target=self _run_all_reduce args= process_group t daemon = True t start t join int get_timeout id assertTrue t is_alive prev_nccl_async_error_handling None os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = prev_nccl_async_error_handling requires_nccl skip_if_lt_x_gpu skip_if_rocm_multiprocess test_nccl_errors_blocking _reduce_timeout os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = store = c d FileStore file_name world_size process_group = c d ProcessGroupNCCL store rank world_size x = torch rand cuda rank process_group allreduce x rank == work = process_group allreduce x assertRaisesRegex dist DistBackendError work wait timeout=timedelta seconds=self op_timeout_sec _test_barrier_error store = c d FileStore file_name world_size process_group = c d ProcessGroupNCCL store rank world_size timeout=timedelta seconds= process_group barrier wait rank == assertRaisesRegex dist DistBackendError It seems error message would different depending whether test run CI machine devGPU Skipping error message check make both sides happy process_group barrier wait timeout=timedelta seconds=self op_timeout_sec with_nccl_blocking_wait requires_nccl requires_nccl_version Need NCCL + error checking skip_if_lt_x_gpu test_nccl_blocking_wait_with_barrier _reduce_timeout _test_barrier_error requires_nccl requires_nccl_version Need NCCL + error checking skip_if_lt_x_gpu test_nccl_non_blocking_wait_with_barrier _reduce_timeout test barrier behavior non blocking wait setting prev_nccl_async_error_handling = os environ get TORCH_NCCL_ASYNC_ERROR_HANDLING None avoid watchdog thread interference os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = _test_barrier_error prev_nccl_async_error_handling None os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = prev_nccl_async_error_handling requires_nccl requires_nccl_version Need NCCL + error checking skip_if_lt_x_gpu test_error_detection_and_propagation assert_fut_success fut assertEqual WorkResult fut value WorkResult SUCCESS test barrier behavior non blocking wait setting prev_nccl_async_error_handling = os environ get TORCH_NCCL_ASYNC_ERROR_HANDLING None avoid watchdog thread interference os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = os environ TORCH_NCCL_PROPAGATE_ERROR = set heartbeat timeout small value so we don t wait too long things shutdown os environ TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC = store = c d FileStore file_name world_size process_group = c d ProcessGroupNCCL store rank world_size timeout=timedelta seconds= assertEqual process_group get_error ErrorType SUCCESS barrier_work = process_group barrier barrier_work wait barrier_result = barrier_work get_future_result wait assertEqual WorkResult barrier_result WorkResult SUCCESS ar_work = process_group allreduce torch rand cuda rank ar_work wait fut = ar_work get_future_result test adding callback function fut then assert_fut_success rank == work = process_group allreduce torch rand cuda rank work wait result = work get_future_result wait assertEqual WorkResult result WorkResult TIMEOUT assertEqual process_group get_error ErrorType TIMEOUT other ranks exiting before rank timeout avoid nccl error happening before rank timeouts time sleep assertEqual process_group get_error ErrorType REMOTE_ERROR Mimicking all ranks sensing timeout abort process_group abort prev_nccl_async_error_handling None os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = prev_nccl_async_error_handling requires_nccl requires_nccl_version Need NCCL + error checking skip_if_rocm_multiprocess skip_if_lt_x_gpu test_restart_pg_after_error _reduce_timeout test barrier behavior non blocking wait setting prev_nccl_async_error_handling = os environ get TORCH_NCCL_ASYNC_ERROR_HANDLING None avoid FR dumping logic during restart os environ TORCH_NCCL_DUMP_ON_TIMEOUT = avoid watchdog thread interference os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = os environ TORCH_NCCL_PROPAGATE_ERROR = store = c d FileStore file_name world_size device = torch device f cuda rank torch cuda device_count initialize pg first time c d init_process_group nccl timeout=timedelta seconds= world_size=self world_size rank=self rank store=store pg = c d distributed_c d _get_default_group nccl_backend = pg _get_backend torch device device assertEqual nccl_backend get_error ErrorType SUCCESS barrier_work = nccl_backend barrier barrier_work wait barrier_result = barrier_work get_future_result wait assertEqual WorkResult barrier_result WorkResult SUCCESS assertEqual nccl_backend get_error ErrorType SUCCESS rank == work = nccl_backend allreduce torch rand cuda rank work wait result = work get_future_result wait assertEqual WorkResult result WorkResult TIMEOUT assertEqual nccl_backend get_error ErrorType TIMEOUT we need brand new fileStore new PG new file name shared through old fileStore new_file_name = tempfile NamedTemporaryFile delete=False name store set file new_file_name other ranks exiting before rank timeout avoid nccl error happening before rank timeouts time sleep assertEqual nccl_backend get_error ErrorType REMOTE_ERROR new_file_name = store get file decode all ranks restart using new store after detecting timeout error nccl_backend abort dist destroy_process_group new_store = c d FileStore new_file_name world_size re-initialize pg c d init_process_group nccl world_size=self world_size rank=self rank store=new_store new_pg = c d distributed_c d _get_default_group new_nccl_backend = new_pg _get_backend torch device device t = torch rand device=device dist all_reduce t assertEqual new_nccl_backend get_error ErrorType SUCCESS torch cuda synchronize dist destroy_process_group give some time other ranks exit first before destroying FileStore rank == time sleep os remove new_file_name prev_nccl_async_error_handling None os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = prev_nccl_async_error_handling _run_invalid_nccl_blocking_wait_env val os environ TORCH_NCCL_BLOCKING_WAIT = val store = c d FileStore file_name world_size assertRaises RuntimeError c d ProcessGroupNCCL store rank world_size requires_nccl skip_if_lt_x_gpu test_invalid_nccl_blocking_wait_env _run_invalid_nccl_blocking_wait_env abc _run_invalid_nccl_blocking_wait_env - _run_invalid_nccl_blocking_wait_env _run_invalid_nccl_blocking_wait_env NcclUserBufferRegistrationTest MultiProcessTestCase setUp super setUp nccl_debug_file = tempfile NamedTemporaryFile nccl_env = TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests use TORCH_NCCL_BLOCKING_WAIT will test expected TORCH_NCCL_ASYNC_ERROR_HANDLING NCCL_ALGO NVLS NCCL_DEBUG INFO NCCL_DEBUG_SUBSYS NVLS NCCL_DEBUG_FILE nccl_debug_file name torch cuda nccl version = nccl_env NCCL_DEBUG_SUBSYS = REG TUNING env_patcher = mock patch dict os environ nccl_env env_patcher start _spawn_processes tearDown env_patcher stop super tearDown try os remove file_name except OSError pass requires_nccl requires_nccl_version Need NCCL user buffer registration skip_if_lt_x_gpu requires_multicast_support test_nccl_user_buffer_registration store = c d FileStore file_name world_size device = torch device f cuda rank c d init_process_group backend= nccl rank=self rank world_size=self world_size store=store device_id=device torch cuda set_device rank pg = c d distributed_c d _get_default_group backend = pg _get_backend torch device device Use NCCL memory allocator pool = torch cuda MemPool backend mem_allocator allocate memory ncclMemAlloc torch cuda use_mem_pool pool tensor = torch arange device=device register buffers NCCL backend register_mem_pool pool allreduce now should use NVIDIA Switches pg allreduce tensor wait torch cuda synchronize device=device de-register buffers NCCL backend deregister_mem_pool pool clean up memory del tensor pool open os environ NCCL_DEBUG_FILE f nccl_debug_file_content = f read buffers registered NVLS reduction ran NCCL_DEBUG should show successful registration debug output torch cuda nccl version = assertRegex nccl_debug_file_content successfully registered NVLS assertRegex nccl_debug_file_content local-registered requires_nccl requires_nccl_version Need NCCL window registration skip_if_lt_x_gpu requires_multicast_support test_nccl_window_registration store = c d FileStore file_name world_size device = torch device f cuda rank torch cuda device device Eager init nccl comm so we don t implicitly create one during register_mem_pool c d init_process_group backend= nccl rank=self rank world_size=self world_size store=store device_id=device pg = c d distributed_c d _get_default_group backend = pg _get_backend torch device device Use NCCL memory allocator enable symmetric memory usage NCCL pool = torch cuda MemPool backend mem_allocator allocate memory ncclMemAlloc note symmetric kernels available dtypes like torch int torch cuda use_mem_pool pool tensor = torch arange device=device dtype=torch float register buffers NCCL backend register_mem_pool pool symm=True allreduce now should use NVIDIA Switches pg allreduce tensor wait check further allocations also registered torch cuda use_mem_pool pool tensor = torch arange device=device dtype=torch float pg allreduce tensor wait torch cuda synchronize device=device de-register buffers NCCL backend deregister_mem_pool pool clean up memory del tensor pool open os environ NCCL_DEBUG_FILE f nccl_debug_file_content = f read buffers registered symmetric kernels ran NCCL_DEBUG should show successful registration debug output assertRegex nccl_debug_file_content Symmetric CommTest test_c d_common AbstractCommTest MultiProcessTestCase property device f cuda rank setUp super setUp TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests use TORCH_NCCL_BLOCKING_WAIT will test expected os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = _spawn_processes tearDown super tearDown try os remove file_name except OSError pass _test_broadcast_coalesced process_group device root_rank half = torch float No support float CPU tensors device == torch device cpu half = torch float target = torch arange dtype=half device=device chunk target += torch arange dtype=torch float device=device chunk target += torch arange dtype=half device=device chunk target += torch arange dtype=torch float device=device chunk target += torch arange dtype=half device=device chunk target += torch arange dtype=torch float device=device chunk The tensors pass broadcast identical target only process root broadcast rank == root_rank tensors = tensor clone tensor target tensors = torch zeros_like tensor tensor target rank = root_rank assertNotEqual tensors target c d _broadcast_coalesced process_group tensors buffer_size= src=root_rank rank = root_rank assertEqual tensors target requires_nccl skip_if_lt_x_gpu test_broadcast_coalesced_nccl store = c d FileStore file_name world_size c d init_process_group backend= nccl store=store rank=self rank world_size=self world_size process_group = c d distributed_c d _get_default_group device = torch device f cuda rank d ranks = root_rank ranks _test_broadcast_coalesced process_group device root_rank requires_nccl skip_if_lt_x_gpu test_all_reduce_coalesced_nccl store = c d FileStore file_name world_size c d init_process_group backend= nccl store=store rank=self rank world_size=self world_size process_group = c d distributed_c d _get_default_group device = torch device f cuda rank d tensors = torch full + i rank + + i device=device dtype=torch float i range torch distributed all_reduce_coalesced tensors group=process_group i t enumerate tensors assertEqual t torch full_like t world_size i + world_size + requires_nccl skip_if_lt_x_gpu test_all_reduce_coalesced_manager_nccl store = c d FileStore file_name world_size c d init_process_group backend= nccl store=store rank=self rank world_size=self world_size process_group = c d distributed_c d _get_default_group device = torch device f cuda rank d tensors = torch full + i rank + + i device=device dtype=torch float i range torch distributed _coalescing_manager group=process_group device=device async_ops=True cm tensor tensors torch distributed all_reduce tensor assertEqual len cm works cm wait i t enumerate tensors assertEqual t torch full_like t world_size i + world_size + requires_nccl skip_if_lt_x_gpu runOnRocmArch MI _ARCH test_intra_node_comm_all_reduce torch _C _distributed_c d _get_intra_node_comm_usage_counter torch testing _internal common_cuda SM OrLater peer range world_size peer == rank continue torch _C _cuda_canDeviceAccessPeer rank peer raise SkipTest Test requires p p access SM OrLater raise SkipTest Test requires sm = store = c d FileStore file_name world_size os environ ENABLE_INTRA_NODE_COMM = os environ TEST_INTRA_NODE_COMM = torch cuda set_device rank c d init_process_group backend= nccl rank=self rank world_size=self world_size store=store expect = world_size world_size - IntraNodeComm currently only supports sum bf Verify used next two configurations t = torch full rank cuda c d all_reduce t c d ReduceOp SUM assertTrue t eq expect all assertEqual _get_intra_node_comm_usage_counter t = torch full rank dtype=torch bfloat cuda c d all_reduce t c d ReduceOp AVG assertEqual _get_intra_node_comm_usage_counter Verify IntraNodeComm used up MB t = torch full rank dtype=torch bfloat cuda c d all_reduce t c d ReduceOp SUM assertTrue t eq expect all assertEqual _get_intra_node_comm_usage_counter t = torch full rank dtype=torch bfloat cuda c d all_reduce t c d ReduceOp SUM assertTrue t eq expect all assertEqual _get_intra_node_comm_usage_counter t = torch full rank dtype=torch bfloat cuda c d all_reduce t c d ReduceOp SUM assertTrue t eq expect all assertEqual _get_intra_node_comm_usage_counter Verify IntraNodeComm used beyond MB t = torch full + rank dtype=torch bfloat cuda c d all_reduce t c d ReduceOp SUM assertTrue t eq expect all assertEqual _get_intra_node_comm_usage_counter c d destroy_process_group requires_nccl requires_nccl_version Need NCCL + configuring estimate comm time skip_if_lt_x_gpu test_time_estimate_nccl store = c d FileStore file_name world_size torch cuda set_device rank c d init_process_group backend= nccl store=store rank=self rank world_size=self world_size process_group = c d distributed_c d _get_default_group device = torch device f cuda rank d t = torch full rank cuda dist _time_estimator group=process_group device=device cm c d all_reduce t c d ReduceOp SUM assertTrue cm estimated_time None assertTrue cm estimated_time requires_nccl skip_if_lt_x_gpu test_sequence_num_set_default_pg_nccl torch cuda set_device rank _test_sequence_num_set_default_pg backend= nccl skip_if_lt_x_gpu requires_nccl test_sequence_num_incremented_nccl_default _test_sequence_num_incremented_default_group nccl skip_if_lt_x_gpu requires_nccl test_sequence_num_incremented_nccl_subgroup world_size skip_but_pass_in_sandcastle Test requires world_size least _test_sequence_num_incremented_subgroup nccl requires_nccl skip_if_lt_x_gpu test_sequence_num_set_nccl_new_group torch cuda set_device rank _test_sequence_num_set_new_group backend= nccl _test_pass_nccl_options pg_opts store = c d FileStore file_name world_size Test init_process_group accepts options dist init_process_group nccl world_size=self world_size rank=self rank store=store pg_options=pg_opts Test new_group pg = c d new_group pg_options=pg_opts test process group works expected t = torch tensor rank + cuda rank pg allreduce t wait expected_tensor = torch tensor cuda rank assertEqual expected_tensor t requires_nccl skip_if_lt_x_gpu test_pass_nccl_options_high_priority_stream pg_opts = c d ProcessGroupNCCL Options pg_opts is_high_priority_stream = True _test_pass_nccl_options pg_opts requires_nccl requires_nccl_version Need NCCL + configuring NCCL communicators skip_if_lt_x_gpu test_pass_nccl_options_config pg_opts = c d ProcessGroupNCCL Options pg_opts config max_ctas = pg_opts config min_ctas = pg_opts config cga_cluster_size = pg_opts config net_name = Socket pg_opts config split_share = nccl_debug_file = tempfile NamedTemporaryFile os environ NCCL_DEBUG = INFO os environ NCCL_DEBUG_FILE = nccl_debug_file name Tests functionality when passing nccl config _test_pass_nccl_options pg_opts Tests comms configured nccl_debug_file_content = nccl_debug_file read max_ctas = re search rb Max CTAs \d+ &#124; $ nccl_debug_file_content group min_ctas = re search rb Min CTAs \d+ &#124; $ nccl_debug_file_content group split_share = re search rb Split share \d+ &#124; $ nccl_debug_file_content group cga_cluster_size = re search rb CGA cluster \d+ &#124; $ nccl_debug_file_content group net_name = re search rb Using network a-zA-z + &#124; $ nccl_debug_file_content group assertEqual pg_opts config max_ctas int max_ctas assertEqual pg_opts config min_ctas int min_ctas assertEqual pg_opts config cga_cluster_size int cga_cluster_size assertEqual pg_opts config net_name net_name decode assertEqual pg_opts config split_share int split_share Tests config inited correctly pg_opts = c d ProcessGroupNCCL Options nccl_cfg = c d ProcessGroupNCCL NCCLConfig assertEqual pg_opts config min_ctas - assertEqual nccl_cfg min_ctas - Tests opts config can copied pg_opts_ = copy deepcopy pg_opts nccl_cfg_ = copy copy pg_opts_ config pg_opts_ config min_ctas = nccl_cfg_ min_ctas = assertEqual pg_opts config min_ctas - assertEqual pg_opts_ config min_ctas assertEqual nccl_cfg_ min_ctas requires_nccl skip_if_lt_x_gpu test_nccl_barrier store = c d FileStore file_name world_size c d init_process_group backend= nccl rank=self rank world_size=self world_size store=store t = torch tensor rank + cuda rank c d all_reduce t expected_tensor = torch tensor cuda rank assertEqual expected_tensor t Test new_group pg = c d new_group t = torch tensor rank + cuda rank pg allreduce t wait assertEqual expected_tensor t pg = c d new_group rank == t = torch tensor rank + cuda rank expected_tensor = torch tensor rank + cuda rank pg allreduce t wait assertEqual expected_tensor t pg = c d new_group rank == t = torch tensor rank + cuda rank expected_tensor = torch tensor rank + cuda rank pg allreduce t wait assertEqual expected_tensor t requires_nccl skip_if_lt_x_gpu test_nccl_barrier_device_ids store = c d FileStore file_name world_size c d init_process_group backend= nccl rank=self rank world_size=self world_size store=store c d barrier device_ids= rank requires_nccl skip_if_lt_x_gpu test_unwaited - None Verify process can terminate gracefully even unwaited tensors store = c d FileStore file_name world_size c d init_process_group backend= nccl rank=self rank world_size=self world_size store=store Case Run collectives under context manager don t call wait them _functional_collectives allow_inflight_collective_as_graph_input_ctx assertEqual torch _C _distributed_c d _get_work_registry_size input = torch full float rank device=f cuda rank dist all_reduce input op=dist ReduceOp SUM async_op=True Non-functional collectives run under context manager registered work registry assertEqual torch _C _distributed_c d _get_work_registry_size Running another collective same tensor should still work dist all_reduce input op=dist ReduceOp SUM async_op=True assertEqual torch _C _distributed_c d _get_work_registry_size Case Run collectives under context manager don t call wait them NOTE Here we intentionally test memory-stressed case assertEqual torch _C _distributed_c d _get_work_registry_size _ range input = torch full float rank device=f cuda rank dist all_reduce input op=dist ReduceOp SUM async_op=True Work registry size unchanged since non-functional collectives run under context manager registered work registry assertEqual torch _C _distributed_c d _get_work_registry_size requires_nccl skip_if_lt_x_gpu test_wait_tensor - None Verify c d_functional wait_tensor can invoked output tensor non-functional collective store = c d FileStore file_name world_size c d init_process_group backend= nccl rank=self rank world_size=self world_size store=store Case under context manager i e work registered registry _functional_collectives allow_inflight_collective_as_graph_input_ctx input = torch full float rank device=f cuda rank assertEqual torch _C _distributed_c d _get_work_registry_size dist all_reduce input op=dist ReduceOp SUM async_op=True assertEqual torch _C _distributed_c d _get_work_registry_size torch ops c d_functional wait_tensor input assertEqual torch _C _distributed_c d _get_work_registry_size input = torch full float rank device=f cuda rank assertEqual torch _C _distributed_c d _get_work_registry_size work = dist all_reduce input op=dist ReduceOp SUM async_op=True assertEqual torch _C _distributed_c d _get_work_registry_size work wait assertEqual torch _C _distributed_c d _get_work_registry_size assertEqual input input Case under context manager i e work registered registry input = torch full float rank device=f cuda rank assertEqual torch _C _distributed_c d _get_work_registry_size dist all_reduce input op=dist ReduceOp SUM async_op=True assertEqual torch _C _distributed_c d _get_work_registry_size does take effect since underlying wait_tensor logic would able find corresponding work object because s registered registry torch ops c d_functional wait_tensor input assertEqual torch _C _distributed_c d _get_work_registry_size input = torch full float rank device=f cuda rank assertEqual torch _C _distributed_c d _get_work_registry_size work = dist all_reduce input op=dist ReduceOp SUM async_op=True assertEqual torch _C _distributed_c d _get_work_registry_size work wait assertEqual torch _C _distributed_c d _get_work_registry_size assertEqual input input requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= DETAIL test_nccl_warn_not_in_group_debug_detail _test_warn_not_in_group backend= nccl requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= INFO test_nccl_warn_not_in_group_debug_info _test_warn_not_in_group backend= nccl requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= OFF test_nccl_warn_not_in_group_debug_off _test_warn_not_in_group backend= nccl requires_nccl skip_if_lt_x_gpu test_nncl_rank_membership _test_rank_membership backend= nccl requires_nccl skip_if_lt_x_gpu test_tensor_dtype_mismatch _test_tensor_dtype_mismatch backend= nccl requires_nccl skip_if_lt_x_gpu test_tensor_dtype_complex _test_tensor_dtype_complex backend= nccl requires_nccl skip_if_lt_x_gpu test_reduce_scatter_base_k store = dist FileStore file_name world_size dist init_process_group nccl world_size=self world_size rank=self rank store=store output_tensor = torch zeros dtype=torch int rank input_tensors = torch arange world_size dtype=torch int rank input_tensors = torch reshape input_tensors world_size dist reduce_scatter_tensor output_tensor input_tensors assertEqual output_tensor input_tensors rank world_size requires_nccl skip_if_lt_x_gpu test_reduce_scatter_tensor_coalesced store = dist FileStore file_name world_size dist init_process_group nccl world_size=self world_size rank=self rank store=store output_tensors = torch zeros rank input_tensors = torch ones rank _ range world_size dist _coalescing_manager i range world_size dist reduce_scatter_tensor output_tensors i input_tensors i assertEqual output_tensors input_tensors rank world_size SetDeviceMethod Enum TORCH_CUDA_SET = auto torch cuda set_device COLLECTIVE_ARGUMENT = auto broadcast_object_list device= NcclProcessGroupWithDispatchedCollectivesTests test_c d_common ProcessGroupWithDispatchedCollectivesTests requires_nccl skip_if_lt_x_gpu test_collectives _test_collectives backend= nccl requires_nccl skip_if_lt_x_gpu test_allreduce_coalesced _test_allreduce_coalesced backend= nccl requires_nccl skip_if_lt_x_gpu test_all_to_all_single _test_all_to_all_single backend= nccl requires_nccl skip_if_lt_x_gpu test_allgather_base store = dist FileStore file_name world_size dist init_process_group nccl world_size=self world_size rank=self rank store=store device = cuda tensor = torch ones device=torch device device output_tensor = torch zeros device=torch device device dist all_gather_into_tensor output_tensor tensor assertEqual output_tensor tensor requires_nccl skip_if_lt_x_gpu parametrize float _dtype torch float _e m fn torch float _e m test_allgather_float float _dtype device = torch device f cuda rank d sm_is_or_higher_than device noqa F skipTest FP reduction support begins sm capable devices store = dist FileStore file_name world_size dist init_process_group nccl world_size=self world_size rank=self rank store=store device = cuda tensor = torch ones device=torch device device float _dtype output_tensor = torch zeros device=torch device device float _dtype dist all_gather_into_tensor output_tensor tensor assertEqual output_tensor view torch float tensor view torch float instantiate_parametrized_tests NcclProcessGroupWithDispatchedCollectivesTests LargeCommTest test_c d_common AbstractLargeCommTest MultiProcessTestCase setUp super setUp TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests use TORCH_NCCL_BLOCKING_WAIT will test expected os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = _spawn_processes tearDown super tearDown try os remove file_name except OSError pass property device rank requires_nccl skip_if_lt_x_gpu test_new_group_local_sync _test_new_group_local_sync backend= nccl requires_nccl skip_if_lt_x_gpu test_new_group_local_sync_sanity_check _test_new_group_local_sync_sanity_check backend= nccl requires_nccl skip_if_lt_x_gpu test_new_group_local_sync_duplicated_pg _test_new_group_local_sync_duplicate_pg backend= nccl _init_two_pg _subgroups world_size int = world_size = raise NotImplementedError f need world size get subgroup PGs got world size world_size store = c d FileStore file_name world_size c d init_process_group backend= nccl store=store rank=self rank world_size=world_size every rank creates same sub groups including unused sub groups current rank a_group = c d new_group b_group = c d new_group a_group rank b_group requires_nccl skip_if_lt_x_gpu parametrize group_rank True False test_gather_subgroup group_rank world_size = rank = world_size just easier write test exactly gpus even test increased gpu later subgroup = _init_two_pg _subgroups world_size device = torch device f cuda rank d input = torch ones device=device rank rank == rank == gather_list = torch empty_like input _ range subgroup size group_rank global_dst= group_dst= my_global_rank= gather_list None=True torch distributed gather input gather_list=gather_list group_dst= group=subgroup async_op=False torch distributed gather input gather_list=gather_list dst=self rank group=subgroup async_op=False src range len gather_list expected = torch ones_like input rank + src assertEqual gather_list src expected group_rank torch distributed gather input gather_list=None group_dst= group=subgroup async_op=False torch distributed gather input gather_list=None dst=self rank - group=subgroup async_op=False requires_nccl skip_if_lt_x_gpu parametrize group_rank True False test_gather_object_subgroup group_rank world_size = rank = world_size just easier write test exactly gpus even test increased gpu later subgroup = _init_two_pg _subgroups world_size discrepancy have set device gather_object gets wrong device current_device = _get_pg_default_device group torch cuda set_device rank input = rank rank rank == rank == discrepancy another weird thing- what s point making me specify some empty objects my list empty list should valid imo throws error gather_list = group_rank torch distributed gather_object input object_gather_list=gather_list group_dst= group=subgroup torch distributed gather_object input object_gather_list=gather_list dst=self rank group=subgroup src range len gather_list assertEqual gather_list src rank rank + src group_rank torch distributed gather_object input object_gather_list=None group_dst= group=subgroup torch distributed gather_object input object_gather_list=None dst=self rank - group=subgroup requires_nccl skip_if_lt_x_gpu parametrize group_rank True False test_reduce_subgroup group_rank world_size = rank = world_size subgroup = _init_two_pg _subgroups world_size device = torch device f cuda rank d x = torch ones device=device rank rank == rank == expected = x + torch ones device=device rank + group_rank c d reduce x group_dst= group=subgroup async_op=False c d reduce x dst=self rank group=subgroup async_op=False assertEqual x expected group_rank c d reduce x group_dst= group=subgroup async_op=False c d reduce x dst=self rank - group=subgroup async_op=False requires_nccl skip_if_lt_x_gpu parametrize group_rank True False parametrize async_op True False test_send_recv_subgroup async_op group_rank world_size = rank = world_size subgroup = _init_two_pg _subgroups world_size device = torch device f cuda rank d rank == rank == x = torch empty device=device async_op group_rank c d irecv x group_src= group=subgroup wait c d irecv x src=self rank + group=subgroup wait group_rank c d recv x group_src= group=subgroup c d recv x src=self rank + group=subgroup expected = torch ones device=device rank + assertEqual x expected x = torch ones device=device rank async_op group_rank c d isend x group_dst= group=subgroup wait c d isend x dst=self rank - group=subgroup wait group_rank c d send x group_dst= group=subgroup c d send x dst=self rank - group=subgroup requires_nccl skip_if_lt_x_gpu parametrize group_rank True False test_batch_send_recv_subgroup group_rank world_size = rank = world_size subgroup = _init_two_pg _subgroups world_size device = torch device f cuda rank d ops = rank == rank == x = torch empty device=device group_rank ops append c d P POp dist irecv x group=subgroup group_peer= ops append c d P POp dist irecv x peer=self rank + group=subgroup work dist batch_isend_irecv ops work wait expected = torch ones device=device rank + assertEqual x expected x = torch ones device=device rank group_rank ops append c d P POp dist isend x group=subgroup group_peer= ops append c d P POp dist isend x peer=self rank - group=subgroup work dist batch_isend_irecv ops work wait requires_nccl skip_if_lt_x_gpu parametrize group_rank True False test_broadcast_subgroup group_rank world_size = rank = world_size subgroup = _init_two_pg _subgroups world_size device = torch device f cuda rank d rank == rank == x = torch empty device=device group_rank c d broadcast x group_src= group=subgroup c d broadcast x src=self rank + group=subgroup expected = torch ones device=device rank + assertEqual x expected x = torch ones device=device rank group_rank c d broadcast x group_src= group=subgroup c d broadcast x src=self rank group=subgroup requires_nccl skip_if_lt_x_gpu parametrize set_device SetDeviceMethod TORCH_CUDA_SET SetDeviceMethod COLLECTIVE_ARGUMENT parametrize group_rank True False test_send_recv_object_list_subgroup set_device SetDeviceMethod group_rank world_size = rank = world_size subgroup = _init_two_pg _subgroups world_size set_device == SetDeviceMethod TORCH_CUDA_SET torch cuda set_device rank device = None device = torch device f cuda rank d rank == rank == x = group_rank c d recv_object_list x group_src= group=subgroup device=device c d recv_object_list x src=self rank + group=subgroup device=device expected = rank rank + assertEqual x expected x = rank rank group_rank c d send_object_list x group_dst= group=subgroup device=device c d send_object_list x dst=self rank - group=subgroup device=device requires_nccl skip_if_lt_x_gpu parametrize set_device SetDeviceMethod TORCH_CUDA_SET SetDeviceMethod COLLECTIVE_ARGUMENT parametrize group_rank True False test_broadcast_object_list_subgroup set_device SetDeviceMethod group_rank world_size = rank = world_size subgroup = _init_two_pg _subgroups world_size set_device == SetDeviceMethod TORCH_CUDA_SET torch cuda set_device rank device = None device = torch device f cuda rank d rank == rank == x = group_rank c d broadcast_object_list x group_src= group=subgroup device=device c d broadcast_object_list x src=self rank + group=subgroup device=device expected = rank rank + assertEqual x expected x = rank rank group_rank c d broadcast_object_list x group_src= group=subgroup device=device c d broadcast_object_list x src=self rank group=subgroup device=device requires_nccl skip_if_lt_x_gpu parametrize group_rank True False test_scatter_subgroup group_rank world_size = rank = world_size subgroup = _init_two_pg _subgroups world_size device = torch device f cuda rank d x = torch empty device=device expected = torch ones device=device rank rank == rank == group_rank c d scatter x scatter_list=None group_src= group=subgroup c d scatter x scatter_list=None src=self rank + group=subgroup scatter_list = torch ones device=device rank - torch ones device=device rank group_rank c d scatter x scatter_list=scatter_list group_src= group=subgroup c d scatter x scatter_list=scatter_list src=self rank group=subgroup assertEqual x expected requires_nccl skip_if_lt_x_gpu parametrize group_rank True False test_scatter_object_list_subgroup group_rank world_size = rank = world_size subgroup = _init_two_pg _subgroups world_size torch cuda set_device rank scatter_object_output_list = None expected = rank rank rank == rank == group_rank c d scatter_object_list scatter_object_output_list=scatter_object_output_list scatter_object_input_list=None group_src= group=subgroup c d scatter_object_list scatter_object_output_list=scatter_object_output_list scatter_object_input_list=None src=self rank + group=subgroup scatter_object_input_list = rank rank - rank rank group_rank c d scatter_object_list scatter_object_output_list=scatter_object_output_list scatter_object_input_list=scatter_object_input_list group_src= group=subgroup c d scatter_object_list scatter_object_output_list=scatter_object_output_list scatter_object_input_list=scatter_object_input_list src=self rank group=subgroup assertEqual scatter_object_output_list expected instantiate_parametrized_tests LargeCommTest SparseCollective MultiProcessTestCase property world_size setUp super setUp TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests use TORCH_NCCL_BLOCKING_WAIT will test expected os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = num_gpus = torch cuda device_count _spawn_processes tearDown super tearDown try os remove file_name except OSError pass ToyModel nn Module __init__ rank vocab_size embedding_dim super __init__ embedding = nn Embedding vocab_size embedding_dim sparse=True rank linear = nn Linear embedding_dim rank forward inputs embedded = embedding inputs embedded shape batch_size sequence_length embedding_dim flattened = torch mean embedded dim= flattened shape batch_size embedding_dim output = linear flattened output shape batch_size output requires_nccl skip_if_lt_x_gpu test_ddp_set_sparse_metadata store = dist FileStore file_name world_size dist init_process_group nccl world_size=self world_size rank=self rank store=store vocab_size = model = SparseCollective ToyModel rank vocab_size=vocab_size embedding_dim= ddp_model = DistributedDataParallel model inputs = torch tensor rank set sparse metadata DDP model indices = torch Tensor list range vocab_size ddp_model _set_sparse_metadata embedding weight indices forward pass try output = ddp_model inputs loss = output sum backward pass loss backward assertTrue ddp_model module embedding weight grad indices indices except RuntimeError e NCCL does support all_reduce sparse tensors str e pass Rethrow exception s different error raise NCCLTraceTestBase MultiProcessTestCase setUp super setUp os environ TORCH_NCCL_ENABLE_TIMING = see timing_enabled parametrized tests os environ TORCH_NCCL_TRACE_BUFFER_SIZE = os environ TORCH_NCCL_DUMP_ON_TIMEOUT = tempdir = tempfile TemporaryDirectory os environ TORCH_NCCL_DEBUG_INFO_TEMP_FILE = _trace_basename os environ TORCH_NCCL_DEBUG_INFO_PIPE_FILE = _trace_basename _spawn_processes classmethod _run cls parent_conn rank int test_name str file_name str parent_pipe kwargs - None cls parent = parent_conn super _run rank test_name file_name parent_pipe property local_device torch device cuda rank_to_GPU rank _join_processes fn We need patch sys exit skip_if will use sys exit exit code process will caught mock patch sys exit fn super _join_processes fn _spawn_processes - None proc = torch multiprocessing get_context spawn Process children_pipes = parent_pipes = _ range world_size parent_conn child_conn = torch multiprocessing Pipe children_pipes append child_conn parent_pipes append parent_conn piter = iter parent_pipes wrap positional args kwargs args = next piter args proc positional args=args kwargs _start_processes wrap _create_process_group_nccl store = dist FileStore file_name world_size c d init_process_group nccl world_size=self world_size rank=self rank store=store pg = c d distributed_c d _get_default_group pg tearDown super tearDown try os remove file_name except OSError pass property world_size property rank_to_GPU rank GPU map init_multigpu_helper world_size nccl _trace_basename we pass base env dump util will append rank os path join tempdir name trace_ _trace_name rank _trace_basename + str rank started_or_scheduled timing_enabled started timing_enabled scheduled NCCLTraceTest NCCLTraceTestBase _verify_trace t include_collectives timing_enabled is_json ver = t version assertEqual ver comm_lib_version = t comm_lib_version torch_comm_lib_version = torch cuda nccl version assertEqual comm_lib_version join str v v torch_comm_lib_version pg_config = t pg_config assertEqual len pg_config default_pg_info = pg_config assertIn name default_pg_info assertIn desc default_pg_info assertIn ranks default_pg_info pg_status = t pg_status assertEqual len pg_status assertEqual str pg_status last_enqueued_collective assertEqual str pg_status last_completed_collective assertEqual str pg_status last_started_collective timing_enabled - global_ranks = pg_config ranks assertEqual len json loads global_ranks world_size include_collectives assertEqual len t entries t = t entries last = t - assertEqual last thread_id str threading current_thread ident assertEqual last thread_name fr_test_thread assertEqual last process_group default_pg assertEqual last state completed s = last time_discovered_started_ns f = last time_discovered_completed_ns assertEqual last record_id assertIsNotNone f timing_enabled assertIsNotNone s assertTrue s = f we don t collect stack traces JSON moment is_json assertIn test_c d_nccl py str last frames assertEqual last input_sizes assertEqual last input_dtypes Float assertEqual last output_sizes assertEqual last output_dtypes Float assertEqual last collective_seq_id assertEqual last timeout_ms now = datetime now event_created_time = datetime fromtimestamp last time_created_ns before_test = now - timedelta minutes= assertTrue before_test event_created_time now timing_enabled very loose bounds measured ms devgpu assertTrue last duration_ms assertTrue duration_ms last assertTrue entries t load_libpthread_or_libc ctypes util base pthread c path = ctypes util find_library base path try ctypes CDLL path except OSError continue raise RuntimeError Could load pthread libc Directly set thread name using threading current_thread name does work because we use pthread_getname_np get thread  s OS-level name C++ set_thread_name name ctypes lib = load_libpthread_or_libc pthread_self = lib pthread_self pthread_self restype = ctypes c_void_p pthread_setname_np = lib pthread_setname_np pthread_setname_np argtypes = ctypes c_void_p ctypes c_char_p Get current pthread handle tid = pthread_self Set name pthread_setname_np tid name encode requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize timing_enabled True False parametrize include_collectives True False test_short_json timing_enabled include_collectives rank == MAIN_PROCESS_RANK pg = _create_process_group_nccl timing_enabled pg _enable_collectives_timing device = local_device set_thread_name fr_test_thread = torch full float rank device=device _ range f = pg allreduce f wait torch cuda synchronize device=device gah ok so now duration_ms populated best-effort since can only happen outside dump api time sleep t = json loads torch _C _distributed_c d _dump_nccl_trace_json includeCollectives=include_collectives _verify_trace t include_collectives timing_enabled True dist destroy_process_group requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize timing_enabled True False parametrize include_collectives True False test_short_pickle timing_enabled include_collectives rank == MAIN_PROCESS_RANK pg = _create_process_group_nccl timing_enabled pg _enable_collectives_timing device = local_device set_thread_name fr_test_thread = torch full float rank device=device _ range f = pg allreduce f wait torch cuda synchronize device=device gah ok so now duration_ms populated best-effort since can only happen outside dump api time sleep t = pickle loads torch _C _distributed_c d _dump_nccl_trace includeCollectives=include_collectives _verify_trace t include_collectives=include_collectives timing_enabled=timing_enabled is_json=True dist destroy_process_group requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize timing_enabled True False test_fr_record_reset timing_enabled rank == MAIN_PROCESS_RANK pg = _create_process_group_nccl timing_enabled pg _enable_collectives_timing device = local_device set_thread_name fr_test_thread = torch full float rank device=device _ range f = pg allreduce f wait torch cuda synchronize device=device gah ok so now duration_ms populated best-effort since can only happen outside dump api time sleep torch _C _distributed_c d _reset_fr_recording_nccl _ range f = pg allreduce f wait torch cuda synchronize device=device time sleep t = pickle loads torch _C _distributed_c d _dump_nccl_trace assertEqual len t entries dist destroy_process_group requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_dump_pipe open_file_with_timeout file_path mode timeout= start_time = time time while time time - start_time timeout os path exists file_path open file_path mode time sleep raise FileNotFoundError rank == MAIN_PROCESS_RANK c children_pipes assertEqual c recv next dump_file = _trace_name rank= pipe_file = dump_file + pipe open_file_with_timeout pipe_file w f f write \n open_file_with_timeout dump_file rb timeout= f assertTrue all_reduce str pickle load f c children_pipes c send next pg = _create_process_group_nccl device = local_device = torch full float rank device=device _ range f = pg allreduce f wait torch cuda synchronize device=device parent send next parent recv requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_long os environ TORCH_NCCL_TRACE_BUFFER_SIZE = rank == MAIN_PROCESS_RANK pg = _create_process_group_nccl device = local_device = torch full float rank device=device _ range test some other primitives make sure their strings valid xs = torch ones device=device pg broadcast xs wait pg allreduce xs wait pg reduce xs wait ys = torch empty device=device _ range world_size pg allgather ys xs wait pg reduce_scatter xs ys wait f = pg allreduce f wait torch cuda synchronize device=device t = pickle loads torch _C _distributed_c d _dump_nccl_trace t = t entries assertEqual len t first = t last = t - assertEqual last profiling_name nccl all_reduce assertEqual last state completed assertIn test_c d_nccl py str last frames assertEqual last input_sizes assertEqual last input_dtypes Float assertEqual last output_sizes assertEqual last output_dtypes Float assertEqual last timeout_ms assertEqual last collective_seq_id - first collective_seq_id dist destroy_process_group requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_barrier_profiling os environ TORCH_NCCL_TRACE_BUFFER_SIZE = rank == MAIN_PROCESS_RANK pg = _create_process_group_nccl device = local_device = torch full float rank device=device f = pg barrier f = pg allreduce f wait torch cuda synchronize device=device t = pickle loads torch _C _distributed_c d _dump_nccl_trace t = t entries assertEqual len t first = t last = t - assertEqual first profiling_name nccl all_reduce_barrier assertEqual last profiling_name nccl all_reduce dist destroy_process_group requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs test_trace_while_all_works_retired os environ TORCH_NCCL_TRACE_BUFFER_SIZE = rank == MAIN_PROCESS_RANK pg = _create_process_group_nccl device = local_device send more works than buffer size overwrite previous entry _ range = torch ones device=device pg broadcast wait torch cuda synchronize device=device wait all works retired pg _wait_for_pending_works t = pickle loads torch _C _distributed_c d _dump_nccl_trace t = t entries assertEqual len t last = t - assertEqual last retired True assertEqual last state completed requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize timing_enabled True False parametrize only_active True False test_trace_while_active timing_enabled only_active rank == MAIN_PROCESS_RANK c children_pipes assertEqual c recv next c children_pipes c send next pg = _create_process_group_nccl timing_enabled pg _enable_collectives_timing device = local_device torch cuda device device = torch full float rank device=device pg allreduce wait e = torch cuda Event e record rank = pg allreduce wait e synchronize t = pickle loads torch _C _distributed_c d _dump_nccl_trace onlyActive=only_active t = t entries only_active rank == assertEqual len t assertEqual len t only_active rank == assertEqual t - profiling_name nccl all_reduce assertEqual t - collective_seq_id assertEqual t - state completed assertEqual t - profiling_name nccl all_reduce assertEqual t - collective_seq_id ROCm runtime used call uSleep s inside defaultsignal busy-wait loop Now sleep removed which lets host thread spin continuously Therefore state can either scheduled started before test dumps trace torch version hip _get_torch_rocm_version = timing_enabled assert t - state scheduled started assertEqual t - state started_or_scheduled timing_enabled parent send next assertEqual next parent recv rank == pg allreduce wait torch cuda synchronize device=device requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize timing_enabled True False test_trace_while_stuck timing_enabled rank == MAIN_PROCESS_RANK c children_pipes assertEqual c recv next c children_pipes c send next pg = _create_process_group_nccl timing_enabled pg _enable_collectives_timing device = local_device torch cuda device device = torch full float rank device=device pg allreduce wait e = torch cuda Event e record gather_trace e synchronize give other thread some time fill cuda buffer time sleep t = pickle loads torch _C _distributed_c d _dump_nccl_trace t = t entries assertEqual t - profiling_name nccl all_reduce rank == assertEqual t - collective_seq_id assertEqual t - state completed assertEqual t - collective_seq_id assertEqual t - state started_or_scheduled timing_enabled assertIsNone t - time_discovered_completed_ns will eventually cause missing rank continue which will unblock non-zero ranks parent send next rank = pg allreduce wait th = threading Thread target=gather_trace th start fill cuda buffer around events will stall _ range = + th join gather_trace assertEqual next parent recv rank == pg allreduce wait torch cuda synchronize device=device requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize op_sizes_per_coalesce parametrize timing_enabled True False test_batched_send_recv op_sizes_per_coalesce timing_enabled WorkEnqueue skipped isendirecv leading segfault dump_entries when update_state tried use destructed Work obj s cuda events rank == MAIN_PROCESS_RANK pg = _create_process_group_nccl timing_enabled pg _enable_collectives_timing num_coalesced_ops = ops_per_coalesce = len op_sizes_per_coalesce _ range num_coalesced_ops ops = input_sizes op_sizes_per_coalesce tensor = torch zeros input_sizes local_device rank == ops append dist P POp dist irecv tensor rank == tensor = ops append dist P POp dist isend tensor dist batch_isend_irecv ops pop wait torch cuda synchronize device=self local_device timing_enabled wait watchdog thread process queue works time sleep t = pickle loads torch _C _distributed_c d _dump_nccl_trace assertEqual len t entries num_coalesced_ops ops_per_coalesce + expected_record_id = expected_seq = expected_op_id = seq range num_coalesced_ops first_op = seq ops_per_coalesce + coalesced_op = first_op + ops_per_coalesce p p_op_idx input_sizes zip range first_op coalesced_op op_sizes_per_coalesce individual ops inside coalescing group individual op metadata timing info coming actual coalesced kernel profiling_name = nccl recv - rank == nccl send - assertEqual t entries p p_op_idx record_id expected_record_id expected_record_id += assertEqual t entries p p_op_idx profiling_name profiling_name we don t increment collective_seq_id p p ops assertEqual t entries p p_op_idx collective_seq_id assertEqual t entries p p_op_idx p p_seq_id expected_seq assertEqual t entries p p_op_idx op_id expected_op_id expected_op_id += assertEqual t entries p p_op_idx input_sizes input_sizes assertEqual t entries p p_op_idx output_sizes input_sizes duration doesn t get tagged onto individual ops yet nor their state updated assertEqual t entries p p_op_idx state scheduled assertTrue duration_ms t entries p p_op_idx coalesced op has no metadata indicates coalescing used accurately reflects timing state info whole group assertEqual t entries coalesced_op record_id expected_record_id expected_record_id += assertEqual t entries coalesced_op profiling_name nccl coalesced assertEqual t entries coalesced_op p p_seq_id expected_seq expected_seq += assertEqual t entries coalesced_op state completed assertEqual t entries coalesced_op input_sizes assertEqual t entries coalesced_op output_sizes timing_enabled duration = t entries coalesced_op duration_ms assertTrue duration duration assertTrue duration_ms t entries coalesced_op assertEqual t entries coalesced_op timeout_ms requires_nccl skip_but_pass_in_sandcastle_if TEST_MULTIGPU NCCL test requires + GPUs parametrize op_sizes parametrize timing_enabled True False test_individual_send_recv op_sizes timing_enabled WorkEnqueue skipped isendirecv leading segfault dump_entries when update_state tried use destructed Work obj s cuda events rank == MAIN_PROCESS_RANK pg = _create_process_group_nccl timing_enabled pg _enable_collectives_timing num_repeats = ops_per_repeat = len op_sizes _ range num_repeats input_sizes op_sizes tensor = torch zeros input_sizes local_device rank == dist recv tensor rank == tensor = dist send tensor torch cuda synchronize device=self local_device timing_enabled wait watchdog thread process queue works time sleep t = pickle loads torch _C _distributed_c d _dump_nccl_trace assertEqual len t entries num_repeats ops_per_repeat expected_seq = expected_op_id = seq range num_repeats ops_per_repeat input_sizes = op_sizes seq ops_per_repeat profiling_name = nccl recv - rank == nccl send - assertEqual t entries seq profiling_name profiling_name we don t increment collective_seq_id p p ops assertEqual t entries seq collective_seq_id assertEqual t entries seq p p_seq_id expected_seq expected_seq += assertEqual t entries seq op_id expected_op_id expected_op_id += assertEqual t entries seq input_sizes input_sizes assertEqual t entries seq output_sizes input_sizes assertEqual t entries seq state completed timing_enabled duration = t entries seq duration_ms assertTrue duration duration assertTrue duration_ms t entries seq requires_nccl skip_if_lt_x_gpu parametrize timing_enabled True False test_allgather_uneven timing_enabled rank == MAIN_PROCESS_RANK pg = _create_process_group_nccl timing_enabled pg _enable_collectives_timing output_split_sizes = i + i range world_size sum_len = sum output_split_sizes output_tensor = torch zeros sum_len rank expected_tensor = torch ones sum_len rank input_tensor = torch ones output_split_sizes rank rank dist all_gather list torch split output_tensor output_split_sizes input_tensor torch cuda synchronize device=self rank assertEqual output_tensor expected_tensor timing_enabled wait watchdog thread process queue works time sleep t = pickle loads torch _C _distributed_c d _dump_nccl_trace assertEqual len t entries world_size + i range world_size assertEqual t entries i profiling_name nccl _broadcast_oop collective_seq_id should incremented once assertEqual t entries i collective_seq_id assertEqual t entries i input_sizes i + assertEqual t entries i output_sizes i + assertEqual t entries i state scheduled No event recorded individual ops assertTrue time_discovered_completed_ns t entries i assertEqual t entries world_size profiling_name nccl ALLGATHER_coalesced TODO whc test out other ops And combinations ops s valid requires_nccl skip_if_lt_x_gpu parametrize timing_enabled True False test_coalescing_manager_collective timing_enabled The coalescing manager api works accumulating operations python via contextmanager then making one call into c++ op _coalesced API It has limited support ops has been added recently avoid overheads making individual py-cpp calls This complicates flight recording For now flight recording coalescing_manager collectives less detailed than cpp coalesced collectives rank == MAIN_PROCESS_RANK pg = _create_process_group_nccl timing_enabled pg _enable_collectives_timing output_tensors = torch zeros rank input_tensors = torch ones rank _ range world_size TODO whc make work bigger world something assertEqual world_size world_size dist _coalescing_manager i range world_size dist reduce_scatter_tensor output_tensors i input_tensors i assertEqual output_tensors input_tensors rank world_size torch cuda synchronize device=self rank timing_enabled wait watchdog thread process queue works time sleep t = pickle loads torch _C _distributed_c d _dump_nccl_trace assertEqual len t entries one reduce_scatter_tensor_coalesced assertEqual t entries profiling_name nccl reduce_scatter_tensor_coalesced collective_seq_id should incremented once assertEqual t entries collective_seq_id assertEqual t entries input_sizes assertEqual t entries output_sizes assertEqual t entries state completed timing_enabled duration = t entries duration_ms assertTrue duration duration assertTrue duration_ms t entries check_if_test_is_skipped fn wrapper args kwargs skip TEST_SKIPS values processes exitcode == skip exit_code MultiProcessTestCase _check_return_codes args kwargs fn args kwargs wrapper NCCLTraceTestDumpOnTimeoutBase NCCLTraceTestBase timeout_sec = _create_process_group_nccl store = dist FileStore file_name world_size c d init_process_group nccl world_size=self world_size rank=self rank store=store timeout=timedelta seconds=NCCLTraceTestDumpOnTimeoutBase timeout_sec pg = c d distributed_c d _get_default_group pg check_if_test_is_skipped _check_return_codes elapsed_time base test infra assumes processes exit matching codes we want rank abort rank exit cleanly test assertEqual processes exitcode - assertEqual processes exitcode _wait_process rank timeout try processes rank join timeout processes rank exitcode except TimeoutError None skip_but_pass_in_sandcastle NCCLTraceTestDumpOnTimeout NCCLTraceTestDumpOnTimeoutBase requires_nccl skip_if_lt_x_gpu parametrize timing_enabled True False test_timeout_dumps timing_enabled dump heartbeatmonitor thread os environ TORCH_NCCL_COORD_CHECK_MILSEC = need rank crash before looking its output file os environ TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC = rank == MAIN_PROCESS_RANK wait rank crash before looking its output file we rely rank holding off its abort long enough dump debug info assertEqual _wait_process timeout= - open _trace_name rank= rb f t = pickle load f t = t entries assertEqual len t assertEqual t collective_seq_id assertEqual t state completed assertEqual t collective_seq_id assertEqual t state started_or_scheduled timing_enabled assertFalse os path exists _trace_name rank= pg = _create_process_group_nccl timing_enabled we force disabled timing setup since there no disable function pg _enable_collectives_timing device = local_device torch cuda device device = torch full float rank device=device pg allreduce wait rank == pg allreduce wait rank will crash before passes sync rank will exit quickly cleanly torch cuda synchronize device=device instantiate_parametrized_tests ProcessGroupNCCLGroupTest instantiate_parametrized_tests NCCLTraceTestDumpOnTimeout instantiate_parametrized_tests NCCLTraceTest skip_but_pass_in_sandcastle NCCLTraceTestTimeoutDumpOnStuckRanks NCCLTraceTestDumpOnTimeoutBase check_if_test_is_skipped _check_return_codes elapsed_time base test infra assumes processes exit matching codes we want rank abort rank exit cleanly test assertEqual processes exitcode - assertEqual processes exitcode - requires_nccl skip_if_lt_x_gpu test_timeout_dumps_on_stuck_ranks need rank crash quicker after detecting timeout os environ TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC = restore env var its prior default case another test changed os environ TORCH_NCCL_COORD_CHECK_MILSEC = rank == MAIN_PROCESS_RANK wait both rank crash before looking both ranks output file we rely rank sleep long enough dump debug info assertEqual _wait_process timeout= - assertEqual _wait_process timeout= - assertTrue os path exists _trace_name rank= assertTrue os path exists _trace_name rank= open _trace_name rank= rb f t = pickle load f t = t entries assertEqual len t open _trace_name rank= rb f t = pickle load f t = t entries assertEqual len t assertEqual t collective_seq_id assertEqual t state completed pg = _create_process_group_nccl device = local_device torch cuda device device = torch full float rank device=device pg allreduce wait rank == pg allreduce wait rank will get stuck timeout then signal timeout all ranks torch cuda synchronize device=device rank == Force rank idle so will eventually timeout well after getting global signal dump debugging info time sleep skip_but_pass_in_sandcastle NcclErrorDumpTest NCCLTraceTestBase _wait_process rank timeout try processes rank join timeout processes rank exitcode except TimeoutError None check_if_test_is_skipped _check_return_codes elapsed_time base test infra assumes processes exit matching codes we want rank abort exception rank exit exit assertEqual processes exitcode - assertEqual processes exitcode requires_nccl requires_nccl_version Need NCCL + error checking skip_if_lt_x_gpu skip_if_rocm_multiprocess test_nccl_errors_dump os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = os environ TORCH_NCCL_TRACE_BUFFER_SIZE = os environ TORCH_NCCL_DUMP_ON_TIMEOUT = need rank dump before abort os environ TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC = rank == MAIN_PROCESS_RANK wait both rank crash before looking dump assertEqual _wait_process timeout= - assertEqual _wait_process timeout= verify trace file exists rank assertTrue os path exists _trace_name rank= store = c d FileStore file_name world_size process_group = c d ProcessGroupNCCL store rank world_size timeout=timedelta seconds= process_group allreduce torch rand cuda rank rank == work = process_group allreduce torch rand cuda rank expect error raised assertRaisesRegex dist DistBackendError Block current stream NCCL stream work wait Run some GPU operations torch rand cuda rank rank == Clean up structures ex files FileStore before going down del process_group sys exit tests needs run larger world size ProcessGroupNCCLLargerScaleTest MultiProcessTestCase _create_process_group_nccl store opts device_id=None create nccl processgroup opts c d init_process_group nccl world_size=self world_size rank=self rank store=store pg_options=opts device_id=device_id pg = c d distributed_c d _get_default_group pg opts high_priority_stream=False opts = c d ProcessGroupNCCL Options opts is_high_priority_stream = high_priority_stream opts setUp super setUp TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests use TORCH_NCCL_BLOCKING_WAIT will test expected os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = num_gpus = torch cuda device_count _spawn_processes tearDown super tearDown try os remove file_name except OSError pass property world_size property rank_to_GPU rank GPU map init_multigpu_helper world_size nccl requires_nccl_version Need NCCL + ncclCommSplit skip_if_lt_x_gpu test_comm_split_group_larger_scale store = c d FileStore file_name world_size device = torch device f cuda rank pg = _create_process_group_nccl store opts device_id=device backend = pg _get_backend torch device device tensor = torch full rank cuda device ng = c d split_group pg comm split happens eagerly since device_id passed init_process_group assertEqual backend comm_split_count dist broadcast take Source rank global process group rank dist broadcast tensor group=ng assertEqual tensor torch full dist broadcast tensor group=ng assertEqual tensor torch full test split only one colored group other ranks should no color split ng = c d split_group pg assertEqual backend comm_split_count rank = tensor = torch full rank cuda device dist broadcast tensor group=ng assertEqual tensor torch full assertEqual ng None barrier cuda sync before destroying all pgs dist barrier pg torch cuda synchronize dist destroy_process_group requires_nccl_version Need NCCL + ncclCommSplit skip_if_lt_x_gpu test_comm_recursive_split_group store = c d FileStore file_name world_size device = torch device f cuda rank pg = _create_process_group_nccl store opts device_id=device backend = pg _get_backend torch device device split default PG into subgroups each subgroup ng has ranks tensor = torch full rank cuda device ng = c d split_group pg backend = ng _get_backend torch device device rank dist broadcast tensor group=ng assertEqual tensor torch full dist broadcast tensor group=ng assertEqual tensor torch full comm split happens eagerly since device_id passed init_process_group assertEqual backend comm_split_count assertEqual backend comm_split_count further split ng into subgroups each subgroup ng has ranks tensor = torch full rank cuda device ng = c d split_group ng backend = ng _get_backend torch device device assertEqual backend comm_split_count assertEqual backend comm_split_count assertEqual backend comm_split_count execute collective calls within each -rank pg rank == rank == dist broadcast tensor group=ng assertEqual tensor torch full rank == rank == dist broadcast tensor group=ng assertEqual tensor torch full rank == rank == dist broadcast tensor group=ng assertEqual tensor torch full rank == rank == dist broadcast tensor group=ng assertEqual tensor torch full barrier cuda sync before destroying all pgs dist barrier pg torch cuda synchronize dist destroy_process_group __name__ == __main__ assert torch cuda _initialized test_distributed must have initialized CUDA context main process run_tests