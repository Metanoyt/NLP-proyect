mypy ignore-errors Torch torch torch cuda torch jit torch jit _logging torch jit frontend torch jit quantized Testing utils torch testing _internal common_dtype floating_and_complex_types_and torch testing _internal common_utils TestCase \ freeze_rng_state TemporaryFileName enable_profiling_mode_for_profiling_tests is_iterable_of_tensors torch testing _internal common_utils enable_profiling_mode noqa F Standard library itertools chain typing Union torch _C TensorType io check_output_types func ref_outputs args kwargs graph = getattr func last_graph None types = o type o graph outputs assertTrue len types == t = types torch _C _jit_assert_is_instance ref_outputs t Test names set only checked single derivative nn_functional_single_grad = frozenset test_nn_ + name name pdist multilabel_margin_loss max_unpool d multi_margin_loss binary_cross_entropy binary_cross_entropy_size_average ctc_loss grid_sample check_against_reference func reference_func output_func args kwargs=None allow_unused=True check_types=True no_grad=False no_gradgrad=False Verifies function performs identically some reference implementation Commonly used verify JIT implementation output_func matches behavior eager implementation reference_func kwargs = kwargs kwargs allSum vs isinstance vs torch Tensor vs = vs sum i + v sum abs v dtype is_complex i + v sum i v enumerate vs v None v dtype floating_and_complex_types_and torch half torch bfloat clone_tensor t preserve_requires_grad require_grad = preserve_requires_grad t requires_grad t detach clone requires_grad_ require_grad clone_inputs preserve_requires_grad bool inputs list Union torch Tensor list torch Tensor = arg args isinstance arg torch Tensor inputs append clone_tensor arg preserve_requires_grad is_iterable_of_tensors arg inputs append clone_tensor t preserve_requires_grad t arg inputs append arg inputs Returns tensors args requires_grad including tensors TensorList args get_recording_tensors args recording_tensors list torch Tensor = arg args isinstance arg torch Tensor arg requires_grad recording_tensors append arg is_iterable_of_tensors arg recording_tensors extend filter lambda t t requires_grad arg recording_tensors test no gradients case nograd_inputs = clone_inputs preserve_requires_grad=False outputs = runAndSaveRNG reference_func nograd_inputs kwargs enable_profiling_mode_for_profiling_tests outputs_test = runAndSaveRNG func nograd_inputs kwargs assertEqual outputs outputs_test check_types check_output_types func outputs_test nograd_inputs kwargs no_grad skip grad tests enable_profiling_mode_for_profiling_tests test single grad case recording_inputs = clone_inputs preserve_requires_grad=True recording_tensors = get_recording_tensors recording_inputs outputs = output_func runAndSaveRNG reference_func recording_inputs kwargs grads = torch autograd grad allSum outputs recording_tensors allow_unused=allow_unused outputs_test = output_func runAndSaveRNG func recording_inputs kwargs grads_test = torch autograd grad allSum outputs_test recording_tensors allow_unused=allow_unused assertEqual outputs outputs_test assertEqual grads grads_test test grad grad case _testMethodName nn_functional_single_grad no_gradgrad outputs = output_func runAndSaveRNG reference_func recording_inputs kwargs l = allSum outputs grads = torch autograd grad l recording_tensors create_graph=True allow_unused=allow_unused l = allSum grads l grads = torch autograd grad l recording_tensors allow_unused=allow_unused recording_inputs = clone_inputs preserve_requires_grad=True recording_tensors = get_recording_tensors recording_inputs outputs_test = output_func runAndSaveRNG func recording_inputs kwargs l _test = allSum outputs_test grads_test = torch autograd grad l _test recording_tensors create_graph=True allow_unused=allow_unused l _test = allSum grads_test l _test grads _test = torch autograd grad l _test recording_tensors allow_unused=allow_unused assertEqual outputs outputs_test assertEqual grads grads_test g g _test zip grads grads _test strict=True g None g _test None continue assertEqual g g _test atol= e- rtol= e- JitCommonTestCase TestCase createFunctionFromGraph trace graph = trace isinstance trace torch _C Graph trace graph torch _C _create_function_from_graph forward graph assertExportImport trace inputs m = createFunctionFromGraph trace assertExportImportModule m inputs assertExportImportModule m inputs m_import = getExportImportCopy m = runAndSaveRNG m inputs b = runAndSaveRNG m_import inputs assertEqual b Results original model exported imported version model differed runAndSaveRNG func inputs kwargs=None kwargs = kwargs kwargs freeze_rng_state results = func inputs kwargs results getExportImportCopy m also_test_file=True map_location=None buffer = io BytesIO torch jit save m buffer buffer seek imported = torch jit load buffer map_location=map_location also_test_file imported TemporaryFileName fname torch jit save imported fname torch jit load fname map_location=map_location autoDiffErrorMessage should_autodiff_node nodes_not_in_diff_graph fusion_nodes_not_found non_fusible_nodes_being_fused fusion_nodes_found nodes_in_diff_graph err_msg = \nFailure testing nodes autodifferentiation should_autodiff_node err_msg += One more nodes expected autodiffed \ found specified fusible nonfusible \ DifferentiableGraph groups \nSpecifically The node intended appear differentiable graph doesn t diff_nodes_missing = The node intended appear differentiable graph outside fusion group instead fusion group diff_nodes_in_fusion = The node intended appear fusion group doesn t fusion_nodes_missing = The node intended appear fusion group instead just outer differentiable graph fusion_nodes_in_diff = node nodes_not_in_diff_graph node non_fusible_nodes_being_fused diff_nodes_in_fusion append node diff_nodes_missing append node node fusion_nodes_not_found node nodes_in_diff_graph fusion_nodes_in_diff append node fusion_nodes_missing append node len diff_nodes_missing err_msg += f \n diff_nodes_missing one \ DifferentiableGraphs when they expected \ Did you intend these nodes autodiffed \ If remove them list nonfusible nodes len diff_nodes_in_fusion err_msg += f \n diff_nodes_in_fusion found one FusionGroups \ when they expected just DifferentiableGraph If \ intended these nodes FusionGroups reclassify these nodes \ fusible nodes If these nodes intended fused your \ autodifferentiation logic might wrong len fusion_nodes_missing err_msg += f \n fusion_nodes_missing one FusionGroups \ DifferentiableGraphs when they expected \ They also found outer DifferentiableGraph Did you \ intend these nodes autodifferentiated If you should \ remove these nodes test s fusible nodes Otherwise your \ autodifferentiation logic might wrong len fusion_nodes_in_diff err_msg += f \n fusion_nodes_in_diff one FusionGroups \ DifferentiableGraphs when they expected \ instead they found just outer DifferentiableGraph \ Did you intend these nodes fused If you should \ move these nodes into test s nonfusible nodes Otherwise your \ autodifferentiation logic might wrong err_msg += One more nodes expected autodiffed \ found DifferentiableGraph FusionGroup \ DifferentiableGraph Did you intend these nodes \ autodiffed If so change test expect autodifferentiation \ \nSpecifically len fusion_nodes_found err_msg += f \n fusion_nodes_found expected \ one DifferentiableGraphs appeared FusionGroup \ DifferentiableGraph len nodes_in_diff_graph err_msg += f \n nodes_in_diff_graph expected \ one DifferentiableGraphs err_msg assertAutodiffNode graph should_autodiff_node nonfusible_nodes fusible_nodes diff_nodes = graph findAllNodes prim DifferentiableGraph diff_subgraphs = node g Subgraph node diff_nodes Note currently no tests have fusible_nodes fusion_nodes = list chain from_iterable g findAllNodes prim FusionGroup g diff_subgraphs fusion_subgraphs = node g Subgraph node fusion_nodes For any non-fusible node must show up one DifferentiableGraphs nodes_in_diff_graph = nodes_not_in_diff_graph = non_fusible_nodes_being_fused = node nonfusible_nodes any g findNode node None g diff_subgraphs nodes_in_diff_graph append node nodes_not_in_diff_graph append node any g findNode node None g fusion_subgraphs non_fusible_nodes_being_fused append node found_all_nonfusible_nodes = len nodes_in_diff_graph == len nonfusible_nodes For any fusible node must show up one FusionGroups one DifferentiableGraphs fusion_nodes_found = fusion_nodes_not_found = node fusible_nodes any g findNode node None g fusion_subgraphs fusion_nodes_found append node fusion_nodes_not_found append node found_all_fusible_nodes = len fusion_nodes_found == len fusible_nodes should_autodiff_node None err_msg = autoDiffErrorMessage should_autodiff_node nodes_not_in_diff_graph fusion_nodes_not_found non_fusible_nodes_being_fused fusion_nodes_found nodes_in_diff_graph assertEqual should_autodiff_node found_all_nonfusible_nodes found_all_fusible_nodes err_msg checkShapeAnalysis out_sizes Union list int list list int traced_graph assert_propagation constant_prop=True repropagte input shapes provided tracing prev_symbolic_shapes_test_enabled = torch _C _jit_symbolic_shapes_test_mode_enabled enable_test_mode True False here we testing allowing disallowing substituting complete shapes constants disallowing constants helps stress test partial eval substitution pipeline torch _C _jit_set_symbolic_shapes_test_mode enable_test_mode torch _C _jit_erase_non_input_shape_information traced_graph constant_prop torch _C _jit_pass_constant_propagation traced_graph torch _C _jit_pass_propagate_shapes_on_graph traced_graph Add sizes default tensor type avoid checking something out scope difficulties tracer leaving other parts tensor type output = next traced_graph outputs type test_type type actual_size sizes = type symbolic_sizes out_type = TensorType get with_sizes sizes actual_type = TensorType get with_sizes actual_size always check actual shape subtype output assertTrue actual_type isSubtypeOf out_type then assertion flag provided check shape analysis successful assert_propagation assertEqual out_type sizes actual_size output isSubtypeOf torch _C TensorType get test_type output out_sizes tuple_elements = output elements i range len tuple_elements test_type tuple_elements i out_sizes i torch _C _jit_set_symbolic_shapes_test_mode prev_symbolic_shapes_test_enabled