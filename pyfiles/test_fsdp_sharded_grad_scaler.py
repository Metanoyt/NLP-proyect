Owner s oncall distributed copy functools itertools sys unittest typing Optional torch torch distributed dist torch cuda amp common amp_definitely_not_available torch distributed fsdp CPUOffload MixedPrecision torch distributed fsdp fully_sharded_data_parallel FullyShardedDataParallel FSDP ShardingStrategy torch distributed fsdp sharded_grad_scaler ShardedGradScaler torch distributed fsdp wrap ModuleWrapPolicy torch nn TransformerDecoderLayer TransformerEncoderLayer torch nn parallel distributed DistributedDataParallel DDP torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp DEVICEInitMode DummyProcessGroup FSDPInitMode FSDPTest NestedWrappedModule NonUniformReqGradNWM subtest_name TransformerWithSharedParams torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN TEST_XPU TestCase dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit device_type = acc type acc = torch accelerator current_accelerator cpu params = cpu_offload sharding_strategy mixed_precision use_orig_params cpu_offload_config = CPUOffload offload_params=True CPUOffload offload_params=False sharding_strategy_config = ShardingStrategy SHARD_GRAD_OP None mixed_precision = enable_mixed_precision None use_orig_params = enable_use_orig_params None configs = list itertools product cpu_offload_config sharding_strategy_config mixed_precision use_orig_params test_name_mapping = str CPUOffload offload_params=True offload_true str CPUOffload offload_params=False offload_false str ShardingStrategy SHARD_GRAD_OP shard_grad_op enable_mixed_precision mixed_precision enable_use_orig_params use_orig_params subtest_name = functools partial subtest_name test_name_mapping TestShardGradScaler TestCase unittest skipIf amp_definitely_not_available TEST_XPU no supported device cuda xla xpu found test_grad_scaling pg = DummyProcessGroup scaler = ShardedGradScaler device=device_type init_scale= process_group=pg enabled=True t = torch full dtype=torch float device= cpu t = torch full dtype=torch float device= cpu outputs = t clone t clone t clone t clone t clone outputs = scaler scale outputs assertTrue outputs == outputs == outputs == assertTrue outputs == outputs == assertTrue scaler _scale device == t device unittest skipIf amp_definitely_not_available TEST_XPU no supported device cuda xla xpu found test_scaling_unscaling_sparse pg = DummyProcessGroup scaler = ShardedGradScaler device=device_type init_scale= process_group=pg enabled=True inv_scale = torch full dtype=torch float device= cpu found_inf = torch full dtype=torch float device= cpu i = torch tensor device= cpu dtype=torch int v = torch tensor dtype=torch float device= cpu s = torch sparse_coo_tensor i v torch Size device= cpu dtype=torch float unscale sparse tensors s = s clone s grad = s clone opt = torch optim SGD s lr= found_inf zero_ found_inf = scaler _unscale_grads_ opt inv_scale found_inf s device assertEqual found_inf assertEqual s grad to_dense s to_dense unscale sparse tensor inf v = torch tensor float inf dtype=torch float device= cpu s grad = torch sparse_coo_tensor i v torch Size device= cpu dtype=torch float found_inf zero_ found_inf = scaler _unscale_grads_ opt inv_scale found_inf s device assertEqual found_inf unscale sparse tensor overflow marked inf i = torch tensor device= cpu dtype=torch int coalescing sparse tensor here will cause value Inf v = torch tensor dtype=torch float device= cpu s = torch sparse_coo_tensor i v torch Size device= cpu dtype=torch float s grad = s clone found_inf zero_ found_inf = scaler _unscale_grads_ opt inv_scale found_inf s device assertEqual found_inf unittest skipIf amp_definitely_not_available TEST_XPU no supported device cuda xla xpu found test_inf_gradients_skip_optim_step pg = DummyProcessGroup scaler = ShardedGradScaler device=device_type init_scale= process_group=pg enabled=True loss = torch full dtype=torch float device= cpu t = torch tensor float inf dtype=torch float device= cpu t grad = t clone opt = torch optim SGD t lr= scaler scale loss ret_val = scaler step opt assertTrue ret_val None TestShardedGradScalerParityWithDDP FSDPTest _get_init_modes_for_test cpu_offload modes = DEVICEInitMode DEVICE_AFTER DEVICEInitMode DEVICE_BEFORE Note DEVICEInitMode DEVICE_NEVER works currently only CPU offload we explicitly bring param back CUDA device In general will work since we try all_gather p data which CPU NCCL only supports GPU cpu_offload offload_params modes append DEVICEInitMode DEVICE_NEVER modes skip_if_lt_x_gpu parametrize params configs subtest_name test_fsdp_ddp_parity_with_grad_scaler cpu_offload CPUOffload sharding_strategy Optional ShardingStrategy mixed_precision Optional str use_orig_params Optional str init_modes = _get_init_modes_for_test cpu_offload mp = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float mixed_precision None None ` ` NonUniformReqGradNWM ` ` model requires we set ` init_scale ` more conservatively than default avoid infs initial steps use_orig_params == enable_use_orig_params use_orig = True model_cls = NonUniformReqGradNWM sharded_grad_scaler_kwargs = init_scale use_orig = False model_cls = NestedWrappedModule type ignore assignment sharded_grad_scaler_kwargs = None device_init_mode init_modes _test_fsdp_parity model_cls FSDPInitMode RECURSIVE device_init_mode=device_init_mode cpu_offload=cpu_offload sharding_strategy=sharding_strategy mixed_precision=mp enable_sharded_grad_scaler=True use_orig_params=use_orig sharded_grad_scaler_kwargs=sharded_grad_scaler_kwargs _build_model_and_optim cpu_offload CPUOffload = CPUOffload offload_params=False use_orig_params bool = False model = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True ref_model = DDP copy deepcopy model device_ids= rank ref_optim = torch optim Adam ref_model parameters lr= e- fsdp_kwargs = use_orig_params use_orig_params cpu_offload cpu_offload auto_wrap_policy ModuleWrapPolicy TransformerEncoderLayer TransformerDecoderLayer device_id rank model = FSDP model fsdp_kwargs optim = torch optim Adam model parameters lr= e- model optim ref_model ref_optim skip_if_lt_x_gpu test_sharded_grad_scaler_found_inf run_subtests use_orig_params False True cpu_offload CPUOffload offload_params=True CPUOffload offload_params=False _test_sharded_grad_scaler_found_inf _test_sharded_grad_scaler_found_inf use_orig_params bool cpu_offload CPUOffload model optim ref_model ref_optim = _build_model_and_optim cpu_offload=cpu_offload use_orig_params=use_orig_params grad_scaler = ShardedGradScaler device=device_type init_scale= ref_grad_scaler = torch amp GradScaler device=device_type init_scale= scaled_losses list torch Tensor = device = torch device device_type torch manual_seed + rank + iter range _model _optim _grad_scaler ref_model ref_optim ref_grad_scaler model optim grad_scaler module = _model module inp = module get_input device _optim zero_grad output = _model inp loss = module get_loss inp output scaled_loss = _grad_scaler scale loss scaled_losses append scaled_loss scaled_loss backward orig_params = param detach clone param _model parameters param grad None should_find_inf = iter == should_find_inf _model ref_model _model model rank == other ranks should find infs rank after collectives param _model parameters param grad None continue param grad fill_ float inf break _grad_scaler step _optim orig_scale = _grad_scaler get_scale _grad_scaler update should_find_inf assertEqual _grad_scaler get_scale orig_scale _grad_scaler get_backoff_factor f rank rank iter iter expect origin scale orig_scale f backed off _grad_scaler get_backoff_factor f got _grad_scaler get_scale assertEqual _grad_scaler get_scale orig_scale f rank rank iter iter expect same scale orig_scale f got _grad_scaler get_scale param orig_param zip param param _model parameters param grad None orig_params should_find_inf assertEqual param orig_param f rank rank iter iter expect same params before f after optim step got param vs orig_param assertNotEqual param orig_param f rank rank iter iter expect updated params after f optim step got param vs orig_param assertEqual scaled_losses scaled_losses f iter iter scaled_losses vs scaled_losses instantiate_parametrized_tests TestShardGradScaler instantiate_parametrized_tests TestShardedGradScalerParityWithDDP __name__ == __main__ run_tests