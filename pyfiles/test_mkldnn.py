Owner s module mkldnn copy itertools functools unittest warnings contextlib nullcontext try torchvision HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False skipIfNoTorchVision = unittest skipIf HAS_TORCHVISION no torchvision torch torch nn functional F torch jit torch backends mkldnn torch utils mkldnn mkldnn_utils torch testing _internal common_utils TestCase \ run_tests TemporaryFileName gradcheck gradgradcheck IS_WINDOWS \ skipIfTorchDynamo xfailIfTorchDynamo recover_orig_fp _precision torch testing _internal common_device_type instantiate_device_type_tests dtypes torch testing _internal common_mkldnn reduced_f _on_and_off batched grad doesn t support mkldnn gradcheck = functools partial gradcheck check_batched_grad=False gradgradcheck = functools partial gradgradcheck check_batched_grad=False types = torch float torch bfloat torch half Comment line below find out CI machines having MKL-DNN build disabled unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled TestMkldnn TestCase test_conversion cpu_tensor torch randn dtype=torch float device=torch device cpu torch randn dtype=torch float device=torch device cpu cpu_tensor requires_grad_ convert_dtypes = torch half torch half torch float torch bfloat torch bfloat torch float torch float torch bfloat torch half float bfloat half cpu tensor mkldnn tensortensor dtype types mkldnn_tensor = cpu_tensor to_mkldnn dtype assertEqual mkldnn_tensor dtype dtype cpu_tensor_ = mkldnn_tensor to_dense given dtype to_dense mkldnn tensor has same dtype cpu tensor assertEqual mkldnn_tensor dtype cpu_tensor_ dtype mkldnn float bfloat tensor cpu float bfloat tensor dtype convert_dtypes dtype cpu_tensor_ = mkldnn_tensor to_dense dtype assertEqual cpu_tensor_ dtype dtype atol = e- dtype == torch float dtype == torch float e- assertEqual cpu_tensor cpu_tensor_ float atol=atol rtol= assertEqual mkldnn_tensor device torch device cpu assertEqual mkldnn_tensor size torch Size assertEqual mkldnn_tensor numel cpu_tensor numel dtype == torch float assertEqual mkldnn_tensor element_size cpu_tensor element_size assertEqual mkldnn_tensor element_size cpu_tensor element_size assertRaisesRegex RuntimeError Cannot access data pointer Tensor doesn t have storage lambda mkldnn_tensor data_ptr = bfloat cpu tensor mkldnn float tensor bfloat tensor orig_dtype torch half torch bfloat cpu_tensor_lower = cpu_tensor dtype=orig_dtype dtype convert_dtypes orig_dtype mkldnn_tensor = cpu_tensor_lower to_mkldnn dtype assertEqual mkldnn_tensor dtype dtype cpu_tensor_ = mkldnn_tensor to_dense given dtype to_dense mkldnn tensor has same dtype cpu tensor assertEqual mkldnn_tensor dtype cpu_tensor_ dtype mkldnn float bfloat half tensor cpu float bfloat half tensor dtype convert_dtypes cpu_tensor_lower dtype cpu_tensor_ = mkldnn_tensor to_dense dtype assertEqual cpu_tensor_ dtype dtype assertEqual cpu_tensor_lower cpu_tensor_ dtype=cpu_tensor_lower dtype atol= e- rtol= assertEqual mkldnn_tensor device torch device cpu assertEqual mkldnn_tensor size torch Size assertEqual mkldnn_tensor numel cpu_tensor numel dtype torch bfloat torch half assertEqual mkldnn_tensor element_size cpu_tensor_lower element_size assertEqual mkldnn_tensor element_size cpu_tensor_lower element_size assertRaisesRegex RuntimeError Cannot access data pointer Tensor doesn t have storage lambda mkldnn_tensor data_ptr = test_conversion_byte_char int _types = torch int torch uint int _type int _types low = - int _type torch int high = cpu_tensor torch randint low=low high=high size= dtype=torch int device=torch device cpu torch randint low=low high=high size= dtype=torch int device=torch device cpu cpu_tensor = cpu_tensor dtype=int _type mkldnn_tensor = cpu_tensor to_mkldnn int _type assertEqual mkldnn_tensor dtype int _type cpu_tensor_ = mkldnn_tensor to_dense assertEqual mkldnn_tensor dtype cpu_tensor_ dtype assertEqual cpu_tensor cpu_tensor_ assertEqual mkldnn_tensor device torch device cpu assertEqual mkldnn_tensor size cpu_tensor size assertEqual mkldnn_tensor numel cpu_tensor numel assertEqual mkldnn_tensor element_size cpu_tensor element_size assertRaisesRegex RuntimeError Cannot access data pointer Tensor doesn t have storage lambda mkldnn_tensor data_ptr = test_copy x = torch randn dtype=torch float mkldnn_x = x to_mkldnn mkldnn_y = torch randn dtype=torch float to_mkldnn mkldnn_z = torch randn dtype=torch float to_mkldnn mkldnn_y copy_ mkldnn_x assertEqual x mkldnn_y to_dense assertRaisesRegex RuntimeError copy_mkldnn_ only support same size tensor lambda mkldnn_z copy_ mkldnn_x assertRaisesRegex RuntimeError copy_mkldnn_ between mkldnn layout dense Tensors implemented Found type = torch FloatTensor src type = Mkldnntorch FloatTensor lambda x copy_ mkldnn_x assertRaisesRegex RuntimeError copy_mkldnn_ between mkldnn layout dense Tensors implemented Found type = Mkldnntorch FloatTensor src type = torch FloatTensor lambda mkldnn_x copy_ x test_unsupported unsupported types unsupported types gpu dtype torch double torch uint torch int torch short torch int torch long assertRaises RuntimeError torch randn dtype=dtype device=torch device cpu to_mkldnn torch cuda is_available assertRaises RuntimeError torch randn dtype=dtype device=torch device cuda to_mkldnn supported type gpu torch cuda is_available assertRaises RuntimeError torch randn dtype=torch float device=torch device cuda to_mkldnn some factory functions creator torch ones torch randn torch rand assertRaises RuntimeError creator dtype=torch float device=torch device cpu layout=torch _mkldnn test_mkldnn_conv_shapecheck input = torch full dtype=torch float w = torch full dtype=torch float b = torch full dtype=torch float w = torch full dtype=torch float b = torch full dtype=torch float options = zip - padding stride dilation groups w w w w w w w weight b b b b b b b bias pad st dil gr w b options assertRaises RuntimeError _ torch mkldnn_convolution input w b pad st dil gr test_autograd_to_mkldnn MKLDNN only supports float root = torch randn dtype=torch float requires_grad=True func root root to_mkldnn to_dense because MKLDNN only supports float we need lessen precision these numbers just empirical results seem work assertWarnsRegex UserWarning double precision floating point lambda gradcheck func root atol= e- rtol= e- assertWarnsRegex UserWarning double precision floating point lambda gradgradcheck func root atol= e- rtol= e- test_autograd_from_mkldnn MKLDNN only supports float root = torch randn dtype=torch float to_mkldnn requires_grad_ func root root to_dense because MKLDNN only supports float we need lessen precision these numbers just empirical results seem work assertWarnsRegex UserWarning double precision floating point lambda gradcheck func root atol= e- rtol= e- test_detach root = torch randn dtype=torch float to_mkldnn requires_grad_ detach = root detach assertEqual detach size assertFalse detach requires_grad assertTrue root requires_grad detach_ = root detach_ assertEqual detach_ size assertFalse detach_ requires_grad assertFalse root requires_grad test_repr assertTrue layout=torch _mkldnn str torch randn dtype=torch float device=torch device cpu to_mkldnn _test_conv_base dim conv_module = torch nn Conv d torch nn Conv d torch nn Conv d input_shapes = options = itertools product True False True False train bias dilation groups options N = torch randint item M = torch randint item groups C = torch randint item groups x_shape = N C + input_shapes dim x = torch randn x_shape dtype=torch float conv = conv_module dim in_channels=C out_channels=M kernel_size= stride= padding= dilation=dilation bias=bias groups=groups float x = x clone x = x clone to_mkldnn train mkldnn_conv = mkldnn_utils to_mkldnn copy deepcopy conv train dim = TODO enable conv d training x requires_grad_ x requires_grad_ mkldnn_conv = copy deepcopy conv torch backends mkldnn flags enabled=False y_aten = conv x train dim = loss = y_aten sum loss backward train train dim = y_mkldnn = mkldnn_conv x to_dense precision = assertEqual y_aten y_mkldnn atol=self precision rtol=self precision assertEqual y_aten y_mkldnn train _test_serialization mkldnn_conv x to_mkldnn _test_tracing mkldnn_conv x to_mkldnn dim = loss = y_mkldnn sum loss backward assertTrue x grad is_mkldnn assertEqual x grad x grad to_dense assertEqual conv weight grad mkldnn_conv weight grad atol= e- rtol= e- bias assertEqual conv bias grad mkldnn_conv bias grad reduced_f _on_and_off test_conv d _test_conv_base dim= reduced_f _on_and_off test_conv d _test_conv_base dim= reduced_f _on_and_off test_conv d _test_conv_base dim= _test_conv_deconv_lower_precision_base dim conv_module dtype input_shapes = options = itertools product True False bias dilation groups options N = torch randint item M = torch randint item groups C = torch randint item groups x_shape = N C + input_shapes dim x = torch randn x_shape dtype=torch float TODO remove when group depthwise supported conv_module torch nn ConvTranspose d torch nn ConvTranspose d torch nn ConvTranspose d groups C == groups continue conv = conv_module in_channels=C out_channels=M kernel_size= stride= padding= dilation=dilation bias=bias groups=groups float x_lower = x dtype=dtype dtype == torch bfloat torch ops mkldnn _is_mkldnn_bf _supported \ dtype == torch half torch ops mkldnn _is_mkldnn_fp _supported mkldnn_conv = mkldnn_utils to_mkldnn copy deepcopy conv mkldnn_conv_lower = mkldnn_utils to_mkldnn copy deepcopy conv dtype y = mkldnn_conv x to_mkldnn to_dense y_lower = mkldnn_conv_lower x_lower to_mkldnn to_dense torch float assertEqual y y_lower atol= e- rtol= e- msg = torch bfloat r bf path needs cpu support avx_ne_convert avx bw avx vl avx dq torch half r fp path needs cpu support avx_ne_convert avx _fp assertRaisesRegex RuntimeError msg dtype mkldnn_conv_lower = mkldnn_utils to_mkldnn copy deepcopy conv dtype y_lower = mkldnn_conv_lower x_lower to_mkldnn to_dense torch float test thnn impl conv_lower = copy deepcopy conv dtype=dtype conv_ref = copy deepcopy conv_lower float torch backends mkldnn flags enabled=False x_ref = x_lower clone float detach requires_grad_ x_lower requires_grad_ y = conv_ref x_ref y_lower = conv_lower x_lower float assertEqual y y_lower atol= e- rtol= e- dtypes torch float torch bfloat test_conv_deconv_ d_lower_precision dtype _test_conv_deconv_lower_precision_base torch nn Conv d dtype=dtype _test_conv_deconv_lower_precision_base torch nn ConvTranspose d dtype=dtype dtypes torch float torch bfloat test_conv_deconv_ d_lower_precision dtype _test_conv_deconv_lower_precision_base torch nn Conv d dtype=dtype _test_conv_deconv_lower_precision_base torch nn ConvTranspose d dtype=dtype dtypes torch float torch bfloat test_conv_deconv_ d_lower_precision dtype _test_conv_deconv_lower_precision_base torch nn Conv d dtype=dtype _test_conv_deconv_lower_precision_base torch nn ConvTranspose d dtype=dtype _test_conv_deconv_nhwc_base conv_module weight_memory_format dtype prec=None input_shapes = options = itertools product True False True False conv_module torch nn Conv d torch nn ConvTranspose d cl_format = torch channels_last input_shape = input_shapes conv_module torch nn Conv d torch nn ConvTranspose d cl_format = torch channels_last_ d input_shape = input_shapes train bias dilation groups options N = torch randint item M = torch randint item groups C = torch randint item groups x_shape = N C + input_shape x = torch randn x_shape dtype=dtype conv mkldnn conv deconv contiguous memory format nchw conv mkldnn conv deconv channels last memory format nhwc conv = conv_module in_channels=C out_channels=M kernel_size= stride= padding= dilation=dilation bias=bias groups=groups dtype=dtype conv = copy deepcopy conv memory_format=weight_memory_format x = x clone x = x clone memory_format=cl_format train x requires_grad_ x requires_grad_ y = conv x y = conv x assertEqual y y atol=prec rtol=prec train y sum backward y sum backward assertTrue x grad is_contiguous memory_format=cl_format assertEqual conv weight grad conv weight grad atol= e- rtol= e- bias assertEqual conv bias grad conv bias grad atol=prec rtol=prec assertEqual x grad x grad atol=prec rtol=prec reduced_f _on_and_off test_conv_nhwc_fp _test_conv_deconv_nhwc_base torch nn Conv d torch contiguous_format dtype=torch float _test_conv_deconv_nhwc_base torch nn Conv d torch channels_last dtype=torch float _test_conv_deconv_nhwc_base torch nn Conv d torch contiguous_format dtype=torch float _test_conv_deconv_nhwc_base torch nn Conv d torch channels_last_ d dtype=torch float dtypes torch float torch bfloat test_conv_nhwc_lower_precision dtype when torch ops mkldnn _is_mkldnn_bf _supported torch ops mkldnn _is_mkldnn_fp _supported returns false bf fp CPU conv will fall back thnn impl support_checks = torch bfloat torch ops mkldnn _is_mkldnn_bf _supported torch float torch ops mkldnn _is_mkldnn_fp _supported support_checks dtype _test_conv_deconv_nhwc_base torch nn Conv d torch contiguous_format dtype=dtype _test_conv_deconv_nhwc_base torch nn Conv d torch channels_last dtype=dtype _test_conv_deconv_nhwc_base torch nn Conv d torch contiguous_format dtype=dtype _test_conv_deconv_nhwc_base torch nn Conv d torch channels_last_ d dtype=dtype BF FP fallback implementations divided into two parts im col+gemm number data type conversions middle more than onednn s direct conv resulting additional accuracy loss precisions = torch bfloat e- torch float e- prec = precisions dtype torch backends mkldnn flags enabled=False _test_conv_deconv_nhwc_base torch nn Conv d torch contiguous_format dtype=dtype prec=prec _test_conv_deconv_nhwc_base torch nn Conv d torch channels_last dtype=dtype prec=prec _test_conv_deconv_nhwc_base torch nn Conv d torch contiguous_format dtype=dtype prec=prec _test_conv_deconv_nhwc_base torch nn Conv d torch channels_last_ d dtype=dtype prec=prec reduced_f _on_and_off test_conv_transpose_nhwc_fp _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch contiguous_format dtype=torch float _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch channels_last dtype=torch float _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch contiguous_format dtype=torch float _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch channels_last_ d dtype=torch float dtypes torch float torch bfloat test_conv_transpose_nhwc_lower_precision dtype when torch ops mkldnn _is_mkldnn_bf _supported torch ops mkldnn _is_mkldnn_fp _supported returns false bf fp CPU conv will fall back thnn impl support_checks = torch bfloat torch ops mkldnn _is_mkldnn_bf _supported torch float torch ops mkldnn _is_mkldnn_fp _supported support_checks dtype _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch contiguous_format dtype=dtype _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch channels_last dtype=dtype _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch contiguous_format dtype=dtype _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch channels_last_ d dtype=dtype BF FP fallback implementations divided into two parts col im+gemm number data type conversions middle more than onednn s direct conv resulting additional accuracy loss precisions = torch bfloat e- torch float e- prec = precisions dtype torch backends mkldnn flags enabled=False _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch contiguous_format dtype=dtype prec=prec _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch channels_last dtype=dtype prec=prec _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch contiguous_format dtype=dtype prec=prec _test_conv_deconv_nhwc_base torch nn ConvTranspose d torch channels_last_ d dtype=dtype prec=prec _test_conv_transpose_base dim conv_module = torch nn ConvTranspose d torch nn ConvTranspose d torch nn ConvTranspose d input_shapes = options = itertools product True False True False train bias dilation groups options N = torch randint item M = torch randint item groups C = torch randint item groups x_shape = N C + input_shapes dim data = torch randn x_shape dtype=torch float conv mkldnn transpose conv fp conv_ref thnn transpose conv fp conv = conv_module dim in_channels=C out_channels=M kernel_size= stride= padding= dilation=dilation bias=bias groups=groups dtype=torch float x = data clone x_ref = x clone train x requires_grad_ x_ref requires_grad_ conv_ref = copy deepcopy conv torch backends mkldnn flags enabled=False y_ref = conv_ref x_ref train y_ref sum backward y = conv x train y sum backward precision = assertEqual y y_ref atol=self precision rtol=self precision assertEqual y y_ref train assertEqual x grad x_ref grad assertEqual conv weight grad conv_ref weight grad atol= e- rtol= e- bias assertEqual conv bias grad conv_ref bias grad reduced_f _on_and_off test_conv_transpose d _test_conv_transpose_base dim= reduced_f _on_and_off test_conv_transpose d _test_conv_transpose_base dim= reduced_f _on_and_off test_conv_transpose d _test_conv_transpose_base dim= test_conv d_legacy_jit_model MKLDNN integration used serialize models d weight grouped convolutions we d like preserve behavior g = conv d = torch nn Conv d groups=g conv d_mkldnn = torch utils mkldnn to_mkldnn conv d contrive legacy conv d module -d weight o i h w = conv d weight shape weight_ d = conv d weight reshape g o g i h w conv d_mkldnn weight = weight_ d to_mkldnn x = torch randn TemporaryFileName fname torch jit save conv d_mkldnn fname conv d_loaded = torch jit load fname assertEqual conv d_mkldnn weight ndimension assertEqual conv d_loaded weight ndimension assertEqual conv d x conv d_loaded x to_mkldnn to_dense This test check whether D conv supported mkldnn tensor which exposed Issue https github com pytorch pytorch issues test_conv d_functional input = torch randn to_mkldnn weight = torch randn to_mkldnn bias = torch randn to_mkldnn output = torch nn functional conv d input weight bias assertEqual output size torch Size test_relu x = torch randn dtype=torch float x = x clone requires_grad_ x = x clone to_mkldnn requires_grad_ y = torch relu x y = torch relu x to_dense loss = y sum loss = y sum loss backward loss backward assertEqual y y assertEqual x grad x grad to_dense test_relu_ x = torch randn dtype=torch float x = x clone requires_grad_ x = x clone to_mkldnn requires_grad_ y = torch relu_ x clone y = torch relu_ x clone to_dense loss = y sum loss = y sum loss backward loss backward assertEqual y y assertEqual x grad x grad to_dense unittest skipIf IS_WINDOWS Limit support bf path _test_relu_bf _base name x = torch randn dtype=torch float x_bf = x bfloat fn = getattr torch name torch ops mkldnn _is_mkldnn_bf _supported y = fn x to_mkldnn to_dense y_bf = fn x_bf to_mkldnn to_dense torch float assertEqual y y_bf atol= e- rtol= e- msg = r bf path needs cpu support avx bw avx vl avx dq assertRaisesRegex RuntimeError msg lambda fn x_bf to_mkldnn test_relu_bf _test_relu_bf _base relu test_relu_inplace_bf _test_relu_bf _base relu_ test_gelu m = torch nn GELU x = torch randn dtype=torch float x = x clone requires_grad_ x = x clone to_mkldnn requires_grad_ y = m x y = m x to_dense loss = y sum loss = y sum loss backward loss backward assertEqual y y assertEqual x grad x grad to_dense unittest skipIf IS_WINDOWS Limit support bf path test_gelu_bf m = torch nn GELU x = torch randn dtype=torch float x = x clone to_mkldnn requires_grad_ x = x clone to_mkldnn torch bfloat requires_grad_ torch ops mkldnn _is_mkldnn_bf _supported y = m x to_dense y = m x to_dense loss = y sum loss = y sum loss backward loss backward assertEqual y y torch float atol= e- rtol= assertEqual x grad to_dense x grad to_dense torch float atol= e- rtol= msg = r bf path needs cpu support avx bw avx vl avx dq assertRaisesRegex RuntimeError msg lambda m x _test_prelu_base size num_channels x = torch randn size dtype=torch float x = x clone requires_grad_ x = x clone to_mkldnn requires_grad_ x = x clone to_mkldnn requires_grad_ m = torch nn PReLU num_channels m = mkldnn_utils to_mkldnn copy deepcopy m m = copy deepcopy m y = m x y = m x to_dense y = m x to_dense Only convert data mkldnn weight Aten tensor loss = y sum loss backward loss = y sum loss backward loss = y sum loss backward assertEqual y y assertEqual y y assertEqual x grad x grad to_dense assertEqual x grad x grad to_dense test_prelu _test_prelu_base torch Size _test_prelu_base torch Size _test_prelu_base torch Size _test_prelu_base torch Size _test_prelu_base torch Size _test_prelu_base torch Size _test_prelu_base torch Size _test_prelu_base torch Size _test_prelu_base torch Size unittest skipIf IS_WINDOWS Limit support bf path _test_prelu_bf _base size num_channels torch ops mkldnn _is_mkldnn_bf _supported x = torch randn size dtype=torch float x_fp = x clone to_mkldnn requires_grad_ x_bf = x clone to_mkldnn torch bfloat requires_grad_ m = mkldnn_utils to_mkldnn torch nn PReLU m_bf = mkldnn_utils to_mkldnn torch nn PReLU torch bfloat y = m x_fp to_dense y_bf = m_bf x_bf to_dense assertEqual y y_bf torch float atol= e- rtol= e- loss = y sum loss backward loss_bf = y_bf sum loss_bf backward assertEqual x_fp grad to_dense x_bf grad to_dense torch float x_bf = torch randn size dtype=torch bfloat requires_grad_ m_bf = mkldnn_utils to_mkldnn torch nn PReLU torch bfloat msg = r bf path needs cpu support avx bw avx vl avx dq assertRaisesRegex RuntimeError msg lambda m_bf x_bf test_prelu_bf _test_prelu_bf _base torch Size _test_prelu_bf _base torch Size _test_prelu_bf _base torch Size _test_prelu_bf _base torch Size _test_prelu_bf _base torch Size _test_prelu_bf _base torch Size _test_prelu_bf _base torch Size _test_max_pool_base dim input pool_module = torch nn MaxPool d torch nn MaxPool d stride ceil_mode False True max_pool = pool_module dim kernel_size= ceil_mode stride=stride padding= ceil_mode=ceil_mode x = input clone requires_grad_ x = input clone to_mkldnn requires_grad_ y = max_pool x y = max_pool x to_dense loss = y sum loss = y sum loss backward loss backward assertEqual y y assertEqual x grad x grad to_dense test_max_pool d N = torch randint item C = torch randint item H W x = torch randn N C H W dtype=torch float _test_max_pool_base dim= input=x test_max_pool d N = torch randint item C = torch randint item D H W x = torch randn N C D H W dtype=torch float _test_max_pool_base dim= input=x unittest skipIf IS_WINDOWS Limit support bf path _test_max_pool_bf _base dim input pool_module = torch nn MaxPool d torch nn MaxPool d x_bf = input bfloat stride ceil_mode False True max_pool = pool_module dim kernel_size= ceil_mode stride=stride padding= ceil_mode=ceil_mode torch ops mkldnn _is_mkldnn_bf _supported y = max_pool input to_mkldnn to_dense y_bf = max_pool x_bf to_mkldnn to_dense torch float assertEqual y y_bf atol= rtol= e- msg = f mkldnn_max_pool dim d d bf path needs cpu support avx bw avx vl avx dq assertRaisesRegex RuntimeError msg lambda max_pool x_bf to_mkldnn test_max_pool d_bf N = torch randint item C = torch randint item H W x = torch randn N C H W dtype=torch float _test_max_pool_bf _base dim= input=x test_max_pool d_bf N = torch randint item C = torch randint item D H W x = torch randn N C D H W dtype=torch float _test_max_pool_bf _base dim= input=x test_max_pool d_stride_none N = torch randint item C = torch randint item H W x = torch randn N C H W dtype=torch float ceil_mode False True y = F max_pool d x kernel_size= ceil_mode stride=None padding= ceil_mode=ceil_mode y = F max_pool d x to_mkldnn kernel_size= ceil_mode stride=None padding= ceil_mode=ceil_mode assertEqual y y to_dense https github com pytorch pytorch issues xfailIfTorchDynamo test_max_pool_unsupported OneDNN support dilation max_pooling will avilabled v N = torch randint item C = torch randint item d dilation case x = torch randn N C dtype=torch float to_mkldnn max_pool d = torch nn MaxPool d kernel_size= stride= padding= dilation= assertRaisesRegex RuntimeError mkldnn_max_pool d does support dilation case lambda max_pool d x d dilation case x = torch randn N C dtype=torch float to_mkldnn max_pool d = torch nn MaxPool d kernel_size= stride= padding= dilation= assertRaisesRegex RuntimeError mkldnn_max_pool d does support dilation case lambda max_pool d x _test_avg_pool_base dim input avg_module = torch nn AvgPool d torch nn AvgPool d count_include_pad True False avg_pool = avg_module dim kernel_size= stride= padding= count_include_pad=count_include_pad x = input clone requires_grad_ x = input clone to_mkldnn requires_grad_ y = avg_pool x y = avg_pool x to_dense loss = y sum loss = y sum loss backward loss backward assertEqual y y assertEqual x grad x grad to_dense test_avg_pool d N = torch randint item C = torch randint item x = torch randn N C dtype=torch float _test_avg_pool_base dim= input=x test_avg_pool d N = torch randint item C = torch randint item x = torch randn N C dtype=torch float _test_avg_pool_base dim= input=x unittest skipIf IS_WINDOWS Limit support bf path _test_avg_pool_bf _base dim input avg_module = torch nn AvgPool d torch nn AvgPool d x_bf = input bfloat count_include_pad True False avg_pool = avg_module dim kernel_size= stride= padding= count_include_pad=count_include_pad torch ops mkldnn _is_mkldnn_bf _supported y = avg_pool input to_mkldnn to_dense y_bf = avg_pool x_bf to_mkldnn to_dense torch float assertEqual y y_bf atol= e- rtol= e- msg = f mkldnn_avg_pool dim d d bf path needs cpu support avx bw avx vl avx dq assertRaisesRegex RuntimeError msg lambda avg_pool x_bf to_mkldnn test_avg_pool d_bf N = torch randint item C = torch randint item x = torch randn N C dtype=torch float _test_avg_pool_bf _base dim= input=x test_avg_pool d_bf N = torch randint item C = torch randint item x = torch randn N C dtype=torch float _test_avg_pool_bf _base dim= input=x test_avg_pool d_stride_none N = torch randint item C = torch randint item x = torch randn N C dtype=torch float count_include_pad True False y = F avg_pool d x kernel_size= stride=None padding= count_include_pad=count_include_pad y = F avg_pool d x to_mkldnn kernel_size= stride=None padding= count_include_pad=count_include_pad assertEqual y y to_dense test_adaptive_avg_pool d N = torch randint item C = torch randint item x = torch randn N C dtype=torch float adaptive_avg_pool d = torch nn AdaptiveAvgPool d x = x clone requires_grad_ x = x clone to_mkldnn requires_grad_ y = adaptive_avg_pool d x y = adaptive_avg_pool d x to_dense loss = y sum loss = y sum loss backward loss backward assertEqual y y assertEqual x grad x grad to_dense unittest skipIf IS_WINDOWS Limit support bf path test_adaptive_avg_pool d_bf N = torch randint item C = torch randint item x = torch randn N C dtype=torch float x_bf = x bfloat adaptive_avg_pool d = torch nn AdaptiveAvgPool d torch ops mkldnn _is_mkldnn_bf _supported y = adaptive_avg_pool d x to_mkldnn to_dense y_bf = adaptive_avg_pool d x to_mkldnn to_dense torch float assertEqual y y_bf atol= e- rtol= e- msg = mkldnn_adaptive_avg_pool d bf path needs cpu support avx bw avx vl avx dq assertRaisesRegex RuntimeError msg lambda adaptive_avg_pool d x_bf to_mkldnn _test_batch_norm_base dim channels input bn_module = torch nn BatchNorm d torch nn BatchNorm d bn = bn_module dim channels float train False mkldnn_bn = mkldnn_utils to_mkldnn copy deepcopy bn assertEqual bn input mkldnn_bn input to_mkldnn to_dense _test_serialization mkldnn_bn input to_mkldnn _test_tracing mkldnn_bn input to_mkldnn _test_batch_norm_train_base dim channels input TODO support d batchnorm training bn_module = torch nn BatchNorm d TODO support none affine options = itertools product True True False affine track_running_stats options bn = bn_module dim num_features=channels affine=affine track_running_stats=track_running_stats float train True mkldnn_bn = copy deepcopy bn x = input clone requires_grad_ x = input clone to_mkldnn requires_grad_ y = bn x y = mkldnn_bn x to_dense loss = y sum loss = y sum loss backward loss backward assertEqual y y assertEqual x grad x grad to_dense assertEqual bn weight grad mkldnn_bn weight grad rtol= e- atol= e- track_running_stats assertEqual bn running_mean mkldnn_bn running_mean assertEqual bn running_var mkldnn_bn running_var rtol= e- atol= e- test_batch_norm_ d N = torch randint item C = torch randint item x = torch randn N C dtype=torch float _test_batch_norm_base dim= channels=C input=x _test_batch_norm_train_base dim= channels=C input=x test_batch_norm_ d N = torch randint item C = torch randint item x = torch randn N C dtype=torch float _test_batch_norm_base dim= channels=C input=x unittest skipIf IS_WINDOWS Limit support bf path _test_batch_norm_bf _base dim channels input bn_module = torch nn BatchNorm d torch nn BatchNorm d x_bf = input bfloat TODO support training train False bn = bn_module dim channels float train train mkldnn_bn = mkldnn_utils to_mkldnn copy deepcopy bn noqa F torch ops mkldnn _is_mkldnn_bf _supported y = bn input to_mkldnn to_dense y_bf = bn input to_mkldnn to_dense torch float assertEqual y y_bf atol= e- rtol= e- msg = mkldnn_batch_norm bf path needs cpu support avx bw avx vl avx dq assertRaisesRegex RuntimeError msg lambda bn x_bf to_mkldnn test_batch_norm_ d_bf N = torch randint item C = torch randint item x = torch randn N C dtype=torch float _test_batch_norm_bf _base dim= channels=C input=x test_batch_norm_ d_bf N = torch randint item C = torch randint item x = torch randn N C dtype=torch float _test_batch_norm_bf _base dim= channels=C input=x test_add N = torch randint item C = torch randint item alpha = torch randn dtype=torch float item x = torch randn N C dtype=torch float y = torch randn N C dtype=torch float mx = x to_mkldnn my = y to_mkldnn add assertEqual x + y mx + my to_dense assertEqual torch add x y alpha=alpha torch add mx my alpha=alpha to_dense add_ x += y mx += my assertEqual x mx to_dense add_out out = x clone mkldnn_out = out to_mkldnn torch add x y alpha=alpha out=out torch add mx my alpha=alpha out=mkldnn_out assertEqual out mkldnn_out to_dense add_out inplace case first input torch add x y alpha=alpha out=x torch add mx my alpha=alpha out=mx assertEqual x mx to_dense add_out inplace case second input torch add x y alpha=alpha out=y torch add mx my alpha=alpha out=my assertEqual y my to_dense test_mul N = torch randint item C = torch randint item value = torch randn dtype=torch float item x = torch randn N C dtype=torch float y = torch randn N C dtype=torch float mx = x to_mkldnn my = y to_mkldnn mul assertEqual x y mx my to_dense assertEqual x value mx value to_dense assertEqual torch mul x y torch mul mx my to_dense assertEqual torch mul x value torch mul mx value to_dense mul_ x = y mx = my assertEqual x mx to_dense x = value mx = value assertEqual x mx to_dense mul_out out = x clone mkldnn_out = out to_mkldnn torch mul x y out=out torch mul mx my out=mkldnn_out assertEqual out mkldnn_out to_dense out = x clone mkldnn_out = out to_mkldnn torch mul x value out=out torch mul mx value out=mkldnn_out assertEqual out mkldnn_out to_dense test_ _dimension_tensor x = torch rand dtype=torch float y = torch rand dtype=torch float unary ops work without modification out_relu = torch relu y out_relu_mkldnn = torch relu y to_mkldnn to_dense assertEqual out_relu out_relu_mkldnn out_mul = x y out_mul_mkldnn = x to_mkldnn y to_mkldnn to_dense assertEqual out_mul out_mul_mkldnn out_add = x + y out_add_mkldnn = x to_mkldnn + y to_mkldnn to_dense assertEqual out_add out_add_mkldnn x requires_grad_ True y requires_grad_ True assertRaisesRegex RuntimeError -dimension Tensor training x to_mkldnn + y to_mkldnn assertRaisesRegex RuntimeError must match torch rand to_mkldnn + torch rand to_mkldnn C = m = torch nn Conv d C C x = torch randn C C dtype=torch float out_eager = m x out_mkldnn = mkldnn_utils to_mkldnn m x assertEqual out_eager out_mkldnn https github com pytorch pytorch issues xfailIfTorchDynamo test_view x = torch randn dtype=torch float to_mkldnn assertRaisesRegex RuntimeError Change use reshape lambda x view x size - test_reshape x = torch randn dtype=torch float size = x size - assertEqual x reshape size x to_mkldnn reshape size to_dense test whether share same memory plain format tensor y = x to_mkldnn z = y reshape size add_ y reshape size assertEqual y reshape size to_dense z to_dense test_reshape_blocked_format construct mkldnn blocked tensor mkldnn conv d C = m = mkldnn_utils to_mkldnn torch nn Conv d C C x = torch randn C to_mkldnn mkldnn tensor w blocked format y_block = m x aten tensor w plain format y_plain = y_block to_dense y_block_reshape = y_block reshape C - y_plain_reshape = y_plain reshape C - assertEqual y_plain_reshape y_block_reshape to_dense test_reshape_backward x = torch randn dtype=torch float size = x size - x = x clone requires_grad_ x = x clone to_mkldnn requires_grad_ in_features = out_features = torch randint item linear = torch nn Linear in_features out_features float y = linear x reshape size sum y = linear x reshape size to_dense sum y backward y backward assertEqual x grad x grad to_dense test_clone x = torch randn dtype=torch float assertEqual x clone x to_mkldnn clone to_dense test whether share same memory y = x to_mkldnn z = y clone add_ y assertNotEqual y to_dense z to_dense test_transpose x = torch randn dtype=torch float dim range x ndim dim range x ndim assertEqual x transpose dim dim x to_mkldnn transpose dim dim to_dense test_transpose_invalid_dime x = torch randn dtype=torch float to_mkldnn assertRaisesRegex IndexError Dimension out range torch _mkldnn_transpose x test_linear_non_contiguous_weight in_features = torch randint item out_features = torch randint item x = torch randn in_features dtype=torch float w = torch randn in_features out_features dtype=torch float bias True False x = x clone requires_grad_ x = x clone to_mkldnn requires_grad_ linear = torch nn Linear in_features out_features float linear weight = torch nn Parameter w t mkldnn_linear = copy deepcopy linear y = linear x sum y = mkldnn_linear x to_dense sum y backward y backward assertEqual x grad x grad to_dense assertEqual linear weight grad mkldnn_linear weight grad bias assertEqual linear bias grad mkldnn_linear bias grad test_linear in_features = torch randint item out_features = torch randint item x = torch randn in_features dtype=torch float bias True False linear = torch nn Linear in_features out_features bias=bias float mkldnn_linear = mkldnn_utils to_mkldnn copy deepcopy linear assertEqual linear x mkldnn_linear x to_mkldnn to_dense _test_serialization mkldnn_linear x to_mkldnn _test_tracing mkldnn_linear x to_mkldnn test_linear_backward in_features = torch randint item out_features = torch randint item x = torch randn in_features dtype=torch float bias True False x = x clone requires_grad_ x = x clone to_mkldnn requires_grad_ linear = torch nn Linear in_features out_features float mkldnn_linear = copy deepcopy linear y = linear x sum y = mkldnn_linear x to_dense sum y backward y backward assertEqual x grad x grad to_dense assertEqual linear weight grad mkldnn_linear weight grad bias assertEqual linear bias grad mkldnn_linear bias grad dtypes torch float torch bfloat test_linear_lowp dtype in_features = torch randint item out_features = torch randint item x = torch randn in_features dtype=torch float x_lowp = x dtype=dtype bias True False linear = torch nn Linear in_features out_features bias=bias float mkldnn_linear = mkldnn_utils to_mkldnn copy deepcopy linear mkldnn_linear_lowp = mkldnn_utils to_mkldnn copy deepcopy linear dtype lowp_support = torch bfloat torch ops mkldnn _is_mkldnn_bf _supported torch half torch ops mkldnn _is_mkldnn_fp _supported lowp_support dtype y = mkldnn_linear x to_mkldnn to_dense y_lowp = mkldnn_linear_lowp x_lowp to_mkldnn to_dense torch float dtype == torch bfloat assertEqual y y_lowp atol= e- rtol= e- assertEqual y y_lowp atol= e- rtol= e- msg = torch bfloat r bf path needs cpu support avx_ne_convert avx bw avx vl avx dq torch half r fp path needs cpu support avx_ne_convert avx _fp assertRaisesRegex RuntimeError msg dtype lambda mkldnn_linear_lowp x_lowp to_mkldnn test_softmax x = torch randn dtype=torch float dim range x ndim softmax = torch nn Softmax dim=dim assertEqual softmax x softmax x to_mkldnn to_dense test_sigmoid x = torch randn dtype=torch float mkldnn_x = x to_mkldnn assertEqual torch sigmoid x torch sigmoid mkldnn_x to_dense inplace torch sigmoid_ x torch sigmoid_ mkldnn_x assertEqual x mkldnn_x to_dense test_tanh x = torch randn dtype=torch float mkldnn_x = x to_mkldnn assertEqual torch tanh x torch tanh mkldnn_x to_dense inplace torch tanh_ x torch tanh_ mkldnn_x assertEqual x mkldnn_x to_dense _test_serialization module inputs TemporaryFileName fname torch jit save module fname loaded = torch jit load fname assertEqual module inputs to_dense loaded inputs to_dense _test_tracing module inputs traced = torch jit trace module inputs assertEqual module inputs to_dense traced inputs to_dense test_set_data_tensorimpl_type Dense tensor has impl type ` TensorImpl ` while MKL-DNN tensor has impl type ` OpaqueTensorImpl IDeepTensorWrapperPtr ` x = torch randn dtype=torch float device=torch device cpu x_mkldnn = x to_mkldnn assertRaisesRegex RuntimeError incompatible tensor type x data = x_mkldnn test_empty x = torch empty dtype=torch float x = torch empty dtype=torch float layout=torch _mkldnn assertEqual x size x to_dense size assertEqual x dtype x to_dense dtype test_zero_ x = torch randn dtype=torch float x = x clone to_mkldnn assertEqual x zero_ x zero_ to_dense test_is_mkldnn x = torch randn dtype=torch float assertFalse x is_mkldnn assertTrue x to_mkldnn is_mkldnn legacy constructor new doesn t support mkldnn tensors skipIfTorchDynamo https github com pytorch torchdynamo issues test_legacy_new_failure x = torch randn dtype=torch float x_mkldnn = x to_mkldnn assertRaises RuntimeError lambda x_mkldnn new device= cpu assertRaises RuntimeError lambda x_mkldnn new x storage assertRaises RuntimeError lambda x_mkldnn new x assertRaises RuntimeError lambda x_mkldnn new torch Size assertRaises RuntimeError lambda x_mkldnn new test_is_mkldnn_jit EnsureMkldnn torch jit ScriptModule torch jit script_method forward x x is_mkldnn x = x to_mkldnn x m = EnsureMkldnn x = torch randn dtype=torch float assertTrue m x is_mkldnn assertTrue m x to_mkldnn is_mkldnn _test_imagenet_model model model = model train False float mkldnn_model = mkldnn_utils to_mkldnn copy deepcopy model x = torch randn dtype=torch float torch no_grad assertEqual model x mkldnn_model x to_mkldnn to_dense skipIfNoTorchVision test_resnet model = torchvision models resnet resnet weights=None _test_imagenet_model model skipIfNoTorchVision test_resnext _ x d model = torchvision models resnet resnext _ x d weights=None _test_imagenet_model model _lstm_params_list params_dict = input_size hidden_size num_layers bidirectional False True bias False True batch_first False True dropout batch_size seq_len training False True params_list = list params_dict values params_list _cast_dtype input dtype dtype == torch bfloat input = input torch bfloat dtype == torch half input = input torch half input test_lstm seed = torch manual_seed seed params_list = _lstm_params_list dtype types bf = dtype == torch bfloat fp = dtype == torch half rtol = e- atol = e- bf rtol = atol = fp rtol = e- atol = e- input_size hidden_size num_layers bidirectional bias batch_first dropout batch_size seq_len training \ itertools product params_list num_directions = bidirectional batch_first input = torch randn batch_size seq_len input_size dtype=torch float input = torch randn seq_len batch_size input_size dtype=torch float h = torch randn num_layers num_directions batch_size hidden_size dtype=torch float c = torch randn num_layers num_directions batch_size hidden_size dtype=torch float fp TODO add training support when oneDNN support lstm FP training training = False model = torch nn LSTM input_size hidden_size num_layers bidirectional=bidirectional bias=bias dropout=dropout batch_first=batch_first float model train training model eval input = input clone requires_grad_ training input = input clone requires_grad_ training h = h clone requires_grad_ training h = h clone requires_grad_ training c = c clone requires_grad_ training c = c clone requires_grad_ training model = copy deepcopy model model = copy deepcopy model torch no_grad training nullcontext torch backends mkldnn flags enabled=False torch manual_seed seed output hn cn = _cast_dtype model dtype _cast_dtype input dtype _cast_dtype h dtype _cast_dtype c dtype torch manual_seed seed output hn cn = _cast_dtype model dtype _cast_dtype input dtype _cast_dtype h dtype _cast_dtype c dtype assertEqual output output rtol=rtol atol=atol assertEqual hn hn rtol=rtol atol=atol assertEqual cn cn rtol=rtol atol=atol training torch backends mkldnn flags enabled=False torch manual_seed seed output sum backward retain_graph=True torch manual_seed seed output sum backward retain_graph=True assertEqual input grad input grad rtol=rtol atol=atol name para model named_parameters assertEqual para getattr model name assertEqual para grad getattr model name grad rtol=rtol atol=atol torch backends mkldnn flags enabled=False torch manual_seed seed hn sum backward retain_graph=True torch manual_seed seed hn sum backward retain_graph=True assertEqual h grad h grad rtol=rtol atol=atol torch backends mkldnn flags enabled=False torch manual_seed seed cn sum backward retain_graph=True torch manual_seed seed cn sum backward retain_graph=True assertEqual c grad c grad rtol=rtol atol=atol dtypes torch float torch bfloat test_matmul_lower_precision dtype support_check = torch bfloat torch ops mkldnn _is_mkldnn_bf _supported torch float torch ops mkldnn _is_mkldnn_fp _supported common shape shape op dtype = torch randn shape dtype=dtype a_ref = float b = torch randn shape dtype=dtype b_ref = b float y = op b y_ref = op a_ref b_ref assertEqual y y_ref exact_dtype=False support_check dtype = torch randn dtype=dtype contiguous tensor s strides default contiguous strides = torch as_strided clone assertTrue is_contiguous b = torch randn dtype=dtype y = torch ops aten bmm b y = torch bmm b assertEqual y y shape shape op torch matmul torch matmul torch matmul torch matmul torch matmul torch bmm torch bmm common shape shape op dtype test_mkldnn_setflags_nowarn device Regression test https github com pytorch pytorch issues warnings catch_warnings record=True w rc = torch backends mkldnn set_flags torch backends mkldnn returns previously set flags That one should able set back without cauinsg warning torch backends mkldnn set_flags rc Above should trigger no warnings regardless configuration assertEqual len w test_mkldnn_error_on_zero_stride device Regression test https github com pytorch pytorch issues x = torch rand to_mkldnn assertRaises ValueError torch mkldnn_max_pool d x kernel_size= stride= test_mkldnn_scaled_mm device - None test input scale weight scale output_scale M N K = x = torch randn M K device=device K y = torch randn N K device=device t K options = itertools product torch float _e m fn torch float _e m torch float _e m fn torch float _e m torch float _e m fn torch float _e m torch bfloat torch float torch float x_dtype y_dtype out_dtype options out_dtype torch float _e m fn torch float _e m x_dtype = out_dtype continue x_fp = x x_dtype y_fp = y y_dtype scale_a = torch randn device=device scale_b = torch randn device=device scale_out = torch randn device=device out_fp = torch mm x_fp torch float scale_a y_fp torch float scale_b out_dtype torch float _e m fn torch float _e m out_emulated = out_fp scale_out out_dtype out_emulated = out_fp out_dtype out = torch _scaled_mm x_fp y_fp scale_a scale_b scale_result=scale_out out_dtype=out_dtype out_dtype None assertEqual out_dtype out dtype assertEqual out_emulated float out float atol= e- rtol= e- recover_orig_fp _precision test_mlkdnn_get_set get set mkldnn ops torch backends mkldnn flags enabled=None fp _precision= bf assertEqual torch backends mkldnn fp _precision bf torch backends mkldnn flags enabled=None fp _precision= tf assertEqual torch backends mkldnn fp _precision tf torch backends mkldnn flags enabled=None fp _precision= none assertEqual torch backends mkldnn fp _precision none get set matmul torch backends mkldnn matmul fp _precision = bf assertEqual torch backends mkldnn matmul fp _precision bf torch backends mkldnn matmul fp _precision = tf assertEqual torch backends mkldnn matmul fp _precision tf torch backends mkldnn matmul fp _precision = none assertEqual torch backends mkldnn matmul fp _precision none get set conv torch backends mkldnn conv fp _precision = bf assertEqual torch backends mkldnn conv fp _precision bf torch backends mkldnn conv fp _precision = tf assertEqual torch backends mkldnn conv fp _precision tf torch backends mkldnn conv fp _precision = none assertEqual torch backends mkldnn conv fp _precision none get set rnn torch backends mkldnn rnn fp _precision = bf assertEqual torch backends mkldnn rnn fp _precision bf torch backends mkldnn rnn fp _precision = tf assertEqual torch backends mkldnn rnn fp _precision tf torch backends mkldnn rnn fp _precision = none assertEqual torch backends mkldnn rnn fp _precision none recover_orig_fp _precision test_generic_precision torch backends flags fp _precision= none assertEqual torch backends fp _precision none torch backends flags fp _precision= tf assertEqual torch backends fp _precision tf recover_orig_fp _precision test_default_use_parent torch backends mkldnn matmul fp _precision = none torch backends mkldnn flags enabled=None fp _precision= bf assertEqual torch backends mkldnn matmul fp _precision bf torch backends mkldnn flags enabled=None fp _precision= tf assertEqual torch backends mkldnn matmul fp _precision tf torch backends mkldnn flags enabled=None fp _precision= none torch backends flags fp _precision= bf assertEqual torch backends mkldnn matmul fp _precision bf torch backends flags fp _precision= tf assertEqual torch backends mkldnn matmul fp _precision tf instantiate_device_type_tests TestMkldnn globals only_for= cpu __name__ == __main__ run_tests