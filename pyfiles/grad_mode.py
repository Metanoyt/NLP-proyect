mypy allow-untyped-defs typing Any Union torch torch utils _contextlib _DecoratorContextManager _NoParamDecoratorContextManager F __all__ = no_grad enable_grad set_grad_enabled inference_mode set_multithreading_enabled no_grad _NoParamDecoratorContextManager r Context-manager disables gradient calculation Disabling gradient calculation useful inference when you sure you will call meth ` Tensor backward ` It will reduce memory consumption computations would otherwise have ` requires_grad=True ` In mode result every computation will have ` requires_grad=False ` even when inputs have ` requires_grad=True ` There exception All factory functions functions create new Tensor take requires_grad kwarg will NOT affected mode This context manager thread local will affect computation other threads Also functions decorator note No-grad one several mechanisms can enable disable gradients locally see ref ` locally-disable-grad-doc ` more information how they compare note This API does apply ref ` forward-mode AD forward-mode-ad ` If you want disable forward AD computation you can unpack your dual tensors Example xdoctest +SKIP x = torch tensor requires_grad=True torch no_grad y = x y requires_grad False torch no_grad doubler x x z = doubler x z requires_grad False torch no_grad tripler x x z = tripler x z requires_grad False factory function exception torch no_grad = torch nn Parameter torch rand requires_grad True __init__ - None torch _jit_internal is_scripting super __init__ prev = False __enter__ - None prev = torch is_grad_enabled torch set_grad_enabled False __exit__ exc_type Any exc_value Any traceback Any - None torch set_grad_enabled prev enable_grad _NoParamDecoratorContextManager r Context-manager enables gradient calculation Enables gradient calculation has been disabled via ` ~no_grad ` ` ~set_grad_enabled ` This context manager thread local will affect computation other threads Also functions decorator note enable_grad one several mechanisms can enable disable gradients locally see ref ` locally-disable-grad-doc ` more information how they compare note This API does apply ref ` forward-mode AD forward-mode-ad ` Example xdoctest +SKIP x = torch tensor requires_grad=True torch no_grad torch enable_grad y = x y requires_grad True y backward x grad tensor torch enable_grad doubler x x torch no_grad z = doubler x z requires_grad True torch enable_grad tripler x x torch no_grad z = tripler x z requires_grad True __enter__ - None prev = torch is_grad_enabled torch _C _set_grad_enabled True __exit__ exc_type Any exc_value Any traceback Any - None torch _C _set_grad_enabled prev set_grad_enabled _DecoratorContextManager r Context-manager sets gradient calculation off ` ` set_grad_enabled ` ` will enable disable grads based its argument attr ` mode ` It can used context-manager function This context manager thread local will affect computation other threads Args mode bool Flag whether enable grad ` ` True ` ` disable ` ` False ` ` This can used conditionally enable gradients note set_grad_enabled one several mechanisms can enable disable gradients locally see ref ` locally-disable-grad-doc ` more information how they compare note This API does apply ref ` forward-mode AD forward-mode-ad ` Example xdoctest +SKIP x = torch tensor requires_grad=True is_train = False torch set_grad_enabled is_train y = x y requires_grad False _ = torch set_grad_enabled True y = x y requires_grad True _ = torch set_grad_enabled False y = x y requires_grad False __init__ mode bool - None prev = torch is_grad_enabled mode = mode torch _C _set_grad_enabled mode __call__ orig_func F - F torch _C _set_grad_enabled prev super __call__ orig_func __enter__ - None torch _C _set_grad_enabled mode __exit__ exc_type Any exc_value Any traceback Any - None torch _C _set_grad_enabled prev __str__ - str f torch typename mode= mode __repr__ - str str clone - set_grad_enabled r Create copy __class__ mode inference_mode _DecoratorContextManager r Context manager enables disables inference mode InferenceMode analogous ` ~no_grad ` should used when you certain your operations will interact autograd e g during data loading model evaluation Compared ` ~no_grad ` removes additional overhead disabling view tracking version counter bumps It also more restrictive tensors created mode cannot used computations recorded autograd This context manager thread-local does affect computation other threads Also functions decorator note Inference mode one several mechanisms can locally enable disable gradients See ref ` locally-disable-grad-doc ` comparison If avoiding use tensors created inference mode autograd-tracked regions difficult consider benchmarking your code without inference mode weigh performance benefits against trade-offs You can always use ` ~no_grad ` instead note Unlike some other mechanisms locally enable disable grad entering inference_mode also disables ref ` forward-mode AD forward-mode-ad ` warning ` inference_mode ` does NOT automatically set model evaluation mode For proper inference behavior e g disabling dropout using running statistics batch normalization you must explicitly set your model evaluation mode using ` model eval ` addition using context manager Args mode bool function Either boolean flag enable disable inference mode Python function decorate inference mode enabled Example xdoctest +REQUIRES env TORCH_DOCTEST_AUTOGRAD torch x = torch ones requires_grad=True torch inference_mode y = x x y requires_grad False xdoctest +SKIP want string isn t quite right y _version Traceback most recent call last File stdin line module RuntimeError Inference tensors do track version counter torch inference_mode func x x x out = func x out requires_grad False torch inference_mode doubler x x out = doubler x out requires_grad False __init__ mode bool = True - None torch _jit_internal is_scripting super __init__ mode = mode __new__ cls mode=True isinstance mode bool super __new__ cls cls mode __enter__ - None _inference_mode_context = torch _C _InferenceMode mode _inference_mode_context __enter__ __exit__ exc_type Any exc_value Any traceback Any - None _inference_mode_context __exit__ exc_type exc_value traceback clone - inference_mode r Create copy __class__ mode _enter_inference_mode mode mode_context = torch _C _InferenceMode mode mode_context __enter__ mode_context _exit_inference_mode mode mode __exit__ None None None set_multithreading_enabled _DecoratorContextManager r Context-manager sets multithreaded backwards off ` ` set_multithreading_enabled ` ` will enable disable multithreaded backwards based its argument attr ` mode ` It can used context-manager function This context manager thread local will affect computation other threads Args mode bool Flag whether enable multithreaded backwards ` ` True ` ` disable ` ` False ` ` note This API does apply ref ` forward-mode AD forward-mode-ad ` __init__ mode bool - None prev = torch _C _is_multithreading_enabled torch _C _set_multithreading_enabled mode mode = mode __enter__ - None pass __exit__ exc_type Any exc_value Any traceback Any - None torch _C _set_multithreading_enabled prev clone - set_multithreading_enabled r Create copy __class__ mode _force_original_view_tracking _DecoratorContextManager r Context-manager sets whether always enable view-replay autograd ` ` set_view_replay_enabled ` ` will enable disable view-replay based its argument attr ` mode ` It can used context-manager function This context manager thread local will affect computation other threads When tensor view mutated autograd engine needs decide whether regenerate updated view either replaying chain views updated base single call as_strided If set_view_replay_enabled set True then autograd will always use view replay Otherwise will fall back its existing logic Args mode bool Flag whether enable view-replay ` ` True ` ` disable ` ` False ` ` __init__ mode bool - None prev = torch _C _is_view_replay_enabled torch _C _set_view_replay_enabled mode mode = mode __enter__ - None pass __exit__ exc_type Any exc_value Any traceback Any - None torch _C _set_view_replay_enabled prev clone __class__ mode _unsafe_preserve_version_counter _DecoratorContextManager r DO NOT USE THIS UNLESS YOU KNOW EXACTLY WHAT YOU RE DOING This context manager can lead arbitrary silent-correctness issues any other part your code even ones touched directly context manager Ordinarily autograd will track mutations tensors incrementing s ` _version ` attribute This generally important correctness example mutating tensor autograd has saved backwards pass can result incorrect gradients autograd uses version counter detect error out situation However there rare instances where might useful hide mutations autograd For example tensor very large you d like free its memory storing elsewhere re-populate tensor right before needed autograd Args tensor torch Tensor tensor question you would like preserve version counter note This API does apply ref ` forward-mode AD forward-mode-ad ` __init__ tensors Union torch Tensor tuple torch Tensor - None tensors = tensors isinstance tensors torch Tensor tensors isinstance tensors tuple raise AssertionError Expected tensors tuple prev_versions = tuple t _version t tensors __enter__ - None pass pyrefly ignore bad-override __exit__ args - None torch _C _autograd _unsafe_set_version_counter tensors prev_versions