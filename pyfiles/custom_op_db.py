mypy allow-untyped-defs torch functools torch testing make_tensor torch testing _internal opinfo core OpInfo SampleInput torch testing _internal common_dtype all_types_and numpy np torch testing _internal autograd_function_db sample_inputs_numpy_cube sample_inputs_numpy_mul sample_inputs_numpy_mul_scalar sample_inputs_numpy_sort sample_inputs_numpy_take torch Tensor torch types Number typing noqa F Note custom op db This collection custom operator test cases written OpInfos so they can easily consumed OpInfo-based tests check subsystems support them correctly to_numpy tensor tensor cpu numpy torch library custom_op _torch_testing numpy_cube mutates_args= numpy_cube x Tensor - tuple Tensor Tensor x_np = to_numpy x dx = torch tensor x_np device=x device torch tensor x_np device=x device dx numpy_cube register_fake _ x x clone x clone numpy_cube_setup_context ctx inputs output x = inputs _cube dx = output ctx save_for_backward x dx numpy_cube_backward ctx grad_out grad_dx x dx = ctx saved_tensors grad_x = numpy_mul grad_out dx + numpy_mul grad_dx x grad_x numpy_cube register_autograd numpy_cube_backward setup_context=numpy_cube_setup_context numpy_cube_vmap info in_dims x result = numpy_cube x result in_dims in_dims numpy_cube register_vmap numpy_cube_vmap torch library custom_op _torch_testing numpy_mul mutates_args= numpy_mul x Tensor y Tensor - Tensor torch tensor to_numpy x to_numpy y device=x device numpy_mul register_fake _ x y assert x device == y device x y contiguous numpy_mul_setup_context ctx inputs output ctx save_for_backward inputs numpy_mul_backward ctx grad_out x y = ctx saved_tensors grad_x = grad_out y ctx needs_input_grad None grad_y = grad_out x ctx needs_input_grad None grad_x grad_y numpy_mul register_autograd numpy_mul_backward setup_context=numpy_mul_setup_context numpy_mul_vmap info in_dims x y x_bdim y_bdim = in_dims x = x movedim x_bdim - x_bdim None x unsqueeze - y = y movedim y_bdim - y_bdim None y unsqueeze - result = x y result = result movedim - result numpy_mul register_vmap numpy_mul_vmap torch library custom_op _torch_testing numpy_mul_scalar mutates_args= numpy_mul_scalar x Tensor scalar float - Tensor torch tensor to_numpy x scalar device=x device numpy_mul_scalar register_fake _ x scalar x scalar contiguous numpy_mul_scalar_setup_context ctx inputs keyword_only_inputs output ctx scalar = keyword_only_inputs scalar numpy_mul_scalar_backward ctx grad_out grad_x = grad_out ctx scalar grad_x numpy_mul_scalar register_autograd numpy_mul_scalar_backward setup_context=numpy_mul_scalar_setup_context numpy_mul_scalar_vmap info in_dims x scalar x_bdim = in_dims x = x movedim x_bdim - x_bdim None x unsqueeze - result = x scalar result = result movedim - result numpy_mul_scalar register_vmap numpy_mul_scalar_vmap torch library custom_op _torch_testing numpy_sort mutates_args= numpy_sort x Tensor dim int - tuple Tensor Tensor Tensor device = x device x = to_numpy x ind = np argsort x axis=dim ind_inv = np argsort ind axis=dim result = np take_along_axis x ind axis=dim torch tensor result device=device torch tensor ind device=device torch tensor ind_inv device=device numpy_sort register_fake _ x dim torch empty_like x torch empty_like x dtype=torch long torch empty_like x dtype=torch long numpy_sort_setup_context ctx inputs output _out ind ind_inv = output ctx dim = inputs ctx save_for_backward ind ind_inv ctx mark_non_differentiable ind ind_inv numpy_sort_backward ctx grad_out grad_ind grad_ind_inv ind ind_inv = ctx saved_tensors numpy_take grad_out ind_inv ind ctx dim None numpy_sort register_autograd numpy_sort_backward setup_context=numpy_sort_setup_context numpy_sort_vmap info in_dims x dim x_bdim _ = in_dims x = x movedim x_bdim dim = dim dim = dim + x dim - result = numpy_sort x dim + result numpy_sort register_vmap numpy_sort_vmap torch library custom_op _torch_testing numpy_take mutates_args= numpy_take x Tensor ind Tensor ind_inv Tensor dim int - Tensor device = x device x = to_numpy x ind = to_numpy ind torch tensor np take_along_axis x ind dim device=device numpy_take register_fake _ x ind ind_inv dim assert x device == ind device assert x device == ind_inv device assert ind dtype == torch long assert ind_inv dtype == torch long torch empty_like x numpy_take_setup_context ctx inputs output _x ind ind_inv dim = inputs ctx dim = dim ctx save_for_backward ind ind_inv numpy_take_backward ctx grad_out ind ind_inv = ctx saved_tensors grad_x = numpy_take grad_out ind_inv ind ctx dim grad_x None None None numpy_take register_autograd numpy_take_backward setup_context=numpy_take_setup_context numpy_take_vmap info in_dims x ind ind_inv dim x_bdim ind_bdim ind_inv_bdim _ = in_dims wrap dim logical_dim = x dim x_bdim None x_bdim - dim = dim dim = dim + logical_dim expand_bdim x x_bdim x_bdim None x expand info batch_size x shape x movedim x_bdim x = expand_bdim x x_bdim ind = expand_bdim ind ind_bdim ind_inv = expand_bdim ind_inv ind_inv_bdim numpy_take x ind ind_inv dim + numpy_take register_vmap numpy_take_vmap torch library custom_op _torch_testing numpy_nonzero mutates_args= numpy_nonzero x Tensor - Tensor x_np = to_numpy x res = np stack np nonzero x_np axis= res shape = raise RuntimeError supported torch tensor res device=x device numpy_nonzero register_fake _ x ctx = torch _custom_op impl get_ctx i = ctx create_unbacked_symint shape = i x dim result = x new_empty shape dtype=torch long result sample_inputs_numpy_nonzero opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=requires_grad shape = result = make_arg shape low= high= mask = make_tensor shape low= high= device=device dtype=torch long torch no_grad result = mask yield SampleInput result args= numpy_nonzero_vmap info in_dims x raise NotImplementedError Operator data-dependent cannot vmapped numpy_nonzero register_vmap numpy_nonzero_vmap torch library custom_op _torch_testing numpy_view_copy mutates_args= numpy_view_copy x Tensor shape Sequence int - Tensor torch tensor np copy to_numpy x reshape shape device=x device numpy_view_copy register_fake _ x shape - Tensor x clone view shape clone numpy_view_copy_setup_context ctx inputs output - None ctx x_shape = inputs shape numpy_view_copy_backward ctx grad_out torch ops _torch_testing numpy_view_copy grad_out ctx x_shape None numpy_view_copy register_autograd numpy_view_copy_backward setup_context=numpy_view_copy_setup_context numpy_view_copy_vmap info in_dims x shape x_bdim _ = in_dims x = x movedim x_bdim x_shape = x shape batch_shape = x_shape shape result = numpy_view_copy x batch_shape result numpy_view_copy register_vmap numpy_view_copy_vmap sample_inputs_numpy_view_copy opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=requires_grad result = make_arg low= high= yield SampleInput result args= torch library custom_op _torch_testing numpy_cat mutates_args= numpy_cat xs Sequence Tensor dim int - Tensor assert len xs assert all x device == xs device x xs assert all x dtype == xs dtype x xs np_xs = to_numpy x x xs np_out = np concatenate np_xs axis=dim torch tensor np_out device=xs device numpy_cat register_fake _ xs dim assert len xs assert all x device == xs device x xs assert all x dtype == xs dtype x xs torch cat xs dim=dim numpy_cat_setup_context ctx inputs output xs dim = inputs ctx dim_sizes = x shape dim x xs ctx dim = dim numpy_cat_backward ctx grad_out dim_sizes = ctx dim_sizes dim = ctx dim splits = list np cumsum dim_sizes - grad_xs = torch ops _torch_testing numpy_split_copy grad_out splits dim grad_xs None numpy_cat register_autograd numpy_cat_backward setup_context=numpy_cat_setup_context numpy_cat_vmap info in_dims x dim x_bdim = in_dims result = numpy_cat x dim result x_bdim numpy_cat register_vmap numpy_cat_vmap sample_inputs_numpy_cat opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=requires_grad r = make_arg low= high= r = make_arg low= high= r = make_arg low= high= yield SampleInput r r r args= torch library custom_op _torch_testing numpy_split_copy mutates_args= numpy_split_copy x Tensor splits Sequence int dim int - List Tensor x_np = to_numpy x arrs = np split x_np splits axis=dim torch tensor arr device=x device dtype=x dtype arr arrs numpy_split_copy register_fake _ x splits dim xi clone xi torch tensor_split x splits dim numpy_split_copy_setup_context ctx inputs output _ _ dim = inputs ctx dim = dim numpy_split_copy_backward ctx grad_out result = torch ops _torch_testing numpy_cat grad_out dim=ctx dim result None None numpy_split_copy register_autograd numpy_split_copy_backward setup_context=numpy_split_copy_setup_context numpy_split_copy_vmap info in_dims x splits dim x_bdim _ _ = in_dims x = x movedim x_bdim result = numpy_split_copy x splits dim + result numpy_split_copy register_vmap numpy_split_copy_vmap sample_inputs_numpy_split_copy opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=requires_grad x = make_arg low= high= yield SampleInput x args= torch library custom_op _torch_testing numpy_split_copy_with_int mutates_args= numpy_split_copy_with_int x Tensor splits Sequence int dim int - tuple List Tensor int x_np = to_numpy x arrs = np split x_np splits axis=dim torch tensor arr device=x device dtype=x dtype arr arrs len splits numpy_split_copy_with_int register_fake _ x splits dim xi clone xi torch tensor_split x splits dim len splits numpy_split_copy_with_int_setup_context ctx inputs output _ _ dim = inputs ctx dim = dim numpy_split_copy_with_int_backward ctx grad_out _ torch ops _torch_testing numpy_cat grad_out dim=ctx dim None None numpy_split_copy_with_int register_autograd numpy_split_copy_with_int_backward setup_context=numpy_split_copy_with_int_setup_context numpy_split_copy_with_int_vmap info in_dims x splits dim x_bdim _ _ = in_dims x = x movedim x_bdim result len_split = numpy_split_copy_with_int x splits dim + result len_split _ range len result None numpy_split_copy_with_int register_vmap numpy_split_copy_with_int_vmap torch library custom_op _torch_testing numpy_nms mutates_args= numpy_nms boxes Tensor scores Tensor iou_threshold Number - Tensor Adapted Ross Girshick s fast-rcnn implementation https github com rbgirshick fast-rcnn blob master lib utils nms py assert boxes device == scores device device = boxes device boxes = to_numpy boxes scores = to_numpy scores N = boxes shape assert boxes shape == N assert scores shape == N x = boxes y = boxes x = boxes y = boxes areas = x - x + y - y + order = scores argsort - keep = while order size i = order keep append i xx = np maximum x i x order yy = np maximum y i y order xx = np minimum x i x order yy = np minimum y i y order w = np maximum xx - xx + h = np maximum yy - yy + inter = w h ovr = inter areas i + areas order - inter inds = np where ovr = iou_threshold order = order inds + result = torch tensor np stack keep device=device Needed data-dependent condition assert result size = result numpy_nms register_fake _ boxes scores iou_threshold assert boxes device == scores device N = boxes shape assert boxes shape == N assert scores shape == N ctx = torch _custom_op impl get_ctx i = ctx create_unbacked_symint result = boxes new_empty i dtype=torch int result numpy_nms_vmap info in_dims boxes scores iou_threshold raise NotImplementedError Operator data-dependent cannot vmapped numpy_nms register_vmap numpy_nms_vmap sample_inputs_numpy_nms opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype N = xs = make_arg N low= high= dx = make_arg N low= high= ys = make_arg N low= high= dy = make_arg N low= high= boxes = torch stack xs ys xs + dx ys + dy dim= requires_grad_ requires_grad scores = make_arg N low= high= requires_grad=requires_grad iou_threshold = make_arg low= high= item yield SampleInput boxes args= scores iou_threshold custom_op_db = OpInfo NumpyCubeCustomOp op=numpy_cube _opoverload sample_inputs_func=sample_inputs_numpy_cube dtypes=all_types_and torch bool torch half supports_out=False OpInfo NumpyMulCustomOp op=numpy_mul _opoverload sample_inputs_func=sample_inputs_numpy_mul dtypes=all_types_and torch bool torch half supports_out=False OpInfo NumpyMulScalarCustomOp op=numpy_mul_scalar _opoverload sample_inputs_func=sample_inputs_numpy_mul_scalar dtypes=all_types_and torch bool torch half supports_out=False OpInfo NumpySortCustomOp op=numpy_sort _opoverload sample_inputs_func=sample_inputs_numpy_sort dtypes=all_types_and torch bool torch half supports_out=False OpInfo NumpyTakeCustomOp op=numpy_take _opoverload sample_inputs_func=sample_inputs_numpy_take dtypes=all_types_and torch bool torch half supports_out=False OpInfo NumpyNonzeroCustomOp op=numpy_nonzero _opoverload sample_inputs_func=sample_inputs_numpy_nonzero dtypes=all_types_and torch bool torch half supports_autograd=False supports_out=False OpInfo NumpyNMSCustomOp op=torch ops _torch_testing numpy_nms sample_inputs_func=sample_inputs_numpy_nms dtypes=all_types_and torch bool torch half supports_autograd=False supports_out=False OpInfo NumpyViewCopyCustomOp op=torch ops _torch_testing numpy_view_copy sample_inputs_func=sample_inputs_numpy_view_copy dtypes=all_types_and torch bool torch half supports_autograd=True supports_out=False OpInfo NumpyCatCustomOp op=torch ops _torch_testing numpy_cat sample_inputs_func=sample_inputs_numpy_cat dtypes=all_types_and torch bool torch half supports_autograd=True check_batched_grad=False check_batched_gradgrad=False supports_out=False OpInfo NumpySplitCopyCustomOp op=torch ops _torch_testing numpy_split_copy sample_inputs_func=sample_inputs_numpy_split_copy dtypes=all_types_and torch bool torch half supports_autograd=True check_batched_grad=False check_batched_gradgrad=False supports_out=False OpInfo NumpySplitCopyWithIntCustomOp op=torch ops _torch_testing numpy_split_copy_with_int sample_inputs_func=sample_inputs_numpy_split_copy dtypes=all_types_and torch bool torch half gradcheck_wrapper=lambda op args kwargs op args kwargs supports_autograd=True check_batched_grad=False check_batched_gradgrad=False supports_out=False ============================================================== some mechanical test cases ============================================================== lib = torch library Library _torch_testing FRAGMENT noqa TOR lib define source Tensor x - Tensor torch library register_fake _torch_testing source lib=lib _ x x clone lib define source Tensor x - Tensor source _fake x x clone torch library register_fake _torch_testing source source _fake lib=lib lib define source Tensor x - Tensor torch library register_fake _torch_testing source lib=lib _ x x clone lib define source Tensor x - Tensor source _fake x x clone torch library register_fake _torch_testing source source _fake lib=lib torch library custom_op _torch_testing source mutates_args= source x Tensor - Tensor x clone source register_fake _ x x clone torch library custom_op _torch_testing source mutates_args= source x Tensor - Tensor x clone source _fake x x clone source register_fake source _fake