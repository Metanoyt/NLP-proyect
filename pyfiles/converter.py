mypy allow-untyped-defs builtins logging operator typing warnings collections abc Callable Sequence contextlib contextmanager typing Any Optional Union torch torch export _trace torch _C torch _export passes replace_quantized_ops_with_standard_ops_pass replace_quantized_ops_with_standard_ops torch export dynamic_shapes _tree_map_with_path Dim torch export exported_program ExportedProgram torch export graph_signature ConstantArgument CustomObjArgument InputKind InputSpec OutputKind OutputSpec TensorArgument torch fx subgraph_rewriter log = logging getLogger __name__ _get_param_count_list method_graph args_params param_count_list = input_ arg_params_ zip method_graph inputs args_params PackedParams str input_ type in_vars _ = torch jit _flatten arg_params_ param_count_list append len in_vars param_count_list append arg_params_ None param_count_list _trace_and_get_graph_from_model model args A basic sanity check make sure state_dict keys same before after running model Fail fast orig_state_dict_keys = torch jit _unique_state_dict model keys Disable Autocast cache because replaces kernel s weight bias undesired constants No perf impact when there reused weights since https github com pytorch pytorch pull prev_autocast_cache_enabled = torch is_autocast_cache_enabled torch set_autocast_cache_enabled False trace_graph torch_out _inputs_states = torch jit _get_trace_graph model args strict=False _force_outplace=False _return_inputs_states=True torch set_autocast_cache_enabled prev_autocast_cache_enabled orig_state_dict_keys = torch jit _unique_state_dict model keys raise RuntimeError state_dict changed after running tracer something weird happening your model trace_graph torch_out _create_jit_graph model Union torch nn Module torch jit ScriptFunction args Sequence Any - tuple torch Graph list _C IValue Any Optional torch ScriptModule isinstance model torch jit ScriptFunction torch jit ScriptModule flattened_args = tuple torch jit _flatten tuple args torch_out = None isinstance model torch jit ScriptModule try graph = model forward graph type ignore attr-defined except AttributeError e raise RuntimeError forward method must script method e _C _jit_pass_onnx_function_substitution graph freezed_module = _C _freeze_module typing cast _C ScriptModule model _c preserveParameters=True module params = _C _jit_onnx_list_model_parameters freezed_module method_graph = module _get_method forward graph args_params = tuple args + tuple params param_count_list = _get_param_count_list method_graph args_params in_vars _ = torch jit _flatten args_params graph = _C _propagate_and_assign_input_shapes method_graph tuple in_vars param_count_list False False graph params torch_out module torch jit ScriptFunction params = graph = model graph _C _jit_pass_onnx_function_substitution graph param_count_list = _get_param_count_list graph args graph = _C _propagate_and_assign_input_shapes graph flattened_args param_count_list False False graph params torch_out None graph torch_out = _trace_and_get_graph_from_model model args _C _jit_pass_onnx_lint graph state_dict = torch jit _unique_state_dict model params = list state_dict values graph_inputs = list graph inputs user_input_num = len graph_inputs - len state_dict param_names = list state_dict keys i inp enumerate graph_inputs i = user_input_num inp setDebugName param_names i - user_input_num _C _jit_pass_onnx_function_substitution graph graph params torch_out None list_add b + b list_append container element container + element execute_subgraph_from_prim_loop subgraph iter_idx len_loop_local_arguments args kwargs subgraph GraphModule sub-block iter_idx The index interaction len_loop_local_arguments The number loop local arguments args Loop local variables TS graph create those inputs because their values updated inside loop loop_local_args = args len_loop_local_arguments Global variables passed inputs loop sub-blocks directly used Most time their values updated only exception when there some operations perform inplace updates global_args = args len_loop_local_arguments subgraph global_args iter_idx loop_local_args kwargs inplace_optimize_sym_size_div gm torch fx GraphModule pattern im dim scale sym_size_int = torch ops aten sym_size int im dim scalar_tensor = torch ops aten scalar_tensor sym_size_int div_scalar_mode = torch ops aten div Scalar_mode scalar_tensor scale rounding_mode= trunc int_tensor = torch ops aten Int Tensor div_scalar_mode int_tensor replacement im dim scale sym_size_int = torch ops aten sym_size int im dim sym_size_int scale subgraph_rewriter replace_pattern gm pattern replacement is_valid_for_codegen name len name == raise RuntimeError Empty argument name codegen name isdigit False True normalize_name name str prefix str = rename - str name = name replace _ is_valid_for_codegen name name f prefix _ name ir_name_to_func_name name str - str prim If - convert_prim_If name_list = name split convert_ + _ join name_list get_node_as_placeholder_or_get_attr fx_graph name is_top_level_graph is_top_level_graph fx_graph get_attr name fx_graph placeholder name _TORCH_DTYPE_TO_ENUM = torch uint torch int torch int torch int torch int torch float torch float torch float torch complex torch complex torch complex torch bool torch qint torch quint torch bfloat _TORCH_ENUM_TO_DTYPE = value key key value _TORCH_DTYPE_TO_ENUM items get_dtype_as_int tensor prim dtype has signature Tensor - int where gets dtype tensor returns integer corresponding dtype based enum ScalarType h dtype = tensor dtype dtype _TORCH_DTYPE_TO_ENUM raise RuntimeError f Unsupported dtype dtype _TORCH_DTYPE_TO_ENUM dtype Those operators will automatically populated instance method TS FXGraphConverter name convert_ namespace _ opname Please check __init__ method population implementations kind_to_standard_operators dict str Callable Any = prim max builtins max prim min builtins min prim TupleIndex operator getitem aten __is__ operator is_ aten __isnot__ operator is_not aten __not__ operator not_ aten __contains__ operator contains prim dtype get_dtype_as_int aten len len Mapping specialized op its symbolic counterpart They currently do have any other overrides aten numel torch ops aten sym_numel aten size torch ops aten sym_size aten storage_offset torch ops aten sym_storage_offset aten stride torch ops aten sym_stride get_ir_value_parent_name_and_attr_name node irv_parent_name irv_name = node input debugName node output debugName attr_name = node s name irv_name irv_parent_name attr_name construct_fqn ir ref_map name_map name_list = while ir ref_map name_list append name_map ir ir = ref_map ir join reversed name_list get_block_to_lifted_attrs graph torch _C Graph - tuple dict torch _C Block set str dict str str Perform two passes get mapping blocks set FQNs its lifted attributes When graph has control flow graph will divided into multiple blocks We want convert each block graph which will passed into torch cond A restriction torch cond model parameters buffers expected lifted inputs subgraphs Before converting model we will run pass which will Figure out which params buffers used within blocks through tracing GetAttr calls Process graph bottom up find lifted attributes each block taking union attributes used current block lifted attributes all its child blocks Returns A mapping blocks set FQNs its lifted attributes mapping node names FQNs its lifted attributes A map block its expected lifted arguments blocks_to_lifted_attrs dict torch _C Block set str = Reference map stores input i e src output i e dest IR GetAttr node By traversing reference map we can figure out full IR aliasing pass figure out FQN attribute E g = GetAttr linear -- node_to_parent_map = node_to_parent_map dict str str = Used reconstructing FQN attribute based reference map In nutshell each GetAttr call GetAttr input IR attribute name - output IR This name map stores which attribute name called src IR -- dest IR action E g = GetAttr linear -- node_to_attr_name = linear node_to_attr_name dict str str = _dfs_get_attr_dependency entry First DFS path construct reference map name map node entry nodes node kind == prim GetAttr irv_name irv_parent_name attr_name = get_ir_value_parent_name_and_attr_name node node_to_parent_map irv_name = irv_parent_name node_to_attr_name irv_name = attr_name block node blocks _dfs_get_attr_dependency block _map_blocks_to_lifted_attrs entry Walk graph bottom-up fashion build expected lifted arguments each block arguments set str = set node entry nodes block node blocks Recursively build arguments = arguments union _map_blocks_to_lifted_attrs block node kind == prim GetAttr irv_name = node output debugName Skip intermediate GetAttr which will anyway result FQN E g node_to_parent_name node_to_attr_name weight linear There only one FQN -- -- linear weight -- FQN linear irv_name set node_to_parent_map values arguments add construct_fqn irv_name node_to_parent_map node_to_attr_name isinstance entry torch _C Graph Skip top level blocks_to_lifted_attrs entry = arguments arguments _dfs_get_attr_dependency graph _map_blocks_to_lifted_attrs graph blocks_to_lifted_attrs node_to_attr_name get_attribute_fqn_from_ts_node name_to_attribute_fqn dict str str node torch _C Node - str get_attr name str name name_to_attribute_fqn name_to_attribute_fqn name raise ValueError f Attribute name found node kind == prim SetAttr input_name = next node inputs debugName node kind == prim GetAttr input_name = node input debugName raise RuntimeError f Unexpected node kind when getting attribute fqn node node attr_name = node s name root_attr_name = get_attr input_name attr_fqn = f root_attr_name attr_name root_attr_name attr_name attr_fqn get_op_overload node torch _C Node schema_str = node schema assert schema_str = no schema f got empty schema node schema torch _C FunctionSchema = torch _C parse_schema schema_str ns op_name = str schema name split override = schema overload_name try op_overload_mod = getattr torch ops ns op_overload_packet = getattr op_overload_mod op_name override op_overload = getattr op_overload_packet override op_overload = op_overload_packet default except Exception e raise RuntimeError f Unable find operator node kind schema node schema e op_overload TS FXGraphConverter __init__ ts_graph Union torch _C Graph torch _C Block name_to_param dict str torch Tensor name_to_buffer dict str torch Tensor blocks_to_lifted_attrs dict torch _C Block set str name_to_non_tensor_attribute dict str Any name_to_constant dict str Any name_to_attribute_fqn dict str str ts_graph = ts_graph Mapping parameter FQN actual parameter value name_to_param = name_to_param Mapping buffer FQN actual buffer value name_to_buffer = name_to_buffer fx_graph torch fx Graph = torch fx Graph input_specs list InputSpec = output_specs list OutputSpec = Mapping TS node name converted FX node name_to_node dict str Union torch fx Node list torch fx Node dict Any torch fx Node = Mapping TS node name constant value int str TorchBind obj tensor constants name_to_constant dict str Any = name_to_constant Mapping torchscript node output name attribute fully qualified name name_to_attribute_fqn dict str str = name_to_attribute_fqn Mapping fully qualified name real values fx graph node During convert represents current value non-tensor attribute One use case forward x c = count count += c = count x + c + c name_to_non_tensor_attribute_node dict str Any = Mapping fully qualified name initial real values inputs We separate name_to_non_tensor_attribute_node since we need initial real value input when we construct fx GraphModule name_to_non_tensor_attribute dict str Any = name_to_non_tensor_attribute subgraphs dict str torch fx GraphModule = Mapping block list attributes need lifted each block blocks_to_lifted_attrs = blocks_to_lifted_attrs Populate methods standard operators k kind_to_standard_operators keys handler_func_name = ir_name_to_func_name k Create indirect function call convert_ namespace _ opname -- lambda node _convert_standard_operator node setattr handler_func_name lambda node _convert_standard_operators node This stores list results do appear original TS graph s outputs The reason we maintain because some operations sub-block might have inplace updates variable defined parent fx graph After execution sub-block variable defined parent fx graph also needs updated name_update_from_subblock_to_parent set str = set _is_get_attr_node fqn fqn name_to_buffer fqn name_to_param fqn name_to_constant isinstance name_to_constant fqn torch ScriptObject _convert_block_to_subgraph node torch _C Node arguments list str subgraph_nodes subgraph_converters = block node blocks subgraph_converter = TS FXGraphConverter block name_to_param name_to_buffer blocks_to_lifted_attrs name_to_constant name_to_attribute_fqn block_arg arguments normalized_block_arg_name = normalize_name block_arg placeholder_node = subgraph_converter fx_graph placeholder normalized_block_arg_name subgraph_converter name_to_node block_arg = placeholder_node subgraph = subgraph_converter convert subgraph_name = add_subgraph subgraph subgraph_nodes append fx_graph get_attr subgraph_name subgraph_converters append subgraph_converter subgraph_nodes subgraph_converters _identify_inputs_as_arguments entry Identify inputs innermost sub-block This needed nested sub-blocks when input hidden nested sub-block E g example IR input hidden nested sub-block Graph x = Block Block x = x arguments set str = set block entry blocks block_node block nodes block_node_in block_node inputs block_node_in debugName name_to_node block_node_in debugName name_to_attribute_fqn arguments add block_node_in debugName arguments = arguments union _identify_inputs_as_arguments block_node arguments is_top_level_graph isinstance ts_graph torch _C Graph add_subgraph subgraph - str name = f subgraph_ len subgraphs subgraphs name = subgraph name get_args_kwargs node torch _C Node schema args = kwargs = input schema_arg zip node inputs schema arguments schema_arg kwarg_only kwargs schema_arg name = get_fx_value_by_ir_value input args append get_fx_value_by_ir_value input tuple args kwargs get_fx_value_by_ir_value value torch _C Value value_name = value debugName value_name name_to_node input_node = name_to_node value_name input_node value_name name_to_constant isinstance name_to_constant value_name torch ScriptObject fx_graph get_attr value_name name_to_constant value_name value_name name_to_attribute_fqn get_fx_value_by_fqn name_to_attribute_fqn value_name raise ValueError f Input value_name found get_fx_value_by_fqn name name name_to_node fx_node = name_to_node name name name_to_constant fx_node = name_to_constant name name name_to_non_tensor_attribute_node fx_node = name_to_non_tensor_attribute_node name name name_to_non_tensor_attribute fx_node = name_to_non_tensor_attribute name raise ValueError f Attribute name found fx_node convert - torch fx GraphModule convert_graph_inputs node ts_graph nodes convert_node node convert_graph_outputs Pass parameter buffer root lookup gm = torch fx GraphModule subgraphs name_to_param name_to_buffer name_to_non_tensor_attribute name_to_constant fx_graph inplace_optimize_sym_size_div gm gm graph lint gm convert_graph_inputs graph_input ts_graph inputs name = graph_input debugName name name_to_param normalized_name = normalize_name name input_specs append InputSpec InputKind PARAMETER arg=TensorArgument name=normalized_name target=name fx_node = get_node_as_placeholder_or_get_attr fx_graph name is_top_level_graph name name_to_buffer normalized_name = normalize_name name input_specs append InputSpec InputKind BUFFER arg=TensorArgument name=normalized_name target=name persistent=True fx_node = get_node_as_placeholder_or_get_attr fx_graph name is_top_level_graph name name_to_constant assert isinstance name_to_constant name torch ScriptObject Input conversion only handles ScriptObject normalized_name = normalize_name name input_specs append InputSpec InputKind CUSTOM_OBJ arg=CustomObjArgument name=normalized_name class_fqn=normalized_name target=name persistent=False fx_node = get_node_as_placeholder_or_get_attr fx_graph name is_top_level_graph isinstance graph_input type torch ClassType Directly skip inputs ScriptObject used graph continue normalized_name = normalize_name name prefix= input input_specs append InputSpec InputKind USER_INPUT arg=TensorArgument name=normalized_name target=name fx_node = fx_graph placeholder normalized_name name_to_node name = fx_node convert_aten_Float node torch _C Node to_float_tensor t t dtype=torch float item inp_list = get_fx_value_by_ir_value inp inp node inputs noqa C fx_node = fx_graph call_function to_float_tensor tuple inp_list name_to_node node output debugName = fx_node convert_aten_tensor node torch _C Node aten tensor creates constant tensor ad-hoc -- GetAttr args kwargs = get_args_kwargs node torch ops aten tensor default _schema k kwargs k == requires_grad kwargs k = bool kwargs k - False - True to_tensor = torch tensor all isinstance int args torch _refs tensor target args kwargs dtype kwargs kwargs dtype None kwargs dtype = _TORCH_ENUM_TO_DTYPE kwargs dtype to_tensor args kwargs to_dynamic_tensor args kwargs dtype kwargs kwargs dtype None kwargs dtype = _TORCH_ENUM_TO_DTYPE kwargs dtype torch _refs tensor args kwargs output_name = node output debugName fx_node = fx_graph call_function target args kwargs name_to_node output_name = fx_node convert_aten_append node torch _C Node special handle python list append aten append t t t c - el - t inplace append list This kinda crazy we inplace mutating list This makes converter non-functional result depends order nodes being converter In sense converter now becomes stateful interpreter warnings warn Converting aten append t which inplace mutation list This makes converter non-functional result depends order append nodes being converter stacklevel= args = tuple get_fx_value_by_ir_value inp inp node inputs fx_node = fx_graph call_function list_append args name_to_node node output debugName = fx_node inplace mutate arg which python list name_to_node node inputsAt debugName = fx_node Variables need updated parent module is_top_level_graph args op == placeholder name_update_from_subblock_to_parent add node inputsAt debugName convert_prim_Constant node torch _C Node name = node output debugName value Any = None node hasAttribute value constant_kind = node kindOf value constant_kind == i value = node i value constant_kind == f value = node f value constant_kind == s value = node s value constant_kind == t alias_name = f lifted_tensor_ name Follow naming convention EP tracing fx_node = fx_graph get_attr alias_name name_to_node name = fx_node name value = alias_name node t value constant_kind == ival value = node ival value raise ValueError f Unsupported constant type node kindOf value value = None name_to_constant name = value convert_prim_CallMethod node torch _C Node inp_list = get_fx_value_by_ir_value inp inp node inputs noqa C fx_node = fx_graph call_method node s name tuple inp_list name_to_node node output debugName = fx_node convert_prim_device node torch _C Node input_type = node input type input_type isSubtypeOf torch _C TensorType get device = input_type device type ignore attr-defined output_name = node output debugName name_to_constant output_name = device raise ValueError f Unsupported JitType input_type when get device convert_prim_GetAttr node torch _C Node Build fully qualified name attr_fqn = get_attribute_fqn_from_ts_node name_to_attribute_fqn node output_name = node output debugName name_to_attribute_fqn output_name = attr_fqn is_top_level_graph _is_get_attr_node attr_fqn We insert get_attr node due two reasons First ts graph does lift tensor constants input nodes So tensor constants may ignored convert_graph_inputs Second attr_fqn may have been written via SetAttr Two GetAttr may give different values name_to_node output_name = fx_graph get_attr attr_fqn attr_fqn name_to_non_tensor_attribute_node name_to_non_tensor_attribute_node attr_fqn = name_to_non_tensor_attribute attr_fqn name_to_node output_name = name_to_non_tensor_attribute_node attr_fqn Special support blocks which do allow SetAttr TorchScript node get_attr FX Graph Node _is_get_attr_node attr_fqn name_to_node output_name = name_to_node attr_fqn convert_prim_SetAttr node torch _C Node attr_fqn = get_attribute_fqn_from_ts_node name_to_attribute_fqn node attr_value = tuple node inputs ts_graph_tensor_input = get_fx_value_by_ir_value attr_value _is_get_attr_node attr_fqn fx_attr_node = fx_graph get_attr attr_fqn fx_graph call_function torch Tensor copy_ fx_attr_node ts_graph_tensor_input name_to_non_tensor_attribute_node attr_fqn = ts_graph_tensor_input convert_call_function_op node torch _C Node target = get_op_overload node args kwargs = get_args_kwargs node target _schema fx_node = fx_graph call_function target args kwargs TODO convert sourceRange into stack_trace fx_node meta stack_trace = node sourceRange node outputsSize == output_name = node output debugName name_to_node output_name = fx_node i outp enumerate node outputs output_name = outp debugName next_fx_node = fx_graph call_function operator getitem fx_node i name_to_node output_name = next_fx_node convert_prim_TupleConstruct node torch _C Node _convert_prim_iterator node convert_prim_ListConstruct node torch _C Node _convert_prim_iterator node _convert_prim_iterator node torch _C Node output_list = get_fx_value_by_ir_value inp inp node inputs output_name = node output debugName name_to_node output_name = output_list convert_prim_DictConstruct node torch _C Node output_dict = k v = None None i inp enumerate node inputs We assume key value stored pair DictConstruct The first element key following value i == k = get_fx_value_by_ir_value inp v = get_fx_value_by_ir_value inp assert k None v None DictConstruct has empty key value pair output_dict k = v k v = None None assert k None v None DictConstruct has odd number elements violating our assumption output_name = node output debugName name_to_node output_name = output_dict convert_prim_ListUnpack node torch _C Node _convert_prim_unpack_iterator node convert_prim_TupleUnpack node torch _C Node _convert_prim_unpack_iterator node _convert_prim_unpack_iterator node torch _C Node Single input multiple outputs unpacking i outp enumerate node outputs outp_name = outp debugName inp = get_fx_value_by_ir_value node input fx_node = fx_graph call_function operator getitem inp i name_to_node outp_name = fx_node convert_aten_Int node torch _C Node converts aten Int aten _to_copy + aten _local_scalar_dense target = torch ops aten _to_copy default args = tuple get_fx_value_by_ir_value input input node inputs to_copy_node = fx_graph call_function target args dtype torch int fx_node = fx_graph call_function torch ops aten _local_scalar_dense default to_copy_node TODO convert sourceRange into stack_trace fx_node meta stack_trace = node sourceRange output_name = node output debugName name_to_node output_name = fx_node convert_prim_NumToTensor node torch _C Node Converts prim NumToTensor aten scalar_tensor prim NumToTensor IRs currently triggered size https github com pytorch pytorch blob main torch csrc jit frontend tracer cpp#L numel https github com pytorch pytorch blob main torch csrc jit frontend tracer cpp#L For both those APIs torch jit trace implicitly sets output tensor type LongTensor target = torch ops aten scalar_tensor args = tuple get_fx_value_by_ir_value input input node inputs fx_node = fx_graph call_function target args dtype torch long output_name = node output debugName name_to_node output_name = fx_node convert_prim_CreateObject node torch _C Node output_name = node output debugName name_to_attribute_fqn output_name = convert_aten__convolution node torch _C Node converts aten _convolution aten convolution since aten _convolution doesn t have meta function target = torch ops aten convolution default args kwargs = get_args_kwargs node target _schema fx_node = fx_graph call_function target args kwargs output_name = node output debugName name_to_node output_name = fx_node convert_aten_div node torch _C Node target = get_op_overload node schema = target _schema args kwargs = get_args_kwargs node schema converts aten div Tensor_mode x tensor_constant aten div Scalar_mode x tensor_constant item schema overload_name == Tensor_mode arg _name = args name arg _name name_to_constant isinstance name_to_constant arg _name torch Tensor tensor_constant = name_to_constant arg _name tensor_constant numel == updated_args = list args updated_args = name_to_constant arg _name item fx_node = fx_graph call_function torch ops aten div Scalar_mode tuple updated_args kwargs TODO convert sourceRange into stack_trace fx_node meta stack_trace = node sourceRange output_name = node output debugName name_to_node output_name = fx_node convert_call_function_op node convert_aten___getitem__ node torch _C Node input_container index = tuple get_fx_value_by_ir_value input input node inputs fx_node = fx_graph call_function operator getitem input_container index output_name = node output debugName name_to_node output_name = fx_node convert_aten_to node torch _C Node target = get_op_overload node args _kwargs = get_args_kwargs node target _schema special handle aten dtype aten prim_dtype followed inplace_mutation_op coz aten + inplace_mutation_op pattern would trigger cannot mutate tensors frozen storage functionalization error To work around issue we override copy True so output sure alias input target torch ops aten dtype target torch ops aten prim_dtype user_nodes = use user use node output uses user_targets = get_op_overload user_node user_node user_nodes user_node schema = no schema has_mutable_target = any target _schema is_mutable target user_targets has_mutable_target assert len args = new_args = list args new_args = True copy override True fx_node = fx_graph call_function torch ops aten dtype tuple new_args temp hack work around issue https github com pytorch pytorch issues When issue fixed clone node would no longer needed clone_node = fx_graph call_function torch ops aten clone default fx_node output_name = node output debugName name_to_node output_name = clone_node convert_call_function_op node convert_aten_add node torch _C Node node schema == no schema isinstance node inputsAt type torch ListType isinstance node inputsAt type torch ListType target = torch ops aten add t raise RuntimeError f unable determined target node target = get_op_overload node target torch ops aten add t special handle python list tuple add aten add t t t b - t RuntimeError aten add Expected value type List t argument instead found type immutable_list args _kwargs = get_args_kwargs node target _schema output_name = node output debugName name_to_node output_name = fx_graph call_function list_add args convert_call_function_op node _check_prim_loop_support node inputs = list node inputs TODO N stage inputs debugName name_to_constant raise RuntimeError prim Loop currently cannot run dynamic value number iterations Make sure condition updated subblock subblock = next node blocks condition_output_name = next subblock outputs debugName node subblock nodes node outputsSize == node output debugName == condition_output_name raise RuntimeError prim Loop currently cannot run dynamic value condition node outputsSize = outp node outputs outp debugName == condition_output_name raise RuntimeError prim Loop currently cannot run dynamic value condition convert_prim_Loop node torch _C Node inputs = list node inputs _check_prim_loop_support node num_iterations = get_fx_value_by_ir_value inputs Find inputs loop_local_arguments = inp debugName inp inputs global_arguments = _identify_inputs_as_arguments node Lift parameters inputs block node blocks global_arguments = global_arguments union blocks_to_lifted_attrs block global_arguments = list global_arguments subgraph_nodes subgraph_converters = _convert_block_to_subgraph node global_arguments assert len subgraph_nodes == subgraph_converter = subgraph_converters is_top_level_graph name_update_from_subblock_to_parent = name_update_from_subblock_to_parent union subgraph_converter name_update_from_subblock_to_parent fx_block_args = get_fx_value_by_fqn name name loop_local_arguments + global_arguments iter_idx range num_iterations loop_node = fx_graph call_function execute_subgraph_from_prim_loop Check execute_node function expected arguments order subgraph_nodes iter_idx len loop_local_arguments fx_block_args Update value loop local variables node outputsSize = i outp enumerate node outputs output_name = outp debugName name_to_node output_name = fx_graph call_function operator getitem loop_node i + + because th element condition fx_block_args i = name_to_node output_name Update value global variables whose values modified inplace i name enumerate subgraph_converter name_update_from_subblock_to_parent name_to_node name = fx_graph call_function operator getitem loop_node i + node outputsSize + + because th element condition global_argument_index = global_arguments index name fx_block_args i + node outputsSize + global_argument_index = name_to_node name _check_set_attr_in_if_block if_node torch _C Node block if_node blocks node block nodes node kind == prim SetAttr raise RuntimeError During converting prim If torch cond found prim SetAttr op which supported yet Please file issue you come across error convert_prim_If node torch _C Node _check_set_attr_in_if_block node inputs = list node inputs assert len inputs == predicate = get_fx_value_by_ir_value inputs Find inputs arguments = _identify_inputs_as_arguments node Lift parameters inputs block node blocks arguments = arguments union blocks_to_lifted_attrs block arguments = list arguments subgraph_nodes _ = _convert_block_to_subgraph node arguments assert len subgraph_nodes == fx_block_args = get_fx_value_by_fqn name name arguments args = predicate subgraph_nodes subgraph_nodes tuple fx_block_args cond_node = fx_graph call_function torch cond args prim If can also have zero output node outputsSize == output_name = node output debugName name_to_node output_name = cond_node node outputsSize i output enumerate node outputs output_name = output debugName getitem = fx_graph call_function operator getitem cond_node i name_to_node output_name = getitem convert_aten_Bool node torch _C Node _convert_as_noop node convert_prim_Enter node torch _C Node export generally treats prim Enter noop The only context manager export supports aten enable_grad Unfortunately TorchScript does support aten enable_grad yet TODO support aten enable_grad both TorchScript Converter convert_prim_Exit node torch _C Node export treats prim Exit noop _convert_as_noop node torch _C Node Converts node no-op mapping its output node arg target = get_op_overload node schema = target _schema args _kwargs = get_args_kwargs node schema output_name = node output debugName name_to_node output_name = args convert_profiler__record_function_exit node torch _C Node _record_function_exit has side effect so we keep fx graph currently _record_function_enter_new _record_function_exit discarded during ` retrace_as_exported_program ` target = torch ops profiler _record_function_exit args = tuple get_fx_value_by_ir_value input input node inputs fx_graph call_function target args convert_prim_tolist node torch _C Node prim tolist cannot supported ` _convert_standard_operators ` since requires call_method instead call_function target = tolist args = get_fx_value_by_ir_value next node inputs fx_node = fx_graph call_method target args output_name = node output debugName name_to_node output_name = fx_node convert_prim_Uninitialized node torch _C Node ` prim Uninitialized ` inserted compiler when can prove value will never used It can introduced exceptions breaks continues returns So we add dummy constant graph output_name = node output debugName name_to_constant output_name = torch Tensor _convert_standard_operators node torch _C Node target = kind_to_standard_operators node kind args = tuple get_fx_value_by_ir_value input input node inputs fx_node = fx_graph call_function target args output_name = node output debugName name_to_node output_name = fx_node convert_node node torch _C Node node_kind = node kind Get handler based namespace operator name Provide default node handler well case we don t find matching converter handler_func_name = ir_name_to_func_name node_kind handler_func = getattr handler_func_name convert_call_function_op str calls print function implemented CPP To avoid repeating entire logic here we simply keep first line node string getting rid sub-blocks IR prints node_str = join str node split \n log debug s converts s handler_func __name__ node_str try handler_func node except Exception e raise RuntimeError f TS EPConverter failed node node_kind e convert_graph_outputs args = outp_name_list = outp debugName outp ts_graph outputs + list name_update_from_subblock_to_parent output_name outp_name_list output_name name_to_node fx_node = name_to_node output_name TODO Revisit later after HigherOrderOp design changes Currently we cannot directly input output is_top_level_graph isinstance fx_node torch fx Node fx_node op == placeholder fx_node = fx_graph call_function torch clone fx_node args append fx_node output_specs append OutputSpec OutputKind USER_OUTPUT arg=TensorArgument name=output_name target=output_name output_name name_to_constant args append name_to_constant output_name output_specs append OutputSpec OutputKind USER_OUTPUT arg=ConstantArgument name=output_name value=self name_to_constant output_name target=output_name raise ValueError f Output output_name found len args == Sub-block prim If can have zero output fx_graph output len args == fx_graph output args Get rid extra list wrapped around final output len args fx_graph output args For prim Loop prim If multiple outputs Sub-block prim Loop can have multiple outputs fx_graph output args ExplainTS FXGraphConverter TS FXGraphConverter Run TS FXGraphConverter explain mode It collects all failed operators conversions provide information users In order collect all failed conversions also mocks some internal attributes e g name_to_node _DictMock dict __init__ dict_data mock_value super __init__ dict_data mock_value = mock_value __getitem__ key If original dictionary has key its value Otherwise mock value super __contains__ key mock_value super __getitem__ key __contains__ key True __init__ ts_graph Union torch _C Graph torch _C Block name_to_param dict str torch Tensor name_to_buffer dict str torch Tensor blocks_to_lifted_attrs dict torch _C Block set str name_to_non_tensor_attribute dict str Any name_to_constant dict str Any name_to_attribute_fqn dict str str super __init__ ts_graph name_to_param name_to_buffer blocks_to_lifted_attrs name_to_non_tensor_attribute name_to_constant name_to_attribute_fqn Data keep track unsupported nodes unsupported_node_list list torch _C Node = Add mock needed attributes name_to_node = ExplainTS FXGraphConverter _DictMock name_to_node Dummy node torch fx Node None type ignore arg-type mock call_function lambda None explain convert_graph_inputs node ts_graph nodes convert_node node convert_graph_outputs convert_node node try super convert_node node except Exception unsupported_node_list append node contextmanager disable_logging log disabled = log disabled log disabled = True try yield finally log disabled = disabled TS EPConverter TorchScript model ExportedProgram converter __init__ ts_model Union torch jit ScriptModule torch jit ScriptFunction sample_args tuple Any sample_kwargs Optional dict str Any = None ts_model = ts_model ts_graph params _ _ = _create_jit_graph ts_model sample_args sample_args = sample_args sample_kwargs = sample_kwargs name_to_param dict str torch Tensor = name_to_buffer dict str torch Tensor = param_list = list ts_model parameters isinstance ts_model torch _C ScriptFunction isinstance ts_model torch _C ScriptFunction k tensor ts_model state_dict items type ignore union-attr Check tensor belongs any parameter any tensor == param all param param_list tensor shape == param shape name_to_param k = tensor name_to_buffer k = tensor name_to_non_tensor_attributes dict str Any = name_to_constant dict str Any = lift_get_attr convert - ExportedProgram log info TS EPConverter logging starts here INFO TORCH_LOGS= export cmd Log TorchScript IR DEBUG TORCH_LOGS= +export cmd additionally Log conversion IR IR format conversion handler name converts IR log info TorchScript graph\n\n s\n ts_graph blocks_to_lifted_attrs name_to_attribute_fqn = get_block_to_lifted_attrs ts_graph graph_converter = TS FXGraphConverter ts_graph name_to_param name_to_buffer blocks_to_lifted_attrs name_to_non_tensor_attributes name_to_constant name_to_attribute_fqn gm = graph_converter convert Post-processing step deal quantized operators replace_quantized_ops_with_standard_ops gm log info GraphModule s gm print_readable print_output=False ep = retrace_as_exported_program gm graph_converter name_to_constant log info s ep Post-processing step ensure ExportedProgram has same state_dict original TorchScript model Throw warnings additionally populated state_dict entries isinstance ts_model torch _C ScriptFunction k tensor ts_model state_dict items type ignore union-attr k ep state_dict warnings warn f Manually populate k into state_dict ExportedProgram never used ExportedProgram stacklevel= ep state_dict k = tensor ep disable_logging log explain print_output=True blocks_to_lifted_attrs name_to_attribute_fqn = get_block_to_lifted_attrs ts_graph graph_converter = ExplainTS FXGraphConverter ts_graph name_to_param name_to_buffer blocks_to_lifted_attrs name_to_non_tensor_attributes name_to_constant name_to_attribute_fqn graph_converter explain len graph_converter unsupported_node_list explain_str = Unsupported nodes found following list i n enumerate graph_converter unsupported_node_list node_str = join str n split \n explain_str += f \n\n i n kind node_str explain_str = Success print_output print explain_str explain_str retrace_as_exported_program gm torch fx GraphModule name_to_constant dict str Any dynamic_shapes = _tree_map_with_path lambda path x Dim AUTO x dim isinstance x torch Tensor None sample_args TODO adjust input orders match GraphSignature convention ep = torch export _trace _export gm sample_args dynamic_shapes=dynamic_shapes strict=False pre_dispatch=True Post-processing make sure ExportedProgram states correct Because during conversion we set tensor constants GetAttr retracing cannot recognize them tensor constants instead treat them buffers We need set them again here ep _constants update k v k v name_to_constant items isinstance v torch Tensor torch ScriptObject k name_to_constant ep state_dict pop k None spec ep graph_signature input_specs Mark constant tensors erroneously traced buffers spec kind == InputKind BUFFER spec target name_to_constant assert isinstance name_to_constant spec target torch Tensor f type name_to_constant spec target has been erroneously marked buffer spec kind = InputKind CONSTANT_TENSOR spec persistent = None ep verifier check ep ep lift_get_attr This function lifts multiple data types Tensor constants attributes e g data = torch tensor buffers Currently when there tensor constants export would error ask users register tensor constants buffers Since hard manually do so TorchScript models e g source code missing function automatically lifts tensor constants buffers ScriptObbject constant It will then converted getattr fx graph This function should happen TS EPConverter instead TS FXGraphConverter since gets attributes ts_model which accessible TS FXGraphConverter It similar where we collect name_to_param name_to_buffer name_to_attribute_fqn dict str str = get_attr fqn str name = fqn split v = ts_model n name v = getattr v n v get_fqn node torch _C Node attr_name = node s name input_name = node input debugName root_attr_name = name_to_attribute_fqn input_name attr_fqn = f root_attr_name attr_name root_attr_name attr_name attr_fqn _dfs_get_attr block node block nodes node kind == prim CreateObject output_name = node output debugName name_to_attribute_fqn output_name = node kind == prim GetAttr attr_fqn = get_fqn node value = get_attr attr_fqn output_name = node output debugName name_to_attribute_fqn output_name = attr_fqn isinstance value torch Tensor attr_fqn name_to_buffer Lift tensor constants buffer name_to_buffer attr_fqn = value isinstance value torch ScriptObject attr_fqn name_to_constant name_to_constant attr_fqn = value name_to_non_tensor_attributes attr_fqn = value subblock node blocks _dfs_get_attr subblock _dfs_get_attr ts_graph