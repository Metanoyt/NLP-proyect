__future__ annotations copy typing TypeVar torch __all__ = fuse_conv_bn_eval fuse_conv_bn_weights fuse_linear_bn_eval fuse_linear_bn_weights ConvT = TypeVar ConvT bound= torch nn modules conv _ConvNd LinearT = TypeVar LinearT bound= torch nn Linear fuse_conv_bn_eval conv ConvT bn torch nn modules batchnorm _BatchNorm transpose bool = False - ConvT r Fuse convolutional module BatchNorm module into single new convolutional module Args conv torch nn modules conv _ConvNd A convolutional module bn torch nn modules batchnorm _BatchNorm A BatchNorm module transpose bool optional If True transpose convolutional weight Defaults False Returns torch nn modules conv _ConvNd The fused convolutional module note Both ` ` conv ` ` ` ` bn ` ` must eval mode ` ` bn ` ` must have its running buffers computed assert conv training bn training Fusion only eval fused_conv = copy deepcopy conv assert bn running_mean None bn running_var None fused_conv weight fused_conv bias = fuse_conv_bn_weights fused_conv weight fused_conv bias bn running_mean bn running_var bn eps bn weight bn bias transpose fused_conv fuse_conv_bn_weights conv_w torch Tensor conv_b torch Tensor &#124; None bn_rm torch Tensor bn_rv torch Tensor bn_eps float bn_w torch Tensor &#124; None bn_b torch Tensor &#124; None transpose bool = False - tuple torch nn Parameter torch nn Parameter r Fuse convolutional module parameters BatchNorm module parameters into new convolutional module parameters Args conv_w torch Tensor Convolutional weight conv_b Optional torch Tensor Convolutional bias bn_rm torch Tensor BatchNorm running mean bn_rv torch Tensor BatchNorm running variance bn_eps float BatchNorm epsilon bn_w Optional torch Tensor BatchNorm weight bn_b Optional torch Tensor BatchNorm bias transpose bool optional If True transpose conv weight Defaults False Returns Tuple torch nn Parameter torch nn Parameter Fused convolutional weight bias conv_weight_dtype = conv_w dtype conv_bias_dtype = conv_b dtype conv_b None conv_weight_dtype conv_b None conv_b = torch zeros_like bn_rm bn_w None bn_w = torch ones_like bn_rm bn_b None bn_b = torch zeros_like bn_rm bn_var_rsqrt = torch rsqrt bn_rv + bn_eps transpose shape = - + len conv_w shape - shape = - + len conv_w shape - fused_conv_w = conv_w bn_w bn_var_rsqrt reshape shape dtype=conv_weight_dtype fused_conv_b = conv_b - bn_rm bn_var_rsqrt bn_w + bn_b dtype=conv_bias_dtype torch nn Parameter fused_conv_w conv_w requires_grad torch nn Parameter fused_conv_b conv_b requires_grad fuse_linear_bn_eval linear LinearT bn torch nn modules batchnorm _BatchNorm - LinearT r Fuse linear module BatchNorm module into single new linear module Args linear torch nn Linear A Linear module bn torch nn modules batchnorm _BatchNorm A BatchNorm module Returns torch nn Linear The fused linear module note Both ` ` linear ` ` ` ` bn ` ` must eval mode ` ` bn ` ` must have its running buffers computed assert linear training bn training Fusion only eval fused_linear = copy deepcopy linear Linear-BN needs fused while preserving shapes linear weight bias To preserve shapes linear weight bias channel dim bn needs broadcastable last dim linear because bn operates over channel dim N C_in H W while linear operates over last dim H_in To broadcastable number features bn number output features linear must satisfy following condition they equal number features bn Otherwise skip folding path assert linear out_features == bn num_features bn num_features == To fuse linear out_features == bn num_features bn num_features == assert bn running_mean None bn running_var None fused_linear weight fused_linear bias = fuse_linear_bn_weights fused_linear weight fused_linear bias bn running_mean bn running_var bn eps bn weight bn bias fused_linear fuse_linear_bn_weights linear_w torch Tensor linear_b torch Tensor &#124; None bn_rm torch Tensor bn_rv torch Tensor bn_eps float bn_w torch Tensor bn_b torch Tensor - tuple torch nn Parameter torch nn Parameter r Fuse linear module parameters BatchNorm module parameters into new linear module parameters Args linear_w torch Tensor Linear weight linear_b Optional torch Tensor Linear bias bn_rm torch Tensor BatchNorm running mean bn_rv torch Tensor BatchNorm running variance bn_eps float BatchNorm epsilon bn_w torch Tensor BatchNorm weight bn_b torch Tensor BatchNorm bias Returns Tuple torch nn Parameter torch nn Parameter Fused linear weight bias linear_weight_dtype = linear_w dtype linear_bias_dtype = linear_b dtype linear_b None linear_weight_dtype linear_b None linear_b = torch zeros_like bn_rm bn_scale = bn_w torch rsqrt bn_rv + bn_eps fused_w = linear_w bn_scale unsqueeze - dtype=linear_weight_dtype fused_b = linear_b - bn_rm bn_scale + bn_b dtype=linear_bias_dtype torch nn Parameter fused_w linear_w requires_grad torch nn Parameter fused_b linear_b requires_grad