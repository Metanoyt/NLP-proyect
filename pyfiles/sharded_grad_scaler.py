mypy allow-untyped-defs logging collections abc defaultdict collections abc Iterable typing Any Optional overload Union torch torch distributed dist torch amp grad_scaler _MultiDeviceReplicator GradScaler OptState torch distributed distributed_c d ProcessGroup logger = logging getLogger __name__ _refresh_per_optimizer_state - dict str Any stage OptState READY found_inf_per_device _is_supported_device tensor torch Tensor - bool tensor is_cuda tensor device type xla cpu hpu mtia xpu torch _C _get_privateuse _backend_name _GeneralMultiDeviceReplicator _MultiDeviceReplicator Lazily serves tensor request device This extends _MultiDeviceReplicator allow support cpu device __init__ master_tensor torch Tensor - None _is_supported_device master_tensor raise AssertionError f Expected supported device got master_tensor device master = master_tensor _per_device_tensors dict torch device torch Tensor = ShardedGradScaler GradScaler ShardedGradScaler helps perform gradient scaling shard aware manner It extends functionality GradScaler Supports Pytorch DDP FSDP implementations Support CPU offloaded tensors used fully sharded data parallel FSDP Supports custom Mixed Precision loss dtype fp bf FSDP returns Sync inf nan scaled gradient tensors any torch device where tensors placed across nodes Example Creates ShardedGradScaler once beginning training scaler = ShardedGradScaler epoch epochs input target data optimizer zero_grad output = model input loss = loss_fn output target Scales loss Calls backward scaled loss create scaled gradients scaler scale loss backward scaler step first unscales gradients optimizer s params If gradients don t contain infs NaNs optimizer step then called otherwise optimizer step skipped scaler step optimizer Updates scale next iteration scaler update See ` GradScaler ` explanation scaling unscaling more use cases Args init_scale float optional default= Initial scale factor growth_factor float optional default= Factor which scale multiplied during meth ` update ` no inf NaN gradients occur ` ` growth_interval ` ` consecutive iterations backoff_factor float optional default= Factor which scale multiplied during meth ` update ` inf NaN gradients occur iteration growth_interval int optional default= Number consecutive iterations without inf NaN gradients must occur scale multiplied ` ` growth_factor ` ` enabled bool optional If ` ` False ` ` disables gradient scaling meth ` step ` simply invokes underlying ` ` optimizer step ` ` other methods become no-ops Default ` ` True ` ` process_group ProcessGroup optional default=torch distributed group WORLD process group sharding __init__ device str = cuda init_scale float = backoff_factor float = growth_factor float = growth_interval int = enabled bool = True process_group Optional ProcessGroup = dist group WORLD - None super __init__ device init_scale=init_scale backoff_factor=backoff_factor growth_factor=growth_factor growth_interval=growth_interval enabled=enabled _enabled process_group = process_group _per_optimizer_states = defaultdict _refresh_per_optimizer_state overload scale outputs torch Tensor - torch Tensor overload scale outputs list torch Tensor - list torch Tensor overload scale outputs tuple torch Tensor - tuple torch Tensor overload scale outputs Iterable torch Tensor - Iterable torch Tensor scale outputs Union torch Tensor Iterable torch Tensor - Union torch Tensor Iterable torch Tensor _enabled outputs isinstance outputs torch Tensor _is_supported_device outputs raise AssertionError f Expected supported device got outputs device _scale None _lazy_init_scale_growth_tracker outputs device _scale None raise AssertionError Expected _scale initialized got None scaled_output = outputs _scale device=outputs device non_blocking=True Here we ensure dtype same outputs dtype For FSDP + Mixed Precision use case loss output Mixed Precision format fp bf so scaled loss should same dtype scaled_output type outputs dtype stash list _GeneralMultiDeviceReplicator = apply_scale val Union torch Tensor Iterable torch Tensor isinstance val torch Tensor _is_supported_device val raise AssertionError f Expected supported device got val device len stash == _scale None _lazy_init_scale_growth_tracker val device _scale None raise AssertionError Expected _scale initialized got None stash append _GeneralMultiDeviceReplicator _scale scaled_val = val stash get val device Here we ensure dtype same outputs dtype For FSDP + Mixed Precision use case loss output Mixed Precision format fp bf so scaled loss should same dtype scaled_val type val dtype isinstance val abc Iterable iterator = map apply_scale val isinstance val list tuple type val iterator iterator raise ValueError outputs must Tensor iterable Tensors apply_scale outputs _unscale_grads_ optimizer torch optim Optimizer inv_scale torch Tensor found_inf torch Tensor allow_fp bool = True - dict torch device torch Tensor per_device_inv_scale = _GeneralMultiDeviceReplicator inv_scale per_device_found_inf = _GeneralMultiDeviceReplicator found_inf To set up _amp_foreach_non_finite_check_and_unscale_ split grads device dtype There could thousands grads so we d like iterate through them just once However we don t know their devices dtypes advance https stackoverflow com questions defaultdict-of-defaultdict Google says mypy struggles defaultdicts type annotations per_device_and_dtype_grads = defaultdict lambda defaultdict list type ignore var-annotated torch no_grad group optimizer param_groups param group params param grad None continue allow_fp param grad dtype == torch float raise ValueError Attempting unscale FP gradients param grad is_sparse is_coalesced == False means sparse grad has values duplicate indices coalesce deduplicates indices adds all values have same index For scaled fp values there s good chance coalescing will cause overflow so we should check coalesced _values param grad dtype torch float coalesce supported torch float param_grad_fp = param grad type torch float coalesce param grad = param_grad_fp type torch float to_unscale = param grad _values to_unscale = param grad per_device_and_dtype_grads to_unscale device to_unscale dtype append to_unscale device per_dtype_grads per_device_and_dtype_grads items grads per_dtype_grads values torch _amp_foreach_non_finite_check_and_unscale_ grads per_device_found_inf get device per_device_inv_scale get device There exist contexts e g w ` use_orig_params=True ` wherein some ranks may have no non-zero sized parameter shards necessitating initialization ` per_device_found_inf _per_device_tensors ` here per_device_found_inf _per_device_tensors _scale None raise AssertionError Expected _scale initialized got None per_device_found_inf get _scale device per_device_found_inf _per_device_tensors unscale_ optimizer torch optim Optimizer - None _enabled _check_scale_growth_tracker unscale_ optimizer_state = _per_optimizer_states id optimizer optimizer_state stage OptState UNSCALED raise RuntimeError unscale_ has already been called optimizer since last update optimizer_state stage OptState STEPPED raise RuntimeError unscale_ being called after step FP division can imprecise certain compile options so we carry out reciprocal FP _scale None raise AssertionError Expected _scale initialized got None inv_scale = _scale double reciprocal float found_inf = torch full dtype=torch float device=self _scale device optimizer_state found_inf_per_device = _unscale_grads_ optimizer inv_scale found_inf True optimizer_state stage = OptState UNSCALED Synchronize detected inf across ranks optimizer_state = _per_optimizer_states id optimizer works = found_inf_on_cpus = found_inf_on_devices = found_inf optimizer_state found_inf_per_device values _device = cpu found_inf device type == cpu found_inf_on_cpus append found_inf found_inf_on_device = found_inf _device found_inf_on_devices append found_inf_on_device works append dist all_reduce found_inf_on_device async_op=True group=self process_group works append dist all_reduce found_inf async_op=True group=self process_group work works work wait found_inf_on_cpus torch _foreach_copy_ found_inf_on_cpus found_inf_on_devices _amp_update_scale_cpu_ found_inf torch Tensor - None If found_inf True then scale multiplied backoff_factor growth_tracker set zero Otherwise scale multiplied growth factor when growth interval reached _scale None _growth_tracker None raise AssertionError Expected _scale _growth_tracker initialized got None found_inf item = _scale = _backoff_factor _growth_tracker fill_ successful = _growth_tracker + successful == _growth_interval _scale = _growth_factor _growth_tracker fill_ _growth_tracker = successful update new_scale Optional Union float torch Tensor = None - None Updates scale factor If any optimizer steps skipped scale multiplied ` ` backoff_factor ` ` reduce If ` ` growth_interval ` ` unskipped iterations occurred consecutively scale multiplied ` ` growth_factor ` ` increase Passing ` ` new_scale ` ` sets new scale value manually ` ` new_scale ` ` used directly s used fill GradScaler s internal scale tensor So ` ` new_scale ` ` tensor later in-place changes tensor will further affect scale GradScaler uses internally Args new_scale float ` torch Tensor ` optional default=None New scale factor warning meth ` update ` should only called end iteration after ` ` scaler step optimizer ` ` has been invoked all optimizers used iteration _enabled _scale _growth_tracker = _check_scale_growth_tracker update type ignore var-annotated new_scale None Accept new user-defined scale isinstance new_scale float _scale fill_ new_scale type ignore union-attr reason = new_scale should float -element torch cuda FloatTensor torch FloatTensor requires_grad=False new_scale device type = _device raise AssertionError reason new_scale numel = raise AssertionError reason new_scale requires_grad False raise AssertionError reason _scale copy_ new_scale type ignore union-attr Consume shared inf nan data collected optimizers update scale If all found_inf tensors same device _scale operation asynchronous found_infs = found_inf device=_scale device non_blocking=True state _per_optimizer_states values found_inf state found_inf_per_device values len found_infs == raise AssertionError No inf checks recorded prior update found_inf_combined = found_infs len found_infs i range len found_infs found_inf_combined += found_infs i _scale device type == cpu _amp_update_scale_cpu_ found_inf_combined torch _amp_update_scale_ _scale type ignore arg-type _growth_tracker type ignore arg-type found_inf_combined _growth_factor type ignore arg-type _backoff_factor type ignore arg-type _growth_interval type ignore arg-type To prepare next iteration clear data collected optimizers iteration _per_optimizer_states = defaultdict _refresh_per_optimizer_state