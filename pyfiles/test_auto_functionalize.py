Owner s module functionalization unittest numpy np torch torch _dynamo testing torch _inductor config inductor_config torch _inductor test_case torch utils _pytree pytree torch utils cpp_extension torch Tensor torch _dynamo testing CompileCounterWithBackend torch _higher_order_ops auto_functionalize try_use_slice torch testing _internal logging_utils logs_to_string AutoFunctionalizeTests torch _inductor test_case TestCase test_auto_functionalize_can_with_default torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor int b Tensor d c=None Tensor d=None int e=- - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib foo_impl b c=None d=None e=- + b f mode torch ops mylib foo = torch tensor dtype=torch int torch compile f test_auto_functionalize_can_with_none_return torch library _scoped_library mylib FRAGMENT lib lib define foo Tensor x Tensor out - None foo_impl x out out copy_ x lib impl foo foo_impl CompositeExplicitAutograd x = torch randn out = torch zeros torch compile f x out torch ops mylib foo x out f x out test_auto_functionalize_self_as_mutate_arg torch library _scoped_library mylib FRAGMENT lib lib define foo Tensor - None foo_impl torch Tensor - None sin_ x = torch randn lib impl foo foo_impl CompositeExplicitAutograd torch compile backend= inductor fullgraph=True f x torch ops mylib foo x f x test_auto_functionalize_tensorlist torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor all_gather_output SymInt all_gather_input_split_sizes int dim Tensor out - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl all_gather_output all_gather_input_split_sizes dim out o out o copy_ all_gather_output f all_gather_output all_gather_input_split_sizes dim out torch ops mylib foo all_gather_output all_gather_input_split_sizes dim out = torch ones b = c = d = torch empty _ range orig_args = b c d compiled_args = pytree tree_map_only torch Tensor torch clone orig_args torch compile f backend= inductor fullgraph=True compiled_args eager_args = pytree tree_map_only torch Tensor torch clone orig_args f eager_args assertEqual compiled_args eager_args test_can_auto_functionalize torch _higher_order_ops auto_functionalize can_auto_functionalize expected_true = Tensor x - Tensor x Tensor y Tensor b z SymInt w Tensor c n - Tensor x Tensor y Tensor b z SymInt w Tensor c n - Tensor x Tensor y Tensor b z SymInt w - Tensor x Tensor y Tensor b z SymInt w Tensor c n - Tensor Tensor x Tensor y Tensor b z SymInt w Tensor c n - Tensor Tensor expected_false = Tensor x - Tensor x - Tensor Tensor x - Tensor Tensor x Tensor y Tensor b z SymInt w Tensor c n - Tensor Tensor x Tensor y Tensor b z SymInt w Tensor c n - Tensor Tensor Tensor x Tensor y Tensor b z SymInt w Tensor c n - Tensor Tensor Tensor x Tensor y Tensor b z SymInt w Tensor c n - Tensor Tensor schema expected_true torch library _scoped_library mylib FRAGMENT lib torch library define mylib schema lib=lib assertTrue can_auto_functionalize torch ops mylib default msg=schema assertFalse can_auto_functionalize torch ops mylib schema expected_false torch library _scoped_library mylib FRAGMENT lib torch library define mylib schema lib=lib assertFalse can_auto_functionalize torch ops mylib default msg=schema assertFalse can_auto_functionalize torch ops mylib torch _inductor config patch enable_auto_functionalized_v =False test_auto_functionalize_old torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor y Tensor b z SymInt w Tensor n - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y z w n x add_ y + w z add_ y + n f x y z n torch ops mylib foo x y z n x = torch randn y = torch randn torch randn z = torch randn n = torch randn orig_args = x y z n compiled_args = pytree tree_map_only torch Tensor torch clone orig_args log_stream ctx = logs_to_string torch _inductor compile_fx post_grad_graphs ctx torch compile f backend= inductor fullgraph=True compiled_args post_grad_graphs = \n join log_stream getvalue strip split \n strip Check graph under static shapes torch _dynamo config assume_static_by_default assertExpectedInline post_grad_graphs \ forward arg _ f cpu arg _ f cpu arg _ f cpu arg _ f cpu arg _ f cpu foo_default = torch ops mylib foo default arg _ arg _ arg _ arg _ arg _ arg _ = arg _ = arg _ = arg _ = arg _ = foo_default = None noqa B ignore_comments=True stack trace should post_grad_graph assertTrue code torch ops mylib foo x y z n post_grad_graphs eager_args = pytree tree_map_only torch Tensor torch clone orig_args f eager_args assertEqual compiled_args eager_args torch _inductor config patch enable_auto_functionalized_v =False test_auto_functionalize_with_returns_old torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor y Tensor b z SymInt w Tensor n - Tensor Tensor tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y z w n x add_ y + w z add_ y + n y + w y + n torch library register_fake mylib foo lib=lib foo_abstract x y z w n y + w y + n f x y z n torch ops mylib foo x y z n x = torch randn y = torch randn torch randn z = torch randn n = torch randn orig_args = x y z n compiled_args = pytree tree_map_only torch Tensor torch clone orig_args log_stream ctx = logs_to_string torch _inductor compile_fx post_grad_graphs ctx compiled_out = torch compile f backend= inductor fullgraph=True compiled_args torch _dynamo config assume_static_by_default post_grad_graphs = \n join log_stream getvalue strip split \n strip assertExpectedInline post_grad_graphs \ forward arg _ f cpu arg _ f cpu arg _ f cpu arg _ f cpu arg _ f cpu foo_default = torch ops mylib foo default arg _ arg _ arg _ arg _ arg _ arg _ = arg _ = arg _ = arg _ = arg _ = None getitem_ f cpu = foo_default getitem_ f cpu = foo_default foo_default = None getitem_ getitem_ noqa B ignore_comments=True eager_args = pytree tree_map_only torch Tensor torch clone orig_args eager_out = f eager_args assertEqual compiled_args eager_args assertEqual compiled_out eager_out test_auto_functionalize_on_view value True False torch library _scoped_library mylib FRAGMENT lib inductor_config patch enable_auto_functionalized_v value torch library define mylib foo Tensor x - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x x_np = x detach numpy view np sin x_np out=x_np x = torch randn expected = x sin torch ops mylib foo x assert torch allclose x expected torch compile backend= aot_eager_decomp_partition fullgraph=True f x x = x clone y = x torch ops mylib foo y x y = f x assertEqual y x sin torch _inductor config patch enable_auto_functionalized_v =False test_auto_functionalize_optional_old torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor y Tensor b z SymInt w Tensor n - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y z w n x None x add_ y + w z None z add_ y + n f x y z n torch ops mylib foo x y z n x = None y = torch randn torch randn z = torch randn n = torch randn orig_args = x y z n compiled_args = pytree tree_map_only torch Tensor torch clone orig_args log_stream ctx = logs_to_string torch _inductor compile_fx post_grad_graphs ctx torch compile f backend= inductor fullgraph=True compiled_args torch _dynamo config assume_static_by_default post_grad_graphs = \n join log_stream getvalue strip split \n strip assertExpectedInline post_grad_graphs \ forward arg _ f cpu arg _ f cpu arg _ f cpu arg _ f cpu foo_default = torch ops mylib foo default None arg _ arg _ arg _ arg _ arg _ = \ arg _ = arg _ = arg _ = foo_default = None ignore_comments=True stack trace should post_grad_graph assertTrue code torch ops mylib foo x y z n post_grad_graphs eager_args = pytree tree_map_only torch Tensor torch clone orig_args f eager_args assertEqual compiled_args eager_args torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True test_unbacked_auto_functionalize_op torch library custom_op mylib mk_image mutates_args= decoder device_types= cpu mk_image decoder Tensor - Tensor torch randn torch library register_fake mylib mk_image _ decoder Tensor - Tensor image_size = torch library get_ctx new_dynamic_size _ range torch empty image_size torch compile fullgraph=True f x torch ops mylib mk_image default x x = torch zeros dtype=torch int f x torch _inductor config patch enable_auto_functionalized_v =True test_auto_functionalize_v _dynamic=False torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor y Tensor b z SymInt w Tensor n - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y z w n x add_ y + w z add_ y + n f x y z n torch ops mylib foo x y z n x = torch randn y = torch randn torch randn z = torch randn n = torch randn orig_args = x y z n compiled_args = pytree tree_map_only torch Tensor torch clone orig_args log_stream ctx = logs_to_string torch _inductor compile_fx post_grad_graphs ctx torch compile f backend= inductor dynamic=_dynamic fullgraph=True compiled_args post_grad_graphs = \n join log_stream getvalue strip split \n strip torch _dynamo config assume_static_by_default _dynamic assertExpectedInline post_grad_graphs \ forward arg _ Sym s arg _ f s cpu arg _ f s cpu arg _ f s cpu arg _ f s cpu arg _ f s cpu foo_default = torch ops mylib foo default arg _ arg _ arg _ arg _ arg _ arg _ = arg _ = arg _ = foo_default = None copy_ f s cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy_ = None copy__ f s cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy__ = None noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline post_grad_graphs \ forward arg _ f cpu arg _ f cpu arg _ f cpu arg _ f cpu arg _ f cpu foo_default = torch ops mylib foo default arg _ arg _ arg _ arg _ arg _ arg _ = arg _ = arg _ = foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy_ = None copy__ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy__ = None noqa B ignore_comments=True ignore_empty_lines=True eager_args = pytree tree_map_only torch Tensor torch clone orig_args f eager_args assertEqual compiled_args eager_args run_aot_eager f orig_args _dynamic=False aot_eager_args = pytree tree_map_only torch Tensor torch clone orig_args log_stream ctx = logs_to_string torch _functorch _aot_autograd graph_capture aot_graphs result = None ctx result = torch compile f backend= aot_eager fullgraph=True dynamic=_dynamic aot_eager_args graph = \n join log_stream getvalue strip split \n strip aot_eager_args result graph run_inductor f orig_args _dynamic=False log_module= torch _inductor compile_fx log_function= post_grad_graphs compiled_args = pytree tree_map_only torch Tensor torch clone orig_args log_stream ctx = logs_to_string log_module log_function result = None ctx result = torch compile f backend= inductor fullgraph=True dynamic=_dynamic compiled_args graph = \n join log_stream getvalue strip split \n strip compiled_args result graph torch _inductor config patch enable_auto_functionalized_v =True test_auto_functionalize_with_returns_v torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor y Tensor b z SymInt w Tensor n - Tensor Tensor tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y z w n x add_ y + w z add_ y + n y + w y + n torch library register_fake mylib foo lib=lib foo_abstract x y z w n y + w y + n f x y z n torch ops mylib foo x y z n x = torch randn y = torch randn torch randn z = torch randn n = torch randn orig_args = x y z n compiled_args = pytree tree_map_only torch Tensor torch clone orig_args log_stream ctx = logs_to_string torch _inductor compile_fx post_grad_graphs ctx compiled_out = torch compile f backend= inductor fullgraph=True compiled_args torch _dynamo config assume_static_by_default post_grad_graphs = \n join log_stream getvalue strip split \n strip assertExpectedInline post_grad_graphs \ forward arg _ f cpu arg _ f cpu arg _ f cpu arg _ f cpu arg _ f cpu foo_default = torch ops mylib foo default arg _ arg _ arg _ arg _ arg _ arg _ = arg _ = arg _ = None getitem_ f cpu = foo_default getitem_ f cpu = foo_default foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy_ = None copy__ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy__ = None getitem_ getitem_ noqa B ignore_comments=True ignore_empty_lines=True eager_args = pytree tree_map_only torch Tensor torch clone orig_args eager_out = f eager_args assertEqual compiled_args eager_args assertEqual compiled_out eager_out foo takes two inputs views torch _inductor config patch enable_auto_functionalized_v =True test_auto_functionalize_extra _dynamic=False torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y x sin_ y sin_ f x y torch ops mylib foo x y x + y orig_args = torch randn torch randn aot_eager_args result graph_aot = run_aot_eager f orig_args _dynamic inductor_args result graph_inductor = run_inductor f orig_args _dynamic eager_args = pytree tree_map_only torch Tensor torch clone orig_args result = f eager_args assertEqual inductor_args eager_args assertEqual inductor_args aot_eager_args assertEqual result result assertEqual result result torch _dynamo config assume_static_by_default _dynamic assertExpectedInline graph_aot \ forward arg _ Sym s arg _ f s cpu arg _ f s cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _y_base_index = _all_bases = arg _ arg _ getitem_ f s cpu = auto_functionalized_v getitem_ f s cpu = auto_functionalized_v auto_functionalized_v = None add f s cpu = torch ops aten add Tensor getitem_ getitem_ copy_ f s cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy_ = None copy__ f s cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy__ = None add noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_aot \ forward arg _ f cpu arg _ f cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _y_base_index = _all_bases = arg _ arg _ getitem_ f cpu = auto_functionalized_v getitem_ f cpu = auto_functionalized_v auto_functionalized_v = None add f cpu = torch ops aten add Tensor getitem_ getitem_ copy_ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy_ = None copy__ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy__ = None add noqa B ignore_comments=True ignore_empty_lines=True torch _dynamo config assume_static_by_default _dynamic assertExpectedInline graph_inductor \ forward arg _ Sym s arg _ f s cpu arg _ f s cpu foo_default = torch ops mylib foo default arg _ arg _ foo_default = None add f s cpu = torch ops aten add Tensor arg _ arg _ copy_ f s cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy_ = None copy__ f s cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy__ = None add ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_inductor \ forward arg _ f cpu arg _ f cpu foo_default = torch ops mylib foo default arg _ arg _ foo_default = None add f cpu = torch ops aten add Tensor arg _ arg _ copy_ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy_ = None copy__ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy__ = None add ignore_comments=True ignore_empty_lines=True foo takes two views same input function does have torch _inductor config patch enable_auto_functionalized_v =True test_auto_functionalize_extra _dynamic=False torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y x sin_ y sin_ f x = x b = x torch ops mylib foo b orig_args = torch randn aot_eager_args result graph_aot = run_aot_eager f orig_args _dynamic inductor_args result graph_inductor = run_inductor f orig_args _dynamic eager_args = pytree tree_map_only torch Tensor torch clone orig_args result = f eager_args assertEqual inductor_args eager_args assertEqual inductor_args aot_eager_args assertEqual result result assertEqual result result torch _dynamo config assume_static_by_default _dynamic assertExpectedInline graph_aot \ forward arg _ Sym s arg _ f s cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_size = _x_stride = _x_storage_offset = _y_base_index = _y_size = _y_stride = _y_storage_offset = _all_bases = arg _ getitem_ f s cpu = auto_functionalized_v auto_functionalized_v = None copy_ f s cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy_ = None noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_aot \ forward arg _ f cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_size = _x_stride = _x_storage_offset = _y_base_index = _y_size = _y_stride = _y_storage_offset = _all_bases = arg _ getitem_ f cpu = auto_functionalized_v auto_functionalized_v = None copy_ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy_ = None noqa B ignore_comments=True ignore_empty_lines=True Run inductor backend torch _dynamo config assume_static_by_default _dynamic assertExpectedInline graph_inductor \ forward arg _ Sym s arg _ f s cpu as_strided_default f cpu = torch ops aten as_strided default arg _ as_strided_default_ f cpu = torch ops aten as_strided default arg _ foo_default = torch ops mylib foo default as_strided_default as_strided_default_ as_strided_default = as_strided_default_ = foo_default = None copy_ f s cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy_ = None noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_inductor \ forward arg _ f cpu as_strided_default f cpu = torch ops aten as_strided default arg _ as_strided_default_ f cpu = torch ops aten as_strided default arg _ foo_default = torch ops mylib foo default as_strided_default as_strided_default_ as_strided_default = as_strided_default_ = foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy_ = None noqa B ignore_comments=True ignore_empty_lines=True foo takes two views same input function returns both views input torch _inductor config patch enable_auto_functionalized_v =True test_auto_functionalize_extra torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y x sin_ y sin_ f x = x b = x torch ops mylib foo b b x orig_args = torch randn aot_eager_args result graph_aot = run_aot_eager f orig_args inductor_args result graph_inductor = run_inductor f orig_args eager_args = pytree tree_map_only torch Tensor torch clone orig_args result = f eager_args assertEqual inductor_args eager_args assertEqual inductor_args aot_eager_args assertEqual result result assertEqual result result torch _dynamo config assume_static_by_default assertExpectedInline graph_aot \ forward arg _ f cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_size = _x_stride = _x_storage_offset = _y_base_index = _y_size = _y_stride = _y_storage_offset = _all_bases = arg _ getitem_ f cpu = auto_functionalized_v auto_functionalized_v = None copy_ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = copy_ = None select_ f cpu = torch ops aten select int getitem_ select_ f cpu = torch ops aten select int getitem_ getitem_ = None select_ select_ noqa B ignore_comments=True ignore_empty_lines=True Run inductor backend torch _dynamo config assume_static_by_default assertExpectedInline graph_inductor \ forward arg _ f cpu as_strided_default f cpu = torch ops aten as_strided default arg _ as_strided_default_ f cpu = torch ops aten as_strided default arg _ foo_default = torch ops mylib foo default as_strided_default as_strided_default_ as_strided_default = as_strided_default_ = foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ copy_ = None select_ f cpu = torch ops aten select int arg _ select_ f cpu = torch ops aten select int arg _ arg _ = None select_ select_ noqa B ignore_comments=True ignore_empty_lines=True foo takes x y both being graph inputs views same shared base do overlap In special case functionlization will have none base x y so they will assumed have unique bases during functionalizations During inplace we notice they both share storage because their memory does overlap we can inplace both see github issue torch _inductor config patch enable_auto_functionalized_v =True test_auto_functionalize_extra torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y x sin_ y sin_ f x y torch ops mylib foo x y base = torch randn orig_args = base base aot_eager_args result graph_aot = run_aot_eager f orig_args inductor_args result graph_inductor = run_inductor f orig_args eager_args = pytree tree_map_only torch Tensor torch clone orig_args result = f eager_args assertEqual inductor_args eager_args assertEqual inductor_args aot_eager_args assertEqual result result assertEqual result result torch _dynamo config assume_static_by_default assertExpectedInline graph_aot \ forward arg _ f cpu arg _ f cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _y_base_index = _all_bases = arg _ arg _ getitem_ f cpu = auto_functionalized_v getitem_ f cpu = auto_functionalized_v auto_functionalized_v = None copy_ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy_ = None copy__ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy__ = None noqa B ignore_comments=True ignore_empty_lines=True Run inductor backend torch _dynamo config assume_static_by_default assertExpectedInline graph_inductor \ forward arg _ f cpu arg _ f cpu foo_default = torch ops mylib foo default arg _ arg _ foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy_ = None copy__ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy__ = None noqa B ignore_comments=True ignore_empty_lines=True foo takes mutable list views addition other args torch _inductor config patch enable_auto_functionalized_v =True test_auto_functionalize_extra torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y x sin_ y sin_ f x y z = x b = z torch ops mylib foo b y orig_args = torch randn torch randn torch randn aot_eager_args result graph_aot = run_aot_eager f orig_args inductor_args result graph_inductor = run_inductor f orig_args eager_args = pytree tree_map_only torch Tensor torch clone orig_args result = f eager_args assertEqual inductor_args eager_args assertEqual inductor_args aot_eager_args assertEqual result result assertEqual result result torch _dynamo config assume_static_by_default assertExpectedInline graph_aot \ forward arg _ f cpu arg _ f cpu arg _ f cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_size = _x_stride = _x_storage_offset = _y_length = _y_ _base_index = _y_ _size = _y_ _stride = _y_ _storage_offset = _y_ _base_index = _all_bases = arg _ arg _ arg _ getitem_ f cpu = auto_functionalized_v getitem_ f cpu = auto_functionalized_v getitem_ f cpu = auto_functionalized_v auto_functionalized_v = None copy_ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy_ = None copy__ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy__ = None copy__ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = getitem_ = copy__ = None noqa B ignore_comments=True ignore_empty_lines=True Run inductor backend torch _dynamo config assume_static_by_default assertExpectedInline graph_inductor \ forward arg _ f cpu arg _ f cpu arg _ f cpu as_strided_default f cpu = torch ops aten as_strided default arg _ as_strided_default_ f cpu = torch ops aten as_strided default arg _ foo_default = torch ops mylib foo default as_strided_default as_strided_default_ arg _ as_strided_default = as_strided_default_ = foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy_ = None copy__ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy__ = None copy__ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy__ = None noqa B ignore_comments=True ignore_empty_lines=True torch _inductor config patch enable_auto_functionalized_v =True test_auto_functionalize_optional_v torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor y Tensor b z SymInt w Tensor n - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y z w n x None x add_ y + w z None z add_ y + n f x y z n torch ops mylib foo x y z n x = None y = torch randn torch randn z = torch randn n = torch randn orig_args = x y z n compiled_args = pytree tree_map_only torch Tensor torch clone orig_args log_stream ctx = logs_to_string torch _inductor compile_fx post_grad_graphs ctx torch compile f backend= inductor fullgraph=True compiled_args torch _dynamo config assume_static_by_default post_grad_graphs = \n join log_stream getvalue strip split \n strip assertExpectedInline post_grad_graphs \ forward arg _ f cpu arg _ f cpu arg _ f cpu arg _ f cpu foo_default = torch ops mylib foo default None arg _ arg _ arg _ arg _ arg _ = arg _ = arg _ = foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ arg _ = copy_ = None noqa B ignore_comments=True ignore_empty_lines=True eager_args = pytree tree_map_only torch Tensor torch clone orig_args f eager_args assertEqual compiled_args eager_args torch _inductor config patch enable_auto_functionalized_v =False test_inference_mode _v torch inference_mode test_auto_functionalize_extra torch _inductor config patch enable_auto_functionalized_v =True test_inference_mode _v torch inference_mode test_auto_functionalize_extra torch _inductor config patch enable_auto_functionalized_v =True test_inference_mode _v torch inference_mode test_auto_functionalize_extra torch _inductor config patch enable_auto_functionalized_v =True test_inference_mode _v torch inference_mode test_auto_functionalize_extra torch _inductor config patch enable_auto_functionalized_v =True test_dynamic_v test_auto_functionalize_v _dynamic=True torch _inductor config patch enable_auto_functionalized_v =True test_dynamic _v test_auto_functionalize_extra _dynamic=True torch _inductor config patch enable_auto_functionalized_v =True test_dynamic _v test_auto_functionalize_extra _dynamic=True torch _inductor config patch enable_auto_functionalized_v =True test_graph_input_is_view torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x pass torch compile fullgraph=True dynamic=False backend= aot_eager f x = x torch ops mylib foo x = torch tensor This would fail auto_functionalized_v uses clone clone_preserve_strides clone not-inplaced args f x torch _inductor config patch enable_auto_functionalized_v =True test_alias _dynamic=False torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y x sin_ y sin_ f x = torch ops aten alias default x b = torch ops aten alias default x torch ops mylib foo b b x orig_args = torch randn aot_eager_args result graph_aot = run_aot_eager f orig_args _dynamic inductor_args result graph_inductor = run_inductor f orig_args _dynamic eager_args = pytree tree_map_only torch Tensor torch clone orig_args result = f eager_args assertEqual inductor_args eager_args assertEqual inductor_args aot_eager_args assertEqual result result assertEqual result result torch _dynamo config assume_static_by_default _dynamic assertExpectedInline graph_aot \ forward arg _ Sym s arg _ f s cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_alias = True _y_base_index = _y_alias = True _all_bases = arg _ getitem_ f s cpu = auto_functionalized_v auto_functionalized_v = None copy_ f s cpu = torch ops aten copy_ default arg _ getitem_ arg _ = copy_ = None alias_ f s cpu = torch ops aten alias default getitem_ alias_ f s cpu = torch ops aten alias default getitem_ getitem_ = None alias_ alias_ noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_aot \ forward arg _ f cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_alias = True _y_base_index = _y_alias = True _all_bases = arg _ getitem_ f cpu = auto_functionalized_v auto_functionalized_v = None copy_ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = copy_ = None alias_ f cpu = torch ops aten alias default getitem_ alias_ f cpu = torch ops aten alias default getitem_ getitem_ = None alias_ alias_ noqa B ignore_comments=True ignore_empty_lines=True Run inductor backend torch _dynamo config assume_static_by_default _dynamic assertExpectedInline graph_inductor \ forward arg _ Sym s arg _ f s cpu alias_default f s cpu = torch ops aten alias default arg _ alias_default_ f s cpu = torch ops aten alias default arg _ foo_default = torch ops mylib foo default alias_default alias_default_ \ alias_default = alias_default_ = foo_default = None copy_ f s cpu = torch ops aten copy_ default arg _ arg _ copy_ = None arg _ arg _ noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_inductor \ forward arg _ f cpu alias_default f cpu = torch ops aten alias default arg _ alias_default_ f cpu = torch ops aten alias default arg _ foo_default = torch ops mylib foo default alias_default alias_default_ \ alias_default = alias_default_ = foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ copy_ = None arg _ arg _ noqa B ignore_comments=True ignore_empty_lines=True Test slice view generated instead as_strided when split used torch _inductor config patch enable_auto_functionalized_v =True test_split _dynamic=False torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y x sin_ y sin_ f x splits = x split dim= = splits b = splits torch ops mylib foo b b x orig_args = torch randn aot_eager_args result graph_aot = run_aot_eager f orig_args _dynamic inductor_args result graph_inductor = run_inductor f orig_args _dynamic eager_args = pytree tree_map_only torch Tensor torch clone orig_args result = f eager_args assertEqual inductor_args eager_args assertEqual inductor_args aot_eager_args assertEqual result result assertEqual result result torch _dynamo config assume_static_by_default _dynamic split forces specialization size so we dont see arg _ dynamic anymore assertExpectedInline graph_aot \ forward arg _ f cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_slice_dim = _x_slice_start = _x_slice_end = _y_base_index = _y_slice_dim = _y_slice_start = _y_slice_end = _all_bases = arg _ getitem_ f cpu = auto_functionalized_v auto_functionalized_v = None copy_ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = copy_ = None split_with_sizes_ = torch ops aten split_with_sizes default getitem_ getitem_ f cpu = split_with_sizes_ split_with_sizes_ = None split_with_sizes_ = torch ops aten split_with_sizes default getitem_ getitem_ = None getitem_ f cpu = split_with_sizes_ split_with_sizes_ = None getitem_ getitem_ noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_aot \ forward arg _ f cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_slice_dim = _x_slice_start = _x_slice_end = _y_base_index = _y_slice_dim = _y_slice_start = _y_slice_end = _all_bases = arg _ getitem_ f cpu = auto_functionalized_v auto_functionalized_v = None copy_ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = copy_ = None split_with_sizes_ = torch ops aten split_with_sizes default getitem_ getitem_ f cpu = split_with_sizes_ split_with_sizes_ = None split_with_sizes_ = torch ops aten split_with_sizes default getitem_ getitem_ = None getitem_ f cpu = split_with_sizes_ split_with_sizes_ = None getitem_ getitem_ noqa B ignore_comments=True ignore_empty_lines=True Run inductor backend torch _dynamo config assume_static_by_default _dynamic split forces specialization size so we dont see arg _ dynamic anymore assertExpectedInline graph_inductor \ forward arg _ f cpu slice_tensor f cpu = torch ops aten slice Tensor arg _ slice_tensor_ f cpu = torch ops aten slice Tensor arg _ foo_default = torch ops mylib foo default slice_tensor slice_tensor_ slice_tensor = slice_tensor_ = foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ copy_ = None split_with_sizes_ = torch ops aten split_with_sizes default arg _ getitem_ f cpu = split_with_sizes_ split_with_sizes_ = None split_with_sizes_ = torch ops aten split_with_sizes default arg _ arg _ = None getitem_ f cpu = split_with_sizes_ split_with_sizes_ = None getitem_ getitem_ noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_inductor \ forward arg _ f cpu slice_tensor f cpu = torch ops aten slice Tensor arg _ slice_tensor_ f cpu = torch ops aten slice Tensor arg _ foo_default = torch ops mylib foo default slice_tensor slice_tensor_ slice_tensor = slice_tensor_ = foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ copy_ = None split_with_sizes_ = torch ops aten split_with_sizes default arg _ getitem_ f cpu = split_with_sizes_ split_with_sizes_ = None split_with_sizes_ = torch ops aten split_with_sizes default arg _ arg _ = None getitem_ f cpu = split_with_sizes_ split_with_sizes_ = None getitem_ getitem_ noqa B ignore_comments=True ignore_empty_lines=True Note split force input tensor get specialized So we do see SymInts when _dynamic=True torch _inductor config patch enable_auto_functionalized_v =True test_split_dynamic test_split _dynamic=True Test slice view generated instead as_strided when slice used torch _inductor config patch enable_auto_functionalized_v =True test_slice _dynamic=False torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y x sin_ y sin_ f x = torch ops aten slice Tensor x b = torch ops aten slice Tensor x torch ops mylib foo b b x orig_args = torch randn aot_eager_args result graph_aot = run_aot_eager f orig_args _dynamic inductor_args result graph_inductor = run_inductor f orig_args _dynamic eager_args = pytree tree_map_only torch Tensor torch clone orig_args result = f eager_args assertEqual inductor_args eager_args assertEqual inductor_args aot_eager_args assertEqual result result assertEqual result result torch _dynamo config assume_static_by_default _dynamic assertExpectedInline graph_aot \ forward arg _ Sym s arg _ f s s s cpu floordiv Sym = arg _ arg _ = None add_ Sym = floordiv + auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_slice_dim = _x_slice_start = floordiv _x_slice_end = add_ _y_base_index = _y_slice_dim = _y_slice_start = _y_slice_end = _all_bases = arg _ floordiv = add_ = None getitem_ f s s s cpu = auto_functionalized_v auto_functionalized_v = None copy_ f s s s cpu = torch ops aten copy_ default arg _ getitem_ arg _ = copy_ = None slice_ f s s cpu = torch ops aten slice Tensor getitem_ slice_ f s s cpu = torch ops aten slice Tensor getitem_ getitem_ = None slice_ slice_ noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_aot \ forward arg _ f cpu auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_slice_dim = _x_slice_start = _x_slice_end = _y_base_index = _y_slice_dim = _y_slice_start = _y_slice_end = _all_bases = arg _ getitem_ f cpu = auto_functionalized_v auto_functionalized_v = None copy_ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = copy_ = None slice_ f cpu = torch ops aten slice Tensor getitem_ slice_ f cpu = torch ops aten slice Tensor getitem_ getitem_ = None slice_ slice_ noqa B ignore_comments=True ignore_empty_lines=True Run inductor backend torch _dynamo config assume_static_by_default _dynamic assertExpectedInline graph_inductor \ forward arg _ Sym s arg _ f s s s cpu floordiv Sym = arg _ arg _ = None add_ Sym = floordiv + floordiv = add_ = None slice_tensor f s s cpu = torch ops aten slice Tensor arg _ slice_tensor_ f s s cpu = torch ops aten slice Tensor arg _ foo_default = torch ops mylib foo default slice_tensor slice_tensor_ slice_tensor = slice_tensor_ = foo_default = None copy_ f s s s cpu = torch ops aten copy_ default arg _ arg _ copy_ = None slice_ f s s cpu = torch ops aten slice Tensor arg _ slice_ f s s cpu = torch ops aten slice Tensor arg _ arg _ = None slice_ slice_ noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_inductor \ forward arg _ f cpu slice_tensor f cpu = torch ops aten slice Tensor arg _ slice_tensor_ f cpu = torch ops aten slice Tensor arg _ foo_default = torch ops mylib foo default slice_tensor slice_tensor_ slice_tensor = slice_tensor_ = foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ copy_ = None slice_ f cpu = torch ops aten slice Tensor arg _ slice_ f cpu = torch ops aten slice Tensor arg _ arg _ = None slice_ slice_ noqa B ignore_comments=True ignore_empty_lines=True Note split force input tensor get specialized So we do see SymInts when _dynamic=True torch _inductor config patch enable_auto_functionalized_v =True test_slice_dynamic test_slice _dynamic=True test_try_use_slice test_round_trip base tensor dim start end = try_use_slice base tensor sliced = torch ops aten slice Tensor base dim start end assertEqual sliced tensor t = torch tensor test_round_trip t t dim range - f = t split dim test_round_trip t f dim range - f = t split dim test_round_trip t f test_round_trip t f t = torch randint test_round_trip t t dim range - f = t split dim test_round_trip t f test_round_trip t f dim range - f = t split dim test_round_trip t f test_round_trip t f test_round_trip t f t = torch rand test_round_trip t t dim range - f = t split dim test_round_trip t f test_round_trip t f test_round_trip t f example where slice won t work selection t = torch ones b = t assertEqual try_use_slice t b None t = torch tensor assertEqual try_use_slice t t None assertEqual try_use_slice t t None t = torch tensor assertEqual try_use_slice t t None assertEqual try_use_slice t t None assertEqual try_use_slice t t None simple slice operations supported test_round_trip t t test_round_trip t t torch _dynamo config patch capture_dynamic_output_shape_ops=True torch _inductor config patch enable_auto_functionalized_v =True test_alias _dynamic=False torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y x sin_ y sin_ f x = torch ops aten alias default x b = x clone c = b nonzero float d = torch ops aten slice c d Tensor unbacked Symint shape torch ops mylib foo d d orig_args = torch randn aot_eager_args result graph_aot = run_aot_eager f orig_args _dynamic inductor_args result graph_inductor = run_inductor f orig_args _dynamic eager_args = pytree tree_map_only torch Tensor torch clone orig_args result = f eager_args assertEqual inductor_args eager_args assertEqual inductor_args aot_eager_args assertEqual result result assertEqual result result torch _dynamo config assume_static_by_default _dynamic assertExpectedInline graph_aot \ forward arg _ Sym s arg _ f s cpu clone f s cpu = torch ops aten clone default arg _ nonzero i u u cpu = torch ops aten nonzero default clone clone = None sym_size_int_ Sym u = torch ops aten sym_size int nonzero ge_ Sym u = = sym_size_int_ = sym_size_int_ = None _assert_scalar = torch ops aten _assert_scalar default ge_ Runtime assertion failed expression u = node ge ge_ = _assert_scalar = None _to_copy f u u cpu = torch ops aten _to_copy default nonzero dtype = torch float nonzero = None auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_alias = True _y_base_index = _y_alias = True _all_bases = arg _ _to_copy _to_copy = None getitem_ f s cpu = auto_functionalized_v getitem_ f u u cpu = auto_functionalized_v auto_functionalized_v = None copy_ f s cpu = torch ops aten copy_ default arg _ getitem_ arg _ = copy_ = None alias_ f s cpu = torch ops aten alias default getitem_ getitem_ = None slice_ f u u cpu = torch ops aten slice Tensor getitem_ getitem_ = None alias_ slice_ noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_aot \ forward arg _ f cpu clone f cpu = torch ops aten clone default arg _ nonzero i u u cpu = torch ops aten nonzero default clone clone = None sym_size_int Sym u = torch ops aten sym_size int nonzero ge_ Sym u = = sym_size_int = _assert_scalar = torch ops aten _assert_scalar default ge_ Runtime assertion failed expression u = node ge ge_ = _assert_scalar = None le Sym u = = sym_size_int = sym_size_int = None _assert_scalar_ = torch ops aten _assert_scalar default le Runtime assertion failed expression u = node le le = _assert_scalar_ = None _to_copy f u u cpu = torch ops aten _to_copy default nonzero dtype = torch float nonzero = None auto_functionalized_v = torch ops higher_order auto_functionalized_v torch ops mylib foo default _x_base_index = _x_alias = True _y_base_index = _y_alias = True _all_bases = arg _ _to_copy _to_copy = None getitem_ f cpu = auto_functionalized_v getitem_ f u u cpu = auto_functionalized_v auto_functionalized_v = None copy_ f cpu = torch ops aten copy_ default arg _ getitem_ arg _ = copy_ = None alias_ f cpu = torch ops aten alias default getitem_ getitem_ = None slice_ f u u cpu = torch ops aten slice Tensor getitem_ getitem_ = None alias_ slice_ noqa B ignore_comments=True ignore_empty_lines=True Run inductor backend torch _dynamo config assume_static_by_default _dynamic assertExpectedInline graph_inductor \ forward arg _ Sym s arg _ f s cpu nonzero i u u cpu = torch ops aten nonzero default arg _ sym_size_int_ Sym u = torch ops aten sym_size int nonzero ge_ Sym u = = sym_size_int_ = sym_size_int_ = None _assert_scalar = torch ops aten _assert_scalar default ge_ Runtime assertion failed expression u = node ge ge_ = _assert_scalar = None convert_element_type f u u cpu = torch ops prims convert_element_type default nonzero torch float nonzero = None alias_default f s cpu = torch ops aten alias default arg _ alias_default_ f u u cpu = torch ops aten alias default convert_element_type foo_default = torch ops mylib foo default alias_default alias_default_ alias_default = alias_default_ = foo_default = None copy_ f s cpu = torch ops aten copy_ default arg _ arg _ copy_ = None slice_ f u u cpu = torch ops aten slice Tensor convert_element_type convert_element_type = None arg _ slice_ noqa B ignore_comments=True ignore_empty_lines=True assertExpectedInline graph_inductor \ forward arg _ f cpu nonzero i u u cpu = torch ops aten nonzero default arg _ sym_size_int Sym u = torch ops aten sym_size int nonzero ge_ Sym u = = sym_size_int = _assert_scalar = torch ops aten _assert_scalar default ge_ Runtime assertion failed expression u = node ge ge_ = _assert_scalar = None le Sym u = = sym_size_int = sym_size_int = None _assert_scalar_ = torch ops aten _assert_scalar default le Runtime assertion failed expression u = node le le = _assert_scalar_ = None convert_element_type f u u cpu = torch ops prims convert_element_type default nonzero torch float nonzero = None alias_default f cpu = torch ops aten alias default arg _ alias_default_ f u u cpu = torch ops aten alias default convert_element_type foo_default = torch ops mylib foo default alias_default alias_default_ alias_default = alias_default_ = foo_default = None copy_ f cpu = torch ops aten copy_ default arg _ arg _ copy_ = None slice_ f u u cpu = torch ops aten slice Tensor convert_element_type convert_element_type = None arg _ slice_ noqa B ignore_comments=True ignore_empty_lines=True torch _inductor config patch enable_auto_functionalized_v =True test_alias _dynamic test_alias _dynamic=True Test view regeneration optimizations do result recompilations By comparing re-compilation eager backend recompilation inductor backend torch fx experimental _config patch use_duck_shape=False test_recompile torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo_impl x y pass run_and_compare func expected= counter_v = CompileCounterWithBackend inductor counter_v = CompileCounterWithBackend inductor v = torch compile func backend=counter_v fullgraph=True dynamic=True v = torch compile func backend=counter_v fullgraph=True dynamic=True inputs = torch rand torch rand torch rand torch rand torch _inductor config patch enable_auto_functionalized_v =True input inputs v input torch _dynamo reset torch _inductor config patch enable_auto_functionalized_v =False input inputs v input assertEqual counter_v frame_count counter_v frame_count assertEqual counter_v frame_count expected func x = x b = x torch ops mylib foo b run_and_compare func func x = torch ops aten alias default x b = torch ops aten alias default x torch ops mylib foo b run_and_compare func func x last row = x x size - first row b = x torch ops mylib foo b run_and_compare func func x = torch ops aten slice Tensor x b = torch ops aten slice Tensor x torch ops mylib foo b recompile here triggered auto_functionalize __recompiles - = L x size = torch ops aten slice Tensor x test inductor test_auto_functionalize py func _decomp decompositions py slice_forward run_and_compare func func x = torch ops aten alias default x b = x clone c = b nonzero float d = torch ops aten slice c d Tensor unbacked Symint shape torch ops mylib foo d d torch _dynamo config patch capture_dynamic_output_shape_ops=True run_and_compare func Test alias optimization alias called instead as_strided preserve fact id x = id base torch _inductor config patch enable_auto_functionalized_v =True unittest skip reason= This test fails because something inductor optimize out alias issue test_alias_id_input_to_custom_op torch library _scoped_library mylib FRAGMENT lib torch library define mylib not_eq Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib not_eq cpu lib=lib torch _dynamo disable not_eq_impl x y assertNotEqual id x id y func x = torch ops aten alias default x torch ops mylib not_eq x compiled = torch compile func backend= inductor fullgraph=True compiled torch rand Test alias optimization alias called instead as_strided preserve fact id x = id base torch _inductor config patch enable_auto_functionalized_v =True test_alias_id_output torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor b y - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo x y pass func x = torch ops aten alias default x torch ops mylib foo x compiled = torch compile func backend= inductor fullgraph=True input = torch rand output = compiled torch rand assertNotEqual id output id input test_inference_mode_view torch library custom_op test_inference_mode_view foo mutates_args= workspace foo x torch Tensor workspace torch Tensor - torch Tensor x clone foo register_fake _ x workspace x clone torch compile fullgraph=True backend= aot_eager f x w y = foo x w z = y view - z sin x = torch randn w = torch randn torch inference_mode y = f x w assertEqual y x sin torch _inductor config patch enable_auto_functionalized_v =True test_scheduling_with_multiple_mutates torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x Tensor y Tensor z - tags=torch Tag pt _compliant_tag lib=lib torch library impl mylib foo cpu lib=lib torch _dynamo disable foo x y z pass func x w = torch empty_like x buf b = torch empty_like x buf torch ops mylib foo b x buf buf buf c = torch mm w buf torch ops mylib foo c b x buf buf buf c input = torch rand weight = torch rand inductor_args output graph_inductor = run_inductor func input weight False torch _inductor scheduler compute_dependencies name_to_users = eval graph_inductor assertNotEqual name_to_users buf name_to_users buf __name__ == __main__ torch _inductor test_case run_tests run_tests