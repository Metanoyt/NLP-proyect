collections logging operator collections defaultdict typing Any Callable Literal TypeAlias torch torch distributed dist torch utils _pytree pytree torch _dispatch python enable_python_dispatcher torch _dynamo utils detect_fake_mode torch _inductor runtime runtime_utils dynamo_timed torch _logging trace_structured torch fx experimental proxy_tensor make_fx torch utils _ordered_set OrderedSet logger logging Logger = logging getLogger __name__ logger setLevel logging INFO BucketMode TypeAlias = Literal default custom_ops custom_ops_multidtype Helper functions moved top better organization _ag_group_key node torch fx Node - tuple str torch dtype type ignore name-defined _ group_size group_name = node args dtype = node meta val dtype assert isinstance group_name str group_name dtype _ag_group_key_multidtype node torch fx Node - tuple str _ group_size group_name = node args assert isinstance group_name str group_name _rs_group_key node torch fx Node - tuple str str torch dtype type ignore name-defined _ reduce_op group_size group_name = node args dtype = node meta val dtype assert isinstance group_name str assert isinstance reduce_op str group_name reduce_op dtype _ar_group_key node torch fx Node - tuple str str torch dtype _ reduce_op group_name = node args dtype = node meta val dtype assert isinstance group_name str assert isinstance reduce_op str group_name reduce_op dtype bucket_key node torch fx Node mode BucketMode &#124; None = None - object &#124; None is_all_gather_into_tensor node group_key_fn = _ag_group_key_multidtype mode multidtype mode _ag_group_key group_key_fn node is_reduce_scatter_tensor node _rs_group_key node is_all_reduce_tensor node _ar_group_key node None pick_bucket_dtype dtypes list torch dtype - torch dtype type ignore name-defined assert len dtypes min dtypes key=operator attrgetter itemsize bucket_cap_mb_by_bucket_idx_default bucket_id int - float Determine size bucket based its ID Args bucket_id int The ID bucket Returns float The size bucket bucket_all_gather gm torch fx GraphModule bucket_cap_mb_by_bucket_idx Callable int float &#124; None = None mode BucketMode = default - None bucket_cap_mb_by_bucket_idx None torch _inductor fx_passes bucketing pyrefly ignore missing-module-attribute bucket_cap_mb_by_bucket_idx_default bucket_cap_mb_by_bucket_idx = bucket_cap_mb_by_bucket_idx_default ag_buckets = bucket_all_gather_by_mb gm bucket_cap_mb_by_bucket_idx None mode len ag_buckets == merge_all_gather gm ag_buckets mode bucket_reduce_scatter gm torch fx GraphModule bucket_cap_mb_by_bucket_idx Callable int float &#124; None = None mode BucketMode = default - None bucket_cap_mb_by_bucket_idx None torch _inductor fx_passes bucketing pyrefly ignore missing-module-attribute bucket_cap_mb_by_bucket_idx_default bucket_cap_mb_by_bucket_idx = bucket_cap_mb_by_bucket_idx_default rs_buckets = bucket_reduce_scatter_by_mb gm bucket_cap_mb_by_bucket_idx None mode len rs_buckets == merge_reduce_scatter gm rs_buckets mode is_all_gather_into_tensor node torch fx Node - bool type ignore arg-type node op == call_function node target torch ops _c d_functional all_gather_into_tensor default is_reduce_scatter_tensor node torch fx Node - bool node op == call_function node target torch ops _c d_functional reduce_scatter_tensor default is_wait_tensor node torch fx Node - bool node op == call_function node target torch ops _c d_functional wait_tensor default is_all_reduce_tensor node torch fx Node - bool node op == call_function node target torch ops _c d_functional all_reduce default is_wait_tensor_from_all_gather_into_tensor node torch fx Node - bool is_wait_tensor node is_all_gather_into_tensor node args type ignore arg-type collect_node_descendants graph torch fx Graph - dict torch fx Node OrderedSet torch fx Node Collects descendants each node graph Args graph torch fx Graph The graph collect descendants Returns dict torch fx Node OrderedSet torch fx Node A dictionary mapping each node its descendants node_descendants dict torch fx Node OrderedSet torch fx Node = collections defaultdict OrderedSet outdegree = collections defaultdict int queue = node graph nodes n_outdegree = len node users n_outdegree == queue append node outdegree node = len node users while queue node = queue pop input_node node all_input_nodes node_descendants input_node &#124; = node_descendants node node_descendants input_node add node outdegree input_node -= outdegree input_node == queue append input_node node_descendants greedy_bucket_collective_by_mb gm torch fx GraphModule bucket_cap_mb_by_bucket_idx Callable int float filter_node Callable torch fx Node bool node_group_key Callable torch fx Node Any filter_wait_node Callable torch fx Node bool &#124; None = None - list list torch fx Node Bucketing adjacent collectives equal node_group_key We can bucket non adjacent collectives will effectively change order collectives Reordering can lead different order different ranks g = gm graph found_candidates = False node g nodes filter_node node found_candidates = True break found_candidates TODO pearce kelly algorithm detecting cycles node_descendents = collect_node_descendants gm graph nodes_groups list list torch fx Node = cur_group list torch fx Node = cur_group_key = None node g nodes is_wait_tensor node filter_node node args filter_wait_node None filter_wait_node node coll_node = node args group_key = node_group_key coll_node group_key == cur_group_key cur_group append coll_node len cur_group nodes_groups append cur_group cur_group = coll_node cur_group_key = group_key len cur_group nodes_groups append cur_group buckets list list torch fx Node = nodes nodes_groups cur_bucket list torch fx Node = cur_bucket_descendents OrderedSet torch fx Node = OrderedSet cur_bucket_size_bytes int = cur_bucket_id int = bucket_size_bytes = int bucket_cap_mb_by_bucket_idx cur_bucket_id node nodes node cur_bucket_descendents there path node current bucket we cannot horizontally fuse bucket continue assert val node meta n_val = node meta val out_size_bytes = n_val numel n_val element_size n_input_val = node all_input_nodes meta val in_size_bytes = n_input_val numel n_input_val element_size size_bytes = max out_size_bytes in_size_bytes cur_bucket_size_bytes + size_bytes bucket_size_bytes cur_bucket Current bucket full create new bucket len cur_bucket buckets append cur_bucket cur_bucket = cur_bucket_size_bytes = cur_bucket_id += cur_bucket_descendents = OrderedSet cur_bucket_size_bytes += size_bytes cur_bucket append node cur_bucket_descendents &#124; = node_descendents node len cur_bucket buckets append cur_bucket buckets bucket_all_gather_by_mb gm torch fx GraphModule bucket_cap_mb_by_bucket_idx Callable int float filter_wait_node Callable torch fx Node bool &#124; None = None mode BucketMode = default - list list torch fx Node Identifies all all_gather nodes groups them into buckets based size limit ` bucket_cap_mb_by_bucket_idx ` Args gm torch fx GraphModule GraphModule where bucket all_gathers bucket_cap_mb_by_bucket_idx Callable int float Callable specify cap bucket megabytes bucket idx The idea ` bucket_cap_mb_by_bucket_idx ` allow specify different sizes buckets start first all_gather usually exposed Interface bucket_cap_mb_by_bucket_idx ` bucket_cap_mb_by_bucket_idx_default ` function default value ` bucket_cap_mb_by_bucket_idx ` filter_wait_node Callable torch fx Node bool &#124; None If specified only all_gather nodes wait_node satisfy ` filter_wait_node ` will bucketed Returns list list torch fx Node List buckets where each bucket list all_gather nodes group_key_fn = _ag_group_key_multidtype mode multidtype mode _ag_group_key greedy_bucket_collective_by_mb gm bucket_cap_mb_by_bucket_idx is_all_gather_into_tensor group_key_fn filter_wait_node bucket_reduce_scatter_by_mb gm torch fx GraphModule bucket_cap_mb_by_bucket_idx Callable int float filter_wait_node Callable torch fx Node bool &#124; None = None mode BucketMode = default - list list torch fx Node Identifies all reduce_scatter nodes groups them into buckets based size limit ` bucket_cap_mb_by_bucket_idx ` Args gm torch fx GraphModule GraphModule where bucket reduce_scatters bucket_cap_mb_by_bucket_idx Callable int float Callable specify cap bucket megabytes bucket idx The idea ` bucket_cap_mb_by_bucket_idx ` allow specify different sizes buckets filter_wait_node Callable torch fx Node bool &#124; None If specified only reduce_scatter nodes wait_node satisfy ` filter_wait_node ` will bucketed Returns list list torch fx Node List buckets where each bucket list reduce_scatter nodes assert multidtype mode reduce scatter bucketing does support multidtype greedy_bucket_collective_by_mb gm bucket_cap_mb_by_bucket_idx is_reduce_scatter_tensor _rs_group_key filter_wait_node bucket_all_reduce_by_mb gm torch fx GraphModule bucket_cap_mb_by_bucket_idx Callable int float filter_wait_node Callable torch fx Node bool &#124; None = None - list list torch fx Node greedy_bucket_collective_by_mb gm bucket_cap_mb_by_bucket_idx is_all_reduce_tensor _ar_group_key filter_wait_node bucket_all_reduce gm torch fx GraphModule bucket_cap_mb_by_bucket_idx Callable int float &#124; None = None mode str &#124; None = None - None bucket_cap_mb_by_bucket_idx None torch _inductor fx_passes bucketing pyrefly ignore missing-module-attribute bucket_cap_mb_by_bucket_idx_default bucket_cap_mb_by_bucket_idx = bucket_cap_mb_by_bucket_idx_default ar_buckets = bucket_all_reduce_by_mb gm bucket_cap_mb_by_bucket_idx len ar_buckets == bucket ar_buckets merge_all_reduce_bucket gm graph bucket mode torch library custom_op bucketing _pre_bucket_reduce_scatter mutates_args= _pre_bucket_reduce_scatter rs_ins list torch Tensor group_size int - torch Tensor rs_ins_flattened = x view group_size - x rs_ins new_rs_in = torch cat rs_ins_flattened dim= flatten new_rs_in _pre_bucket_reduce_scatter_fake rs_ins list torch Tensor group_size int - torch Tensor out_numel = sum rs_in numel rs_in rs_ins torch empty out_numel device=rs_ins device dtype=rs_ins dtype _pre_bucket_reduce_scatter register_fake _pre_bucket_reduce_scatter_fake reduce_scatter_merge_fn_to_trace_custom_ops rs_ins list torch Tensor group_size int group_name str reduce_op str reduce_dtype torch dtype type ignore name-defined device torch device type ignore name-defined - list torch Tensor type ignore no-untyped-def new_out_sizes = x shape group_size + x shape x rs_ins new_out_numels = x numel group_size x rs_ins new_rs_in = torch ops bucketing _pre_bucket_reduce_scatter rs_ins group_size TODO - either use torch cat make sure inductor foreach codegen fires more reliably new_rs_out = torch ops c d_functional wait_tensor torch ops _c d_functional reduce_scatter_tensor default new_rs_in reduce_op group_size group_name new_out_flat = new_rs_out split new_out_numels new_outs = x view s x s zip new_out_flat new_out_sizes new_outs reduce_scatter_merge_fn_to_trace rs_ins list torch Tensor group_size int group_name str reduce_op str reduce_dtype torch dtype type ignore name-defined device torch device type ignore name-defined - list torch Tensor type ignore no-untyped-def rs_ins_flattened = x view group_size - x rs_ins new_out_sizes = x shape group_size + x shape x rs_ins new_out_numels = x numel group_size x rs_ins new_rs_in = torch cat rs_ins_flattened dim= flatten new_rs_out = torch ops c d_functional wait_tensor torch ops _c d_functional reduce_scatter_tensor default new_rs_in reduce_op group_size group_name new_out_flat = new_rs_out split new_out_numels new_outs = x view s x s zip new_out_flat new_out_sizes new_outs all_reduce_merge_fn_to_trace ar_ins list torch Tensor group_name str reduce_op str reduce_dtype torch dtype type ignore name-defined device torch device type ignore name-defined - list torch Tensor type ignore no-untyped-def ar_ins_flattened = x view - x ar_ins new_ar_in = torch cat ar_ins_flattened new_ar_out = torch ops c d_functional wait_tensor torch ops _c d_functional all_reduce default new_ar_in reduce_op group_name split_sizes = x numel x ar_ins new_outs_flat = new_ar_out split split_sizes new_outs = x view ar_in shape x ar_in zip new_outs_flat ar_ins new_outs torch library custom_op bucketing _pre_bucket_all_gather mutates_args= _pre_bucket_all_gather ag_ins list torch Tensor group_size int group_name str dtype torch dtype type ignore name-defined rank int - torch Tensor ins_split_sizes_bytes = ag_in numel ag_in element_size ag_in ag_ins bucket_dtype_size_bytes = dtype itemsize ins_split_sizes = _bytes bucket_dtype_size_bytes _bytes ins_split_sizes_bytes ag_input_numel = sum ins_split_sizes device = ag_ins device new_ag_out = torch empty ag_input_numel group_size dtype=dtype device=device new_ag_in = new_ag_out narrow ag_input_numel rank ag_input_numel foreach_copy_dsts = torch split new_ag_in ins_split_sizes ag_ins_flattened = ag_in reshape - view dtype ag_in ag_ins torch _foreach_copy_ foreach_copy_dsts ag_ins_flattened new_ag_out _pre_bucket_all_gather_fake ag_ins list torch Tensor group_size int group_name str dtype torch dtype type ignore name-defined rank int - torch Tensor ins_split_sizes_bytes = ag_in numel ag_in element_size ag_in ag_ins bucket_dtype_size_bytes = dtype itemsize ins_split_sizes = _bytes bucket_dtype_size_bytes _bytes ins_split_sizes_bytes ag_input_numel = sum ins_split_sizes device = ag_ins device new_ag_out = torch empty ag_input_numel group_size dtype=dtype device=device new_ag_out _pre_bucket_all_gather register_fake _pre_bucket_all_gather_fake all_gather_merge_fn_to_trace_custom_ops _ag_ins list torch Tensor group_size int group_name str dtype torch dtype type ignore name-defined out_dtypes list torch dtype type ignore name-defined rank int - list torch Tensor ag_ins = torch _prims convert_element_type _ag_in out_dtype _ag_in dtype = out_dtype _ag_in _ag_in out_dtype zip _ag_ins out_dtypes ins_sizes = ag_in shape ag_in ag_ins ins_split_sizes_bytes = ag_in numel out_dtype itemsize ag_in out_dtype zip ag_ins out_dtypes bucket_dtype_size_bytes = dtype itemsize ins_split_sizes = _bytes bucket_dtype_size_bytes _bytes ins_split_sizes_bytes ag_input_numel = sum ins_split_sizes new_ag_out = torch ops bucketing _pre_bucket_all_gather ag_ins group_size group_name dtype rank new_ag_in = new_ag_out narrow ag_input_numel rank ag_input_numel wait_tensor = torch ops c d_functional wait_tensor torch ops _c d_functional all_gather_into_tensor_out default new_ag_in group_size group_name out=new_ag_out new_ag_out_reshaped = wait_tensor reshape group_size - outs_bucket_dtype = torch split_with_sizes new_ag_out_reshaped ins_split_sizes dim= outs_reshaped = o view out_dtype reshape shape group_size + shape o shape out_dtype zip outs_bucket_dtype ins_sizes out_dtypes outs_reshaped all_gather_merge_fn_to_trace ag_ins list torch Tensor group_size int group_name str dtype torch dtype type ignore name-defined out_dtypes list torch dtype type ignore name-defined rank int - list torch Tensor ins_sizes = ag_in shape ag_in ag_ins ins_split_sizes = ag_in numel ag_in ag_ins ag_input_numel = sum ins_split_sizes device = ag_ins device new_ag_out = torch empty ag_input_numel group_size dtype=dtype device=device new_ag_in = new_ag_out narrow ag_input_numel rank ag_input_numel foreach_copy_dsts = torch split new_ag_in ins_split_sizes ag_ins_flattened = ag_in reshape - ag_in ag_ins torch _foreach_copy_ foreach_copy_dsts ag_ins_flattened wait_tensor = torch ops c d_functional wait_tensor torch ops _c d_functional all_gather_into_tensor_out default new_ag_in group_size group_name out=new_ag_out new_ag_out_reshaped = wait_tensor reshape group_size - outs = torch split_with_sizes new_ag_out_reshaped ins_split_sizes dim= outs_reshaped = o reshape shape group_size + shape o shape zip outs ins_sizes outs_reshaped all_gather_merge_fn_to_trace_functional ag_ins list torch Tensor group_size int group_name str dtype torch dtype type ignore name-defined out_dtypes list torch dtype type ignore name-defined rank int use_fsdp_ag_copy_in bool = False - list torch Tensor Implementation functional graph uses custom op torch ops fsdp all_gather_copy_in ins_sizes = ag_in shape ag_in ag_ins ins_split_sizes = ag_in numel ag_in ag_ins ag_input_numel = sum ins_split_sizes device = ag_ins device new_ag_out = torch empty ag_input_numel group_size dtype=dtype device=device ag_ins_flattened = ag_in reshape - ag_in ag_ins use_fsdp_ag_copy_in new_ag_in new_ag_out = torch ops fsdp all_gather_copy_in ag_ins_flattened new_ag_out ins_split_sizes ag_input_numel rank new_ag_in = torch cat ag_ins_flattened dim= wait_tensor = torch ops c d_functional wait_tensor torch ops _c d_functional all_gather_into_tensor_out default new_ag_in group_size group_name out=new_ag_out new_ag_out_reshaped = wait_tensor reshape group_size - outs = torch split_with_sizes new_ag_out_reshaped ins_split_sizes dim= outs_reshaped = o reshape shape group_size + shape o shape zip outs ins_sizes outs_reshaped _trace fn inps - torch fx GraphModule type ignore no-untyped-def dynamo_timed fx bucketing _trace log_pt _compile_event=True fake_mode = detect_fake_mode inps assert fake_mode None fake_mode enable_python_dispatcher out = make_fx fn inps node out graph find_nodes op= call_function target=torch ops aten detach default node replace_all_uses_with node args out graph erase_node node out _insert_fn_trace_before_node type ignore no-untyped-def g torch fx Graph fn_to_trace inps insert_before_node torch fx Node g_fn_inps list torch fx Node g_fn_outs list torch fx Node - tuple dict torch fx Node torch fx Node list torch fx Node type ignore no-untyped-def Helper function traces attr ` fn_to_trace ` inputs attr ` inps ` The result function graph will inserted before attr ` insert_before_node ` using attr ` g_fn_inps ` nodes original graph inputs function graph function graph outputs will replace attr ` g_fn_outs ` original graph Returns replacements new_nodes Dictionary mapping old new nodes list all newly inserted nodes dynamo_timed fx bucketing _insert_fn_trace_before_node log_pt _compile_event=True fn_gm = _trace fn_to_trace inps fn_g = fn_gm graph fn_g_ins = fn_g find_nodes op= placeholder env = fn_g_ins idx g_fn_inps idx idx range len g_fn_inps g_fn_new_outs list torch fx Node = new_nodes list torch fx Node = Track all newly inserted nodes g inserting_before insert_before_node _n fn_g nodes _n op == placeholder continue _new_n = g node_copy _n lambda x env x env _n = _new_n _n op == output g_fn_new_outs = _new_n args type ignore assignment g erase_node _new_n new_nodes append _new_n Track non-output nodes replacements = noqa C orig_out new_out orig_out new_out zip g_fn_outs g_fn_new_outs orig_out new_out zip g_fn_outs g_fn_new_outs orig_out replace_all_uses_with new_out replacements new_nodes process_collective_bucket g torch fx Graph bucket_nodes list torch fx Node fn_to_trace Callable list torch Tensor trace_args_fn Callable list torch fx Node tuple Any insert_before torch fx Node &#124; None = None wait_insertion_point torch fx Node &#124; None = None - tuple list torch fx Node dict torch fx Node torch fx Node Process single bucket collective operation nodes flexible insertion control Args g The graph modify bucket_nodes Nodes current bucket process fn_to_trace Function trace insert trace_args_fn Function create trace arguments inputs insert_before Where insert traced function default after last bucket node wait_insertion_point If provided move all nodes wait onwards before node Returns new_nodes List all newly inserted nodes replacements Dictionary mapping old wait nodes new output nodes Collect inputs waits current bucket bucket_ins list torch fx Node = bucket_waits list torch fx Node = ag_node_to_pre_nodes dict torch fx Node list torch fx Node = defaultdict list n bucket_nodes assert len n users == f Expected single user n got n users wait_n = next iter n users Handle convert_element_type operations all_gather node_in = n args is_all_gather_into_tensor n isinstance node_in torch fx Node Add type check node_in op == call_function node_in target torch ops prims convert_element_type default len node_in users == ag_node_to_pre_nodes n append node_in node_in = node_in args assert isinstance node_in torch fx Node Ensure node_in Node bucket_ins append node_in bucket_waits append wait_n Create trace arguments trace_args = trace_args_fn bucket_ins Determine insertion point insert_before None insert_before = bucket_nodes - next Insert traced function get replacements + new nodes replacements new_nodes = _insert_fn_trace_before_node g fn_to_trace trace_args insert_before bucket_ins bucket_waits If requested move wait nodes everything after specified location wait_insertion_point None Find first wait node new_nodes wait_start_idx = None i node enumerate new_nodes is_wait_tensor node wait_start_idx = i break Move all nodes wait onwards including wait wait_start_idx None nodes_to_move = new_nodes wait_start_idx node nodes_to_move wait_insertion_point prepend node Erase old nodes node wait_n zip bucket_nodes bucket_waits g erase_node wait_n g erase_node node Erase any convert_element_type nodes we tracked pre_node reversed ag_node_to_pre_nodes node g erase_node pre_node new_nodes replacements merge_reduce_scatter_bucket g torch fx Graph rs_nodes list torch fx Node mode BucketMode = default insert_before torch fx Node &#124; None = None wait_insertion_point torch fx Node &#124; None = None - tuple list torch fx Node dict torch fx Node torch fx Node Validate bucket consistency rs = rs_nodes rs _val = rs meta val _ reduce_op group_size group_name = rs args reduce_dtype = rs _val dtype device = rs _val device n rs_nodes rs_val = n meta val assert n args == reduce_op n args == group_size n args == group_name rs_val device == device rs_val dtype == reduce_dtype Choose merge function based mode rs_merge_fn = reduce_scatter_merge_fn_to_trace mode custom_ops mode rs_merge_fn = reduce_scatter_merge_fn_to_trace_custom_ops Process bucket lazy input collection create_trace_args bucket_ins list torch fx Node - tuple Any pytree tree_map lambda node node meta val bucket_ins group_size group_name reduce_op reduce_dtype device process_collective_bucket g rs_nodes rs_merge_fn create_trace_args insert_before=insert_before wait_insertion_point=wait_insertion_point merge_all_reduce_bucket g torch fx Graph ar_nodes list torch fx Node mode str &#124; None = None insert_before torch fx Node &#124; None = None wait_insertion_point torch fx Node &#124; None = None - tuple list torch fx Node dict torch fx Node torch fx Node ar = ar_nodes ar _val = ar meta val _ reduce_op group_name = ar args reduce_dtype = ar _val dtype device = ar _val device n ar_nodes ar_val = n meta val assert n args == reduce_op n args == group_name ar_val device == device ar_val dtype == reduce_dtype ar_merge_fn = all_reduce_merge_fn_to_trace create_trace_args bucket_ins list torch fx Node - tuple Any pytree tree_map lambda node node meta val bucket_ins group_name reduce_op reduce_dtype device process_collective_bucket g ar_nodes ar_merge_fn create_trace_args insert_before=insert_before wait_insertion_point=wait_insertion_point merge_all_gather_bucket g torch fx Graph ag_nodes list torch fx Node mode BucketMode = default insert_before torch fx Node &#124; None = None wait_insertion_point torch fx Node &#124; None = None - tuple list torch fx Node dict torch fx Node torch fx Node torch distributed distributed_c d _resolve_process_group ag = ag_nodes _ group_size group_name = ag args assert isinstance group_name str _ag_dtypes list torch dtype = type ignore name-defined n ag_nodes assert n args == group_size n args == group_name _ag_dtypes append n meta val dtype bucket_dtype = pick_bucket_dtype _ag_dtypes Choose merge function based mode ag_merge_fn = all_gather_merge_fn_to_trace mode None custom_ops mode ag_merge_fn = all_gather_merge_fn_to_trace_custom_ops type ignore assignment Process bucket lazy input collection rank int = dist get_rank _resolve_process_group group_name create_trace_args bucket_ins list torch fx Node - tuple Any pytree tree_map lambda node node meta val bucket_ins group_size group_name bucket_dtype _ag_dtypes rank process_collective_bucket g ag_nodes ag_merge_fn create_trace_args wait_insertion_point=wait_insertion_point merge_reduce_scatter gm torch fx GraphModule rs_buckets list list torch fx Node mode BucketMode = default - None Merges specified buckets reduce_scatter joint reduce_scatter dynamo_timed fx bucketing merge_reduce_scatter log_pt _compile_event=True trace_structured artifact metadata_fn=lambda name fx_bucketing_passes_reduce_scatter_buckets encoding string payload_fn=lambda str rs_buckets g = gm graph rs_nodes rs_buckets merge_reduce_scatter_bucket g rs_nodes mode merge_all_gather gm torch fx GraphModule ag_buckets list list torch fx Node mode BucketMode = default - None Merges specified buckets all_gather joint all_gather dynamo_timed fx bucketing merge_all_gather log_pt _compile_event=True trace_structured artifact metadata_fn=lambda name fx_bucketing_passes_all_gather_buckets encoding string payload_fn=lambda str ag_buckets g = gm graph ag_nodes ag_buckets merge_all_gather_bucket g ag_nodes mode