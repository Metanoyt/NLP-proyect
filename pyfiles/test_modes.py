Owner s module dynamo operator unittest mock patch torch torch _dynamo test_case torch _dynamo testing torch _C _len_torch_function_stack _pop_torch_function_stack _push_on_torch_function_stack torch _dynamo utils counters torch overrides _get_current_function_mode_stack BaseTorchFunctionMode TorchFunctionMode torch testing _internal common_utils skipIfXpu torch testing _internal inductor_utils GPU_TYPE torch testing _internal triton_utils requires_gpu torch utils _device DeviceContext torch utils _python_dispatch TorchDispatchMode device_type = acc type acc = torch accelerator current_accelerator True cpu TestMode BaseTorchFunctionMode __torch_function__ func types args kwargs=None kwargs kwargs = func == torch add torch zeros super __torch_function__ func types args kwargs HopDetectionError Exception pass TestModeRaises BaseTorchFunctionMode __torch_function__ func types args kwargs=None kwargs kwargs = torch _higher_order_ops func == torch _higher_order_ops flex_attention raise HopDetectionError test super __torch_function__ func types args kwargs TorchDispatchModeTests torch _dynamo test_case TestCase classmethod setUpClass cls super setUpClass classmethod tearDownClass cls super tearDownClass test_torch_dispatch_ignore_compile_internals counters clear torch utils _python_dispatch TorchDispatchMode torch library custom_op mylib foo mutates_args= foo x torch Tensor - torch Tensor x clone checksum x x abs sum _checksums = ChecksumFoo TorchDispatchMode classmethod ignore_compile_internals cls True __init__ - None super __init__ __torch_dispatch__ func types args kwargs=None kwargs = kwargs func torch ops mylib foo default Do some compute smoketest see there s bad interaction _checksums append args abs sum func args kwargs test e e Inductor smoketest torch _dynamo error_on_graph_break True torch compile backend= inductor g x x sin cos x = torch randn ChecksumFoo foo x g x foo x assertEqual len _checksums The correct result here Dynamo should capture ` g ` frame assertEqual counters frames total assertEqual counters frames ok test_skip_torch_dispatch_modes RewriteAddToMul TorchDispatchMode __torch_dispatch__ func types args= kwargs=None func torch ops aten add Tensor func = torch ops aten mul Tensor func args kwargs fn x x + x cnt = torch _dynamo testing CompileCounter x = torch tensor RewriteAddToMul eager_res = fn x compiled_res = torch compile fn backend=cnt x assertEqual eager_res compiled_res assertEqual cnt frame_count TorchFunctionModeTests torch _dynamo test_case TestCase classmethod setUpClass cls cls default_device_old = torch get_default_device super setUpClass classmethod tearDownClass cls torch set_default_device cls default_device_old super tearDownClass setUp torch set_default_device None torch _dynamo reset tearDown torch set_default_device None torch _dynamo reset _run_torch_function_mode_guard_test TestMode BaseTorchFunctionMode pass TestMode BaseTorchFunctionMode pass cnt = torch _dynamo testing CompileCounter torch compile backend=cnt __call__ fn x x + inp = torch ones fn inp assertEqual cnt frame_count TestMode fn inp assertEqual cnt frame_count TestMode TestMode fn inp assertEqual cnt frame_count TestMode TestMode fn inp assertEqual cnt frame_count TestMode fn inp assertEqual cnt frame_count torch _dynamo config patch enable_cpp_guard_manager False test_torch_function_mode_guards_py _run_torch_function_mode_guard_test test_torch_function_mode_guards_cpp _run_torch_function_mode_guard_test requires_gpu test_torch_function_mode_preserves_cuda_rng_state ConstantReturnMode TorchFunctionMode __torch_function__ func types args= kwargs=None - torch _dynamo optimize eager fn ConstantReturnMode assertEqual fn test_stack_state_mutation_default_device m = BaseTorchFunctionMode m = BaseTorchFunctionMode m m torch compile fullgraph=True fn x torch set_default_device cpu _pop_torch_function_stack fn torch ones _push_on_torch_function_stack m stack = _get_current_function_mode_stack assertIsInstance stack DeviceContext assertEqual stack device torch device cpu assertIs stack m assertIs stack m test_stack_state_clear_default_device torch compile fullgraph=True fn x torch set_default_device None x + fn torch ones stack = _get_current_function_mode_stack assertEqual len stack m = BaseTorchFunctionMode m = BaseTorchFunctionMode Stack populated add device m m torch compile fullgraph=True fn x torch set_default_device cpu torch set_default_device None torch set_default_device cpu x + fn torch ones stack = _get_current_function_mode_stack assertEqual stack device torch device cpu assertIs stack m assertIs stack m Stack populated remove device torch set_default_device cpu m m torch compile fullgraph=True fn x torch set_default_device None x + fn torch ones stack = _get_current_function_mode_stack assertIs stack m assertIs stack m torch compile fullgraph=True fn x torch set_default_device cpu torch set_default_device cpu x + fn torch ones stack = _get_current_function_mode_stack assertEqual stack device torch device cpu torch set_default_device None test_pop_torch_function_mode m = BaseTorchFunctionMode m torch compile fullgraph=True fn x _pop_torch_function_stack x + fn torch ones assertEqual _len_torch_function_stack reset stack so __exit__ doesn t crash _push_on_torch_function_stack m assertEqual _len_torch_function_stack test_is_torch_function_all_disabled torch compile fullgraph=True fn x torch _C _is_torch_function_all_disabled torch add x input = torch ones res _ = fn input assertFalse res test_error_empty_stack_pop_torch_function_mode torch compile fullgraph=True fn x _pop_torch_function_stack x + assertRaisesRegex torch _dynamo exc Unsupported Attempted pop empty torch function mode stack lambda fn torch ones test_push_torch_function_mode m = BaseTorchFunctionMode m torch compile fullgraph=True fn x m _push_on_torch_function_stack m x + fn torch ones m assertEqual _len_torch_function_stack reset stack state _pop_torch_function_stack assertEqual _len_torch_function_stack test_len_torch_function_mode m = BaseTorchFunctionMode m torch compile fullgraph=True fn x z = _len_torch_function_stack x + z res = fn torch ones assertEqual res torch ones + assertEqual _len_torch_function_stack test_intermedate_torch_function_mode_construction_mutation TestMode BaseTorchFunctionMode __init__ x x = x torch compile fullgraph=True fn x z = TestMode z y = x + z fn torch ones test_torch_function_mode_enabled_guard cnt = torch _dynamo testing CompileCounter inp = torch ones torch compile backend=cnt __call__ fn x x + BaseTorchFunctionMode torch _C DisableTorchFunctionSubclass torch _C DisableTorchFunction fn inp fn inp assertEqual cnt frame_count test_nested_torch_function_mode mode_ _called = False mode_ _called = False reset_state nonlocal mode_ _called nonlocal mode_ _called mode_ _called = False mode_ _called = False ones = torch ones zeros = torch zeros TestMode BaseTorchFunctionMode __torch_function__ func types args kwargs=None kwargs kwargs = nonlocal mode_ _called mode_ _called = True func == torch add zeros super __torch_function__ func types args kwargs TestMode BaseTorchFunctionMode __torch_function__ func types args kwargs=None kwargs kwargs = nonlocal mode_ _called mode_ _called = True func == torch mul ones super __torch_function__ func types args kwargs fn x torch add x fn_ x torch mul x + torch add x inp = torch ones + fn_i fn fn_ fn_opt = torch compile fn_i fullgraph=True TestMode TestMode expected = fn_i inp mode_ _called mode_ _called reset_state actual = fn_opt inp mode_ _called mode_ _called reset_state assertEqual expected actual test_torch_function_mode_disable TestSubclass torch Tensor classmethod __torch_function__ cls func types args kwargs=None kwargs kwargs = func == torch add torch ones super __torch_function__ func types args kwargs TestMode BaseTorchFunctionMode __torch_function__ func types args kwargs=None kwargs kwargs = func == torch add torch zeros super __torch_function__ func types args kwargs fn x torch add x inp = torch ones + as_subclass TestSubclass fn_opt = torch compile fn fullgraph=True TestMode torch _C DisableTorchFunctionSubclass expected = fn inp actual = fn_opt inp assertEqual expected actual torch _C DisableTorchFunction expected = fn inp actual = fn_opt inp assertEqual expected actual test_torch_function_mode_highest_priority TestSubclass torch Tensor classmethod __torch_function__ cls func types args kwargs=None kwargs kwargs = func == torch add torch ones super __torch_function__ func types args kwargs fn x torch add x inp = torch ones + as_subclass TestSubclass fn_opt = torch compile fn fullgraph=True TestMode expected = fn inp actual = fn_opt inp assertEqual expected actual test_torch_function_mode_enter_exit fn x y TestMode o = torch add x torch add o y inp = torch ones + torch ones + fn_opt = torch compile fn fullgraph=True expected = fn inp actual = fn_opt inp assertEqual expected actual test_torch_function_mode_graph_break fn x y TestMode torch _dynamo graph_break o = torch add x torch add o y inp = torch ones + torch ones + fn_opt = torch compile fn expected = fn inp actual = fn_opt inp assertEqual expected actual test_torch_function_mode_and_pop_graph_break fn x y TestMode z = _pop_torch_function_stack torch _dynamo graph_break _push_on_torch_function_stack z o = torch add x torch add o y inp = torch ones + torch ones + fn_opt = torch compile fn expected = fn inp actual = fn_opt inp assertEqual expected actual test_torch_function_mode_restore_on_exc torch _dynamo disable err raise RuntimeError test torch compile fn x TestMode x += err x += x try fn torch ones except RuntimeError pass assertEqual _len_torch_function_stack test_torch_function_mode_and_pop_graph_break_mutation fn x y TestMode z = _pop_torch_function_stack z y = torch _dynamo graph_break _push_on_torch_function_stack z o = torch add x o = torch mul o z y torch add o y inp = torch ones + torch ones + fn_opt = torch compile fn expected = fn inp actual = fn_opt inp assertEqual expected actual Needs larger cache size since we recompile each op patch object torch _dynamo config recompile_limit test_builtin_equivalent_funcs torch _dynamo variables builtin BUILTIN_TO_TENSOR_FN_MAP BUILTIN_TO_TENSOR_RFN_MAP torch _dynamo variables torch_function bin_int_ops bin_ops tensor_and_int_ops un_int_ops un_ops expected_func = None valid = False FuncEquivMode BaseTorchFunctionMode __torch_function__ func types args= kwargs=None nonlocal expected_func nonlocal valid kwargs kwargs = torch _dynamo is_compiling valid = expected_func == func super __torch_function__ func types args kwargs inp = torch ones inp = torch ones inp _int = torch ones dtype=torch int inp _int = torch ones dtype=torch int torch compile fullgraph=True fn_un op inp op inp torch compile fullgraph=True fn_un_int op inp op inp torch compile fullgraph=True fn_bin op inp inp op inp inp torch compile fullgraph=True fn_bin_int op inp inp op inp inp torch compile fullgraph=True fn_tensor_and_int op inp inp op inp inp setups_and_oplists = lambda o fn_un o inp un_ops lambda o fn_un_int o inp _int un_int_ops lambda o fn_bin o inp inp bin_ops lambda o fn_bin_int o inp _int inp _int bin_int_ops lambda o fn_tensor_and_int o inp _int tensor_and_int_ops gather reverse functions rsetups_and_oplists = lambda o fn_bin o inp bin_ops Get r ops ex __sub__ int Tensor - __rsub__ Tensor int lambda o fn_bin_int o inp _int bin_int_ops lambda o fn_tensor_and_int o inp _int tensor_and_int_ops skips = operator not_ Has local scalar dense call which graph breaks rskips = operator matmul operator imatmul operator getitem Doesn t type check reversed args run_checks setups_and_oplists skips ref_map nonlocal valid nonlocal expected_func setup_fn op_list setups_and_oplists op op_list op skips op ref_map continue FuncEquivMode expected_func = ref_map op setup_fn op assertTrue valid expected_func = None valid = False run_checks setups_and_oplists skips BUILTIN_TO_TENSOR_FN_MAP run_checks rsetups_and_oplists rskips BUILTIN_TO_TENSOR_RFN_MAP test_expand torch distributions AffineTransform ComposeTransform Normal TanhTransform TransformedDistribution https github com pytorch pytorch issues torch device cpu torch compile fullgraph=True func d = TransformedDistribution Normal ComposeTransform TanhTransform AffineTransform b = d log_prob d rsample b func torch randn requires_gpu test_flex_attention torch torch nn attention flex_attention create_block_mask flex_attention torch set_default_device device_type flex_attention = torch compile flex_attention dynamic=False prefix_lengths = torch arange prefix_lm b h q kv prefix_lengths b = kv This runs fullgraph already create_block_mask prefix_lm None _compile=True device=device_type test_register_hook functools my_hook grad k= grad + k hook = functools partial my_hook k= MyMod torch nn Module forward x x register_hook hook y = x mul z = y mul z mod = MyMod x = torch ones requires_grad=True torch device cpu torch compile mod fullgraph=True x requires_gpu skipIfXpu msg= XPU does support flex attention test_hop torch torch _higher_order_ops torch nn attention flex_attention flex_attention flex_attention_eager torch device GPU_TYPE flex_attention = torch compile flex_attention_eager dynamic=False assertRaisesRegex torch _dynamo exc Unsupported raised exception HopDetectionError ConstantVariable str test This runs fullgraph already TestModeRaises flex_attention torch ones torch ones torch ones requires_gpu skipIfXpu msg= XPU does support flex attention test_hop_eager torch torch _higher_order_ops torch nn attention flex_attention flex_attention flex_attention_eager torch device GPU_TYPE assertRaisesRegex torch _dynamo exc Unsupported raised exception HopDetectionError ConstantVariable str test TestModeRaises flex_attention_eager torch ones torch ones torch ones __name__ == __main__ torch _dynamo test_case run_tests run_tests