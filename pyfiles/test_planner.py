Owner s oncall distributed sys torch torch distributed checkpoint dcp torch nn nn torch distributed _shard sharded_tensor Shard ShardedTensor ShardedTensorMetadata ShardMetadata torch distributed _shard sharded_tensor metadata TensorProperties TensorProperties_Shard torch distributed checkpoint _dedup_save_plans dedup_save_plans torch distributed checkpoint api CheckpointException torch distributed checkpoint default_planner _create_default_local_metadata create_default_global_save_plan create_default_local_load_plan create_default_local_save_plan DefaultLoadPlanner DefaultSavePlanner torch distributed checkpoint filesystem CURRENT_DCP_VERSION torch distributed checkpoint metadata BytesStorageMetadata ChunkStorageMetadata MetadataIndex TensorProperties TensorStorageMetadata torch distributed checkpoint planner LoadItemType SavePlan SavePlanner WriteItemType torch distributed checkpoint planner_helpers _compare_save_plans _merge_delta_local_plans create_read_items_for_chunk_list torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN TestCase torch testing _internal distributed checkpoint_utils with_temp_dir torch testing _internal distributed distributed_utils with_dist with_fake_comms TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit create_sharded_tensor rank world_size shards_per_rank shard_size= shards_metadata = local_shards = idx range world_size shards_per_rank shard_rank = idx shards_per_rank shard_md = ShardMetadata shard_offsets= idx shard_size shard_sizes= shard_size placement=f rank shard_rank cpu shards_metadata append shard_md shard_rank == rank shard = Shard from_tensor_and_offsets torch rand shard_md shard_sizes shard_offsets=shard_md shard_offsets rank=rank local_shards append shard sharded_tensor_md = ShardedTensorMetadata shards_metadata=shards_metadata size=torch Size shard_size len shards_metadata tensor_properties=TensorProperties_Shard create_from_tensor torch zeros ShardedTensor _init_from_local_shards_and_global_metadata local_shards=local_shards sharded_tensor_metadata=sharded_tensor_md TestSavePlan TestCase with_fake_comms rank= world_size= test_local_plan tensor = torch rand val = st = create_sharded_tensor rank= world_size= shards_per_rank= state_dict = tensor tensor value val st st plan = create_default_local_save_plan state_dict False assertEqual len plan items wi = plan items assertEqual wi index MetadataIndex tensor assertEqual wi type WriteItemType TENSOR assertEqual wi tensor_data size tensor size assertEqual wi tensor_data properties TensorProperties create_from_tensor torch zeros assertEqual wi tensor_data chunk offsets torch Size assertEqual wi tensor_data chunk sizes torch Size st_wi = plan items assertEqual st_wi index MetadataIndex st assertEqual st_wi type WriteItemType SHARD assertEqual st_wi tensor_data size st size assertEqual st_wi tensor_data properties TensorProperties create_from_tensor torch zeros assertEqual st_wi tensor_data chunk offsets torch Size assertEqual st_wi tensor_data chunk sizes torch Size Coordinator rank should include replicated items well plan = create_default_local_save_plan state_dict True assertEqual len plan items tensor_wi = next wi wi plan items wi type == WriteItemType TENSOR assertEqual tensor_wi index MetadataIndex tensor assertEqual tensor_wi tensor_data size tensor size assertEqual tensor_wi tensor_data properties TensorProperties create_from_tensor tensor assertEqual tensor_wi tensor_data chunk offsets torch Size assertEqual tensor_wi tensor_data chunk sizes torch Size bytes_wi = next wi wi plan items wi type == WriteItemType BYTE_IO assertEqual bytes_wi index MetadataIndex value assertIsNone bytes_wi tensor_data with_fake_comms rank= world_size= test_local_plan_with_caching tensor = torch rand val = st = create_sharded_tensor rank= world_size= shards_per_rank= state_dict = tensor tensor value val st st planner = DefaultSavePlanner enable_plan_caching=True planner set_up_planner state_dict is_coordinator=False First iteration should create new plan first_plan = planner create_local_plan Validate plan has been cached cached_plan = SavePlanner _cached_save_plan planner _cached_plans_key assertEqual first_plan cached_plan second iteration should create empty unusable plan second_plan = planner create_local_plan assertFalse second_plan usable assertEqual len second_plan items assertIsNone second_plan planner_data assertIsNone second_plan storage_data test_global_plan create_data rank with_dist rank=rank world_size= tensor = torch rand val = st = create_sharded_tensor rank=rank world_size= shards_per_rank= state_dict = tensor tensor value val st st create_default_local_save_plan state_dict rank == all_plans = create_data create_data create_data create_data all_plans = dedup_save_plans all_plans final_plans metadata = create_default_global_save_plan all_plans=all_plans The default global plan updates all indexes include hints new_plan old_plan zip final_plans all_plans new_item old_item zip new_plan items old_plan items assertEqual new_item index old_item index assertEqual new_item type old_item type assertEqual new_item tensor_data old_item tensor_data assertIn new_item index fqn metadata state_dict_metadata item_md = metadata state_dict_metadata new_item index fqn new_item type == WriteItemType BYTE_IO assertTrue isinstance item_md BytesStorageMetadata assertTrue isinstance item_md TensorStorageMetadata assertEqual item_md size old_item tensor_data size assertEqual item_md properties old_item tensor_data properties assertIsNotNone new_item index index Make sure hint correct assertEqual item_md chunks new_item index index old_item tensor_data chunk test_dedup_plans create_data rank with_dist rank=rank world_size= tensor = torch rand val = st = create_sharded_tensor rank=rank world_size= shards_per_rank= state_dict = tensor tensor value val st st create_default_local_save_plan state_dict rank == all_plans = create_data create_data create_data create_data deduped_plans = dedup_save_plans all_plans Number plans should remain unchanged assertEqual len all_plans len deduped_plans Number items deduped plans should less than original plans new_plan old_plan zip deduped_plans all_plans assertFalse _compare_save_plans new_plan old_plan assertTrue len new_plan items len old_plan items test_global_plan_with_caching create_data rank with_dist rank=rank world_size= planner = DefaultSavePlanner enable_plan_caching=True tensor = torch rand val = st = create_sharded_tensor rank=rank world_size= shards_per_rank= state_dict = tensor tensor value val st st planner set_up_planner state_dict is_coordinator= rank == planner create_local_plan all_plans = create_data create_data create_data create_data planner = DefaultSavePlanner enable_plan_caching=True First iteration should create new plan first_global_plan first_metadata = planner create_global_plan all_plans Validate plan has been cached cached_global_plan = SavePlanner _cached_global_plan planner _cached_plans_key assertEqual cached_global_plan first_global_plan Validate all_plans cached cached_all_plans = SavePlanner _cached_all_plans planner _cached_plans_key assertEqual cached_all_plans all_plans Second iteration should empty plans Recreate plans previous ones deduped all_plans = create_data create_data create_data create_data second_global_plan second_metadata = planner create_global_plan all_plans All plans should empty usable plan second_global_plan assertFalse plan usable assertEqual len plan items assertIsNone plan planner_data assertIsNone plan storage_data assertEqual first_metadata second_metadata assertEqual second_metadata planner _cached_metadata planner _cached_plans_key Validate all_plans cached remain unchanged cached_all_plans = SavePlanner _cached_all_plans planner _cached_plans_key assertEqual cached_all_plans all_plans Third iteration changed plans create_data_v rank with_dist rank=rank world_size= planner = DefaultSavePlanner enable_plan_caching=True tensor = torch rand val = st = create_sharded_tensor rank=rank world_size= shards_per_rank= state_dict = tensor tensor value val st st planner set_up_planner state_dict is_coordinator= rank == planner create_local_plan all_plans = create_data_v create_data_v create_data_v create_data_v third_global_plan third_metadata = planner create_global_plan all_plans Only rank plan should non-empty The rest should empty tensor_plan = third_global_plan assertNotEqual len tensor_plan items assertTrue tensor_plan usable Validate all_plans updated cached cached_all_plans = SavePlanner _cached_all_plans planner _cached_plans_key assertEqual cached_all_plans all_plans plan third_global_plan assertFalse plan usable assertEqual len plan items assertIsNone plan planner_data assertIsNone plan storage_data Global metadata should different one plan has changed assertNotEqual second_metadata third_metadata Validate metadata cached assertEqual third_metadata planner _cached_metadata planner _cached_plans_key Validate new plan has been cached cached_global_plan = SavePlanner _cached_global_plan planner _cached_plans_key assertEqual cached_global_plan tensor_plan test_finish_plan_with_caching planner = DefaultSavePlanner enable_plan_caching=True tensor = torch rand val = state_dict = tensor tensor value val planner set_up_planner state_dict is_coordinator=True plan = planner create_local_plan First iteration should create new plan first_finished_plan = planner finish_plan plan Validate plan has been cached cached_finished_plan = SavePlanner _cached_final_save_plan planner _cached_plans_key assertEqual first_finished_plan cached_finished_plan second iteration should cached plan second_finished_plan = planner finish_plan SavePlan usable=False assertEqual second_finished_plan first_finished_plan test_local_load_plan create_state_dict rank with_dist rank=rank world_size= tensor = torch rand val = st = create_sharded_tensor rank=rank world_size= shards_per_rank= tensor tensor value val st st state_dict = create_state_dict metadata = _create_default_local_metadata state_dict load_plan = create_default_local_load_plan state_dict metadata This will create entries assertEqual len load_plan items st_item = next ri ri load_plan items ri dest_index fqn == st tensor_item = next ri ri load_plan items ri dest_index fqn == tensor bytes_item = next ri ri load_plan items ri dest_index fqn == value assertEqual st_item type LoadItemType TENSOR This exact copy assertEqual st_item dest_index MetadataIndex st assertEqual st_item dest_offsets torch Size assertEqual st_item storage_index MetadataIndex st assertEqual st_item storage_offsets torch Size assertEqual st_item lengths torch Size assertEqual tensor_item type LoadItemType TENSOR assertEqual tensor_item dest_index MetadataIndex tensor assertEqual tensor_item dest_offsets torch Size assertEqual tensor_item storage_index MetadataIndex tensor assertEqual tensor_item storage_offsets torch Size assertEqual tensor_item lengths torch Size assertEqual bytes_item type LoadItemType BYTE_IO assertEqual bytes_item dest_index MetadataIndex value test_load_with_resharding create_state_dict rank world_size with_dist rank=rank world_size=world_size st create_sharded_tensor rank=rank world_size=world_size shards_per_rank= shard_size= world_size Rank has bytes shard world _state_dict = create_state_dict rank= world_size= world _metadata = _create_default_local_metadata world _state_dict Rank has bytes shard world _state_dict = create_state_dict rank= world_size= world _metadata = _create_default_local_metadata world _state_dict First scenario going world= world= need load shards Each -world shard has elements so needs load shards load_plan = create_default_local_load_plan world _state_dict world _metadata assertEqual len load_plan items low_ri = next ri ri load_plan items ri dest_offsets == torch Size high_ri = next ri ri load_plan items ri dest_offsets == torch Size assertEqual low_ri storage_index MetadataIndex st assertEqual low_ri storage_offsets torch Size assertEqual low_ri dest_index MetadataIndex st assertEqual low_ri dest_offsets torch Size assertEqual low_ri lengths torch Size assertEqual high_ri storage_index MetadataIndex st assertEqual high_ri storage_offsets torch Size assertEqual high_ri dest_index MetadataIndex st assertEqual high_ri dest_offsets torch Size assertEqual high_ri lengths torch Size Second scenario going world= world= need load half shard rank -world needs load upper half rank -world shard load_plan = create_default_local_load_plan world _state_dict world _metadata assertEqual len load_plan items ri = load_plan items assertEqual ri storage_index MetadataIndex st assertEqual ri storage_offsets torch Size assertEqual ri dest_index MetadataIndex st assertEqual ri dest_offsets torch Size assertEqual ri lengths torch Size test_load_with_world_size_diff_by_one create_state_dict rank world_size with_dist rank=rank world_size=world_size st create_sharded_tensor rank=rank world_size=world_size shards_per_rank= shard_size= world_size rank has bytes shard world _state_dict = create_state_dict rank= world_size= world _metadata = _create_default_local_metadata world _state_dict rank has bytes shard world _state_dict = create_state_dict rank= world_size= load_plan = create_default_local_load_plan world _state_dict world _metadata assertEqual len load_plan items load low_ri = next ri ri load_plan items ri dest_offsets == torch Size load high_ri = next ri ri load_plan items ri dest_offsets == torch Size assertEqual low_ri storage_index MetadataIndex st assertEqual low_ri storage_offsets torch Size assertEqual low_ri dest_index MetadataIndex st assertEqual low_ri dest_offsets torch Size assertEqual low_ri lengths torch Size assertEqual high_ri storage_index MetadataIndex st assertEqual high_ri storage_offsets torch Size assertEqual high_ri dest_index MetadataIndex st assertEqual high_ri dest_offsets torch Size assertEqual high_ri lengths torch Size TestPlannerHelpers TestCase test_create_read_item_from_chunks tensor_md = TensorStorageMetadata properties=TensorProperties create_from_tensor torch empty size=torch Size chunks= ChunkStorageMetadata offsets=torch Size sizes=torch Size ChunkStorageMetadata offsets=torch Size sizes=torch Size chunk = ChunkStorageMetadata offsets=torch Size sizes=torch Size read_items = create_read_items_for_chunk_list foo tensor_md chunk assertEqual len read_items assertEqual MetadataIndex foo read_items dest_index assertEqual torch Size read_items dest_offsets assertEqual MetadataIndex foo read_items storage_index assertEqual torch Size read_items storage_offsets assertEqual torch Size read_items lengths assertEqual MetadataIndex foo read_items dest_index assertEqual torch Size read_items dest_offsets assertEqual MetadataIndex foo read_items storage_index assertEqual torch Size read_items storage_offsets assertEqual torch Size read_items lengths test_merge_delta_local_plans create_data rank with_dist rank=rank world_size= tensor = torch rand val = st = create_sharded_tensor rank=rank world_size= shards_per_rank= state_dict = tensor tensor value val st st create_default_local_save_plan state_dict rank == _validate_plans plan SavePlan plan SavePlan assertEqual len plan items len plan items item item zip plan items plan items assertEqual item index item index assertEqual item type item type assertEqual item tensor_data item tensor_data cached_plans = create_data create_data delta_plans = create_data create_data Both plans changed Merge plan should have both plans delta plans merged_plans = _merge_delta_local_plans cached_plans delta_plans assertEqual len merged_plans _validate_plans delta_plans merged_plans _validate_plans delta_plans merged_plans Only first plan changed Merge plan should have first plan delta plans second plan cached plans delta_plans = create_data SavePlan usable=False merged_plans = _merge_delta_local_plans cached_plans delta_plans _validate_plans delta_plans merged_plans _validate_plans cached_plans merged_plans Only second plan changed Merge plan should have first plan cached plans second plan delta plans delta_plans = SavePlan usable=False create_data merged_plans = _merge_delta_local_plans cached_plans delta_plans _validate_plans cached_plans merged_plans _validate_plans delta_plans merged_plans None plans changed Cached plans should returned delta_plans = SavePlan usable=False SavePlan usable=False merged_plans = _merge_delta_local_plans cached_plans delta_plans _validate_plans cached_plans merged_plans _validate_plans cached_plans merged_plans test_compare_save_plans create_data rank with_dist rank=rank world_size= tensor = torch rand val = st = create_sharded_tensor rank=rank world_size= shards_per_rank= state_dict = tensor tensor value val st st create_default_local_save_plan state_dict rank == plan = create_data plan = create_data assertFalse _compare_save_plans plan plan assertTrue _compare_save_plans plan plan assertTrue _compare_save_plans plan plan TestLoadPlanner TestCase with_temp_dir test_strict original_module = nn Linear dcp save state_dict= module original_module checkpoint_id=self temp_dir new_module = nn Linear new_module extra_param = nn Parameter torch randn dcp load state_dict= module new_module checkpoint_id=self temp_dir planner=DefaultLoadPlanner allow_partial_load=True assertRaisesRegex CheckpointException Missing key checkpoint dcp load state_dict= module new_module checkpoint_id=self temp_dir planner=DefaultLoadPlanner allow_partial_load=False with_temp_dir test_load_different_sizes_throws original_module = nn Linear dcp save state_dict= module original_module checkpoint_id=self temp_dir new_module = nn Linear assertRaisesRegex CheckpointException Size mismatch dcp load state_dict= module new_module checkpoint_id=self temp_dir planner=DefaultLoadPlanner with_temp_dir test_version_key_in_planner_data original_module = nn Linear dcp save state_dict= module original_module checkpoint_id=self temp_dir new_module = nn Linear planner = DefaultLoadPlanner dcp load state_dict= module new_module checkpoint_id=self temp_dir planner=planner assertEqual planner metadata version CURRENT_DCP_VERSION __name__ == __main__ run_tests