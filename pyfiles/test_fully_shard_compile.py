Owner s oncall distributed contextlib copy functools itertools logging unittest collections defaultdict unittest mock torch torch _dynamo testing torch nn functional F torch nn torch _dynamo utils counters torch _inductor comms torch _inductor utils is_fallback_op run_and_get_code torch distributed device_mesh init_device_mesh torch distributed fsdp fully_shard FullyShardedDataParallel FSDP ShardingStrategy torch distributed fsdp _fully_shard _fsdp_common TrainingState torch distributed fsdp _fully_shard _fsdp_param_group FSDPParamGroup torch testing FileCheck torch testing _internal common_distributed at_least_x_gpu skip_if_lt_x_gpu sm_is_or_higher_than torch testing _internal common_fsdp FSDPTest get_devtype MLP torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor ModelArgs Transformer torch testing _internal inductor_utils HAS_GPU device_type = torch device get_devtype log = logging getLogger __name__ _count_op_in_graph graph op sum node graph nodes node target op _is_fallback_op_in_snodes snodes op any is_fallback_op snode node op snode snodes orig_F_scaled_dot_product_attention = F scaled_dot_product_attention Mod torch nn Module __init__ super __init__ encoder = torch nn Sequential torch nn Linear device=device_type torch nn Linear device=device_type torch nn Linear device=device_type forward x encoder x TestFullyShardCompileCompute FSDPTest unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_disable_compiling_hooks run_subtests skip_fsdp_hooks False True _test_disable_compiling_hooks _test_disable_compiling_hooks skip_fsdp_hooks bool torch _dynamo reset trace_rules_check_count = HOOKS_FILE_NAME = torch distributed fsdp _fully_shard _fsdp_state py HOOK_WRAPPER_NAME = fsdp_hook_wrapper patched_trace_rules_check args kwargs nonlocal trace_rules_check_count f_code = args hasattr f_code co_filename f_code co_filename endswith HOOKS_FILE_NAME f_code co_name = HOOK_WRAPPER_NAME trace_rules_check_count += orig_trace_rules_check args kwargs original_skip_fsdp_hooks = torch _dynamo config skip_fsdp_hooks orig_trace_rules_check = torch _dynamo trace_rules check torch distributed barrier torch _dynamo config skip_fsdp_hooks = skip_fsdp_hooks torch _dynamo trace_rules check = patched_trace_rules_check model = MLP device_type fully_shard model model compile model torch randn device=device_type torch distributed barrier torch _dynamo config skip_fsdp_hooks = original_skip_fsdp_hooks torch _dynamo trace_rules check = orig_trace_rules_check skip_fsdp_hooks assertEqual trace_rules_check_count assertTrue trace_rules_check_count unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch TestFullyShardCompile FSDPTest fake_pg = at_least_x_gpu This method override base Tests requires bf support so SM arch must higher skipTestForOldSm Assumption This test only run GPU See ` HAS_GPU ` check top device = torch device device_type type rank torch get_device_module device_type device_count device_type type == cuda torch version hip sm_is_or_higher_than device skipTest bf requires sm = test_dynamo_trace_use_training_state torch _dynamo reset Construct dummy FSDPParamGroup since we just want test ` use_training_state ` ctx manager param_group = FSDPParamGroup params List nn Parameter torch nn Linear module Tuple nn Module None mesh_info FSDPMeshInfo None post_forward_mesh_info Optional FSDPMeshInfo device_type device torch device None shard_placement_fn Optional Callable None mp_policy MixedPrecisionPolicy None offload_policy OffloadPolicy f x param_group _training_state = TrainingState IDLE param_group use_training_state TrainingState FORWARD param_group _training_state == TrainingState FORWARD x + x inp = torch zeros assertEqual param_group _training_state TrainingState IDLE eager_out = f inp assertEqual param_group _training_state TrainingState IDLE assertEqual eager_out inp + cnt = torch _dynamo testing CompileCounterWithBackend aot_eager compiled_out = torch compile f backend=cnt fullgraph=True inp assertEqual param_group _training_state TrainingState IDLE assertEqual eager_out compiled_out assertEqual cnt frame_count assertEqual cnt op_count assertEqual len cnt graphs test_trace_fsdp_copy_ torch library custom_op mylib add_one_out mutates_args= out add_one_out x torch Tensor out torch Tensor - None torch add x out=out f x buf = torch zeros buf_view = buf view - torch ops mylib add_one_out x out=buf_view buf_view = buf view - torch ops fsdp copy_ x buf_view ref_x = torch zeros x = copy deepcopy ref_x f ref_x torch compile f backend= aot_eager x assertEqual x ref_x _get_resize_count_in_fx_graph graph torch fx Graph resize_count = node graph nodes node op == call_function node target == torch ops inductor resize_storage_bytes_ default resize_count += resize_count _assert_no_aliased_unsharded_params_in_graph_inputs model graph torch fx Graph - None FSDP unsharded params mutated graph without going through functionalization Therefore we want make sure they don t have aliases graph inputs make easier us do replacement unsharded params all-gathered temporary buffer directly downstream users graph storage_id_to_graph_inputs = defaultdict list unsharded_param_graph_inputs = set node graph nodes node op == call_function node target torch ops inductor resize_storage_bytes_ default torch ops fsdp copy_ default node args op == placeholder unsharded_param_graph_inputs add node args assert len unsharded_param_graph_inputs assert len unsharded_param_graph_inputs == len list model parameters \ Expected all model parameters wrapped FSDP have their unsharded version graph input s true no_aliased_unsharded_params_in_graph_inputs = True err_msg = aliased_graph_inputs storage_id_to_graph_inputs values len aliased_graph_inputs any x unsharded_param_graph_inputs x aliased_graph_inputs no_aliased_unsharded_params_in_graph_inputs = False err_msg += f \n Found aliased unsharded param graph inputs aliased_graph_inputs val shape node meta val shape node aliased_graph_inputs assertTrue no_aliased_unsharded_params_in_graph_inputs err_msg _remove_fsdp _unsharded_param_graph_input_usage_with_optional_checks model bwd_resize_count_before_pass=None fwd_fullgraph=False _run_with_checks graph orig_fn _is_bwd_fx_graph graph bwd_resize_count_before_pass None assertEqual bwd_resize_count_before_pass _get_resize_count_in_fx_graph graph _assert_no_aliased_unsharded_params_in_graph_inputs model graph orig_fn graph fwd_fullgraph mock patch object comms remove_fsdp _unsharded_param_graph_input_usage functools partial _run_with_checks orig_fn=comms remove_fsdp _unsharded_param_graph_input_usage contextlib nullcontext _check_fsdp_copy_and_resize_ops_count_in_graph graph fwd_copy_count fwd_resize_count bwd_copy_count bwd_resize_count _check_count copy_count resize_count actual_copy_count = _count_op_in_graph graph torch ops fsdp copy_ default assertEqual actual_copy_count copy_count f Unexpected number ` fsdp copy_ ` ops expected copy_count got actual_copy_count graph graph actual_resize_count = _count_op_in_graph graph torch ops inductor resize_storage_bytes_ default assertEqual actual_resize_count resize_count f Unexpected number ` inductor resize_storage_bytes_ ` ops expected resize_count got actual_resize_count graph graph noqa B torch _dynamo compiled_autograd in_compiled_autograd_region _check_count fwd_copy_count fwd_resize_count fwd graph _check_count bwd_copy_count bwd_resize_count bwd graph _reinplace_all_gather_with_optional_checks fwd_fullgraph _run_with_checks graph orig_fn world_size assertGreater _count_op_in_graph graph torch ops _c d_functional all_gather_into_tensor default world_size == assertEqual _count_op_in_graph graph torch ops _c d_functional all_gather_into_tensor default orig_fn graph assertEqual _count_op_in_graph graph torch ops _c d_functional all_gather_into_tensor default world_size assertGreater _count_op_in_graph graph torch ops _c d_functional all_gather_into_tensor_out default assertEqual _count_op_in_graph graph torch ops _c d_functional all_gather_into_tensor_out default fwd_fullgraph mock patch object comms reinplace_fsdp_all_gather functools partial _run_with_checks orig_fn=comms reinplace_fsdp_all_gather contextlib nullcontext _is_fwd_graph snodes ag_copy_in_snode = None snode snodes is_fallback_op snode node torch ops fsdp all_gather_copy_in default ag_copy_in_snode = snode break assertTrue ag_copy_in_snode None any dep name startswith primals_ dep ag_copy_in_snode read_writes reads True False _is_bwd_fx_graph graph node graph nodes node op == call_function node target == torch ops _c d_functional reduce_scatter_tensor default True False _maybe_run_decide_global_ordering_of_comms_with_checks fwd_fullgraph _check_fsdp_ops_in_snodes snodes is_fwd_graph expect=True assert_method = assertTrue expect assertFalse common_ops = torch ops fsdp all_gather_copy_in default torch ops _c d_functional all_gather_into_tensor_out default torch ops fsdp split_with_sizes_copy default bwd_only_ops = torch ops fsdp chunk_cat default torch ops _c d_functional reduce_scatter_tensor default op common_ops assert_method _is_fallback_op_in_snodes snodes op msg=f op is_fwd_graph op bwd_only_ops assert_method _is_fallback_op_in_snodes snodes op msg=f op _decide_global_ordering_of_comms_with_checks snodes name_to_buf name_to_fused_node orig_fn is_fwd_graph = _is_fwd_graph snodes _check_fsdp_ops_in_snodes snodes is_fwd_graph expect=True new_snodes = orig_fn snodes name_to_buf name_to_fused_node _check_fsdp_ops_in_snodes new_snodes is_fwd_graph expect=False new_snodes fwd_fullgraph mock patch object comms decide_global_ordering_of_comms functools partial _decide_global_ordering_of_comms_with_checks orig_fn=comms decide_global_ordering_of_comms contextlib nullcontext inductor_code_check_no_compute_op file_check file_check check_not = aten check_not = extern_kernels check_not = triton_ check_not = torch ops check_not = inductor_ops check_not aten check_not extern_kernels check_not triton_ check_not torch ops check_not inductor_ops inductor_code_check_fsdp_all_gather file_check overlapped_compute_op_str last_all_gather=False file_check = file_check check torch ops fsdp all_gather_copy_in file_check = inductor_code_check_no_compute_op file_check file_check = file_check check torch ops _c d_functional all_gather_into_tensor_out Checks AGWait delayed making AG overlap some compute op overlapped_compute_op_str None file_check = file_check check f overlapped_compute_op_str file_check = file_check check torch ops _c d_functional wait_tensor file_check = inductor_code_check_no_compute_op file_check file_check = file_check check torch ops fsdp split_with_sizes_copy last_all_gather Checks there no compute op between AGWait next AG file_check = inductor_code_check_no_compute_op file_check file_check inductor_code_check_fsdp_reduce_scatter file_check overlapped_compute_op_str file_check = file_check check torch ops fsdp chunk_cat file_check = inductor_code_check_no_compute_op file_check file_check = file_check check torch ops _c d_functional reduce_scatter_tensor Checks RSWait delayed making RS overlap some compute op overlapped_compute_op_str None file_check = file_check check f overlapped_compute_op_str file_check = file_check check torch ops _c d_functional wait_tensor file_check unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_compiled_autograd_ctx skipTestForOldSm torch _dynamo config patch skip_fsdp_hooks=False torch _functorch config patch recompute_views=True inputs = torch randn model = torch nn Linear fully_shard model model_compiled = torch compile model backend= inductor i range torch compiler set_stance force_eager i default eager warmup iteration torch _dynamo compiled_autograd _enable torch compile backend= inductor fullgraph=True out = model_compiled inputs out sum backward _test_traceable_fsdp model_init_fn input_creation_fn backend fwd_fullgraph bwd_resize_count_before_inductor=None fwd_bwd model inp out = model inp loss = out sum loss backward loss run_iters fwd_bwd_func optim n_iter= compiled_autograd_backend=None torch manual_seed losses = i range n_iter eager warmup iteration so all FSDP lazy-initialization done eager torch compiler set_stance force_eager i default inp = input_creation_fn loss = fwd_bwd_func inp losses append loss item optim step optim zero_grad set_to_none=True losses test_compiled model optim = model_init_fn fwd_bwd_fn = functools partial fwd_bwd model counters clear _remove_fsdp _unsharded_param_graph_input_usage_with_optional_checks model bwd_resize_count_before_pass=bwd_resize_count_before_inductor fwd_fullgraph=fwd_fullgraph fwd_bwd_fn_compiled = torch compile fwd_bwd_fn backend=backend NOTE we can t set ` fullgraph=True ` here because we will always graph-break ` loss backward ` call ` fwd_bwd ` This okay long s only graph-break forward pass fullgraph=False res = run_iters fwd_bwd_fn_compiled optim compiled_autograd_backend=backend fwd_fullgraph assertEqual len counters graph_break assertExpectedInline next iter counters graph_break keys \ Unsupported Tensor backward call Explanation Dynamo currently does support tracing ` Tensor backward ` Hint This graph break fundamental - unlikely Dynamo will ever able trace through your code Consider finding workaround Developer debug context call_method TensorVariable backward For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html noqa B assertGreater len counters graph_break res test_eager model optim = model_init_fn fwd_bwd_fn = functools partial fwd_bwd model res = run_iters fwd_bwd_fn optim res torch _dynamo reset torch _dynamo compiled_autograd reset torch _dynamo config patch compiled_autograd=True compiled_autograd_kwargs_override= fullgraph True inline_inbuilt_nn_modules=True skip_fsdp_hooks=False torch _functorch config patch enable_autograd_cache=False recompute_views=True torch _inductor config patch force_disable_caches=True reorder_for_compute_comm_overlap=True reorder_for_compute_comm_overlap_passes= sink_waits raise_comms reorder_compute_for_overlap losses_compiled = test_compiled losses_eager = test_eager fake_pg loss_compiled loss_eager zip losses_compiled losses_eager assertTrue torch allclose torch tensor loss_compiled torch tensor loss_eager rtol= e- atol= e- f loss_compiled vs loss_eager _create_simple_mlp_factory_fns hidden_dim = model_init_fn torch manual_seed rank fsdp_config = model = nn Sequential nn Linear hidden_dim hidden_dim device=device_type nn ReLU nn Linear hidden_dim hidden_dim device=device_type nn ReLU nn Linear hidden_dim hidden_dim device=device_type fully_shard model reshard_after_forward=True fsdp_config optim = torch optim SGD model parameters lr= e- model optim input_creation_fn torch manual_seed rank inp = torch randn hidden_dim device=device_type requires_grad=False inp model_init_fn input_creation_fn unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_simple_mlp_fullgraph_backend_aot_eager _test_traceable_fsdp _create_simple_mlp_factory_fns aot_eager fwd_fullgraph=True unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_simple_mlp_fullgraph_backend_aot_eager_decomp_partition _test_traceable_fsdp _create_simple_mlp_factory_fns aot_eager_decomp_partition fwd_fullgraph=True unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_simple_mlp_fullgraph_backend_inductor skipTestForOldSm _test_traceable_fsdp _create_simple_mlp_factory_fns inductor fwd_fullgraph=True _create_nested_fully_shard_factory_fns fwd_fullgraph hidden_dim = TestSubmodule nn Module __init__ hidden_dim super __init__ param = nn Parameter torch zeros hidden_dim hidden_dim dtype=torch float device=device_type param = nn Parameter torch zeros hidden_dim dtype=torch float device=device_type forward x ret = torch matmul x param fwd_fullgraph torch _dynamo graph_break ret = ret param ret = torch relu ret ret TestModule nn Module __init__ n_layers super __init__ layers = torch nn ModuleList _ range n_layers layers append TestSubmodule hidden_dim forward x Intentionally reusing all layers few times test multiple all-gathers same parameter case Case rerun same layer twice layer_id range len layers _ range x = layers layer_id x Case iterate through all layers twice layer layers x = layer x layer layers x = layer x x model_init_fn torch manual_seed rank fsdp_config = mesh = init_device_mesh device_type type world_size model = TestModule n_layers= mod model layers fully_shard mod mesh=mesh reshard_after_forward=True fsdp_config model = fully_shard model mesh=mesh reshard_after_forward=True fsdp_config optim = torch optim SGD model parameters lr= e- model optim input_creation_fn torch manual_seed rank inp = torch randn hidden_dim device=device_type requires_grad=False inp model_init_fn input_creation_fn unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_nested_fully_shard_backend_aot_eager TODO fix fwd_fullgraph=False case fwd_fullgraph True _test_traceable_fsdp _create_nested_fully_shard_factory_fns fwd_fullgraph=fwd_fullgraph aot_eager fwd_fullgraph=fwd_fullgraph unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_nested_fully_shard_backend_aot_eager_decomp_partition TODO fix fwd_fullgraph=False case fwd_fullgraph True _test_traceable_fsdp _create_nested_fully_shard_factory_fns fwd_fullgraph=fwd_fullgraph aot_eager_decomp_partition fwd_fullgraph=fwd_fullgraph _test_nested_fully_shard_backend_inductor_fullgraph_True skipTestForOldSm fwd_fullgraph True _reinplace_all_gather_with_optional_checks fwd_fullgraph torch _inductor config patch post_grad_custom_post_pass= functools partial _check_fsdp_copy_and_resize_ops_count_in_graph fwd_copy_count= fwd_resize_count= bwd_copy_count= bwd_resize_count= fwd_fullgraph None _ triton_codes = run_and_get_code lambda _test_traceable_fsdp _create_nested_fully_shard_factory_fns fwd_fullgraph=fwd_fullgraph inductor fwd_fullgraph=fwd_fullgraph bwd_resize_count_before_inductor= fwd_fullgraph None fwd_fullgraph assertEqual len triton_codes Expected two separate lowerings Triton code one FWD graph one Compiled Autograd BWD graph fwd_code = triton_codes extra_str_from_graph_partition = torch _inductor config graph_partition file_check = FileCheck check f call extra_str_from_graph_partition args fwd_ag_block_info dict overlapped_compute_op_str=None dict overlapped_compute_op_str= extern_kernels mm dict overlapped_compute_op_str= extern_kernels mm dict overlapped_compute_op_str= extern_kernels mm dict overlapped_compute_op_str= extern_kernels mm dict overlapped_compute_op_str= extern_kernels mm dict overlapped_compute_op_str= extern_kernels mm dict overlapped_compute_op_str= extern_kernels mm dict overlapped_compute_op_str= extern_kernels mm last_all_gather=True file_check = inductor_code_check_fsdp_all_gather file_check fwd_ag_block_info pass file_check run fwd_code bwd_code = triton_codes file_check = FileCheck check f call extra_str_from_graph_partition args bwd_ag_block_info dict overlapped_compute_op_str=None dict overlapped_compute_op_str= extern_kernels mm dict overlapped_compute_op_str= extern_kernels mm last_all_gather=True file_check = inductor_code_check_fsdp_all_gather file_check bwd_ag_block_info pass bwd_rs_block_info dict overlapped_compute_op_str= extern_kernels addmm dict overlapped_compute_op_str=None TODO improve compute comm overlap so ` overlapped_compute_op_str ` None dict overlapped_compute_op_str=None file_check = inductor_code_check_fsdp_reduce_scatter file_check bwd_rs_block_info pass file_check run bwd_code unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_nested_fully_shard_backend_inductor_fullgraph_True _test_nested_fully_shard_backend_inductor_fullgraph_True unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch graph_partition True test_nested_fully_shard_backend_inductor_fullgraph_True_graph_partition _test_nested_fully_shard_backend_inductor_fullgraph_True unittest skip TODO fix fwd_fullgraph=False case unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_nested_fully_shard_backend_inductor_fullgraph_False skipTestForOldSm _ triton_codes = run_and_get_code lambda _test_traceable_fsdp _create_nested_fully_shard_factory_fns fwd_fullgraph=False inductor fwd_fullgraph=False TODO when fwd_fullgraph=False there graph break FWD graph there several recompiles need figure out why assertGreater len triton_codes Expected least separate lowerings Triton code which means least graph break FWD graph _create_transformer_factory_fns all_requires_grad activation_checkpoint=False seq_len = vocab_size = n_layers = model_init_fn torch manual_seed rank fsdp_config = mesh = init_device_mesh device_type type world_size model_args = ModelArgs vocab_size=vocab_size n_layers=n_layers checkpoint_activations=activation_checkpoint model = Transformer model_args all_requires_grad requires_grad_params = attention wq attention wv requires_grad_param_count = k v model named_parameters substring requires_grad_params substring k v requires_grad_ True requires_grad_param_count += v requires_grad_ False assert requires_grad_param_count == n_layers len requires_grad_params _ mod enumerate model layers fully_shard mod mesh=mesh reshard_after_forward=True fsdp_config model = fully_shard model mesh=mesh reshard_after_forward=False fsdp_config optim = torch optim SGD model parameters lr= e- model optim input_creation_fn torch manual_seed rank inp = torch randint vocab_size seq_len device=device_type requires_grad=False inp model_init_fn input_creation_fn _maybe_add_graph_break_to_sdpa fwd_fullgraph _sdpa_with_graph_break args kwargs torch _dynamo graph_break orig_F_scaled_dot_product_attention args kwargs fwd_fullgraph mock patch object F scaled_dot_product_attention _sdpa_with_graph_break contextlib nullcontext unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_transformer_backend_aot_eager TODO fix fwd_fullgraph=False case fwd_fullgraph all_requires_grad itertools product True True False _maybe_add_graph_break_to_sdpa fwd_fullgraph _reinplace_all_gather_with_optional_checks fwd_fullgraph _test_traceable_fsdp _create_transformer_factory_fns all_requires_grad=all_requires_grad aot_eager fwd_fullgraph=fwd_fullgraph unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch TODO native_dropout has worse accuracy after decomp need figure out why torch _inductor config patch fallback_random=True test_transformer_backend_aot_eager_decomp_partition TODO fix fwd_fullgraph=False case fwd_fullgraph all_requires_grad itertools product True True False _maybe_add_graph_break_to_sdpa fwd_fullgraph _test_traceable_fsdp _create_transformer_factory_fns all_requires_grad=all_requires_grad aot_eager_decomp_partition fwd_fullgraph=fwd_fullgraph _test_transformer_backend_inductor_fullgraph_True skipTestForOldSm fwd_fullgraph all_requires_grad activation_checkpoint itertools product True True False True False log warning f fwd_fullgraph= fwd_fullgraph all_requires_grad= all_requires_grad activation_checkpoint= activation_checkpoint noqa G G B _reinplace_all_gather_with_optional_checks fwd_fullgraph torch _inductor config patch post_grad_custom_post_pass= functools partial _check_fsdp_copy_and_resize_ops_count_in_graph NOTE For root unsharded params we don t reshard after forward since training parameters would freed all-gathered immediately Hence we still have their resize copy ops graph fwd_copy_count= fwd_resize_count= bwd_copy_count= bwd_resize_count= fwd_fullgraph None _ triton_codes = run_and_get_code lambda _test_traceable_fsdp _create_transformer_factory_fns all_requires_grad=all_requires_grad activation_checkpoint=activation_checkpoint inductor fwd_fullgraph=fwd_fullgraph bwd_resize_count_before_inductor= fwd_fullgraph None fwd_fullgraph assertEqual len triton_codes Expected two separate lowerings Triton code one FWD graph one Compiled Autograd BWD graph fwd_code = triton_codes extra_str_from_graph_partition = torch _inductor config graph_partition file_check = FileCheck check f call extra_str_from_graph_partition args fwd_ag_block_info dict overlapped_compute_op_str= triton_ all_requires_grad None dict overlapped_compute_op_str= aten native_dropout dict overlapped_compute_op_str= aten _scaled_dot_product_efficient_attention dict overlapped_compute_op_str= aten _scaled_dot_product_efficient_attention last_all_gather=True file_check = inductor_code_check_fsdp_all_gather file_check fwd_ag_block_info pass file_check run fwd_code bwd_code = triton_codes file_check = FileCheck check f call extra_str_from_graph_partition args bwd_ag_block_info dict overlapped_compute_op_str= extern_kernels mm dict overlapped_compute_op_str= aten _scaled_dot_product_efficient_attention_backward dict overlapped_compute_op_str= aten _scaled_dot_product_efficient_attention_backward last_all_gather=True bwd_ag_block_info None file_check = inductor_code_check_fsdp_all_gather file_check bwd_ag_block_info pass bwd_rs_block_info dict overlapped_compute_op_str= extern_kernels mm all_requires_grad None dict overlapped_compute_op_str=None TODO improve compute comm overlap so ` overlapped_compute_op_str ` None dict overlapped_compute_op_str=None dict overlapped_compute_op_str=None all_requires_grad None bwd_rs_block_info None file_check = inductor_code_check_fsdp_reduce_scatter file_check bwd_rs_block_info pass file_check run bwd_code unittest skip Traceable FSDP being maintained anymore unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch TODO native_dropout causes CUDA IMA error need figure out why torch _inductor config patch fallback_random=True test_transformer_backend_inductor_fullgraph_True _test_transformer_backend_inductor_fullgraph_True unittest skip Traceable FSDP being maintained anymore unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch TODO native_dropout causes CUDA IMA error need figure out why torch _inductor config patch fallback_random=True torch _inductor config patch graph_partition True test_transformer_backend_inductor_fullgraph_True_graph_partition _test_transformer_backend_inductor_fullgraph_True unittest skip TODO fix fwd_fullgraph=False case unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch TODO native_dropout causes CUDA IMA error need figure out why torch _inductor config patch fallback_random=True test_transformer_backend_inductor_fullgraph_False skipTestForOldSm fwd_fullgraph = False TODO fix numerical issue activation_checkpoint=True case all_requires_grad activation_checkpoint itertools product True False False log warning f fwd_fullgraph= fwd_fullgraph all_requires_grad= all_requires_grad activation_checkpoint= activation_checkpoint noqa G G B _maybe_add_graph_break_to_sdpa fwd_fullgraph _ triton_codes = run_and_get_code lambda _test_traceable_fsdp _create_transformer_factory_fns all_requires_grad=all_requires_grad activation_checkpoint=activation_checkpoint inductor fwd_fullgraph=fwd_fullgraph TODO when fwd_fullgraph=False there graph break FWD graph there several recompiles need figure out why assertGreater len triton_codes Expected least separate lowerings Triton code which means least graph break FWD graph test_dynamo_recompiles_on_fsdp_layers m = Mod name child m encoder named_children isinstance child torch nn Linear new_child = torch compile child setattr m encoder name new_child m = FSDP m sharding_strategy=ShardingStrategy FULL_SHARD use_orig_params=True inp = torch randn device=device_type m inp __name__ == __main__ run_tests