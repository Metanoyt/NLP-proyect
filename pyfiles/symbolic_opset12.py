mypy allow-untyped-defs mypy disable-error-code=arg-type __future__ annotations functools sys torch torch _C _onnx _C_onnx torch onnx errors torch onnx _internal torchscript_exporter _type_utils jit_utils registration symbolic_helper symbolic_opset opset utils EDITING THIS FILE READ THIS FIRST see Note Edit Symbolic Files README md This file exports ONNX ops opset __all__ = argmax argmin binary_cross_entropy_with_logits celu cross_entropy_loss dropout einsum ge le native_dropout nll_loss nll_loss d nll_loss_nd outer pow tensordot unfold _onnx_symbolic = functools partial registration onnx_symbolic opset= _einsum_helper g jit_utils GraphContext equation tensors tensors raise RuntimeError Einsum inputs empty ONNX does support bool Einsum inputs symbolic_helper _is_bool tensors tensors = g op Cast tensor to_i=_C_onnx TensorProtoDataType INT tensor tensors g op Cast g op Einsum tensors equation_s=equation to_i=_C_onnx TensorProtoDataType BOOL g op Einsum tensors equation_s=equation _onnx_symbolic aten einsum symbolic_helper parse_args s v einsum g jit_utils GraphContext equation tensor_list path=None tensors = symbolic_helper _unpack_list tensor_list _einsum_helper g equation tensors _onnx_symbolic aten outer symbolic_helper parse_args v v outer g jit_utils GraphContext input other make sure cast other s type _type_utils JitScalarType from_value other _type_utils JitScalarType UNDEFINED = _type_utils JitScalarType from_value input other = g op Cast other to_i=_type_utils JitScalarType from_value input onnx_type _einsum_helper g i j- ij input other _dropout_returns_masked_input_and_mask g jit_utils GraphContext input torch _C Value p float train bool - tuple torch _C Value torch _C Value &#124; None symbolic_helper check_training_mode train dropout In eval mode dropout non-op That node s train param set False dropout just returns its inputs train input None p = g op Constant value_t=torch tensor p t = g op Constant value_t=torch tensor train dtype=torch bool r mask = g op Dropout input p t outputs= r mask _onnx_symbolic aten dropout symbolic_helper parse_args v f b dropout g jit_utils GraphContext input p train masked _ = _dropout_returns_masked_input_and_mask g input p train masked _onnx_symbolic aten native_dropout symbolic_helper parse_args v f b native_dropout g jit_utils GraphContext input p train _dropout_returns_masked_input_and_mask g input p train _onnx_symbolic aten nll_loss nll_loss g jit_utils GraphContext target weight reduction ignore_index none reduction onnx Constant value= mean reduction onnx Constant value= sum reduction onnx Constant value= reduction = symbolic_helper _maybe_get_const reduction i reduction_vals = none mean sum reduction = reduction_vals reduction onnx NegativeLogLikelihoodLoss specification ignore_index optional without default value therefore we need set ignore_index attribute even specified e g ignore_index=- ignore_index = symbolic_helper _maybe_get_const ignore_index i weight node mustBeNone nllloss = g op NegativeLogLikelihoodLoss target reduction_s=reduction ignore_index_i=ignore_index nllloss = g op NegativeLogLikelihoodLoss target weight reduction_s=reduction ignore_index_i=ignore_index nllloss _onnx_symbolic aten nll_loss d nll_loss d g jit_utils GraphContext target weight reduction ignore_index nll_loss g target weight reduction ignore_index _onnx_symbolic aten nll_loss_nd nll_loss_nd g jit_utils GraphContext target weight reduction ignore_index nll_loss g target weight reduction ignore_index _onnx_symbolic aten cross_entropy_loss cross_entropy_loss g jit_utils GraphContext target weight reduction ignore_index label_smoothing none reduction onnx Constant value= mean reduction onnx Constant value= sum reduction onnx Constant value= reduction = symbolic_helper _maybe_get_const reduction i reduction_vals = none mean sum reduction = reduction_vals reduction label_smoothing = symbolic_helper _maybe_get_const label_smoothing f label_smoothing None label_smoothing raise errors SymbolicValueError Unsupported ONNX does support label_smoothing onnx SoftmaxCrossEntropyLoss specification ignore_index optional without default value therefore we need set ignore_index attribute even specified e g ignore_index=- ignore_index = symbolic_helper _maybe_get_const ignore_index i weight node mustBeNone celoss = g op SoftmaxCrossEntropyLoss target reduction_s=reduction ignore_index_i=ignore_index celoss = g op SoftmaxCrossEntropyLoss target weight reduction_s=reduction ignore_index_i=ignore_index celoss _onnx_symbolic aten binary_cross_entropy_with_logits symbolic_helper parse_args v v v v i binary_cross_entropy_with_logits g jit_utils GraphContext input target weight pos_weight reduction p = g op Constant value_t=torch tensor sig_x = opset sigmoid g input log_sig_x = opset log g sig_x sub_ _x = opset sub g p sig_x sub_ _y = opset sub g p target log_ _x = opset log g sub_ _x pos_weight None symbolic_helper _is_none pos_weight output = opset neg g opset add g opset mul g target log_sig_x opset mul g sub_ _y log_ _x output = opset neg g opset add g opset mul g opset mul g target log_sig_x pos_weight opset mul g sub_ _y log_ _x weight None symbolic_helper _is_none weight output = opset mul g weight output reduction = symbolic_helper _maybe_get_const reduction i reduction == output reduction == g op ReduceMean output keepdims_i= reduction == g op ReduceSum output keepdims_i= symbolic_helper _onnx_unsupported binary_cross_entropy_with_logits reduction other than none mean sum input _onnx_symbolic aten celu celu g jit_utils GraphContext alpha alpha = symbolic_helper _maybe_get_const alpha f input type double cast float _type_utils JitScalarType from_value _type_utils JitScalarType UNDEFINED == _type_utils JitScalarType DOUBLE = g op Cast to_i=_C_onnx TensorProtoDataType FLOAT out = g op Celu alpha_f=alpha g op Cast out to_i=_C_onnx TensorProtoDataType DOUBLE g op Celu alpha_f=alpha _onnx_symbolic aten argmax symbolic_helper parse_args v v b argmax g jit_utils GraphContext input torch _C Value dim torch _C Value keepdim bool symbolic_helper _argmin_argmax_helper g input dim keepdim ArgMax _onnx_symbolic aten argmin symbolic_helper parse_args v v b argmin g jit_utils GraphContext input torch _C Value dim torch _C Value keepdim bool symbolic_helper _argmin_argmax_helper g input dim keepdim ArgMin _onnx_symbolic aten pow pow g jit_utils GraphContext exponent g op Pow exponent _onnx_symbolic aten ge ge g jit_utils GraphContext input other g op GreaterOrEqual input other _onnx_symbolic aten le le g jit_utils GraphContext input other g op LessOrEqual input other _onnx_symbolic aten unfold symbolic_helper parse_args v i v v unfold g jit_utils GraphContext input dimension size step const_size = symbolic_helper _maybe_get_const size i const_step = symbolic_helper _maybe_get_const step i symbolic_helper _is_value const_size symbolic_helper _is_value const_step opset unfold g input dimension const_size const_step sizedim = symbolic_helper _get_tensor_dim_size input dimension sizedim None low_start = g op Constant value_t=torch tensor low_end = g op Constant value_t=torch tensor sizedim hi_end = g op Constant value_t=torch tensor sizedim + low_indices = g op Range low_start low_end step hi_indices = g op Range size hi_end step low_size = symbolic_helper _size_helper g low_indices g op Constant value_t=torch tensor hi_size = symbolic_helper _size_helper g hi_indices g op Constant value_t=torch tensor ndim = symbolic_helper _get_tensor_rank input assert ndim None perm = list range ndim perm append perm pop dimension unsqueeze_list = loop_condition = g op Constant value_t=torch tensor loop_condition = g op Cast loop_condition to_i=_C_onnx TensorProtoDataType BOOL loop_len = g op Min low_size hi_size loop loop_context _ = jit_utils add_op_with_blocks g Loop loop_len loop_condition n_blocks= loop_block = loop_context block block_input_iter = utils _add_input_to_block loop_block cond = utils _add_input_to_block loop_block noqa F starts = loop_context op Gather low_indices block_input_iter ends = loop_context op Gather hi_indices block_input_iter axes = loop_context op Constant value_t=torch tensor starts = symbolic_helper _unsqueeze_helper loop_context starts ends = symbolic_helper _unsqueeze_helper loop_context ends stack = loop_context op Slice input starts ends axes unsqueeze = symbolic_helper _unsqueeze_helper loop_context loop_context op Transpose stack perm_i=perm dimension unsqueeze_list append unsqueeze concat = loop_context op Concat unsqueeze_list axis_i= cond_out = loop_context op Cast loop_condition pyrefly ignore bad-argument-type _C_onnx TensorProtoDataType BOOL utils _add_output_to_block loop_block cond_out utils _add_output_to_block loop_block concat loop_output = loop node output perm = perm perm dimension + = perm dimension + perm transpose = g op Transpose loop_output perm_i=perm squeeze = symbolic_helper _squeeze_helper g transpose squeeze symbolic_helper _unimplemented Unfold input size accessible _onnx_symbolic aten tensordot symbolic_helper parse_args v v v tensordot g jit_utils GraphContext input_a input_b dims_a dims_b out=None out None symbolic_helper _unimplemented Tensordot Out parameter supported tensordot dim_count_a = symbolic_helper _get_tensor_rank input_a dim_count_a None raise errors SymbolicValueError Unsupported ONNX export tensordot tensor input_a unknown rank input_a dim_count_b = symbolic_helper _get_tensor_rank input_b dim_count_b None raise errors SymbolicValueError Unsupported ONNX export tensordot tensor input_b unknown rank input_b dims_a = dims_a i + dim_count_a dims_a i dims_a i i range len dims_a dims_b = dims_b i + dim_count_b dims_b i dims_b i i range len dims_b left_dims_a = i i range dim_count_a i dims_a left_dims_b = i i range dim_count_b i dims_b new_input_a = opset permute g input_a left_dims_a + dims_a new_input_b = opset permute g input_b dims_b + left_dims_b input_shape = g op Shape new_input_a left_sizes_a = symbolic_helper _slice_helper g input_shape axes= starts= ends= len left_dims_a shape_sizes = left_sizes_a g op Constant value_t=torch tensor - dtype=torch long output_a = opset _reshape_from_tensor g new_input_a shape_sizes input_shape = g op Shape output_a slices = symbolic_helper _slice_helper g input_shape axes= starts= - ends= sys maxsize shape_sizes = g op Constant value_t=torch tensor - dtype=torch long slices output_a = opset _reshape_from_tensor g new_input_a shape_sizes input_shape = g op Shape new_input_b left_sizes_b = symbolic_helper _slice_helper g input_shape axes= starts= len dims_b ends= sys maxsize slices = symbolic_helper _slice_helper g input_shape axes= starts= ends= len dims_b shape_sizes = slices g op Constant value_t=torch tensor - dtype=torch long output_b = opset _reshape_from_tensor g new_input_b shape_sizes input_shape = g op Shape output_b slices = symbolic_helper _slice_helper g input_shape axes= starts= - ends= sys maxsize shape_sizes = g op Constant value_t=torch tensor - dtype=torch long slices output_b = opset _reshape_from_tensor g new_input_b shape_sizes output = einsum g ij jk- ik g op prim ListConstruct output_a output_b shape_sizes = left_sizes_a left_sizes_b opset _reshape_from_tensor g output shape_sizes