Owner s oncall distributed collections copy functools itertools typing Any Optional Union torch torch distributed dist torch nn nn torch distributed fsdp fully_shard torch nn parallel scatter_gather _is_namedtuple torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp check_sharded_parity DoubleLinear FSDPTest FSDPTestMultiThread get_devtype MLP torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor ModelArgs Transformer device_type = torch device get_devtype TestFullyShardAutograd FSDPTest property world_size - int min torch get_device_module device_type device_count _reduce_ d_partial_grads module nn Module group Optional dist ProcessGroup = None - None group = group dist distributed_c d _get_default_group param module parameters param grad None param grad div_ group size skip_if_lt_x_gpu test_unused_forward_output Tests gradients propagate when running backward where some forward output used compute loss motivated https github com pytorch pytorch pull run_subtests reshard_after_forward True False _test_unused_forward_output _test_unused_forward_output reshard_after_forward Union bool int torch manual_seed local_batch_size = global_batch_size dim = world_size local_batch_size model = DoubleLinear dim=dim use_second_linear=True ref_model = copy deepcopy model device_type fully_shard model lin reshard_after_forward=reshard_after_forward fully_shard model reshard_after_forward=reshard_after_forward ref_optim = torch optim Adam ref_model parameters lr= e- optim = torch optim Adam model parameters lr= e- torch manual_seed same all ranks iter_idx range Use all forward outputs loss backward first half iterations only st forward output rest global_inp = torch rand global_batch_size dim device=device_type local_inp = global_inp rank local_batch_size rank + local_batch_size detach out out = model local_inp loss = out out sum iter_idx out sum loss backward optim step ref_out ref_out = ref_model global_inp ref_loss = ref_out ref_out sum iter_idx ref_out sum ref_loss backward _reduce_ d_partial_grads ref_model ref_optim step dist all_reduce loss partial - replicated assertEqual loss ref_loss optim zero_grad set_to_none= iter_idx ref_optim zero_grad set_to_none= iter_idx check_sharded_parity ref_model model skip_if_lt_x_gpu test_unused_forward_module Tests gradients propagate when running backward where some forward module used compute loss motivated https github com pytorch pytorch pull run_subtests reshard_after_forward True False _test_unused_forward_module _test_unused_forward_module reshard_after_forward Union bool int torch manual_seed local_batch_size dim = global_batch_size = world_size local_batch_size model = DoubleLinear dim=dim use_second_linear=False ref_model = copy deepcopy model device_type fully_shard model lin reshard_after_forward=reshard_after_forward fully_shard model lin reshard_after_forward=reshard_after_forward fully_shard model reshard_after_forward=reshard_after_forward ref_optim = torch optim Adam ref_model parameters lr= e- optim = torch optim Adam model parameters lr= e- torch manual_seed same all ranks iter_idx range global_inp = torch rand global_batch_size dim device=device_type local_inp = global_inp rank local_batch_size rank + local_batch_size detach losses list torch Tensor = _model inp ref_model global_inp model local_inp losses append _model inp sum losses - backward _reduce_ d_partial_grads ref_model dist all_reduce losses partial - replicated assertEqual losses losses check_sharded_parity ref_model model _optim optim ref_optim _optim step _optim zero_grad set_to_none= iter_idx skip_if_lt_x_gpu test_nontensor_activations Tests gradients propagate when running forward nontensor data structures wrapping activations This mainly test hook registration run_subtests container_type list collections namedtuple tuple dict _test_nontensor_activations _test_nontensor_activations container_type type Module nn Module __init__ dim int super __init__ lin = nn Linear dim dim lin = nn Linear dim dim relu = nn ReLU forward inp Any Assume th element ` inp ` tensor run some forward computation pack back into same data structure type ` inp ` isinstance inp list _forward inp _is_namedtuple inp type inp _forward inp + list inp isinstance inp tuple _forward inp isinstance inp dict x _forward inp x raise NotImplementedError f Unsupported input type type inp inp _forward x torch Tensor - torch Tensor relu lin relu lin x ToContainerType nn Module __init__ container_type type super __init__ container_type = container_type forward x torch Tensor container_type list x container_type collections namedtuple nt = collections namedtuple NT x y nt x torch ones_like x container_type tuple x container_type dict x x raise NotImplementedError f Unsupported container type container_type FromContainerType nn Module __init__ container_type type super __init__ container_type = container_type forward x torch Tensor container_type list collections namedtuple tuple x container_type dict x x raise NotImplementedError f Unsupported container type container_type torch manual_seed local_batch_size dim = global_batch_size = world_size local_batch_size model = nn Sequential ToContainerType container_type Module dim Module dim Module dim FromContainerType container_type ref_model = copy deepcopy model device_type module model fully_shard module fully_shard model ref_optim = torch optim Adam ref_model parameters lr= e- optim = torch optim Adam model parameters lr= e- torch manual_seed same all ranks iter_idx range global_inp = torch rand global_batch_size dim device=device_type local_inp = global_inp rank local_batch_size rank + local_batch_size detach losses list torch Tensor = _model inp ref_model global_inp model local_inp losses append _model inp sum losses - backward _reduce_ d_partial_grads ref_model dist all_reduce losses partial - replicated assertEqual losses losses check_sharded_parity ref_model model _optim optim ref_optim _optim step _optim zero_grad set_to_none= iter_idx TestFullyShardPostAccGradHookMultiThread FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu test_post_acc_grad_hook_runs param_name_to_hook_count = collections defaultdict int hook param_name str param torch Tensor - None nonlocal param_name_to_hook_count param_name_to_hook_count param_name += model = MLP module model in_proj model out_proj model fully_shard module param_name param model named_parameters param_hook = functools partial hook param_name param register_post_accumulate_grad_hook param_hook inp = torch randn device=device_type model inp sum backward param_names = param_name param_name _ model named_parameters assertEqual param_names set param_name_to_hook_count keys count param_name_to_hook_count values assertEqual count TestFullyShardPostAccGradHookMultiProcess FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_post_acc_grad_hook_optim_parity Tests parity running optimizer via post-accumulate-grad hook vs normally torch manual_seed model_args = ModelArgs dropout_p= model = Transformer model_args ref_model = copy deepcopy model device_type module itertools chain ref_model layers ref_model fully_shard module optim_kwargs = lr e- foreach False ref_optim = torch optim AdamW ref_model parameters optim_kwargs lr_scheduler_kwargs = step_size ref_lr_scheduler = torch optim lr_scheduler StepLR ref_optim lr_scheduler_kwargs module itertools chain model layers model fully_shard module param_to_optim = param_to_lr_scheduler = param model parameters param_to_optim param = torch optim AdamW param optim_kwargs param_to_lr_scheduler param = torch optim lr_scheduler StepLR param_to_optim param lr_scheduler_kwargs optim_hook param nn Parameter - None param_to_optim param step param_to_optim param zero_grad param_to_lr_scheduler param step param model parameters param register_post_accumulate_grad_hook optim_hook torch manual_seed + rank inp = torch randint model_args vocab_size device=device_type _ range ref_loss = ref_model inp sum ref_loss backward ref_optim step ref_optim zero_grad ref_lr_scheduler step loss = model inp sum loss backward assertTrue torch equal ref_loss loss ref_param param zip ref_model parameters model parameters assertTrue torch equal ref_param param __name__ == __main__ run_tests