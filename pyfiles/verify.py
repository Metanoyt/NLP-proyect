difflib io numpy np onnx onnx helper torch torch jit torch onnx colonize msg sep= msg msg + sep Errors An error-collecting object which supports error recovery It intended used like context manager Errors Top-level error message errs __init__ msg rtol= e- atol= e- msg = msg errors = context = rtol = rtol atol = atol Allocated upon instance creation so multiple Errors can used ShortCircuit Exception pass exc_class = ShortCircuit requireAlmostEqual x y msg=None Test x y nearly equal equal within rtol precision aborts execution they almostEqualAndThen x y msg failWith checkAlmostEqual x y msg=None Test x y nearly equal equal within rtol precision continue execution even they equal To prevent error cascades you should remember call failIfErrs some later point time almostEqualAndThen x y msg addErr almostEqualAndThen x y msg k Helper implementing requireAlmostEqual checkAlmostEqual Upon failure invokes continuation k error message At moment only tests numpy ndarray supported isinstance x np ndarray isinstance y np ndarray np testing assert_allclose x y rtol=self rtol atol=self atol equal_nan=True verbose=True raise RuntimeError Unsupported almost equal test requireEqual x y msg=None Test x y equal aborts execution they equalAndThen x y msg failWith checkEqual x y msg=None Test x y equal continue execution even they equal To prevent error cascades you should remember call failIfErrs some later point time equalAndThen x y msg addErr Bit-for-bit accuracy test equalAndThen x y msg k Helper implementing requireEqual checkEqual Upon failure invokes continuation k error message isinstance x onnx TensorProto isinstance y onnx TensorProto equalAndThen x name y name msg k Use numpy comparison t = onnx numpy_helper to_array x t = onnx numpy_helper to_array y new_msg = f colonize msg In embedded parameter x name equalAndThen t t new_msg k isinstance x np ndarray isinstance y np ndarray np testing assert_equal x y x = y TODO Better algorithm lists sx = str x sy = str y len sx len sy \n sx \n sy long form l = = k \n The value\n \n \n \n\ndoes equal\n\n \n \n format colonize msg \n l sx l l sy l k f colonize msg sx = sy requireMultiLineEqual x y msg=None Test long multi-line strings x y equal aborts execution they multiLineEqualAndThen x y msg failWith multiLineEqualAndThen x y msg k Helper implementing requireMultiLineEqual Upon failure invokes continuation k error message msg None msg = Strings equal x = y diff = difflib ndiff x splitlines True y splitlines True k format colonize msg \n\n join diff addErr msg Add error error context continue executing TODO instead immediately concatenating context msg attach metadata make decision how format later c reversed context msg += \n\n + \n join c splitlines errors append msg fail Immediately fail short-circuit next recovery context NB It error fail without having added any errors error context raise exc_class failWith msg Add error error context then short-circuit addErr msg fail failIfErrs If there any errors error context short-circuit This used prevent error cascades errors fail recover Returns context manager which can used recover case error Example usage errs recover parent_self = Recover __enter__ pass __exit__ exc_type exc_value traceback exc_type == parent_self exc_class True Recover addErrCtxt msg Returns context manager which encloses fragment code extra contextual message e g where error occurred hint applicable all errors area Example usage errs addErrCtx Some text parent_self = AddContext __enter__ parent_self context append msg __exit__ exc_type exc_value traceback parent_self context pop AddContext __enter__ __exit__ exc_type exc_value traceback errors errors_msg = \n\n join ERROR + x x errors final_msg = \n \n format msg - errors_msg raise AssertionError final_msg exc_type == exc_class raise RuntimeError ShortCircuit raised no errors recorded verify model args backend verbose=False training=torch onnx TrainingMode EVAL rtol= e- atol= e- test_args= do_constant_folding=True opset_version=None keep_initializers_as_inputs=True add_node_names=False operator_export_type=torch onnx OperatorExportTypes ONNX input_names=None dynamic_axes=None remained_onnx_input_idx=None Export model into ONNX into specified ONNX backend then few random inputs verify PyTorch backend produced same results Requires onnx installed This function may spuriously fail some operators implemented different numerical precision ONNX backend which case unstable network e g Inception may blow up these numerical instabilities This situation less likely happen your model has been trained However case you may have found bug Please report PyTorch developers You can also debug issue yourself removing suffixes operators your model until verification passes For reproducibility we recommend explicitly setting PyTorch s seed before invoking function Args model torch nn Module model exported verified args tuple arguments inputs model e g such ` ` model args ` ` valid invocation model Any non-Variable arguments will hard-coded into exported model any Variable arguments will become inputs exported model order they occur args If args Variable equivalent having called -ary tuple Variable Note passing keyword arguments model currently supported Give us shout you need backend onnx backend module ONNX backend verify verbose bool default False specified we will print out debug description trace being exported training bool default False export model training mode At moment ONNX oriented towards exporting models inference only so you will generally need set True rtol float default e- relative precision required test_args int iterable args default either integer specifying number random arguments generate iterable producing arguments test under opset_version int default None opset version model export If specified default value symboli_helper will used utils _export operator_export_type enum default OperatorExportTypes ONNX operator export type use when exporting model The default value converts all operators ONNX ops input_names list string list input names dynamic_axes dict string list dynamic_axes remained_onnx_input_idx list int default None The remained ONNX input index _nested_map condition fn condition_msg=None _map obj condition obj fn obj obj None None isinstance obj list tuple type obj _map x x obj raise ValueError Auto nesting doesn t know how process input object type + torch typename obj + Accepted types + condition_msg + lists tuples them condition_msg _map _iter_filter condition allow_unknown=False condition_msg=None _iter obj condition obj yield obj obj None isinstance obj list tuple o obj yield _iter o allow_unknown yield obj raise ValueError Auto nesting doesn t know how process input object type + torch typename obj + Accepted types + condition_msg + lists tuples them condition_msg _iter is_tensor o isinstance o torch Tensor _iter_tensors = _iter_filter is_tensor condition_msg= Tensors randomize_arg arg new_data = arg data clone For now don t try randomizing non-float tensors these likely things like indices where just randomly spattering some longs unlikely work One way we could make work apply random permutation something arg is_floating_point new_data uniform_ torch autograd Variable new_data requires_grad=arg requires_grad randomize_args = _nested_map is_tensor randomize_arg backend_args args TODO onnx should accept iterables tuple v data cpu numpy v _iter_tensors args load_bytes b b seek x = onnx load b doc_string has stack traces - let s remove them make comparison sane onnx helper strip_doc_string x x Special case common case passing single Tensor isinstance args torch Tensor args = args torch onnx select_model_mode_for_export model training proto_bytes = io BytesIO torch_out = torch onnx utils _export model args proto_bytes verbose=verbose do_constant_folding=do_constant_folding opset_version=opset_version keep_initializers_as_inputs=keep_initializers_as_inputs add_node_names=add_node_names operator_export_type=operator_export_type input_names=input_names dynamic_axes=dynamic_axes isinstance model torch jit ScriptModule torch_out = model args proto = load_bytes proto_bytes prepared = backend prepare proto run args remained_onnx_input_idx alt_proto_bytes = io BytesIO torch_out = torch onnx utils _export model args alt_proto_bytes verbose=verbose do_constant_folding=do_constant_folding opset_version=opset_version keep_initializers_as_inputs=keep_initializers_as_inputs add_node_names=add_node_names operator_export_type=operator_export_type input_names=input_names dynamic_axes=dynamic_axes isinstance model torch jit ScriptModule torch_out = model args alt_proto = load_bytes alt_proto_bytes proto SerializeToString = alt_proto SerializeToString OK let s try figure out what happened msg = When I exported your model different inputs result different verbose msg += \n To get more information run torch onnx verify verbose=True Errors msg rtol=rtol atol=atol errs First check we have same number parameters they re same order If they don t something has really gone wrong initializer_order_hint = This really strange The second time I exported your model \n had different set parameters Are you assigning Parameters\n forward your model definition errs addErrCtxt initializer_order_hint errs requireEqual x name x proto graph initializer x name x alt_proto graph initializer msg= Parameters list differs Now check embedded parameters actually same initializer_hint = A difference embedded parameters usually means that\n your model updating parameters buffers even inference\n mode Look buggy nn Module which isn t respecting train \n errs recover errs addErrCtxt initializer_hint x y zip proto graph initializer alt_proto graph initializer errs checkEqual x y Next check model structure lines up structure_hint = A difference model structure usually means that\n your model has dynamic control flow These models not\n currently supported exporter errs recover errs addErrCtxt structure_hint Delete initializers since we already tested them stripped_proto = onnx ModelProto stripped_proto CopyFrom proto del stripped_proto graph initializer stripped_alt_proto = onnx ModelProto stripped_alt_proto CopyFrom alt_proto del stripped_alt_proto graph initializer Compare printable graph representations first errs requireMultiLineEqual onnx helper printable_graph stripped_proto graph onnx helper printable_graph stripped_alt_proto graph Compare actual protobuf text formats now very user-friendly errs requireMultiLineEqual str stripped_proto str stripped_alt_proto One last ditch effort using built-in equality protobufs errs requireEqual stripped_proto stripped_alt_proto errs failIfErrs At point we should have figured out why binary protobufs differed short-circuited out code helpful error message But what we didn t We better still try give good error message case We EXPECT these requires fail If they don t bug verify errs requireEqual proto alt_proto errs requireEqual proto_bytes getvalue alt_proto_bytes getvalue raise AssertionError TODO test traced model also returns same thing run_helper torch_out args remained_onnx_input_idx Factored out so we can avoid one run model run_helper torch_out args remained_onnx_input_idx onnx_input = backend_args args remained_onnx_input_idx None input_onnx = idx remained_onnx_input_idx input_onnx append onnx_input idx onnx_input = tuple input_onnx backend_out = prepared run onnx_input isinstance torch_out torch Tensor torch_out = torch_out torch_out _ = torch jit _flatten torch_out NB onnx backend NEVER returns bare numpy array msg = ONNX backend returned different results PyTorch result_hint = If you using trained parameters difference results\n could mean your network numerically unstable Otherwise\n indicates bug PyTorch ONNX please file bug report Errors msg rtol=rtol atol=atol errs errs addErrCtxt result_hint i x y enumerate zip torch_out backend_out errs checkAlmostEqual x data cpu numpy y f In output i run_helper torch_out args remained_onnx_input_idx isinstance test_args int _ range test_args run randomize_args args remained_onnx_input_idx test_arg test_args run test_arg remained_onnx_input_idx