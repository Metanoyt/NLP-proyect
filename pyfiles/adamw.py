mypy allow-untyped-defs typing Optional Union torch Tensor adam Adam adam optimizer _capturable_doc _differentiable_doc _foreach_doc _fused_doc _maximize_doc _params_doc ParamsT __all__ = AdamW adamw AdamW Adam __init__ params ParamsT lr Union float Tensor = e- betas tuple Union float Tensor Union float Tensor = eps float = e- weight_decay float = e- amsgrad bool = False maximize bool = False foreach Optional bool = None capturable bool = False differentiable bool = False fused Optional bool = None super __init__ params lr betas eps weight_decay amsgrad foreach=foreach maximize=maximize capturable=capturable differentiable=differentiable fused=fused decoupled_weight_decay=True Preserve decoupled_weight_decay AdamW backwards compatibility The following guarantees decoupled_weight_decay will always True loading any state into AdamW __setstate__ state super __setstate__ state group param_groups group decoupled_weight_decay = True AdamW __doc__ = r Implements AdamW algorithm where weight decay does accumulate momentum nor variance math \begin aligned \rule mm pt \\ \textbf input \gamma \text lr \ \beta_ \beta_ \text betas \ \theta_ \text params \ f \theta \text objective \ \epsilon \text epsilon \\ \hspace mm \lambda \text weight decay \ \textit amsgrad \ \textit maximize \\ \textbf initialize m_ \leftarrow \text first moment v_ \leftarrow \text second moment \ v_ ^ max \leftarrow \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm \textbf \ \textit maximize \\ \hspace mm g_t \leftarrow -\nabla_ \theta f_t \theta_ t- \\ \hspace mm \textbf \\ \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \theta_t \leftarrow \theta_ t- - \gamma \lambda \theta_ t- \\ \hspace mm m_t \leftarrow \beta_ m_ t- + - \beta_ g_t \\ \hspace mm v_t \leftarrow \beta_ v_ t- + -\beta_ g^ _t \\ \hspace mm \widehat m_t \leftarrow m_t \big -\beta_ ^t \big \\ \hspace mm \textbf \ amsgrad \\ \hspace mm v_t^ max \leftarrow \mathrm max v_ t- ^ max v_t \\ \hspace mm \widehat v_t \leftarrow v_t^ max \big -\beta_ ^t \big \\ \hspace mm \textbf \\ \hspace mm \widehat v_t \leftarrow v_t \big -\beta_ ^t \big \\ \hspace mm \theta_t \leftarrow \theta_t - \gamma \widehat m_t \big \sqrt \widehat v_t + \epsilon \big \\ \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned For further details regarding algorithm we refer ` Decoupled Weight Decay Regularization ` _ + rf Args _params_doc lr float Tensor optional learning rate default e- A tensor LR yet supported all our implementations Please use float LR you also specifying fused=True capturable=True betas tuple Union float Tensor Union float Tensor optional coefficients used computing running averages gradient its square If tensor provided must -element default eps float optional term added denominator improve numerical stability default e- weight_decay float optional weight decay coefficient default e- amsgrad bool optional whether use AMSGrad variant algorithm paper ` On Convergence Adam Beyond ` _ default False _maximize_doc _foreach_doc _capturable_doc _differentiable_doc _fused_doc Note A prototype implementation Adam AdamW MPS supports ` torch float ` ` torch float ` _Decoupled Weight Decay Regularization https arxiv org abs _On Convergence Adam Beyond https openreview net forum id=ryQu f-RZ _disable_dynamo_if_unsupported logic occurs decorator s applied F adam adamw params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor max_exp_avg_sqs list Tensor state_steps list Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim foreach Optional bool = None capturable bool = False differentiable bool = False fused Optional bool = None grad_scale Optional Tensor = None found_inf Optional Tensor = None has_complex bool = False amsgrad bool beta Union float Tensor beta Union float Tensor lr Union float Tensor weight_decay float eps float maximize bool r Functional API performs AdamW algorithm computation See ` ~torch optim AdamW ` details adam params grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps foreach=foreach capturable=capturable differentiable=differentiable fused=fused grad_scale=grad_scale found_inf=found_inf has_complex=has_complex amsgrad=amsgrad beta =beta beta =beta lr=lr weight_decay=weight_decay eps=eps maximize=maximize decoupled_weight_decay=True