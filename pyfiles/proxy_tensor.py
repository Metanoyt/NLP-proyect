mypy allow-untyped-decorators Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree __future__ annotations functools inspect logging operator threading typing typing_extensions weakref collections defaultdict OrderedDict collections abc Callable Generator Mapping Sequence contextlib _GeneratorContextManager contextmanager ExitStack nullcontext dataclasses dataclass typing Any Concatenate Optional overload Protocol TYPE_CHECKING TypeVar Union typing_extensions ParamSpec Self TypeVarTuple Unpack weakref WeakKeyDictionary torch torch _ops torch fx fx torch fx traceback fx_traceback torch utils _pytree pytree torch SymBool SymInt Tensor torch _dispatch python enable_python_dispatcher torch _library fake_class_registry FakeScriptObject torch _logging trace_structured torch _subclasses fake_impls fast_detach torch _subclasses fake_tensor FakeTensor FakeTensorMode is_fake unset_fake_temporarily torch _subclasses meta_utils is_sparse_any torch fx GraphModule Proxy Tracer torch fx graph_module _assign_attr torch fx node _side_effectful_need_to_be_preserved_pre_dispatch Argument Target torch fx passes shape_prop _extract_tensor_metadata torch nn Module torch overrides TorchFunctionMode torch utils _python_dispatch _disable_infra_mode _push_mode _unset_infra_mode TorchDispatchMode torch utils _stats count torch utils _thunk Thunk torch utils weak _WeakHashRef WeakIdKeyDictionary WeakTensorKeyDictionary _backward_state BackwardState sym_node SymNode TYPE_CHECKING types collections abc MutableMapping sympy torch _ops OpOverload torch fx _symbolic_trace PHBase torch types IntLikeType __all__ = PythonKeyTracer dispatch_trace make_fx DecompositionInterpreter py_sym_types get_innermost_proxy_mode get_proxy_mode handle_sym_dispatch maybe_enable_thunkify maybe_disable_thunkify _ProxyTracer = Union PythonKeyTracer _GraphAppendingTracerEx _AnyScriptObject = torch ScriptObject FakeScriptObject _AnyScriptObjectType = Union torch ScriptObject FakeScriptObject aten = torch ops aten prim = torch ops prim log = logging getLogger __name__ not_implemented_log = torch _logging getArtifactLogger __name__ not_implemented CURRENT_DECOMPOSITION_TABLE Mapping OpOverload Callable = CONSTANT_NUMEL_LIMIT = T = TypeVar T U = TypeVar U _P = ParamSpec _P R = TypeVar R _Ts = TypeVarTuple _Ts null_ctx_type = type nullcontext We currently convert all SymInt proxies before we use them This could plausibly handled Dynamo level pytree register_pytree_node torch Size lambda xs list xs None lambda xs _ tuple xs pyrefly ignore bad-argument-type flatten_with_keys_fn=lambda xs pytree SequenceKey i x i x enumerate xs None serialized_type_name= torch Size Ideally unflattening should lose info we unflatten torch Size tuple see above This necessary because torch Size constructor only accepts ints whereas our infra often transforms them non-ints e g symint proxies Anyway losing such info can cause pytree mapping spec matching fail so work around problem using following dict needed _pytree_subclasses_that_lose_info = torch Size tuple fake_signature fn Callable _P R nargs int - Callable _P R FX gets confused varargs de-confuse argnames = join f arg i i range nargs eval f lambda argnames fn argnames fn fn contextmanager decompose decomposition_table Optional Mapping OpOverload Callable - Generator Mapping OpOverload Callable None None global CURRENT_DECOMPOSITION_TABLE old_decomposition_table = CURRENT_DECOMPOSITION_TABLE CURRENT_DECOMPOSITION_TABLE = decomposition_table try yield CURRENT_DECOMPOSITION_TABLE finally CURRENT_DECOMPOSITION_TABLE = old_decomposition_table ensure we cannot collide other properties proxy_slot = object _NoDefault pass no_default = _NoDefault torch types py_sym_types PySymType _HasMeta Protocol meta dict str PySymType is_sym_node node _HasMeta - bool assert hasattr node meta All nodes traced proxy_tensor should have meta val node meta isinstance node meta val py_sym_types overload type ignore no-overload-impl set_proxy_slot obj Tensor tracer _ProxyTracer proxy _ProxyTensor - None overload set_proxy_slot obj _AnyScriptObjectType tracer _ProxyTracer proxy Proxy - None overload set_proxy_slot obj PySymType tracer _ProxyTracer proxy _PySymProxyType - None _DisableUpdateTensorTracker threading local value bool = False _disable_update_tensor_tracker_tls = _DisableUpdateTensorTracker _FAKE_TENSOR_ID_TO_PROXY_MAP_FOR_EXPORT dict int torch fx Node = _is_proxy_tensor_update_tensor_tracker_disabled - bool Returns current state disabling update tensor tracker _disable_update_tensor_tracker_tls value contextmanager _proxy_tensor_disable_update_tensor_tracker - Generator None None None NOTE Do clobber inplace ops By default tensor_tracker updated every time This leads chaining every operation FakeTensor For example mutable ops we have several consecutive mutable operations f x y z x copy_ y x copy_ z x Default graph result f_graph x y z x_ = x copy_ y x_ = x_ copy_ z x_ This chaining simplifies fx passes helps prevent reordering But some cases we want those nodes disconnected E g case splitting joint graph into forward backward If first inplace op happened forward second backward we want them after split properly placed Enabling context manager copy_ will result f_graph_ x y z x_ = x copy_ y x_ = x copy_ z x Results copy_ x x will have empty users graph The reason why behavior enabled all inplace ops some fx passes e g fx quantization rely chaining inplace ops like add_ their fusions passes We could revisit enabling logic all inplace ops future orig_value = _disable_update_tensor_tracker_tls value _disable_update_tensor_tracker_tls value = True try yield finally _disable_update_tensor_tracker_tls value = orig_value set_proxy_slot type ignore no-redef obj Union PySymType _AnyScriptObjectType Tensor tracer _ProxyTracer proxy object - None log debug set_proxy_slot s s s obj id obj proxy isinstance obj Tensor We DO want clobber proxies whenever we run inplace operation tensor affects metadata proxy assert isinstance proxy _ProxyTensor see NOTE Do clobber inplace ops _is_proxy_tensor_update_tensor_tracker_disabled tracer tensor_tracker obj = proxy isinstance obj _AnyScriptObject We DO want clobber proxies similar rationale tensors assert isinstance proxy Proxy tracer script_object_tracker obj = proxy NB Never clobber pre-existing proxy Although proxies principle equivalent when we do graph partitioning we need there spurious dependencies tangent inputs This works because primals get their SymInts set first THEN later we allocate tangent inputs Make sure SymInt derivable primal we use assert isinstance obj py_sym_types type obj obj tracer symnode_tracker proxy = typing cast _PySymProxyType proxy tracer symnode_tracker obj = proxy WAR python test dynamo test_subclasses py TestNestedTensor test_basic_autograd AOTAutograd doesn t pass outer sizes actual argument make_fx made use internally AOTAutograd s call tensor unflatten Because outer sizes isn t passed argument therefore untracked However turns out you luck out because Dynamo will manually add outer sizes argument so you can fix up proxy ness This probably fixed https github com pytorch pytorch pull sympy isinstance obj node expr sympy Symbol tracer sympy_expr_tracker obj node expr = _SympyExprTrackerValue proxy obj has_proxy_slot obj Tensor tracer _ProxyTracer - bool assert isinstance obj Tensor SymNode type obj pyrefly ignore no-matching-overload bool get_proxy_slot obj tracer False lambda _ True _PySymProxyType = Thunk Proxy overload get_proxy_slot obj Tensor tracer _ProxyTracer - _ProxyTensor overload get_proxy_slot obj Tensor tracer _ProxyTracer default U - Union _ProxyTensor U overload get_proxy_slot obj Tensor tracer _ProxyTracer default U transform Callable _ProxyTensor R - Union R U overload get_proxy_slot obj _AnyScriptObjectType tracer _ProxyTracer - Proxy overload get_proxy_slot obj _AnyScriptObjectType tracer _ProxyTracer default U - Union Proxy U overload get_proxy_slot obj _AnyScriptObjectType tracer _ProxyTracer default U transform Callable Proxy R - Union R U overload get_proxy_slot obj PySymType tracer _ProxyTracer - _PySymProxyType overload get_proxy_slot obj PySymType tracer _ProxyTracer default T - Union T _PySymProxyType overload get_proxy_slot obj PySymType tracer _ProxyTracer default U transform Callable _PySymProxyType R - Union R U default argument what slot set transform argument handy you need extract subfield successfully looked up result NOT default get_proxy_slot obj Union Tensor _AnyScriptObjectType PySymType tracer _ProxyTracer default object = no_default transform Callable = lambda x x - object tracker Any isinstance obj Tensor tracker = tracer tensor_tracker isinstance obj _AnyScriptObject tracker = tracer script_object_tracker assert isinstance obj py_sym_types type obj tracker = tracer symnode_tracker pyrefly ignore index-error pyrefly ignore no-matching-overload bad-argument-type value = tracker get obj value None isinstance obj py_sym_types obj node is_symbolic Last ditch - we found SymInt SymBool etc we don t know about tmp = tracer sympy_expr_tracker get obj node expr None value = tmp proxy Attempt build first principles _build_proxy_for_sym_expr tracer obj node expr obj pyrefly ignore no-matching-overload value = tracker get obj value None We don t know value - default isinstance default _NoDefault raise RuntimeError f obj type obj id obj tracked proxy tracer default res = transform value res functools cache _sympy_handlers - dict type sympy Expr Callable Any Returns dict converting sympy functions python operators i e ` sympy Mul ` - ` operator mul ` torch utils _sympy interp handlers = k v torch utils _sympy interp handlers items op = getattr operator v None op None handlers k = op handlers _build_proxy_for_sym_expr tracer _ProxyTracer expr sympy Expr out PySymType &#124; None = None - PySymType &#124; None Decompose ` expr ` look pieces inputs If ` out ` provided then will resulting SymNode ` out expr ` must same ` expr ` This function used when ProxyTorchDispatchMode sees SymNode hasn t seen before try associate traced inputs How can happen First thing remember although sympy Exprs interned so ` sympy Expr s s ` will always have same ` id ` will always compare equal SymNode does so doing ` SymNode s SymNode s ` twice row will give two unique SymNodes - On way happen we turn off tracing compute intermediate value then USE value tracing turned - example we turn off tracing do some FakeTensor propagation compute size dtensor does then turn tracing back use computed size - Another way we compute size one graph stash somewhere hidden such some meta-data later use different graph dtensor does too Since size computed first graph s official input second graph s tracked properly This often going show up usually works fullgraph graph break causes failure To handle we decompose sympy Expr look pieces inputs But there problems approach - We lose operation provanance We end up figuring out where get inputs - those may actually correct If we have s coming both tensor tensor we pick wrong one we could end up keeping tensor alive longer than intended - There s no guarantee those values inputs graph If we have s s computed graph used graph there s no guarantee input holds s actually input graph - The decomposition isn t guaranteed same Sympy can simplify expressions so s possible our inputs s s s we decompose into s s s - which wouldn t found Other ways we could handle - Don t Just require all inputs tracked properly This correct solution harder because you need track down each potential problem one one fix them And when fails s lot work figure out both why s failing right way fix This complicated fact stashed value could incorrect work fine until we happen get graph break wrong place - so may while before bug found Maybe we need dynamo abuse mode where we run tests many graph breaks inserted possible - Track SymNode ops separately proxy tracing Right now SymNode operations tracked part proxy tracing - so when we disable proxy tracing we also disable SymNode tracing But we don t have do - we could instead always have SymNodes track where they came just use when needed This solves problem tracing being temporarily turned off doesn t help input isn t present after graph break - Better decomposition Right now decomposition pretty simple We do have sat-solver available us so we could theoretically do better job figuring out correct decomposition But still relies having inputs available all - which isn t guarantee value = tracer sympy_expr_tracker get expr None assert out value value args = arg expr args arg_value = _build_proxy_for_sym_expr tracer arg None None args append arg_value args = tuple args func OpOverload &#124; None = _sympy_handlers get expr func type ignore assignment func Handler found None out None out = func args _sym_register tracer func args out out snapshot_fake val Tensor include_real bool = False - Optional Tensor val detach will also eventually call fast_detach saves us full trip into __torch_dispatch__ snapshot_fake called lot isinstance val FakeTensor fast_detach val fake_mode val include_real val detach _ExtractValType = Optional Union PySymType _AnyScriptObjectType BackwardState list _ExtractValType tuple _ExtractValType dict str _ExtractValType Tensor int float bool extract_val val _ExtractValType include_real bool = False - _ExtractValType is_fake val snapshot_fake val include_real=include_real isinstance val py_sym_types val isinstance val _AnyScriptObject val isinstance val BackwardState val isinstance val list tuple val __class__ extract_val x x val isinstance val dict k extract_val v k v val items isinstance val Tensor val is_sparse NB Kinda hacky we should try get val metadata everywhere TODO This doesn t properly track storages A more robust approach would maintain per-trace FakeTensorMode from_real_tensor create fake values don t forget snapshot_fake torch _guards detect_fake_mode fake_tensor_mode = detect_fake_mode val fake_tensor_mode fake_tensor_mode = FakeTensorMode allow_fallback_kernels=True fake_tensor_mode torch empty_strided val shape val stride device=val device dtype=val dtype None isinstance val int float bool val val None None typing_extensions assert_never val contextmanager _enable_thunkify tracer _ProxyTracer enable bool = True - Generator None None None Enable thunkification inside context manager Thunkification prevents SymNode computation directly being traced into FX graph instead compute only added graph actually used This helps us track SymNode compute when computed since we need something put tracker even unlikely used old = tracer enable_thunkify tracer enable_thunkify = enable try yield finally tracer enable_thunkify = old contextmanager maybe_disable_thunkify - Generator None None None Within context disable thunkification See func ` maybe_enable_thunkify ` more details This helpful you have wrapper function which you want enable thunkification some segment inside say original user function you want disable thunkification you know needed there proxy_mode = get_proxy_mode proxy_mode None _enable_thunkify proxy_mode tracer enable=False yield yield contextmanager maybe_enable_thunkify - Generator None None None Within context manager you doing make_fx tracing we will thunkify all SymNode compute avoid tracing into graph unless actually needed You should prefer avoid using much possible lazy evaluation SymNode tracing can lead long chains thunks which will stack overflow you evaluate them However currently sometimes necessary there buggy parts PT which will fail s tracked proxy error due insufficient tracing SymNode computation proxy_mode = get_proxy_mode proxy_mode None _enable_thunkify proxy_mode tracer yield yield Note invariants node meta val What invariants do we have val set FX node It has accurate metadata only metadata exists below all other subsystems most notably autograd also vmap functorch transforms etc This means you can get dtype shape stride storage you CANNOT get requires_grad grad_fn _base _base actually may set due recursive call ADInplaceOrView you shouldn t rely set_meta proxy Proxy val _ExtractValType - Proxy proxy node meta val = extract_val val include_real= proxy node op == placeholder _enable_thunkify proxy tracer type ignore arg-type Best effort tensor_meta setting prefer using val is_fake val proxy node meta tensor_meta = _extract_tensor_metadata val isinstance val Tensor val is_sparse proxy node meta tensor_meta = _extract_tensor_metadata val proxy thunkify tracer _ProxyTracer f Callable _P R args _P args kwargs _P kwargs - Thunk R Delays computation f until s called again Also caches result tracer enable_thunkify Thunk functools partial f args kwargs r = f args kwargs Thunk lambda r track_tensor tensor Tensor proxy Proxy constant Optional Tensor tracer _ProxyTracer - None try_set_proxy_slot outer_s IntLikeType proxy_callable Callable Concatenate PySymType _P Proxy args _P args kwargs _P kwargs - None assert callable proxy_callable isinstance outer_s SymInt _enable_thunkify tracer set_proxy_slot outer_s tracer thunkify tracer proxy_callable outer_s args kwargs The basic idea we need associate each tensor SymInt Proxy How do we setup association We just store proxy proxy slot object keyed tracer so we have multiple tracers same time they don t clobber each other i s enumerate tensor shape try_set_proxy_slot s lambda x i set_meta tracer create_proxy call_function torch ops aten sym_size int proxy i x i is_sparse_any tensor i s enumerate tensor stride try_set_proxy_slot s lambda x i set_meta tracer create_proxy call_function torch ops aten sym_stride int proxy i x i try_set_proxy_slot tensor numel lambda x set_meta tracer create_proxy call_function torch ops aten sym_numel default proxy x is_sparse_any tensor try_set_proxy_slot tensor storage_offset lambda x set_meta tracer create_proxy call_function torch ops aten sym_storage_offset default proxy x set_proxy_slot tensor tracer _ProxyTensor proxy constant _NestedProxys = Union Proxy Sequence _NestedProxys Mapping object _NestedProxys _NestedTensors = Union Tensor Sequence _NestedTensors Mapping object _NestedTensors track_tensor_tree inner_res T proxy_res _NestedProxys constant Optional _NestedTensors tracer _ProxyTracer - T NB We call set_unbacked_bindings only topmost call track_tensor_tree recursive calls This because there must only ONE unbacked_binding proxy call should one where all unbacked SymInts actually first come into existence If you call again inner proxies tuple projections you will have multiple unbacked_bindings same symbol they re going show up anywhere I briefly deceived into setting unbacked bindings recursively when working https github com pytorch pytorch pull because I observed some extra unbacked bindings needed handle some higher order operator code But actually looks like just unrelated bug needed fixed separately _set_unbacked_bindings inner_res proxy_res wrap_with_proxy e object proxy _NestedProxys constant Optional _NestedTensors - None isinstance e Tensor assert isinstance proxy Proxy assert constant None isinstance constant Tensor track_tensor e proxy tracer=tracer constant=constant set_meta proxy e isinstance e py_sym_types assert isinstance proxy Proxy NB eagerly set meta here so numbering order set_meta proxy e set_proxy_slot e tracer thunkify tracer lambda proxy isinstance e _AnyScriptObject assert isinstance proxy Proxy set_proxy_slot e tracer proxy set_meta proxy e isinstance e tuple list example use case allreduce_ returns tensor work isinstance proxy fx Proxy set_meta proxy e get_constant c Optional _NestedTensors idx int - Optional _NestedTensors c None None assert isinstance c list tuple c idx idx ee enumerate e Use indexer here - proxy List then will unwrap If s Proxy then will proxy getelem wrap_with_proxy ee proxy idx get_constant constant idx type ignore index isinstance e dict example use case triton_kernel_wrapper takes arguments kwargs In theory we could support const-prop when proxy-tensor-tracing operators returns dicts tensors we have no use case today since only op we currently trace can dict triton_kernel_wrapper_functional mutation which does participate const-prop assert constant None isinstance proxy fx Proxy set_meta proxy e key val e items wrap_with_proxy val proxy key None type ignore index isinstance e BackwardState assert isinstance proxy Proxy set_meta proxy e e proxy = proxy intentionally pass primitives pass wrap_with_proxy inner_res proxy_res constant inner_res dataclass _ProxyTensor proxy Proxy constant Optional Tensor fetch_sym_proxy tracer _ProxyTracer - Callable PySymType Union bool int float Proxy inner e PySymType - Union int bool float Proxy n = e node n constant None n constant e node expr is_number isinstance e SymBool bool e node expr isinstance e SymInt int e node expr float e node expr assert isinstance e py_sym_types NB we REQUIRE all symints tracked get_proxy_slot e tracer force inner overload fetch_object_proxy tracer _ProxyTracer t Tensor - Union _ProxyTensor Tensor overload fetch_object_proxy tracer _ProxyTracer t _AnyScriptObjectType - Union Proxy _AnyScriptObjectType overload fetch_object_proxy tracer _ProxyTracer t PySymType - Union _PySymProxyType PySymType fetch_object_proxy tracer _ProxyTracer t Union Tensor _AnyScriptObjectType PySymType - object get_proxy_slot t tracer t HANDLED_TYPES = Tensor torch nn Parameter FakeTensor _maybe_record_pointwise_barrier func object proxy_mode ProxyTorchDispatchMode - None Records operators whose tensor outputs inputs fp bf so downstream pointwise code can emulate eager s rounding behavior when emulate_precision_casts enabled proxy_mode decomp_layers proxy_mode emulate_precision_casts isinstance func torch _ops OpOverload last_node = next iter reversed proxy_mode tracer graph nodes t = last_node meta get val low_pr_fp = torch bfloat torch float output_low_precision = isinstance t torch Tensor t dtype low_pr_fp output_low_precision input_node last_node all_input_nodes val = input_node meta get val hasattr input_node meta None isinstance val torch Tensor val dtype low_pr_fp output_low_precision = True break output_low_precision last_node meta low_precision_pointwise_barrier = True _fetch_proxies_and_all_constant_flag flat_args_kwargs Union list object tuple object tracer _ProxyTracer - tuple list object tuple object bool Given flat arguments fetch proxies whether they all constants This later used proxy_call when someone trying stitch together graph node tf td modes f_flat_args_kwargs = fetch_object_proxy tracer x isinstance x Tensor _AnyScriptObject x x flat_args_kwargs If there SymInts we also should consider constant However fake tensor handling SymInts sufficiently broken I couldn t write test case all_constant = any t constant None t f_flat_args_kwargs isinstance t _ProxyTensor TODO maybe constant SymInts should also allowed Not sure can happen any isinstance x py_sym_types x flat_args_kwargs proxy_flat_args_kwargs = e proxy isinstance e _ProxyTensor e e f_flat_args_kwargs proxy_flat_args_kwargs = fetch_sym_proxy tracer e isinstance e py_sym_types e e proxy_flat_args_kwargs f_flat_args_kwargs tuple proxy_flat_args_kwargs all_constant proxy_call proxy_mode ProxyTorchDispatchMode func OpOverload pre_dispatch bool args tuple object kwargs dict str object - object unrecognized_types list type = flat_args_kwargs spec = pytree tree_flatten args kwargs can_handle_tensor x Tensor - bool r = type x HANDLED_TYPES has_proxy_slot x proxy_mode tracer proxy_mode _allow_fake_constant r = r type x torch _subclasses FakeTensor r unrecognized_types append type x r If there any tensor subclasses we need handle those tensor subclasses first TODO we could use types test all can_handle_tensor x x flat_args_kwargs isinstance x Tensor not_implemented_log debug ProxyTensorMode tensors without proxy had unrecognized subclasses s unrecognized_types NotImplemented r = maybe_handle_decomp proxy_mode func args kwargs r NotImplemented _maybe_record_pointwise_barrier func proxy_mode r For pre-autograd tracing we do want run CompositeImplicit decomps pre_dispatch func torch ops aten size default torch ops aten stride default torch ops aten storage_offset default proxy_mode r = func decompose args kwargs r NotImplemented r func torch ops aten is_nonzero default proxy_mode torch _check args numel == type ignore attr-defined lambda Boolean value Tensor more than one value ambiguous args = item type ignore attr-defined tracer = proxy_mode tracer f_flat_args_kwargs proxy_flat_args_kwargs all_constant = _fetch_proxies_and_all_constant_flag flat_args_kwargs tracer torch Tag data_dependent_output func tags Check all Tensor inputs constants all_constant const_flat_args_kwargs = t constant isinstance t _ProxyTensor t t f_flat_args_kwargs const_args const_kwargs = pytree tree_unflatten const_flat_args_kwargs spec unset_fake_temporarily func const_args const_kwargs If any Tensor inputs real FakeTensor we may incorrectly burn constants allowing access Raise error case proxy_mode _error_on_data_dependent_ops pytree tree_all_only Tensor lambda t is_fake t args kwargs raise RuntimeError f It appears you re trying get value out tracing tensor func - erroring out It s likely caused data-dependent control flow similar It may possible trace dynamic shapes try setting tracing_mode= symbolic your make_fx call proxy_args proxy_kwargs = pytree tree_unflatten proxy_flat_args_kwargs spec When we trace through torch tensor invocation you never actually see torch ops aten tensor call Instead way function implemented internally we allocate plain tensor guaranteed plain tensor we disable all modes when doing so then call lift_fresh give modes chance do their stuff Furthermore tensor argument lift_fresh guaranteed freshly allocated so we want lift_fresh no-op directly returning input argument Here basic problem when we trace sequence executions into FX graph what happens call sequence Traditionally tensor constants get interned buffers FX GraphModule But dangerous Consider x = torch tensor x add_ Naively traces into t = _tensor_constant initialized torch tensor x = torch ops aten lift_fresh t x add_ If lift_fresh returns t directly subsequent add_ call will modify tensor constant Really problem we ve violated invariant argument lift fresh So what we should preserve invariant replacing lift_fresh lift_fresh_copy t = _tensor_constant initialized torch tensor x = torch ops aten lift_fresh_copy t x add_ This what overload modification does func torch ops aten lift_fresh default func = torch ops aten lift_fresh_copy default proxy_out = proxy_mode tracer create_proxy call_function func proxy_args proxy_kwargs name=proxy_mode tracer graph _target_to_str func overloadpacket __name__ _enable_thunkify proxy_mode tracer out = func args kwargs In some circumstances we will tracing situation where tensor statically known constant currently only happens you run torch tensor deterministic factory functions like torch arange don t get treatment When tensor question small s helpful due constant propagation case we call item which case we can constant value known rather than give error The logic here tests constant propagation possible because all inputs constant If so we disable fake tensor mode do true compute constant It s worth highlighting we re making policy decision here There potential tensor actually quite large we don t actually want run compute The tensor being quite large one reasons why factory functions don t get treatment since they can quite large parameter initialized constant value will Similarly there also potential run operator blows up size small tensor we don t protect against case we could force e g only single element constant computation testing numel result before propagating const-ness Similarly we don t require constant live CPU we could any_constant = any t constant None t f_flat_args_kwargs isinstance t _ProxyTensor constant = None tensor_numel_in_limit t Tensor - bool t numel = CONSTANT_NUMEL_LIMIT If lift input tensor guaranteed constant so we keep copy original argument along so we can query we re asked item some later point func torch ops aten lift_fresh_copy default out numel = CONSTANT_NUMEL_LIMIT unset_fake_temporarily assert isinstance args Proxy Tensor type args constant = args clone torch Tag nondeterministic_seeded func tags all_constant any_constant pytree tree_all_only Tensor tensor_numel_in_limit out NB do NOT include factories constants unset_fake_temporarily const_flat_args_kwargs = t constant isinstance t _ProxyTensor t t f_flat_args_kwargs const_args const_kwargs = pytree tree_unflatten const_flat_args_kwargs spec constant = func const_args const_kwargs constant = None track_tensor_tree out proxy_out constant=constant tracer=tracer _maybe_record_pointwise_barrier func proxy_mode out _SymNodeDict Wrapper around dictionary will hash SymInts their nodes __init__ - None sym_node_dict dict PySymType _PySymProxyType = __setitem__ key PySymType value _PySymProxyType - None sym_node_dict key node = value __getitem__ key PySymType - _PySymProxyType sym_node_dict key node __contains__ key PySymType - bool key node sym_node_dict get key PySymType default Optional _PySymProxyType = None - _PySymProxyType dict get s annotation doesn t accept ` None ` when value type isn t Optional sym_node_dict get key node default type ignore arg-type return-value __iter__ - Any raise NotImplementedError __len__ - int len sym_node_dict dataclass _SympyExprTrackerValue proxy _PySymProxyType value PySymType PythonKeyTracer Tracer script_object_tracker MutableMapping _AnyScriptObjectType Proxy symnode_tracker _SymNodeDict sympy_expr_tracker dict sympy Symbol _SympyExprTrackerValue tensor_tracker MutableMapping Tensor _ProxyTensor torch_fn_counts dict OpOverload int enable_thunkify bool = False __init__ - None super __init__ autowrap_modules= type ignore arg-type tensor_tracker = WeakTensorKeyDictionary symnode_tracker = _SymNodeDict script_object_tracker = WeakIdKeyDictionary dict=None ref_type=_WeakHashRef sympy_expr_tracker = Stores torch function called during tracing torch_fn_metadata = None Stores counts every torch function called This help distinguish between different calls same torch function torch_fn_counts = enable_thunkify = False In general we don t want make modules leaves In principle users tracer might want override order turn couple specific modules into leaves traced graph call_module m Module forward Callable Any args tuple Any kwargs dict str Any - Any forward args kwargs We don t want turn getattr calls into proxies So we just actual value getattr attr str attr_val object parameter_proxy_cache dict str Proxy - object attr_val create_arg object - fx node Node isinstance torch nn Parameter n p root named_parameters p create_node get_attr n qualname = get_fresh_qualname _param_constant setattr root qualname create_node get_attr qualname isinstance py_sym_types assert node constant None node constant super create_arg type ignore return-value overload unwrap_proxy e Tensor - Union Proxy Tensor overload unwrap_proxy e PySymType - Union Proxy PySymType overload unwrap_proxy e _AnyScriptObjectType - Union Proxy _AnyScriptObjectType unwrap_proxy e T - object isinstance e Tensor get_proxy_slot e e lambda x x proxy type ignore attr-defined isinstance e py_sym_types get_proxy_slot e e lambda e e force isinstance e _AnyScriptObject get_proxy_slot e e e create_node kind str target Target args tuple Argument kwargs dict str Argument name Optional str = None type_expr Optional Any = None - torch fx Node node = super create_node kind target args kwargs name type_expr type ignore arg-type node op placeholder output stack_trace node meta del node meta stack_trace kind == get_attr assert isinstance target str attr = getattr root target isinstance attr torch Tensor disable_proxy_modes_tracing node meta val = extract_val attr map_fn v Any - Optional _ExtractValType isinstance v torch fx Node val v meta None val = v meta val other subclasses like FunctionalTensor error ` extract_val ` Attempting use FunctionalTensor its own just store FakeTensors now isinstance val torch Tensor isinstance val FakeTensor None extract_val v meta val _should_save_eager_input_vals target args kwargs NOTE eager_input_vals We save original args kwargs FakeTensor values nodes have exact stride requirements This useful downstream We use information inside Inductor ensure inputs stride-sensitive operators have correct strides arg_inp kwarg_inp = torch fx node map_aggregate args kwargs map_fn type ignore misc arg-type node meta eager_input_vals = arg_inp kwarg_inp node _should_save_eager_input_vals target Any args_kwargs Optional tuple tuple Argument dict str Argument = None - bool torch _higher_order_ops invoke_subgraph InvokeSubgraphHOP callable target False isinstance target torch _higher_order_ops triton_kernel_wrap TritonKernelWrapperFunctional torch _higher_order_ops triton_kernel_wrap TritonKernelWrapperMutation InvokeSubgraphHOP True args_kwargs None target torch ops higher_order auto_functionalized target torch ops higher_order auto_functionalized_v args = args_kwargs assert isinstance args torch _ops OpOverload torch _ops HigherOrderOperator _should_save_eager_input_vals args None target torch ops higher_order with_effects TODO inductor lowering with_effects needs updated propagate arg_kwarg_vals False isinstance target torch _ops HigherOrderOperator pytree tree_any _should_save_eager_input_vals args_kwargs raise RuntimeError f NYI The HOP target has input OpOverload f needs exact strides We probably need special logic f propagate FakeTensor vals Please file issue isinstance target torch _ops OpOverload torch _library utils get_layout_constraint_tag get_layout_constraint_tag target == torch _C Tag needs_exact_strides False _make_temp_remove_mode_context_manager mode_ty type TorchFunctionMode - Callable _GeneratorContextManager Optional TorchFunctionMode contextmanager context_manager_fn - Generator Optional TorchFunctionMode None None torch overrides _len_torch_function_stack _pop_mode _push_mode temp_elements = removed_mode = None while _len_torch_function_stack mode = _pop_mode isinstance mode mode_ty removed_mode = mode break temp_elements append mode mode reversed temp_elements _push_mode mode try yield removed_mode finally removed_mode None count = len temp_elements while count mode = _pop_mode count -= temp_elements append removed_mode mode reversed temp_elements _push_mode mode context_manager_fn torch _disable_dynamo dispatch_trace root Union Module Callable tracer Tracer concrete_args Optional tuple Any = None - GraphModule graph = tracer trace root concrete_args type ignore arg-type NB careful DCE item calls impure_pred n fx Node - bool symbolic_shapes is_accessor_node Always defer built-in notion impure n is_impure True Accessors always OK DCE is_accessor_node n False If operator question takes SymInt args SymInt output we assume s pure OK DCE isinstance n meta get val py_sym_types NB constant args ok all isinstance meta get val py_sym_types n args isinstance fx Node False No idea just assume s OK True graph eliminate_dead_code impure_pred torch _inductor fx_passes dedupe_symint_uses dedupe_symints dedupe_symints graph name = root __class__ __name__ isinstance root Module root __name__ fx _lazy_graph_module _make_graph_module tracer root graph name wrap_key f Callable Unpack _Ts R tensors tuple Unpack _Ts tracer _ProxyTracer pre_dispatch bool - Callable _P R flat_tensors _tensors_spec = pytree tree_flatten tensors functools wraps f wrapped proxies _P args _unused _P kwargs - R nonlocal tensors flat_proxies _proxies_spec = pytree tree_flatten proxies assert len flat_proxies == len flat_tensors disable_proxy_modes_tracing m assert isinstance m ProxyTorchDispatchMode track_tensor_tree flat_tensors flat_proxies constant=None tracer=tracer getattr tracer proxy_module_inputs False tensors = type ignore assignment var-annotated p isinstance t torch nn Module t t p zip tensors proxies type ignore arg-type get_tensor_proxy_slot t Tensor - Union Tensor Proxy get_proxy_slot t tracer t lambda x x proxy type ignore attr-defined out = f tensors type ignore call-arg out = pytree tree_map_only Tensor get_tensor_proxy_slot out out = pytree tree_map_only _AnyScriptObject lambda t get_proxy_slot t tracer t lambda x x out get_sym_proxy_slot t PySymType - Proxy get_proxy_slot t tracer force out = pytree tree_map_only py_sym_types get_sym_proxy_slot out out wrapped TODO Make downstream users work OperatorBase ORIGINAL_ATEN Optional object = None contextmanager set_original_aten_op func OpOverload - Generator None None None global ORIGINAL_ATEN ORIGINAL_ATEN None fx_traceback has_preserved_node_meta ORIGINAL_ATEN = func fx_traceback current_meta original_aten = func try yield finally ORIGINAL_ATEN = None fx_traceback current_meta original_aten = None yield TorchFunctionMetadataMode TorchFunctionMode __init__ tracer _ProxyTracer - None tracer = tracer __torch_function__ func OpOverload types tuple torch _C _TensorMeta args tuple object = kwargs Optional dict str object = None - object kwargs = kwargs pyrefly ignore bad-assignment tracer torch_fn_metadata = func tracer torch_fn_counts func = tracer torch_fn_counts get func + func args kwargs _temp_remove_metadata_torch_function_mode = _make_temp_remove_mode_context_manager TorchFunctionMetadataMode This mode only used pre_dispatch tracing In particular we need make sure autograd autocast API s do desugar into dispatcher operators stay graph PreDispatchTorchFunctionMode TorchFunctionMode __init__ tracer _ProxyTracer - None tracer = tracer The input torch amp autocast_mode _exit_autocast graph node should enter_autocast node So we have save enter autocast node here assign exit_autocast call_function node enter_autocast_nodes list torch fx Node = __torch_function__ func Union OpOverload Callable types tuple torch _C _TensorMeta args tuple object = kwargs Optional dict str object = None - object kwargs = kwargs func _side_effectful_need_to_be_preserved_pre_dispatch It s passing export verifier which needs verify meta val TODO tmanlaibaatar we should systematically couple export verifier instead hardcoding here T func torch amp autocast_mode _exit_autocast enter_node = enter_autocast_nodes pop args = enter_node node = tracer create_node call_function func args type ignore arg-type func torch amp autocast_mode _enter_autocast enter_autocast_nodes append node func torch _C _set_grad_enabled torch amp autocast_mode _enter_autocast torch amp autocast_mode _exit_autocast node meta val = None For autocast python APIs run so we don t have run them again here func torch _C _set_grad_enabled pyrefly ignore bad-argument-type func args kwargs node We need more complicated handling here because inputs these functions sometimes tensors symints where we need fetch proxies properly func torch _functorch predispatch _add_batch_dim torch _functorch predispatch _remove_batch_dim torch _functorch predispatch _vmap_increment_nesting torch _functorch predispatch _vmap_decrement_nesting torch _functorch vmap lazy_load_decompositions _ proxies _ = _fetch_proxies_and_all_constant_flag args tracer out_proxy = tracer create_proxy call_function func proxies res = func args kwargs track_tensor_tree res out_proxy constant=None tracer=self tracer res func args kwargs _temp_remove_pre_dispatch_torch_function_mode = _make_temp_remove_mode_context_manager PreDispatchTorchFunctionMode ProxyTorchDispatchMode TorchDispatchMode Ensure read-only exists only legacy reasons property enable_tracing - bool True __init__ tracer _ProxyTracer tracing_mode str pre_dispatch bool = False _allow_fake_constant bool = False _error_on_data_dependent_ops bool = True - None dk = torch _C DispatchKey PreDispatch pre_dispatch None super __init__ dk tracer = tracer tracing_mode = tracing_mode pre_dispatch = pre_dispatch _allow_fake_constant = _allow_fake_constant _error_on_data_dependent_ops = _error_on_data_dependent_ops Indicates our torch_dispatch dispatching infra infra mode lower dispatching precedence _mode_key = torch _C _TorchDispatchModeKey PROXY Every time we enter mode we maintain stack telling us what previous ProxyTorchDispatchMode state there any This lets us properly reset state exit enter_stack list Optional ProxyTorchDispatchMode = decomp_layers int = torch _inductor config emulate_precision_casts bool = config emulate_precision_casts count __torch_dispatch__ func OpOverload types tuple torch _C _TensorMeta args tuple object = kwargs Optional dict str object = None - object set_original_aten_op func kwargs = kwargs func == prim device default func args kwargs proxy_call func pre_dispatch args kwargs __enter__ - Self Stash store previous proxy mode there may may one maybe_prev_proxy_mode = _unset_infra_mode torch _C _TorchDispatchModeKey PROXY enter_stack append maybe_prev_proxy_mode super __enter__ __exit__ exc_type Optional type BaseException exc_value Optional BaseException traceback Optional types TracebackType - Optional bool b = super __exit__ exc_type exc_value traceback Re-enable previous proxy mode there one mb_previous_proxy_mode = enter_stack pop mb_previous_proxy_mode None _push_mode mb_previous_proxy_mode b classmethod is_infra_mode cls - bool True __sym_dispatch__ func OpOverload types tuple torch _C _TensorMeta args tuple object kwargs dict str object - object Peephole optimize multiply one NB careful trigger guards here func operator mul isinstance args int args == args isinstance args int args == args For speed we assume there no nested data structures otherwise we could use tree_map We also assume there no keyword arguments assert kwargs out = func args kwargs _sym_register tracer func args out out _sym_register tracer _ProxyTracer func OpOverload args tuple object out object - None If func returned constant we don t need trace we have determined result constant no matter inputs symbolic no longer necessary trace computation This could occur func triggered some guards isinstance out py_sym_types p_out_thunk = thunkify tracer _compute_proxy tracer func=func args=args out=out set_proxy_slot out tracer p_out_thunk _compute_proxy tracer _ProxyTracer func OpOverload args tuple object out PySymType - Proxy Handle torch sym_sum n_args tuple object len args == isinstance args list tuple n_args = tuple get_proxy_slot tracer force node isinstance py_sym_types args n_args = tuple get_proxy_slot tracer force node isinstance py_sym_types args func doesn t have __torch_function__ Proxy can interpose so we gotta do manually n_out = tracer create_node call_function func n_args type ignore arg-type p_out = fx Proxy n_out tracer set_meta p_out out p_out _GraphAppendingTracerEx fx proxy GraphAppendingTracer script_object_tracker MutableMapping _AnyScriptObjectType Proxy symnode_tracker MutableMapping PySymType _PySymProxyType tensor_tracker MutableMapping Tensor _ProxyTensor sympy_expr_tracker dict sympy Symbol _SympyExprTrackerValue torch_fn_metadata Optional OpOverload torch_fn_counts dict OpOverload int enable_thunkify bool = False __init__ graph fx graph Graph - None super __init__ graph symnode_tracker = weakref WeakKeyDictionary tensor_tracker = WeakTensorKeyDictionary sympy_expr_tracker = script_object_tracker = WeakIdKeyDictionary dict=None ref_type=_WeakHashRef Stores torch function called during tracing torch_fn_metadata = None Stores counts every torch function called This help distinguish between different calls same torch function torch_fn_counts = TODO I m sure what point you can just make_fx through regular Interpreter DecompositionInterpreter fx Interpreter __init__ module fx GraphModule new_graph fx Graph decomposition_table Optional Mapping OpOverload Callable = None kwargs object - None super __init__ module kwargs type ignore arg-type new_graph = new_graph tracer = _GraphAppendingTracerEx new_graph Blegh decomposition_table = decomposition_table mode = ProxyTorchDispatchMode tracer tracing_mode= real pyrefly ignore bad-override placeholder target str type ignore override args tuple object kwargs dict str object - object out = super placeholder target args kwargs type ignore arg-type proxy = fx Proxy new_graph placeholder target tracer track_tensor_tree out proxy constant=None tracer=self tracer TODO handle case where first character target out pyrefly ignore bad-override get_attr target str type ignore override args tuple object kwargs dict str object - object out = super get_attr target args kwargs type ignore arg-type proxy = fx Proxy new_graph get_attr target tracer track_tensor_tree out proxy constant=None tracer=self tracer out call_function call_method call_module get traced automatically outer mode pyrefly ignore bad-override output target str type ignore override args tuple object kwargs dict str object - object out = super output target args kwargs type ignore arg-type get_proxy_node x _ProxyTensor - fx node Node x proxy node unwrap e Tensor - Union Tensor fx Node get_proxy_slot e tracer e get_proxy_node new_graph output pytree tree_map unwrap out out run args object kwargs object - object Should enter mode least once being able restore later See https github com pytorch pytorch pull #discussion_r decompose decomposition_table mode super run args kwargs type ignore arg-type wrapper_and_args_for_make_fx func Callable R args tuple object kwargs dict str object - tuple Callable list object R list object make_fx doesn t support kwargs so we need do flattening then unflatten args before calling func flat_args spec = pytree tree_flatten args kwargs wrapped flat_args list object - R fn_args fn_kwargs = pytree tree_unflatten flat_args spec func fn_args fn_kwargs wrapped flat_args contextmanager disable_autocast_cache - Generator None None None old_value = torch is_autocast_cache_enabled torch set_autocast_cache_enabled False try yield finally torch set_autocast_cache_enabled old_value _ModuleNotInstalledAsSubmoduleError NameError pass Base inline _ModuleStackTracer __init__ AttrProxy _AttrProxy reset_proxy_mapping base Module path str - None pass _ModuleStackTracer PythonKeyTracer r Customized version PythonKeyTracer retains module stack information node meta nn_module_stack FX symbolic trace actually does already relies ` root ` being actual module being traced Since make_fx traces lambda our creation things don t work properly So version we hold onto reference original module scope_root use match path Also when we see A \ B C \ D we want record path A B D recording only one path See Note Preserving nn module stack metadata during export non-strict mode noqa W __init__ scope_root GraphModule - None super __init__ record_stack_traces = True _record_forward_stack_traces_only = True scope_root = scope_root enable_attr_proxy = False submodule_paths = name m scope_root named_modules remove_duplicate=False m submodule_paths log info Shared module found between s s AttrProxy enabled submodule_paths m name enable_attr_proxy = True submodule_paths m = name proxy_paths WeakKeyDictionary _AttrProxy str = WeakKeyDictionary attr_proxy_map WeakKeyDictionary Module _AttrProxy = WeakKeyDictionary proxy_modules WeakKeyDictionary _AttrProxy Module = WeakKeyDictionary counter = module_id_cache = defaultdict list name mod scope_root named_modules remove_duplicate=False module_id_cache id mod append name Build wrapper around _AttrProxy provide tracer We can t store _AttrProxy itself beceause we mimic underlying including its attributes tracer = AttrProxy _AttrProxy __init__ base Union Module _AttrProxy path str - None isinstance base _AttrProxy base = base get_base type ignore attr-defined assert isinstance base Module Class modified subclass torch nn Module Warning We blow away our own attributes here mimic base - so don t expect ` x ` do anything useful pyrefly ignore no-matching-overload pyrefly ignore bad-override __class__ = type base __class__ __name__ __class__ base __class__ __dict__ = base __dict__ __class__ __module__ = base __class__ __module__ __class__ __qualname__ = base __class__ __qualname__ This overwrites any existing paths ` base ` AttrProxy tracer proxy_paths = path tracer proxy_modules = base __getattr__ name str - AttrProxy assert isinstance Module Calling into torch nn Module __getattr__ super That __getattr__ patched module_getattr_wrapper _symbolic_trace py which then calls into _ModuleStackTracer getattr attr_val = super __getattr__ name type ignore misc isinstance attr_val Module attr_val pyrefly ignore index-error AttrProxy attr_val tracer proxy_paths + + name get_base - Module tracer proxy_modules __getitem__ idx Union int slice - AttrProxy isinstance idx slice isinstance torch nn Sequential Copied nn modules container py res = torch nn Sequential OrderedDict list _modules items idx pyrefly ignore index-error AttrProxy res f tracer proxy_paths idx isinstance torch nn ModuleList Copied nn modules container py res = torch nn ModuleList list _modules values idx pyrefly ignore index-error AttrProxy res f tracer proxy_paths idx super __getitem__ idx type ignore misc property _modules - dict str AttrProxy assert _modules __dict__ submodules = __dict__ _modules assert isinstance submodules dict key AttrProxy value tracer proxy_paths + + str key type ignore misc value None value key value submodules items proxy_type = AttrProxy path_of_module mod Module - str Use tracked access path during tracing instead default BFS behavior Still use all possible module paths verify result mod scope_root isinstance mod _AttrProxy proxy_paths mod try Tracer path_of_module mod except NameError e raise _ModuleNotInstalledAsSubmoduleError e getattr attr str attr_val object parameter_proxy_cache dict str Proxy - object isinstance attr_val Module isinstance attr_val fx GraphModule enable_attr_proxy super getattr attr attr_val parameter_proxy_cache isinstance attr_val _AttrProxy attr_val See NOTE caching AttrProxy attr_val attr_proxy_map attr_proxy_map attr_val = proxy_type attr_val attr attr_proxy_map attr_val reset_proxy_mapping attr_val attr attr_proxy_map attr_val trace type ignore override root Union Module Callable concrete_args Optional dict str object - fx Graph res = super trace root concrete_args NOTE export non-strict fake tensor leak detection In non-strict export we don t have dynamo s side effect tracking logic which makes some cases hard detect In general our detecting strategy We instrument fake tensor creation log all fake tensors created during export We dump proxy fake tensor map make_fx tracer _FAKE_TENSOR_ID_TO_PROXY_MAP_FOR_EXPORT Filter out fake tensors logged during Associated TrackedFake input tracking thing symbolic_shapes Associated gm meta Do ID match proxies global _FAKE_TENSOR_ID_TO_PROXY_MAP_FOR_EXPORT _FAKE_TENSOR_ID_TO_PROXY_MAP_FOR_EXPORT clear key val tensor_tracker items _FAKE_TENSOR_ID_TO_PROXY_MAP_FOR_EXPORT id key = val proxy node Since we making _AttrProxy mimic original submodule when someone registers module directly tracer while tracing proxy object gets registered first So we need replace proxy modules real ones This can happen during HOO tracing proxy_module_names_to_be_replaced list tuple str _AttrProxy = name module root named_modules module proxy_modules proxy_module_names_to_be_replaced append name module _delete_proxy_attr obj Module target str - bool Copied fx graph_module py Customized proxy type atoms = target split path target_submod = atoms - atoms - assert isinstance obj Module mod = obj Get parent module item path hasattr mod item False mod = getattr mod item isinstance mod _AttrProxy Module False hasattr mod target_submod False At least leaf module should proxy type isinstance getattr mod target_submod _AttrProxy False delattr mod target_submod True proxy_module_name proxy_module proxy_module_names_to_be_replaced _delete_proxy_attr root proxy_module_name actual_module = proxy_modules proxy_module _assign_attr actual_module root proxy_module_name res call_module m Module forward Callable args tuple object kwargs dict str object - None PythonKeyTracer overrides call_module avoid scope handling we actually want torch _dynamo OptimizedModule FIXME tmanlaibaatar When we call torch compile inside HOO we will end up invoking module registered root For now we just inline them But once we start supporting mark_strict export we do need properly handle Right now doesn t matter because current non-strict use cases don t need work HOO isinstance m OptimizedModule GraphModule forward args kwargs try Tracer call_module m forward args kwargs except _ModuleNotInstalledAsSubmoduleError log debug Unable find path module s This might because module properly registered submodule which good practice We will trace through module without recording stack information str m forward args kwargs is_leaf_module m Module module_qualified_name str - bool False create_node args object kwargs object - fx node Node Create node add metadata Add nn_module_stack here instead TracerBase since calls make_fx might want record module stack metadata Add torch_fn looking torch_fn_metadata torch_fn_counts Add stack_trace filtering out forward stack frames node = super create_node args kwargs type ignore arg-type nn_module_stack node op placeholder output node meta get nn_module_stack None node meta nn_module_stack = module_stack copy convert nn_module_stack Dict key FQN - Dict str Tuple str str key fqn mod_cls node meta nn_module_stack items isinstance mod_cls type node meta nn_module_stack key = fqn mod_cls __module__ + + mod_cls __qualname__ torch_fn node op == call_function torch_fn_metadata None torch_fn node meta node meta torch_fn = f torch_fn_metadata __name__ _ torch_fn_counts torch_fn_metadata f torch_fn_metadata __class__ __name__ torch_fn_metadata __name__ node _MakefxTracer __init__ decomposition_table Optional Mapping OpOverload Callable tracing_mode str _allow_non_fake_inputs bool pre_dispatch bool record_module_stack bool _allow_fake_constant bool _error_on_data_dependent_ops bool record_stack_traces bool = False parent_tracer Optional _MakefxTracer = None proxy_module_inputs bool = False - None Configurations used initialize context managers their states Should modify them during tracing decomposition_table dict OpOverload Callable = dict decomposition_table decomposition_table setdefault torch ops aten sym_numel default torch _decomp decompositions sym_numel tracing_mode str = tracing_mode _allow_non_fake_inputs bool = _allow_non_fake_inputs pre_dispatch bool = pre_dispatch record_module_stack bool = record_module_stack _allow_fake_constant bool = _allow_fake_constant _error_on_data_dependent_ops bool = _error_on_data_dependent_ops All context managers their states should initialized before tracing based inputs configurations After tracing their states should cleaned except shape_env Remember specify how initialize user inputs parent tracer whenever adding new modes _MakefxTracer fake_tensor_mode Optional FakeTensorMode = None proxy_mode Union nullcontext ProxyTorchDispatchMode = nullcontext proxy_function_mode Union nullcontext PreDispatchTorchFunctionMode = nullcontext fx_tracer Optional PythonKeyTracer = None python_dispatcher_mode Union nullcontext Any = nullcontext torch_fn_metadata_mode Union nullcontext TorchFunctionMetadataMode = nullcontext record_stack_traces = record_stack_traces parent_tracer Optional _MakefxTracer = parent_tracer proxy_module_inputs = proxy_module_inputs _checkpoint_modes - list Any fake_tensor_mode proxy_mode proxy_function_mode fx_tracer python_dispatcher_mode torch_fn_metadata_mode _restore_modes prev_fake_tensor_mode Optional FakeTensorMode prev_proxy_mode Union nullcontext ProxyTorchDispatchMode prev_proxy_function_mode Union nullcontext PreDispatchTorchFunctionMode prev_fx_tracer Optional PythonKeyTracer prev_python_dispatcher_mode Union nullcontext Any prev_torch_fn_metadata_mode Union nullcontext TorchFunctionMetadataMode - None fake_tensor_mode = prev_fake_tensor_mode proxy_mode = prev_proxy_mode proxy_function_mode = prev_proxy_function_mode fx_tracer = prev_fx_tracer python_dispatcher_mode = prev_python_dispatcher_mode torch_fn_metadata_mode = prev_torch_fn_metadata_mode contextmanager _init_modes_from_inputs f Callable args tuple object - Generator None None None prev_modes = _checkpoint_modes try Avoid importing sympy module level symbolic_shapes ShapeEnv hasattr f _orig_mod record_module_stack scope_root = f _orig_mod _ModuleStackTracer always try preserve stack trace forward functions fx_tracer = _ModuleStackTracer scope_root fx_tracer = PythonKeyTracer fx_tracer record_stack_traces = record_stack_traces record_stack_traces fx_tracer _record_forward_stack_traces_only = True tracing_mode == fake torch _dynamo fake_tensor_mode = torch _dynamo utils detect_fake_mode args fake_tensor_mode None torch _functorch config _config _config patch fake_tensor_allow_unsafe_data_ptr_access=False fake_tensor_mode = FakeTensorMode allow_fallback_kernels=True allow_non_fake_inputs=self _allow_non_fake_inputs shape_env=ShapeEnv static_shapes=True fake_tensor_mode = fake_tensor_mode tracing_mode == symbolic torch _dynamo fake_tensor_mode = torch _dynamo utils detect_fake_mode args fake_tensor_mode None shape_env = ShapeEnv torch _functorch config _config _config patch fake_tensor_allow_unsafe_data_ptr_access=False fake_tensor_mode = FakeTensorMode allow_fallback_kernels=False allow_non_fake_inputs=self _allow_non_fake_inputs shape_env=shape_env assert fake_tensor_mode shape_env None shape_env should set tracing symbolic fake_tensor_mode = fake_tensor_mode tracing_mode == real raise AssertionError f Unexpected tracing type tracing_mode _construct_modes_with_fx_tracer fx_tracer yield finally _restore_modes prev_modes _construct_modes_with_fx_tracer fx_tracer _ProxyTracer - None proxy_mode = ProxyTorchDispatchMode fx_tracer tracing_mode pre_dispatch=self pre_dispatch _allow_fake_constant=self _allow_fake_constant _error_on_data_dependent_ops=self _error_on_data_dependent_ops pre_dispatch proxy_function_mode = PreDispatchTorchFunctionMode fx_tracer pre-autograd tracing uses per-dispatch-key modes which requires python dispatcher tracing_mode == symbolic pre_dispatch python_dispatcher_mode = enable_python_dispatcher torch_fn_metadata_mode = TorchFunctionMetadataMode fx_tracer fx_tracer proxy_module_inputs = proxy_module_inputs type ignore union-attr contextmanager _init_modes_from_parent parent_tracer _MakefxTracer - Generator None None None By default subtracer creates new modes based parent tracer s config However there cases where we want share same modes parent tracer For example fake_tensor_mode we want example value s fake_mode parent graph subgraphs same prev_modes = _checkpoint_modes try fake_tensor_mode = parent_tracer fake_tensor_mode _create_sub_fx_tracer parent_tracer _ProxyTracer - PythonKeyTracer type parent_tracer PythonKeyTracer PythonKeyTracer type parent_tracer _ModuleStackTracer _ModuleStackTracer parent_tracer scope_root raise RuntimeError f Unexpected tracer type type parent_tracer assert parent_tracer fx_tracer None fx_tracer = _create_sub_fx_tracer parent_tracer fx_tracer _construct_modes_with_fx_tracer fx_tracer yield finally _restore_modes prev_modes _trace_inner f Callable args object - GraphModule TODO We need explicitly torch _dynamo before calling dispatch_trace because dispatch_trace will introduce lazy torch _dynamo some contexts set before calling dispatch_trace will cause problems torch _dynamo such some torch API torch ones so populate_builtin_to_tensor_fn_map will affected context set before dispatch_trace torch _dynamo phs = pytree tree_map lambda _ torch fx _symbolic_trace PH args _wrap_fake args T - T arg_count = inner_wrap_fake x object - object nonlocal arg_count TODO would nice line these up names FX will choose placeholders we don t actually know what names will point yet NB Source here actually meaningless torch _dynamo source ConstantSource assert fake_tensor_mode None source = ConstantSource f input arg_count isinstance x Tensor arg_count += fake_tensor_mode from_tensor x source=source NB don t match bools type x int tracing_mode == symbolic assert fake_tensor_mode shape_env None shape_env should set tracing symbolic fake_tensor_mode shape_env create_symintnode fake_tensor_mode shape_env create_symbol x source positive=None hint=x source=source isinstance x torch ScriptObject torch _library fake_class_registry maybe_to_fake_obj fake_tensor_mode x assert isinstance x FakeScriptObject f ScriptObject x has been fakified Cannot wrap_fake again x wrap_fn_map = real lambda x x fake inner_wrap_fake symbolic inner_wrap_fake pytree tree_map wrap_fn_map tracing_mode args _wrap_func f Callable _P R phs Sequence PHBase - Callable _P R hasattr inspect unwrap f __code__ inspect unwrap f __code__ co_flags inspect CO_VARARGS FX doesn t support varargs so we gotta fake up wrapper TODO Would nice fix source fake_signature f len phs f args = _wrap_fake args func = _wrap_func f phs We disable autocast cache autocast cache causes type conversions parameters check cache which introduces untracked tensors into graph We also disable tracing any other tensor proxy-based tracers except current The purpose ` make_fx ` produce graphmodules side effect its internal execution thus irrelevant any external functional trace proxy_mode ProxyTorchDispatchMode = typing cast ProxyTorchDispatchMode proxy_mode ExitStack stack stack enter_context decompose decomposition_table fake_tensor_mode stack enter_context fake_tensor_mode stack enter_context python_dispatcher_mode stack enter_context proxy_function_mode stack enter_context torch_fn_metadata_mode stack enter_context proxy_mode stack enter_context disable_autocast_cache stack enter_context _set_make_fx_tracer assert fx_tracer None try t = dispatch_trace wrap_key func args fx_tracer pre_dispatch tracer=self fx_tracer concrete_args=tuple phs except Exception trace_structured artifact metadata_fn=lambda name make_fx_fail_partial encoding string payload_fn=lambda fx_tracer graph python_code type ignore union-attr root_module= verbose=True include_stride=True include_device=True src raise is_hop_subgraph_tracer fake_mode = torch _guards detect_fake_mode args fake_mode shape_env None torch fx passes runtime_assert insert_deferred_runtime_asserts insert_deferred_runtime_asserts t fake_mode shape_env reenter_make_fx t recompile TODO kind bad way do should maybe figure out better way tracing_mode == symbolic assert fake_tensor_mode None t shape_env = fake_tensor_mode shape_env type ignore assignment t trace f Callable args object - fx GraphModule _init_modes_from_inputs f args _trace_inner f args is_hop_subgraph_tracer - bool parent_tracer None trace_subgraph f Callable args object - GraphModule Create new tracer based parent s config sub_tracer = _MakefxTracer decomposition_table real _allow_non_fake_inputs pre_dispatch record_module_stack _allow_fake_constant _error_on_data_dependent_ops parent_tracer=self sub_tracer _init_modes_from_parent sub_tracer _trace_inner f args _CURRENT_MAKE_FX_TRACER Optional _MakefxTracer = None contextmanager _set_make_fx_tracer tracer _MakefxTracer - Generator None None None global _CURRENT_MAKE_FX_TRACER prev_tracer = _CURRENT_MAKE_FX_TRACER try _CURRENT_MAKE_FX_TRACER = tracer yield finally _CURRENT_MAKE_FX_TRACER = prev_tracer make_fx f Callable decomposition_table Optional Mapping OpOverload Callable = None tracing_mode str = real _allow_non_fake_inputs bool = False pre_dispatch bool = False record_module_stack bool = False _allow_fake_constant bool = False _error_on_data_dependent_ops bool = True record_stack_traces bool = False proxy_module_inputs bool = False - Callable GraphModule Given function f new function which when executed valid arguments f returns FX GraphModule representing set operations executed during course execution If record_stack_traces True stack trace will preserved node meta stack_trace assert tracing_mode real fake symbolic torch _inductor config make_fx_tracer = _MakefxTracer decomposition_table tracing_mode _allow_non_fake_inputs pre_dispatch record_module_stack _allow_fake_constant _error_on_data_dependent_ops record_stack_traces=record_stack_traces config trace provenance_tracking_level == proxy_module_inputs=proxy_module_inputs functools wraps f wrapped args object - GraphModule make_fx_tracer trace f args wrapped get_torch_dispatch_modes - list TorchDispatchMode torch utils _python_dispatch _get_current_dispatch_mode_stack TODO legacy name there only ever one proxy mode s infra mode get_innermost_proxy_mode - Optional ProxyTorchDispatchMode get_proxy_mode get_proxy_mode - Optional ProxyTorchDispatchMode Current currently active proxy tracing mode None we currently tracing This includes pre-dispatch proxy tracing pre_dispatch_mode = torch _ops _get_dispatch_mode_pre_dispatch torch _C _TorchDispatchModeKey PROXY mode = torch _C _get_dispatch_mode torch _C _TorchDispatchModeKey PROXY assert pre_dispatch_mode None mode None f pre_dispatch_mode= pre_dispatch_mode mode= mode pre_dispatch_mode mode handle_sym_dispatch func Callable _P R args _P args type ignore valid-type allowed use _P args here kwargs _P kwargs type ignore valid-type allowed use _P kwargs here - R Call into currently active proxy tracing mode do SymInt SymFloat SymBool dispatch trace function operates these arguments mode = get_proxy_mode assert mode Have do manually because we re doing normal torch dispatch machinery which disables us disable_proxy_modes_tracing TODO properly compute types types list type = mode __sym_dispatch__ func types args kwargs type ignore arg-type return-value contextmanager disable_proxy_modes_tracing - Generator ProxyTorchDispatchMode None None _disable_infra_mode torch _C _TorchDispatchModeKey PROXY maybe_handle_decomp proxy_mode ProxyTorchDispatchMode op OpOverload args tuple object kwargs dict str object - object torch _inductor compiler_bisector CompilerBisector op CURRENT_DECOMPOSITION_TABLE CompilerBisector disable_subsystem aot_eager_decomp_partition decomposition lambda repr op NotImplemented proxy_mode proxy_mode decomp_layers += out = CURRENT_DECOMPOSITION_TABLE op args kwargs proxy_mode decomp_layers -= out NotImplemented get_isolated_graphmodule func Callable args tuple object kwargs dict str object tracing_mode str = real decomposition_table Optional Mapping OpOverload Callable = None - GraphModule A helper function used get GraphModule given func It s expected used ProxyTensor tracing context It detaches args kwargs current tracer so trace current graph module can created without any side-effects wrapped all_args = wrapper_and_args_for_make_fx func args kwargs disable_proxy_modes_tracing gm = make_fx wrapped decomposition_table=decomposition_table tracing_mode=tracing_mode all_args gm _set_unbacked_bindings out object out_proxy _NestedProxys - None A helper function setting up unbacked_bindings destination FX graph symbolic_shapes compute_unbacked_bindings Can t use detect_fake_mode here python test distributed _tensor test_dtensor_compile py -k test_tp_compile_fullgraph_is_seq_parallel_False will fail Very strange probably isn t right them using two fake modes there fake_mode = torch _C _get_dispatch_mode torch _C _TorchDispatchModeKey FAKE fake_mode fake_mode shape_env symbol_to_path = compute_unbacked_bindings fake_mode shape_env out assert isinstance out_proxy Proxy out_proxy out_proxy node meta unbacked_bindings = symbol_to_path