mypy allow-untyped-defs copy functools itertools logging types collections abc Sequence torch torch nn nn torch _dynamo utils counters detect_fake_mode torch _logging trace_structured torch fx experimental optimization matches_module_pattern replace_node_module torch fx passes graph_transform_observer GraphTransformObserver GraphTransformObserverBase torch fx passes shape_prop ShapeProp torch nn functional F torch nn utils fusion fuse_conv_bn_eval fuse_conv_bn_weights config fx_utils matches_module_function_pattern pattern_matcher init_once_fakemode PatternMatcherPass PatternMatcherPassBase stable_topological_sort utils is_cpu_device pass_execution_and_save group_batch_fusion group_batch_fusion_passes PRE_GRAD_FUSIONS misc_patterns numpy_compat_normalization split_cat PRE_GRAD_PATTERNS PatternMatcherPass = functools partial PatternMatcherPassBase subsystem= pre_grad_passes GraphTransformObserver = functools partial GraphTransformObserverBase subsystem= pre_grad_passes log = logging getLogger __name__ efficient_conv_bn_eval_pass = PatternMatcherPass pass_name= efficient_conv_bn_eval_pass fuse_split_linear_add_pass = PatternMatcherPass pass_name= fuse_split_linear_add_pass fuse_chunk_squeeze_cat_pass = PatternMatcherPass pass_name= fuse_chunk_squeeze_cat_pass remove_reshape_pass = PatternMatcherPass pass_name= remove_reshape_pass based predispatch aten IR normalization_pass_aten = PatternMatcherPass pass_name= normalization_pass_aten merge_splits_pass_aten = PatternMatcherPass pass_name= merge_splits_pass_aten split_cat_pass_aten = PatternMatcherPass pass_name= split_cat_pass_aten unbind_stack_pass_aten = PatternMatcherPass pass_name= unbind_stack_pass_aten merge_getitem_cat_pass_aten = PatternMatcherPass pass_name= merge_getitem_cat_pass_aten merge_stack_tahn_unbind_pass_aten = PatternMatcherPass pass_name= merge_stack_tahn_unbind_pass_aten mutate_cat_pass_aten = PatternMatcherPass pass_name= mutate_cat_pass_aten remove_split_with_size_one_pass_aten = PatternMatcherPass pass_name= remove_split_with_size_one_pass_aten save_inductor_dict pass_to_compare=None pass_to_compare pass_to_compare = list config pre_grad_fusion_options keys + list config post_grad_fusion_options keys p dict counters inductor get p p pass_to_compare is_same_dict inductor_dict optimus_dict pass_name count optimus_dict items count = dict inductor_dict get pass_name False True shape_prop mod - None None normalize_node_kwargs_pass graph None fuse_parallel_linear_pass graph None remove_split_ops graph shape_prop None remove_split_ops_pass graph remove_split_ops graph owning_module shape_prop fuse_chunk_reshape_unsqueeze_concat_pass graph None fuse_chunk_reshape_concat_pass graph None remove_noop_pass graph None stack_to_unsqueeze_pass graph None merge_concats_pass graph None relu_nan_to_num graph None fuse_split_getitem_squeeze_cat graph None use_triton_dot_compress graph None use_triton_lce_replace_simple_LCE_helper gm shape_prop None use_triton_lce_replace_simple_LCE graph use_triton_lce_replace_simple_LCE_helper graph owning_module shape_prop use_triton_lce_replace_normal_LCE_helper gm shape_prop None use_triton_lce_replace_normal_LCE graph use_triton_lce_replace_simple_LCE_helper graph owning_module shape_prop use_matmul_lce_replace_normal_LCE graph None use_matmul_fuse_lce_replace_first_LCE graph None init_once_fakemode lazy_init efficient_conv_bn_eval split_cat noqa F config is_fbcode fb type ignore attr-defined noqa F _get_pass_name_func p isinstance p PatternMatcherPassBase pass_name = p pass_name pass_func = p apply isinstance p types FunctionType pass_name = p __name__ lstrip _ pass_func = p pass_name = None pass_func = None pass_name pass_func _run_pre_dispatch_passes gm torch fx GraphModule example_inputs Sequence object = add_passes str &#124; None = None remove_passes str &#124; None = None - None order matters default_pass_list = normalize passes must called first passes normalization_pass_aten normalize_node_kwargs_pass remove_noop_pass relu_nan_to_num fuse_chunk_reshape_concat_pass group_batch_fusion_passes normalize_node_kwargs_pass fuse_chunk_squeeze_cat_pass merge_concats_pass fuse_split_linear_add_pass remove_reshape_pass fuse_parallel_linear_pass remove_split_ops_pass stack_to_unsqueeze_pass run before fuse_chunk_reshape_unsqueeze_concat_pass fuse_chunk_reshape_unsqueeze_concat_pass full_pass_list = default_pass_list + fuse_split_getitem_squeeze_cat use_triton_dot_compress use_triton_lce_replace_simple_LCE use_triton_lce_replace_normal_LCE use_matmul_fuse_lce_replace_first_LCE use_matmul_lce_replace_normal_LCE log info f pre_grad_passes add_passes add_passes remove_pass remove_passes noqa G add_passes_list = remove_passes_list = add_passes add_passes_list = add_passes split remove_passes remove_passes_list = remove_passes split shape_prop = lambda mod ShapeProp noqa E gm=mod pyre-fixme Module ` torch _dynamo utils ` has no attribute ` detect_fake_mode ` fake_mode=detect_fake_mode example_inputs propagate tuple example_inputs p default_pass_list pass_name pass_func = _get_pass_name_func p should happen pass_name None pass_func None continue pass_name remove_passes_list continue pass_execution_and_save pass_func gm example_inputs f Pre grad predispatch IR Apply pass_name pass p full_pass_list pass_name pass_func = _get_pass_name_func p pass_name None pass_func None continue pass_name add_passes_list pass_execution_and_save pass_func gm example_inputs f Pre grad predispatch IR Apply pass_name pass Remove noops end which may generated other passes pass_execution_and_save remove_noop_pass gm example_inputs Pre grad predispatch IR Apply remove_noop pass shape_prop gm pre_grad_passes gm torch fx GraphModule example_inputs Sequence object = add_passes str &#124; None = None remove_passes str &#124; None = None - torch fx GraphModule Apply passes input FX graph using Torch IR WARNING The IR before grad functional normalized so harder write passes IR Passes must safe respect aliasing mutation need handle all possible arg schemas Consider adding new pass post_grad py joint_graph py which after functionalization normalization config pattern_matcher lazy_init hasattr config fx_passes_numeric_check config fx_passes_numeric_check get pre_grad False gm_before_fx_passes = gm __copy__ explicitly run predispatch atenIR based passes config is_predispatch _run_pre_dispatch_passes gm example_inputs add_passes remove_passes We only log graph changes avoid excessive compilation time https fb workplace com groups permalink example_inputs None gm = fuse_fx gm example_inputs numpy_compat_normalization gm graph We should always do normalization_pass first normalization_pass config pre_grad_fusion_options pattern_matcher_pass = PRE_GRAD_PATTERNS normalization_pass pattern_matcher_pass apply gm graph type ignore arg-type group_batch_fusion_passes gm graph pre_grad=True pass_name config pre_grad_fusion_options skip all patterns group batch fusions pass_name PRE_GRAD_FUSIONS pass_name == normalization_pass continue pattern_matcher_pass = PRE_GRAD_PATTERNS pass_name inductor_before_change = save_inductor_dict pattern_matcher_pass pass_name we support run same pattern multiple times default run only once counter = config pre_grad_fusion_options pass_name get counter _ range counter pattern_matcher_pass apply gm graph type ignore arg-type is_same_dict counters inductor inductor_before_change trace_structured artifact metadata_fn=lambda name f pattern_matcher_pass pass_name _pre_grad encoding string payload_fn=lambda gm print_readable print_output=False include_stride=True include_device=True TODO move efficient_conv_bn_eval_pass fusions dict too efficient_conv_bn_eval_pass apply gm graph type ignore arg-type config pre_grad_custom_pass None GraphTransformObserver gm pre_grad_custom_pass apply_graph_pass config pre_grad_custom_pass stable_topological_sort gm graph quantization quant_lift_up quant_lift_up gm gm graph lint gm recompile config pattern_matcher hasattr config fx_passes_numeric_check config fx_passes_numeric_check get pre_grad False example_inputs None numeric_utils numeric_check_if_enabled gm_after_fx_passes = gm __copy__ numeric_check_if_enabled gm_before_fx_passes type ignore possibly-undefined gm_after_fx_passes example_inputs config fx_passes_numeric_check get num_iterations config fx_passes_numeric_check get precision e- gm fuse_fx gm torch fx GraphModule example_inputs - torch fx GraphModule is_cpu = is_cpu_device example_inputs pyre-fixme Module ` torch _dynamo utils ` has no attribute ` detect_fake_mode ` fake_mode = detect_fake_mode example_inputs gm = sink_cat_after_pointwise gm config permute_fusion is_cpu For linear permute fusion we need check input info identify perform proper permutation transpose ShapeProp gm fake_mode=fake_mode propagate example_inputs GraphTransformObserver gm linear_permute_fusion gm = linear_permute_fusion gm GraphTransformObserver gm permute_linear_fusion gm = permute_linear_fusion gm GraphTransformObserver gm permute_matmul_fusion gm = permute_matmul_fusion gm make sure autograd disabled torch is_grad_enabled is_cpu gm config freezing GraphTransformObserver gm remove_identity gm = remove_identity gm GraphTransformObserver gm fuse_conv_bn gm = fuse_conv_bn gm gm fetch_attr target str mod target_atoms = target split attr_itr = mod i atom enumerate target_atoms hasattr attr_itr atom raise RuntimeError f Node referenced nonexistent target join target_atoms i attr_itr = getattr attr_itr atom attr_itr remove_identity gm torch fx GraphModule - torch fx GraphModule Removes all identity layers module IdentityRemover torch fx Transformer call_module target args kwargs isinstance submodules target nn Identity assert len args == args super call_module target args kwargs IdentityRemover gm transform fuse_conv_bn gm torch fx GraphModule inplace=False - torch fx GraphModule Fuses Convolution BN layers inference purposes modules_patterns = torch nn Conv d torch nn BatchNorm d torch nn Conv d torch nn BatchNorm d torch nn Conv d torch nn BatchNorm d module_function_patterns = torch nn Conv d F batch_norm torch nn Conv d F batch_norm torch nn Conv d F batch_norm modules = dict gm named_modules ConvBNFusion __init__ bn_node conv_module bn_module=None For BN Module bn_running_mean=None For Functional BN bn_running_var=None bn_eps=None bn_weight=None bn_bias=None - None bn_nodes = bn_node conv_module = conv_module bn_module = bn_module bn_running_mean = bn_running_mean bn_running_var = bn_running_var bn_eps = bn_eps bn_weight = bn_weight bn_bias = bn_bias fusion_enabled = True add_bn_node bn_node bn_nodes append bn_node disable_fusion fusion_enabled = False is_fusion_enabled fusion_enabled conv_bn_to_fuse dict int ConvBNFusion = pattern modules_patterns conv_bn_to_fuse clear node gm graph nodes matches_module_pattern pattern node modules len node args users Output conv used other nodes continue conv = modules node args target bn = modules node target eval_mode = all n training n conv bn eval_mode continue bn track_running_stats continue Do hash based module name conv hash_id = hash node args target hash_id conv_bn_to_fuse conv_bn_to_fuse hash_id = ConvBNFusion node conv bn bn == conv_bn_to_fuse hash_id bn_module Do fusion same bn module conv_bn_to_fuse hash_id add_bn_node node Disable conv bn folding conv shared different bn conv_bn_to_fuse hash_id disable_fusion conv_bn_fusion conv_bn_to_fuse values conv_bn_fusion is_fusion_enabled bn_nodes = conv_bn_fusion bn_nodes conv = conv_bn_fusion conv_module bn = conv_bn_fusion bn_module pyrefly ignore bad-argument-type fused_conv = fuse_conv_bn_eval conv bn bn_node bn_nodes replace_node_module bn_node args modules fused_conv bn_node replace_all_uses_with bn_node args gm graph erase_node bn_node gm graph lint pattern module_function_patterns conv_bn_to_fuse clear node gm graph nodes matches_module_function_pattern pattern node modules TODO support kwargs len node args = continue conv = modules node args target bn_training = node args bn_eps = node args conv training bn_training continue type bn_eps float continue _used_by_same_conv_module users conv_module_name = users args target all conv_module_name == user args target user users bn_args_is_constant = all n op == get_attr len n users == _used_by_same_conv_module list n users n node args bn_args_is_constant continue bn_running_mean = fetch_attr node args target gm bn_running_var = fetch_attr node args target gm bn_weight = fetch_attr node args target gm bn_bias = fetch_attr node args target gm bn_running_mean None bn_running_var None continue Do hash based module name conv hash_id = hash node args target hash_id conv_bn_to_fuse conv_bn_to_fuse hash_id = ConvBNFusion node conv bn_running_mean=bn_running_mean bn_running_var=bn_running_var bn_eps=bn_eps bn_weight=bn_weight bn_bias=bn_bias hash bn_running_mean == hash conv_bn_to_fuse hash_id bn_running_mean hash bn_running_var == hash conv_bn_to_fuse hash_id bn_running_var torch allclose torch tensor bn_eps torch tensor conv_bn_to_fuse hash_id bn_eps hash bn_weight == hash conv_bn_to_fuse hash_id bn_weight hash bn_bias == hash conv_bn_to_fuse hash_id bn_bias Do fusion same functional bn conv_bn_to_fuse hash_id add_bn_node node Disable conv bn folding conv shared different bn conv_bn_to_fuse hash_id disable_fusion conv_bn_fusion conv_bn_to_fuse values conv_bn_fusion is_fusion_enabled bn_nodes = conv_bn_fusion bn_nodes conv = conv_bn_fusion conv_module bn_running_mean = conv_bn_fusion bn_running_mean bn_running_var = conv_bn_fusion bn_running_var bn_eps = conv_bn_fusion bn_eps bn_weight = conv_bn_fusion bn_weight bn_bias = conv_bn_fusion bn_bias fused_conv = copy deepcopy conv fused_conv weight fused_conv bias = fuse_conv_bn_weights fused_conv weight fused_conv bias pyrefly ignore bad-argument-type bn_running_mean pyrefly ignore bad-argument-type bn_running_var pyrefly ignore bad-argument-type bn_eps bn_weight bn_bias bn_node bn_nodes replace_node_module bn_node args modules fused_conv bn_node replace_all_uses_with bn_node args gm graph erase_node bn_node gm graph lint gm recompile gm NormalizedLinearNode __init__ node torch fx Node - None assert node op == call_function assert node target torch nn functional linear node torch fx Node = node get_input - torch fx Node len node args node args type ignore return-value node kwargs input type ignore return-value get_weight - torch fx Node len node args node args type ignore return-value node kwargs weight type ignore return-value get_bias - torch fx Node len node args node args type ignore return-value node kwargs get bias None type ignore return-value NormalizedMatmulNode __init__ node torch fx Node - None assert node op == call_function assert node target torch bmm torch matmul node torch fx Node = node get_input - torch fx Node len node args node args type ignore return-value node kwargs input type ignore return-value get_other - torch fx Node len node args node args type ignore return-value node kwargs other type ignore return-value check_permute node torch fx Node - bool ranks = len node meta tensor_meta shape len node args permutation = node args i ranks i range ranks + type ignore operator permutation node kwargs node kwargs permutation None len node kwargs permutation type ignore arg-type permutation = i ranks i node kwargs permutation type ignore operator union-attr False allowed_permutation = list range ranks allowed_permutation - = ranks - allowed_permutation - = ranks - permutation == allowed_permutation sink_cat_after_pointwise module torch fx GraphModule - torch fx GraphModule one_user node users = list node users users len users == None is_view node node op == call_method node target == view is_pointwise_unary node ops = call_function call_method pointwise = torch relu torch tanh relu tanh node op ops node target pointwise g = module graph node g nodes node op = call_function node target = torch cat continue cat_or_view = node while True user = one_user cat_or_view user is_view user break cat_or_view = user user is_pointwise_unary user g inserting_before node cat_args tensors dim= tensors dim tensors dim = cat_args node args node kwargs new_kwargs = name val name val user kwargs items name = input new_tensors = g create_node user op user target args= arg kwargs=new_kwargs arg tensors new_cat = g create_node call_function torch cat args= new_tensors dim user replace_all_uses_with cat_or_view node replace_all_uses_with new_cat g erase_node user g erase_node node g lint module recompile module linear_permute_fusion module torch fx GraphModule - torch fx GraphModule node module graph find_nodes op= call_method target= permute check_permute node len node args input_node = node args input_node = node kwargs input input_node op == call_function input_node target torch nn functional linear normalized = NormalizedLinearNode input_node input = normalized get_input weight = normalized get_weight bias = normalized get_bias module graph inserting_before node fused_node = module graph call_function linear_transpose args= input weight bias node replace_all_uses_with fused_node module graph erase_node node len input_node users == module graph erase_node input_node module graph lint module recompile module Y = X W^T + bias Y = Y permute ---- Y = W X^T + bias unsqueeze - ^T linear_transpose input torch Tensor weight torch Tensor bias torch Tensor &#124; None - torch Tensor bias None torch matmul weight input transpose - - torch matmul weight input transpose - - + bias unsqueeze - permute_linear_fusion module torch fx GraphModule - torch fx GraphModule node module graph find_nodes op= call_function target=torch nn functional linear len node args input_node = node args input_node = node kwargs input input_node op == call_method input_node target == permute check_permute input_node normalized = NormalizedLinearNode node len input_node args input = input_node args input = input_node kwargs input weight = normalized get_weight bias = normalized get_bias module graph inserting_before node fused_node = module graph call_function transpose_linear args= input weight bias node replace_all_uses_with fused_node module graph erase_node node len input_node users == module graph erase_node input_node module graph lint module recompile module permute_matmul_fusion module torch fx GraphModule - torch fx GraphModule node itertools chain module graph find_nodes op= call_function target=torch bmm module graph find_nodes op= call_function target=torch matmul normalized = NormalizedMatmulNode node input_A_node = normalized get_input input_B_node = normalized get_other input_A = input_A_node input_B = input_B_node Atrans = Btrans = False input_A_node op == call_method input_A_node target == permute check_permute input_A_node Atrans = True len input_A_node args input_A = input_A_node args type ignore assignment input_A = input_A_node kwargs input type ignore assignment input_B_node op == call_method input_B_node target == permute check_permute input_B_node Btrans = True len input_B_node args input_B = input_B_node args type ignore assignment input_B = input_B_node kwargs input type ignore assignment Atrans Btrans module graph inserting_before node fused_node = module graph call_function transpose_matmul args= input_A input_B Atrans Btrans node replace_all_uses_with fused_node module graph erase_node node Atrans len input_A_node users == module graph erase_node input_A_node Btrans len input_B_node users == module graph erase_node input_B_node module graph lint module recompile module X = X permute Y = X W ^T + bias ---- Y = X transpose - - W ^T + bias transpose_linear input torch Tensor weight torch Tensor bias torch Tensor &#124; None - torch Tensor bias None torch matmul input transpose - - weight t torch matmul input transpose - - weight t + bias transpose_matmul A torch Tensor B torch Tensor Atrans bool Btrans bool - torch Tensor Atrans A = A transpose - - Btrans B = B transpose - - torch matmul A B