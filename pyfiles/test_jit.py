Owner s oncall jit ruff noqa F torch __name__ == __main__ torch testing _internal common_utils parse_cmd_line_args The value GRAPH_EXECUTOR depends command line arguments so make sure they re parsed before instantiating tests parse_cmd_line_args This how we include tests located test jit They included here so they invoked when you call ` test_jit py ` do run these test files directly jit test_tracer TestTracer TestMixTracingScripting noqa F jit test_recursive_script TestRecursiveScript noqa F jit test_type_sharing TestTypeSharing noqa F jit test_logging TestLogging noqa F jit test_backends TestBackends TestBackendsWithCompiler noqa F jit test_backend_nnapi TestNnapiBackend noqa F jit test_list_dict TestList TestDict TestNamedTuple TestScriptDict TestScriptList noqa F jit test_async TestAsync noqa F jit test_await TestAwait noqa F jit test_data_parallel TestDataParallel noqa F jit test_models TestModels noqa F jit test_modules TestModules noqa F jit test_autodiff TestAutodiffJit noqa F jit test_autodiff_subgraph_slicing TestAutodiffSubgraphSlicing noqa F jit test_custom_operators TestCustomOperators noqa F jit test_graph_rewrite_passes TestGraphRewritePasses noqa F jit test_class_type TestClassType noqa F jit test_builtins TestBuiltins TestTensorBuiltins noqa F jit test_ignore_context_manager TestIgnoreContextManager noqa F jit test_symbolic_shape_analysis TestSymbolicShapeAnalysis noqa F jit test_op_decompositions TestOpDecompositions noqa F jit test_unsupported_ops TestUnsupportedOps noqa F jit test_freezing TestFreezing TestFrozenOptimizations TestMKLDNNReinplacing noqa F jit test_peephole TestPeephole noqa F jit test_alias_analysis TestAliasAnalysis noqa F jit test_save_load TestSaveLoad TestSaveLoadFlatbuffer noqa F jit test_save_load_for_op_version TestSaveLoadForOpVersion noqa F jit test_module_containers TestModuleContainers noqa F jit test_python_bindings TestPythonBindings noqa F jit test_python_ir TestPythonIr noqa F jit test_functional_blocks TestFunctionalBlocks noqa F jit test_remove_mutation TestRemoveMutation noqa F jit test_torchbind TestTorchbind noqa F jit test_module_interface TestModuleInterface noqa F jit test_with TestWith noqa F jit test_enum TestEnum noqa F jit test_string_formatting TestStringFormatting noqa F jit test_profiler TestProfiler noqa F jit test_slice TestSlice noqa F jit test_ignorable_args TestIgnorableArgs noqa F jit test_hooks TestHooks noqa F jit test_warn TestWarn noqa F jit test_isinstance TestIsinstance noqa F jit test_cuda TestCUDA noqa F jit test_python_builtins TestPythonBuiltinOP noqa F jit test_typing TestTyping noqa F jit test_hash TestHash noqa F jit test_complex TestComplex noqa F jit test_jit_utils TestJitUtils noqa F jit test_scriptmod_ann TestScriptModuleInstanceAttributeTypeAnnotation noqa F jit test_types TestTypesAndAnnotation noqa F jit test_misc TestMisc noqa F jit test_upgraders TestUpgraders noqa F jit test_pdt TestPDT noqa F jit test_tensor_creation_ops TestTensorCreationOps noqa F jit test_module_apis TestModuleAPIs noqa F jit test_script_profile TestScriptProfile noqa F jit test_convert_activation TestFunctionalToInplaceActivation TestInplaceToFunctionalActivation noqa F jit test_parametrization TestParametrization noqa F jit test_attr TestGetDefaultAttr noqa F jit test_aten_pow TestAtenPow noqa F jit test_optimize_for_mobile_preserve_debug_info TestOptimizeForMobilePreserveDebugInfo noqa F jit test_union TestUnion noqa F jit test_batch_mm TestBatchMM noqa F jit test_dtype_analysis TestDtypeAnalysis TestDtypeCustomRulesCPU noqa F jit test_device_analysis TestDeviceAnalysis noqa F jit test_dce TestDCE noqa F jit test_sparse TestSparse noqa F jit test_tensor_methods TestTensorMethods noqa F jit test_dataclasses TestDataclasses noqa F jit test_generator TestGenerator noqa F Torch torch Tensor torch _C TensorType BoolType parse_ir _propagate_shapes torch autograd Variable torch jit annotations BroadcastingList BroadcastingList Any noqa F torch nn utils rnn PackedSequence torch testing FileCheck make_tensor torch autograd profiler torch cuda torch jit torch jit _logging torch jit frontend torch nn nn torch nn functional F Testing utils torch testing _internal jit_utils torch testing _internal common_jit check_against_reference torch testing _internal common_utils run_tests IS_WINDOWS \ GRAPH_EXECUTOR suppress_warnings IS_SANDCASTLE ProfilingMode \ TestCase freeze_rng_state slowTest TemporaryFileName \ enable_profiling_mode_for_profiling_tests TEST_MKL set_default_dtype num_profiled_runs \ skipIfCrossRef skipIfTorchDynamo torch testing _internal jit_utils JitTestCase enable_cpu_fuser disable_autodiff_subgraph_inlining \ _trace do_input_map get_execution_plan make_global \ execWrapper _inline_everything _tmp_donotuse_dont_inline_everything \ RUN_CUDA torch testing _internal jit_metaprogramming_utils get_script_args create_input unpack_variables get_all_nn_module_tests EXCLUDE_SCRIPT_MODULES get_nn_module_name_from_kwargs get_nn_mod_test_name script_method_template torch testing _internal common_nn criterion_tests For testing truediv python torch testing _internal test_module future_div div_int_future div_float_future torch testing _internal test_module no_future_div div_int_nofuture div_float_nofuture Standard library collections defaultdict namedtuple OrderedDict copy deepcopy itertools product textwrap dedent typing List Dict NamedTuple Optional Tuple Union copy functools inspect io itertools math numpy np os pickle pickletools random re shutil string sys tempfile types typing unittest warnings zipfile tracemalloc canonical graph torch _C _jit_pass_canonicalize graph str False LSTMCellF input hx cx params LSTMCell input hx cx params doAutodiffCheck testname TODO setting false test itself working test_t_ testname testname == test_t False assert GRAPH_EXECUTOR GRAPH_EXECUTOR == ProfilingMode SIMPLE False GRAPH_EXECUTOR == ProfilingMode LEGACY True these tests disabled because BailOut nodes inserted ProfilingExecutor interfere subgraph slicing Differentiable Graphs test_exceptions = functional test_nn_dropout test_nn_log_softmax test_nn_relu test_nn_softmax test_nn_threshold test_nn_lp_pool d test_nn_lp_pool d test_nn_gumbel_softmax_hard test_nn_gumbel_softmax test_nn_multilabel_soft_margin_loss test_nn_batch_norm test_nn_max_pool d_with_indices AutogradJitGenerated test___rdiv___constant test___rdiv___scalar_constant test_split test_split_dim test_split_dim_neg test_split_size_list test_split_size_list_dim test_split_size_list_dim_neg test_split_with_sizes test_split_with_sizes_dim test_split_with_sizes_dim_neg test_split_with_sizes_size_ test_nn_max_pool d_with_indices testname test_exceptions assert GRAPH_EXECUTOR TODO enable TE PE when all tests fixed torch _C _jit_set_texpr_fuser_enabled GRAPH_EXECUTOR == ProfilingMode PROFILING torch _C _jit_set_profiling_executor GRAPH_EXECUTOR = ProfilingMode LEGACY LSTMCell input hidden w_ih w_hh b_ih=None b_hh=None hx cx = hidden gates = F linear input w_ih b_ih + F linear hx w_hh b_hh ingate forgetgate cellgate outgate = gates chunk ingate = torch sigmoid ingate forgetgate = torch sigmoid forgetgate cellgate = torch tanh cellgate outgate = torch sigmoid outgate cy = forgetgate cx + ingate cellgate hy = outgate torch tanh cy hy cy LSTMCellC args kwargs hy cy = LSTMCellF args kwargs torch cat hy cy LSTMCellS x hx cx w_ih w_hh b_ih b_hh gates = x mm w_ih t + hx mm w_hh t + b_ih + b_hh ingate forgetgate cellgate outgate = gates chunk ingate = torch sigmoid ingate forgetgate = torch sigmoid forgetgate cellgate = torch tanh cellgate outgate = torch sigmoid outgate cy = forgetgate cx + ingate cellgate hy = outgate torch tanh cy hy cy Code reference https github com pytorch translate blob master pytorch_translate rnn_cell py#L MiLSTMCell x hx cx w_ih w_hh alpha beta_i beta_h bias Wx = x mm w_ih t Uz = hx mm w_hh t Section https arxiv org pdf pdf gates = alpha Wx Uz + beta_i Wx + beta_h Uz + bias Same LSTMCell after point ingate forgetgate cellgate outgate = gates chunk ingate = ingate sigmoid forgetgate = forgetgate sigmoid cellgate = cellgate tanh outgate = outgate sigmoid cy = forgetgate cx + ingate cellgate hy = outgate cy tanh hy cy get_lstm_inputs device training=False seq_length=None input_shape = seq_length None seq_length input = torch randn input_shape dtype=torch float device=device requires_grad=training hx = torch randn dtype=torch float device=device requires_grad=training cx = torch randn dtype=torch float device=device requires_grad=training module = nn LSTMCell device torch float Just allocate weights correct sizes training params = tuple module parameters params = tuple p requires_grad_ False p module parameters input hx cx + params get_milstm_inputs device training=False minibatch = input_size = hidden_size = x = torch randn minibatch input_size device=device dtype=torch float hx = torch randn minibatch hidden_size device=device dtype=torch float cx = torch randn minibatch hidden_size device=device dtype=torch float ih = torch randn hidden_size input_size device=device dtype=torch float requires_grad=training hh = torch randn hidden_size hidden_size device=device dtype=torch float requires_grad=training alpha = torch randn hidden_size dtype=torch float device=device requires_grad=training ibeta = torch randn hidden_size dtype=torch float device=device requires_grad=training hbeta = torch randn hidden_size dtype=torch float device=device requires_grad=training bias = torch randn hidden_size dtype=torch float device=device requires_grad=training x hx cx ih hh alpha ibeta hbeta bias get_fn file_name script_path importlib util spec = importlib util spec_from_file_location file_name script_path module = importlib util module_from_spec spec spec loader exec_module module fn = module fn fn get_grad_executor plan_state diff_graph_idx=None skip_check=False diff_graph_idx None nodes = list plan_state graph nodes skip_check nodes = list filter lambda n n kind = prim BailOut n kind = prim BailoutTemplate nodes len nodes == len nodes == nodes kind == prim TupleConstruct pass len nodes == nodes kind == prim RequiresGradCheck nodes kind == prim If pass raise RuntimeError Can t get grad_executor non-differentiable graph grad_executors = list plan_state code grad_executor_states grad_executors diff_graph_idx all_backward_graphs script_module diff_graph_idx=None Note Python order seems unstable ge_state = script_module get_debug_state fwd_plan = get_execution_plan ge_state grad_executor_state = get_grad_executor fwd_plan diff_graph_idx=diff_graph_idx bwd_plans = list grad_executor_state execution_plans values p graph copy p bwd_plans backward_graph script_module diff_graph_idx=None skip_check=False ge_state = script_module get_debug_state fwd_plan = get_execution_plan ge_state grad_executor_state = get_grad_executor fwd_plan diff_graph_idx=diff_graph_idx skip_check=skip_check bwd_plan = get_execution_plan grad_executor_state Running JIT passes requires we own graph shared_ptr The debug state struct does own its graph so we make copy bwd_plan graph copy helper function get sum List Tensor _sum_of_list tensorlist s = t tensorlist s += t sum s has top level Pickle complains FooToPickle torch nn Module __init__ - None super __init__ bar = torch jit ScriptModule TestJitProfiler JitTestCase This runs tests requires setting some global states like torch _C _set_graph_executor_optimize restore values afterward i e test_profiler This address flaky issue https github com pytorch pytorch issues which test_profiler flaky failed middle without chance restore torch _C _set_graph_executor_optimize its original value This causes issues all future tests running after Using separate test here so there no need run setup teardown all tests TestJit setUp super setUp graph_executor_optimize_opt = torch _C _get_graph_executor_optimize tearDown super tearDown Resetting torch _C _set_graph_executor_optimize graph_executor_optimize_opt test_profiler torch _C _set_graph_executor_optimize False other_fn x x x = torch rand traced_other_fn = torch jit trace other_fn x fn x y = traced_other_fn x fut = torch jit _fork traced_other_fn x y = torch jit _wait fut y traced_fn = torch jit trace fn x torch autograd profiler profile prof traced_fn x expecting see other_fn TS function call cpu time = mul cpu time forked other_fn mul_events = defaultdict int other_fn_events = defaultdict int e prof function_events e name == aten mul assertTrue e thread mul_events mul_events e thread = e time_range elapsed_us e name == other_fn assertTrue e thread other_fn_events other_fn_events e thread = e time_range elapsed_us assertTrue len mul_events == assertTrue len other_fn_events == thread mul_time mul_events items assertTrue thread other_fn_events assertTrue other_fn_events thread = mul_time TestJit JitTestCase unittest skip Requires lot RAM test_big m = torch jit ScriptModule gig = int small tensor first GB m v = nn Parameter torch full dtype=torch float large tensor first GB ends outside m v = nn Parameter torch full gig dtype=torch float small tensor GB space m v = nn Parameter torch full dtype=torch float s large tensor GB space m v = nn Parameter torch full gig dtype=torch float m = getExportImportCopy m assertEqual tuple m parameters tuple m parameters test_inferred_as_tensor assertRaisesRegex RuntimeError Inferred value argument dim type Tensor because annotated explicit type torch jit script dot points query dim points query sum dim test_constants_pkl This test asserts serialization archive includes ` constants pkl ` file This file used ` torch load ` determine whether zip file normal eager-mode serialization zip jit serialization zip If you deleting ` constants pkl ` make sure update ` torch serialization load ` so still able figure out which which torch jit script fn x x buf = io BytesIO torch jit save fn buf buf seek files = zipfile ZipFile buf filelist assertTrue any archive constants pkl == f filename f files test_script_fn_pkl assertRaisesRegex pickle PickleError ScriptFunction cannot pickled torch jit script fn x torch Tensor - torch Tensor x pkl_fn = pickle dumps fn protocol= test_script_fn_valid_name torch jit script fn x torch Tensor - torch Tensor x assertIsNotNone fn __name__ assertIsNotNone fn __qualname__ test_restore_device M torch jit ScriptModule __init__ cpu_device_str super __init__ p = nn Parameter torch tensor dtype=torch float device=cpu_device_str b = torch tensor dtype=torch float device=cpu_device_str main purpose checking map_location works m = M cpu m = getExportImportCopy m assertEqual tuple m parameters tuple m parameters assertEqual tuple m buffers tuple m buffers assertFalse m p is_cuda assertFalse m b is_cuda unittest skipIf RUN_CUDA restore device requires CUDA test_restore_device_cuda MyModule torch jit ScriptModule __init__ - None super __init__ b = nn Buffer torch randn p = nn Parameter torch randn torch jit script_method forward x x + b + p m = MyModule m cuda torch cuda device_count - cuda_device_str = cuda + str torch cuda device_count - assertTrue m p is_cuda assertTrue m b is_cuda restore saved devices m = getExportImportCopy m assertEqual tuple m parameters tuple m parameters assertEqual tuple m buffers tuple m buffers assertEqual str m p device cuda_device_str assertEqual str m b device cuda_device_str restore all cpu using string cpu_device_str = cpu m = getExportImportCopy m map_location=cpu_device_str assertEqual str m p device cpu_device_str assertEqual str m b device cpu_device_str restore all first gpu using device m = getExportImportCopy m map_location=torch device cuda assertEqual str m p device cuda assertEqual str m b device cuda compute compare results input = torch rand cuda torch cuda device_count - origin_result = m input assertEqual origin_result m input assertEqual origin_result m input cpu assertEqual origin_result m input cuda test_trace_retains_train M torch nn Module forward x x m = M m eval tm = torch jit trace m torch rand assertEqual tm training m training unittest skipIf RUN_CUDA restore device requires CUDA test_restore_shared_storage_on_cuda Foo torch jit ScriptModule __init__ - None super __init__ whole_tensor = torch randn dtype=torch float device= cpu p = nn Parameter whole_tensor narrow b = nn Buffer whole_tensor narrow m = Foo m = getExportImportCopy m map_location=torch device cuda assertEqual tuple m parameters tuple m parameters assertEqual tuple m buffers tuple m buffers assertTrue m p is_cuda assertTrue m b is_cuda assertTrue m p is_shared assertTrue m b is_shared assertEqual m b storage data_ptr m p storage data_ptr test_add_relu_fusion M torch nn Module __init__ relu_op super __init__ relu_op = relu_op forward b c tmp = torch add b x = relu_op tmp d = torch add c x + d = torch rand = - = + b = torch rand c = torch rand m = torch jit script M torch relu orig_res = m b c torch _C _jit_pass_fuse_add_relu m graph buffer = io BytesIO torch jit save m buffer buffer seek m = torch jit load buffer new_res = m b c FileCheck check_not aten relu \ check aten _add_relu \ run m graph torch testing assert_close orig_res new_res add relu_ = torch rand = - = + b = torch rand c = torch rand m = torch jit script M torch relu_ orig_res = m b c torch _C _jit_pass_fuse_add_relu m graph buffer = io BytesIO torch jit save m buffer buffer seek m = torch jit load buffer new_res = m b c FileCheck check_not aten relu_ \ check aten _add_relu \ run m graph torch testing assert_close orig_res new_res Madd_ torch nn Module __init__ relu_op super __init__ relu_op = relu_op forward b x = add_ b x = relu_op x x add_ relu_ = torch rand = - = + b = torch rand Because place add_ will overwrite a_copy = clone m = torch jit script Madd_ torch relu_ orig_res = m b torch _C _jit_pass_fuse_add_relu m graph buffer = io BytesIO torch jit save m buffer buffer seek m = torch jit load buffer new_res = m a_copy b FileCheck check_not aten add_ \ check_not aten relu_ \ check aten _add_relu_ \ run m graph torch testing assert_close orig_res new_res Since _add_relu_ does inplace mutation ensure a_copy modified torch testing assert_close orig_res a_copy Madd_out torch nn Module __init__ relu_op super __init__ relu_op = relu_op forward b x = torch add b out=a x = relu_op x x = torch rand = - = + b = torch rand add_out relu_ = torch rand = - = + b = torch rand Because place add_ will overwrite a_copy = clone m = torch jit script Madd_out torch relu_ orig_res = m b torch _C _jit_pass_fuse_add_relu m graph buffer = io BytesIO torch jit save m buffer buffer seek m = torch jit load buffer new_res = m a_copy b FileCheck check_not aten add \ check_not aten relu_ \ check aten _add_relu \ run m graph torch testing assert_close orig_res new_res Since _add_relu_ out=a does inplace mutation ensure a_copy modified torch testing assert_close orig_res a_copy test_repeat_interleave_script fn input torch Tensor repeats torch Tensor - torch Tensor output = input repeat_interleave repeats output fn_scripted = torch jit script fn input = torch tensor dtype=torch int repeats = torch tensor dtype=torch int output = fn input repeats output_scripted = fn_scripted input repeats assertEqual output_scripted output unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY Simple executor doesn t have shape information test_peephole_optimize_shape_ops test_input func input result result == we will trigger bailout unprofiled graph should correct result assertEqual func input profile_and_replay=True result gre = func graph_for input FileCheck check_not prim If run gre test_dim torch jit script func x x dim == test_input func torch tensor test_input func torch tensor test_dim test_size_index torch jit script func x x size == test_input func torch rand test_input func torch rand torch jit script neg_index x x size - == test_input neg_index torch rand test_input neg_index torch rand GRAPH_EXECUTOR == ProfilingMode PROFILING test_size_index test_dtype torch jit script func x x dtype == torch float test_input func torch tensor dtype=torch float test_input func torch tensor dtype=torch int test_dtype test_is_floating_poiint torch jit script func x x is_floating_point test_input func torch tensor dtype=torch float test_input func torch tensor dtype=torch int test_is_floating_poiint test_device torch jit script func_ x x device == torch device cuda = = torch jit script func_ x x is_cuda = = test_input func_ torch tensor test_input func_ torch tensor RUN_CUDA test_input func_ torch tensor device= cuda test_input func_ torch tensor device= cuda test_device test_attrs foo x x dtype TODO dtype long - instance conversion x device x shape x is_cuda x is_mkldnn x is_quantized x requires_grad x T x mT x H x mH x layout TODO layout long - instance conversion scripted = torch jit script foo x = torch rand assertEqual scripted x foo x test_layout torch jit script check x y x layout == y layout x = torch rand y = torch rand assertTrue check x y test_matrix_transpose torch jit script check x torch equal x mT x transpose - - x = torch rand assertTrue check x test_transpose torch jit script check x torch equal x T x t x = torch rand assertTrue check x test_matrix_conj_transpose torch jit script check x torch equal x mH x transpose - - conj x = torch rand assertTrue check x x = make_tensor device= cpu dtype=torch complex assertTrue check x test_conj_transpose torch jit script check x torch equal x H x t conj x = torch rand assertTrue check x x = make_tensor device= cpu dtype=torch complex assertTrue check x test_T_mT_H_mH T x x mT mT x x mT H x x H mH x x mH x = torch rand y = make_tensor device= cpu dtype=torch complex checkScript T x checkScript mT x checkScript H x checkScript mH x checkScript T y checkScript mT y checkScript H y checkScript mH y test_nn_conv Mod nn Module __init__ conv super __init__ conv = conv forward input conv input inputs = Conv Mod nn Conv d stride= torch randn Mod nn Conv d stride= torch randn Mod nn Conv d stride= torch randn ConvTransposed Mod nn ConvTranspose d stride= torch randn Mod nn ConvTranspose d stride= torch randn Mod nn ConvTranspose d stride= torch randn m inp inputs checkModule m inp unittest skipIf GRAPH_EXECUTOR = ProfilingMode PROFILING Not implemented Simple Legacy test_debug_flush_compilation_cache foo x x + Mod nn Module forward t t + m = torch jit script Mod x = torch rand enable_profiling_mode_for_profiling_tests jitted = checkScript foo x shouldn t throw states = jitted get_debug_state after flushing there shouldn t no opt plan jitted _debug_flush_compilation_cache assertRaisesRegex RuntimeError INTERNAL ASSERT FAILED states = jitted get_debug_state NUM_RUNS = num_profiled_runs NUM_RUNS m x m x fwd = m _c _get_method forward states = m get_debug_state after flushing there shouldn t no opt plan fwd _debug_flush_compilation_cache assertRaisesRegex RuntimeError INTERNAL ASSERT FAILED states = m get_debug_state test_numel torch jit script get_numel_script x x numel x = torch rand numel = get_numel_script x assertEqual numel x numel test_element_size torch jit script get_element_size_script x x element_size x = torch rand element_size = get_element_size_script x assertEqual element_size x element_size test_Sequential Seq nn Module __init__ - None super __init__ seq = nn Sequential nn Linear nn Linear torch jit script_method forward x l seq x = l x x m = torch jit script Seq assert m graph ensure jit able compile test_ModuleList Mod nn Module __init__ - None super __init__ model = nn ModuleList nn Linear _ range model += nn Linear model append nn Linear model extend nn Linear nn Linear forward v m model v = m v v m = torch jit script Mod assert m graph ensure jit able compile test_disabled torch jit _state disable try f x y x + y assertIs torch jit trace f torch randn torch randn f assertIs torch jit script f f MyModule torch jit ScriptModule torch jit script_method method x x XXX Unfortunately ScriptModule won t simply become Module now because requires disabling JIT startup time which we can t do here We need those two conditions make work all versions Python assertTrue inspect ismethod MyModule method inspect isfunction MyModule method finally torch jit _state enable test_train_eval Sub nn Module forward input training input -input MyModule torch jit ScriptModule __init__ module super __init__ module = module torch jit script_method forward input module input + m = MyModule Sub input = torch rand assertEqual input + m input m eval assertEqual -input + m input test batchnorm dropout train eval input = torch randn batchnorm = nn BatchNorm d dropout = nn Dropout p= m_batchnorm = MyModule batchnorm assertEqual batchnorm input + m_batchnorm input batchnorm eval m_batchnorm eval assertEqual batchnorm input + m_batchnorm input m_dropout = MyModule dropout dropout eval m_dropout eval assertEqual dropout input + m_dropout input test_nn_lp_pool d Mod torch nn Module __init__ - None super __init__ l = torch nn LPPool d n = torch nn LPPool d forward x l x n x torch nn functional lp_pool d x float torch nn functional lp_pool d x torch nn functional lp_pool d x float checkModule Mod torch rand test_nn_lp_pool d Mod torch nn Module __init__ - None super __init__ l = torch nn LPPool d n = torch nn LPPool d forward x l x n x torch nn functional lp_pool d x float torch nn functional lp_pool d x torch nn functional lp_pool d x float checkModule Mod torch rand test_nn_padding_functional Mod nn Module __init__ pad super __init__ pad = pad forward x F pad x pad mode= constant value= inputs = Mod torch randn D Mod torch randn D Mod torch randn D m inp inputs checkModule m inp test_nn_padding Mod nn Module __init__ padding super __init__ padding = padding forward input padding input inputs = Mod nn ConstantPad d torch randn Mod nn ConstantPad d torch randn Mod nn ConstantPad d torch randn Mod nn ReflectionPad d torch arange dtype=torch float reshape Mod nn ReflectionPad d torch arange dtype=torch float reshape Mod nn ReflectionPad d torch randn Mod nn ReplicationPad d torch arange dtype=torch float reshape Mod nn ReplicationPad d torch arange dtype=torch float reshape Mod nn ReplicationPad d torch randn Mod nn ZeroPad d torch randn m inp inputs checkModule m inp test_script_autograd_grad test_simple_grad x y type Tensor Tensor - List Optional Tensor z = x + y + x y torch autograd grad z sum x y test_simple_grad_with_grad_outputs x y type Tensor Tensor - List Optional Tensor z = x + y + x y grad_outputs = torch jit annotate List Optional torch Tensor torch ones torch autograd grad z x y grad_outputs test_one_output_not_requires_grad x y type Tensor Tensor - List Optional Tensor z = y + y torch autograd grad z sum x y allow_unused=True test_retain_graph x y type Tensor Tensor - None z = x + y + x y torch autograd grad z sum x y retain_graph=True torch autograd grad z sum x y x = torch randn requires_grad=True y = torch randn requires_grad=True checkScript test_simple_grad x y inputs_requires_grad=True checkScript test_simple_grad_with_grad_outputs x y inputs_requires_grad=True checkScript test_one_output_not_requires_grad x y inputs_requires_grad=True checkScript test_retain_graph x y inputs_requires_grad=True test_script_backward checkBackwardScript fn inputs scripted_fn = torch jit script fn FileCheck check torch autograd backward run scripted_fn code recording_inputs = do_input_map lambda t t detach requires_grad_ inputs fn inputs scripted_fn recording_inputs inp inp zip inputs recording_inputs assertEqual inp grad inp grad test_tensor_backward input type Tensor - None output = torch relu input output = output softmax sum_out = output sum sum_out backward test_torch_autograd_backward input type Tensor - None output = torch relu input output = output softmax torch autograd backward output sum test_torch_autograd_backward_with_grad_tensors input type Tensor - None output = torch relu input output = output softmax grad_outputs = torch jit annotate List Optional torch Tensor torch ones torch autograd backward output grad_outputs inp = torch randn requires_grad=True checkBackwardScript test_tensor_backward inp checkBackwardScript test_torch_autograd_backward inp checkBackwardScript test_torch_autograd_backward_with_grad_tensors inp test_script_backward_twice checkBackwardTwiceScript fn inputs retain_graph_=False jit_profiling_executor_false __enter__ torch _C _jit_set_profiling_executor False __exit__ args torch _C _jit_set_profiling_executor GRAPH_EXECUTOR = ProfilingMode LEGACY jit_profiling_executor_false torch jit optimized_execution True scripted_fn = torch jit script fn inputs FileCheck check prim DifferentiableGraph run scripted_fn graph_for inputs result = scripted_fn inputs result sum backward retain_graph=retain_graph_ retain_graph_ assertRaisesRegex RuntimeError Specify retain_graph=True lambda result sum backward result sum backward test_script_backward_twice_with_saved_values input input type Tensor Tensor - Tensor tmp = torch mul input input tmp = torch abs tmp torch equal input input tmp = torch acos tmp tmp = torch atan tmp result = torch add tmp input result inp = torch randn requires_grad=True inp = torch randn requires_grad=True checkBackwardTwiceScript test_script_backward_twice_with_saved_values inp inp False checkBackwardTwiceScript test_script_backward_twice_with_saved_values inp inp True test_diff_subgraph_clones_constants torch jit script f x y x + x + y + x + y + x + y + x + y + x count_constants graph sum node kind == prim Constant node graph nodes graph = f graph copy run_pass cse graph run_pass create_autodiff_subgraphs graph nodes = list graph nodes assertEqual count_constants graph assertEqual count_constants nodes g Subgraph TODO adapt test check GraphExecutor treats them differently unittest skip Need adjusted Graph Executor test_arg_configurations Different arg configurations should trigger different traces x = Variable torch FloatTensor uniform_ x_double = Variable x data double x_grad = Variable x data clone requires_grad=True y = Variable torch randn configurations = x x_double x_grad y x x x y torch cuda is_available x_cuda = Variable x data cuda configurations += x_cuda x x_cuda x_cuda x x_cuda x torch cuda device_count x_cuda_ = Variable x data cuda configurations += x_cuda_ x_cuda x_cuda_ torch jit compile nderivs= fn args in_vars _ = torch _C _jit_flatten args in_vars + i config enumerate configurations assertFalse fn has_trace_for config fn config assertTrue fn has_trace_for config unk_config configurations i + assertFalse fn has_trace_for unk_config assertEqual fn hits test_torch_sum fn x torch sum x fn x dim int torch sum x dim x = torch randn checkScript fn x checkScript fn x checkScript fn x test_cse x = torch tensor requires_grad=True y = torch tensor requires_grad=True fn x y w = x + y x + y x + y t = torch tanh w + torch tanh w z = x + y x + y x + y + t z g _ = torch jit _get_trace_graph fn x y run_pass cse g do_exactly = True FileCheck check_count add check_count mul do_exactly \ check_count tanh do_exactly check_count add do_exactly check_next \ run str g assertExportImport g x y test_cse_not_introduce_aliasing torch jit script tensor_alias_outputs x x + x x + x run_pass cse tensor_alias_outputs graph FileCheck check_count aten add run tensor_alias_outputs graph torch jit script ints_alias_outputs x type int - Tuple int int x + x x + x non-aliasing types can CSEd run_pass cse ints_alias_outputs graph FileCheck check_count aten add exactly=True run ints_alias_outputs graph test_recursive_cse input_str = graph x Tensor y Tensor int int = prim Constant value= Tensor = aten add x y int = aten add bool = aten Bool z int = prim If CHECK block block CHECK-NOT aten add z int = aten add - z block - z graph = parse_ir input_str run_pass cse graph FileCheck run input_str graph test_pattern_based_rewrite mul mul mul mul x y z x y -- mul mul mulmul x y z x y -- -- mulmul mulmul x y z x y input_str = graph x y z CHECK-NOT aten mul CHECK my fused_mulmul t = aten mul x y p = aten mul t z CHECK my fused_mulmul u = aten mul p x o = aten mul u y o graph = parse_ir input_str torch _C _jit_pass_custom_pattern_based_rewrite_graph graph b c q = aten mul b r = aten mul q c r graph b c r = my fused_mulmul b c r graph FileCheck run input_str graph Check overlapping matches handled correctly mul mul mul x y z x -- mul mulmul x y z x input_str = graph x y z CHECK-NOT aten mul CHECK my fused_mulmul t = aten mul x y p = aten mul t z CHECK-NEXT aten mul u = aten mul p x u graph = parse_ir input_str torch _C _jit_pass_custom_pattern_based_rewrite_graph graph b c q = aten mul b r = aten mul q c r graph b c r = my fused_mulmul b c r graph FileCheck run input_str graph Check add mul x y z -- muladd x y z replacement input_str = graph x y z CHECK-NOT aten mul CHECK-NOT aten add c = prim Const value= t = aten mul x y p = aten add t z c CHECK my muladd CHECK-NEXT p graph = parse_ir input_str torch _C _jit_pass_custom_pattern_based_rewrite_graph graph b c d q = aten mul b r = aten add q c d r graph b c d r = my muladd b c d r graph FileCheck run input_str graph Check add mul x y z -- sub add x y z replacement input_str = graph x y z CHECK-NOT aten mul c = prim Const value= CHECK aten add t = aten mul x y CHECK-NEXT aten sub p = aten add t z c CHECK-NOT aten add CHECK-NEXT p graph = parse_ir input_str torch _C _jit_pass_custom_pattern_based_rewrite_graph graph b c d q = aten mul b r = aten add q c d r graph b c d q = aten add b d r = aten sub q c d r graph FileCheck run input_str graph Check mul x y -- x replacement input_str = graph x y z c = prim Const value= CHECK-NOT aten mul t = aten mul x y CHECK aten add x z p = aten add t z c CHECK-NEXT p graph = parse_ir input_str torch _C _jit_pass_custom_pattern_based_rewrite_graph graph Pa Pb Pq = aten mul Pa Pb Pq graph Ra Rb Ra graph FileCheck run input_str graph _tmp_donotuse_dont_inline_everything test_pattern_based_module_rewrite Check match module behavior Test torch nn Module __init__ - None super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d num_features= forward x x = conv x x = bn x x m = torch jit script Test torch _C _jit_pass_custom_pattern_based_rewrite_graph graph x conv = match module name= Conv d y = prim CallMethod name= forward conv x bn = match module name= BatchNorm d z = prim CallMethod name= forward bn y z graph x z = my matched_conv_bn x z m _c _get_method forward graph FileCheck check my matched_conv_bn run m _c _get_method forward graph test_pattern_based_rewrite_with_source_range_preserved TestModule torch nn Module forward x y z w x = x + y x = x z w - x input_pattern = graph x y z const t = aten add x y const o = aten mul t z o replacement_pattern = graph x y z const o = my add_mul x y z const o scripted_model = torch jit script TestModule graph = scripted_model graph value_mappings = o t node graph nodes node kind == aten add source_range_ = node sourceRange torch _C _jit_pass_custom_pattern_based_rewrite_graph input_pattern replacement_pattern scripted_model graph value_name_pairs=value_mappings graph = scripted_model graph node graph nodes node kind == my add_mul source_range_ = node sourceRange assertTrue source_range_ == source_range_ TestModule torch nn Module forward x y z w x = x + y x = x + z x = x z x = x w x - Check source range preservation two node transforms add - my_add input_pattern = graph x y const o = aten add x y const o replacement_pattern = graph x y const o = my add x y const o scripted_model = copy deepcopy torch jit script TestModule graph_copy = scripted_model graph copy value_mappings = o o source_range_add_ = None node graph_copy nodes source_range_add_ None node kind == aten add source_range_add_ = node sourceRange source_range_add_ None node kind == aten add source_range_add_ = node sourceRange torch _C _jit_pass_custom_pattern_based_rewrite_graph input_pattern replacement_pattern graph_copy value_name_pairs=value_mappings source_range_my_add_ = None node graph_copy nodes source_range_my_add_ None node kind == my add source_range_my_add_ = node sourceRange source_range_my_add_ None node kind == my add source_range_my_add_ = node sourceRange assertTrue source_range_add_ == source_range_my_add_ assertTrue source_range_add_ == source_range_my_add_ Check source range preservation add-add - double_add transform fuse nodes input_pattern = graph x y z const t = aten add x y const o = aten add t z const o replacement_pattern = graph x y z const o = my double_add x y z const o scripted_model = torch jit script TestModule graph_copy = scripted_model graph copy value_mappings = o t source_range_ = None source_range_ = None node graph_copy nodes node kind == aten add source_range_ = node sourceRange break torch _C _jit_pass_custom_pattern_based_rewrite_graph input_pattern replacement_pattern graph_copy value_name_pairs=value_mappings node graph_copy nodes node kind == my double_add source_range_ = node sourceRange assertTrue source_range_ == source_range_ Check source range preservation mul - add + add transform split node input_pattern = graph x y t = aten mul x y t replacement_pattern = graph x y t = my add x y o = my add t y o scripted_model = torch jit script TestModule graph_copy = scripted_model graph copy value_mappings = t t o t source_range_mul_ = None node graph_copy nodes source_range_mul_ None node kind == aten mul source_range_mul_ = node sourceRange source_range_mul_ None node kind == aten mul source_range_mul_ = node sourceRange torch _C _jit_pass_custom_pattern_based_rewrite_graph input_pattern replacement_pattern graph_copy value_name_pairs=value_mappings source_range_add_ = None node graph_copy nodes source_range_add_ None node kind == my add source_range_add_ = node sourceRange source_range_add_ None node kind == my add source_range_add_ = node sourceRange assertTrue source_range_mul_ == source_range_add_ assertTrue source_range_mul_ == source_range_add_ Check lack source range preservation mul-mul- double_mul transform input_pattern = graph x y z t = aten mul x y o = aten mul t z o replacement_pattern = graph x y z o = my double_mul x y z o scripted_model = torch jit script TestModule graph_copy = scripted_model graph copy node graph_copy nodes node kind == aten mul source_range_ = node sourceRange torch _C _jit_pass_custom_pattern_based_rewrite_graph input_pattern replacement_pattern graph_copy node graph_copy nodes node kind == my double_mul source_range_ = node sourceRange assertFalse source_range_ == source_range_ test_expand_quantlint pass test_expand_fold_quant_inputs pass test_shape_analysis_broadcast broadcast b + b x = torch randn requires_grad=True y = torch randn requires_grad=True graph = torch jit script broadcast graph torch _C _jit_pass_complete_shape_analysis graph x y False FileCheck check Float strides= device=cpu run str graph test_shape_analysis_unsqueeze_in_loop input_str = graph x Tensor bool = prim Constant value= int = prim Constant value= int = prim Constant value= CHECK FloatTensor requires_grad= device=cpu = prim Loop x Tensor = prim Loop x CHECK FloatTensor requires_grad= device=cpu block i int x Tensor CHECK FloatTensor requires_grad= device=cpu = aten unsqueeze x Tensor = aten unsqueeze x - x x graph = parse_ir input_str torch _C _jit_pass_complete_shape_analysis graph torch zeros dtype=torch float False FileCheck run input_str graph test_script_tensor_type foo x t torch dtype x type t scr = torch jit script foo x = torch rand t torch int torch float torch float torch bfloat torch complex torch complex torch bool assertEqual scr x t foo x t test_script_bool_literal_conversion foo x torch mul x True scr = torch jit script foo x = torch rand assertEqual scr x foo x test_shape_analysis_masked_select input_str = graph Float Bool CHECK Float requires_grad= device=cpu = aten masked_select Tensor = aten masked_select test test_jit py graph = parse_ir input_str x = torch ones dtype=torch float mask = x ge torch _C _jit_pass_complete_shape_analysis graph x mask False FileCheck run input_str graph TODO update verify work GraphExecutors unittest skip verify needs updated work GraphExecutors test_verify x = torch tensor requires_grad=True y = torch tensor requires_grad=True torch jit compile f x y z = torch sigmoid x x + y w = torch abs x x x + y + Variable torch ones z w torch jit verify f x y loss_fn=lambda z w z w devices= TODO adapt GraphExecutor test unittest skip Need instrument GraphExecutors bit more test_flags x y = torch randn y = Variable torch randn torch jit compile fn x y x x + y y + x y sum grads = rx ry product True False repeat= x requires_grad = rx y requires_grad = ry assertFalse fn has_trace_for x y out = fn x y assertFalse fn has_trace_for x y v name compute x x rx y y ry compute continue grad_v = torch autograd grad out v retain_graph=True expected_grad = grads setdefault name grad_v assertEqual grad_v expected_grad assertEqual fn has_trace_for x y rx ry test_python_ir x = torch tensor requires_grad=True y = torch tensor requires_grad=True doit x y torch sigmoid torch tanh x x + y g _ = torch jit _get_trace_graph doit x y run_pass dce g run_pass canonicalize g g = torch _C Graph g_to_g = node g inputs g_to_g node = g addInput node g nodes n_ = g createClone node lambda x g_to_g x g appendNode n_ g_to_g update zip node outputs n_ outputs node g outputs g registerOutput g_to_g node t_node = g create prim TensorTest t_ torch ones assertEqual t_node attributeNames g appendNode t_node assertTrue torch equal torch ones t_node t node g nodes assertTrue g findNode node kind None unittest skipIf IS_SANDCASTLE gtest runs these sandcastle unittest skipIf RUN_CUDA covered test_cpp_cuda unittest skipIf torch _C _jit_has_cpp_tests Tests built use BUILD_TEST= test_cpp cpp jit tests_setup tests_setup setup torch _C _jit_run_cpp_tests tests_setup shutdown test_batchnorm x = torch ones g outputs inputs = torch jit _get_trace_graph nn BatchNorm d x _force_outplace=True return_inputs=True m = createFunctionFromGraph g assertEqual outputs m inputs test_dropout x = torch ones torch random fork_rng devices= g outputs inputs = torch jit _get_trace_graph nn Dropout x return_inputs=True torch random fork_rng devices= m = createFunctionFromGraph g assertEqual outputs m inputs unittest skipIf RUN_CUDA test requires CUDA unittest skipIf GRAPH_EXECUTOR = ProfilingMode PROFILING skip profiling isn t enabled test_native_dropout_corner_case disable_autodiff_subgraph_inlining t x p float t bool o = torch dropout x p t o jit_t = torch jit script t x = torch randn requires_grad_ FileCheck check prim DifferentiableGraph run jit_t graph_for x True profile_and_replay=True train True False p device cuda cpu x = torch randn device=device requires_grad_ x_ref = x detach requires_grad_ o = jit_t x p train o_ref = t x_ref p train o sum backward o_ref sum backward assert o equal o_ref assert x grad equal x_ref grad slowTest unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY Testing differentiable graph test_dropout_module_requires_grad enable_profiling_mode_for_profiling_tests MyModule torch nn Module __init__ M super __init__ dropout = torch nn Dropout linear = torch nn Linear M M forward input input = dropout input output = linear input output profile func X torch autograd profiler profile prof func X e name e prof function_events M = scripted = torch jit script MyModule M To reduce confusion about expected behaviors requires_grad controls whether dropout symbolically differentiated training controls whether bernoulli_ called inside symbolic differentiation dropout When requires_grad == training expected behaviors obvious When requires_grad=True training=False bernoulli_ might still show up graph But s branch s called That s why we have separate checks autograd profiler make sure s run When requires_grad=False training=True bernoulli_ must run since s expected behavior dropout layer training mode It s independent whether graph requires gradient In fact bernoulli_ comes autograd instead autodiff case training True False training scripted train scripted eval requires_grad True False X = torch randn M M requires_grad=requires_grad requires_grad FileCheck check aten native_dropout run scripted graph_for X profile_and_replay=True assertEqual training aten bernoulli_ profile scripted X unittest skipIf GRAPH_EXECUTOR == ProfilingMode SIMPLE Testing differentiable graph skipIfTorchDynamo Torchdynamo cannot correctly handle profiler profile calls test_dropout_func_requires_grad dropout_training input F dropout input training=True dropout_eval input F dropout input training=False profile func X torch autograd profiler profile prof func X e name e prof function_events M = scripted_training = torch jit script dropout_training scripted_eval = torch jit script dropout_eval See comments test_dropout_module_requires_grad disable_autodiff_subgraph_inlining requires_grad True False X = torch randn M M requires_grad=requires_grad requires_grad FileCheck check aten native_dropout run scripted_training graph_for X profile_and_replay=True assertIn aten bernoulli_ profile scripted_training X assertNotIn aten bernoulli_ profile scripted_eval X unittest skipIf RUN_CUDA test_dropout_cuda require CUDA test_dropout_cuda Dropout AD dispatched _fused_dropout CUDA case which included TestJitGeneratedFunctional _zero_rate t torch true_divide t == sum t numel x = torch ones cuda requires_grad_ enable_profiling_mode_for_profiling_tests torch jit script func x torch nn functional dropout x freeze_rng_state out_ref = torch nn functional dropout x grad_ref = torch autograd grad out_ref sum x freeze_rng_state out = func x grad = torch autograd grad out sum x TODO previously we assert exact matches between eager JIT result assertEqual out out_ref assertEqual grad grad_ref This test disabled during legacy - profiling executor transition Currently JIT fused results doesn t match eager result exactly due some changes merged between We temporarily only check statstical difference should reverted once issue fixed assertEqual _zero_rate out _zero_rate out_ref rtol= e- atol= e- assertEqual _zero_rate grad _zero_rate grad_ref rtol= e- atol= e- test_torch_ops_overloaded assertRaisesRegex RuntimeError failed match any schema torch ops aten add assertEqual ab torch ops aten add b b = torch rand torch rand assertEqual + b torch ops aten add b assertEqual + torch ops aten add test_torch_ops_kwonly b = torch rand torch rand assertRaisesRegex RuntimeError positional argument torch ops aten add b h t Chillee ambiguous case assertEqual prod torch ops aten prod test_torch_complex fn real img torch complex real img fn_out real img out torch complex real img out=out checkScript fn torch rand torch rand checkScript fn torch ones torch ones checkScript fn torch zeros torch ones checkScript fn torch zeros torch zeros checkScript fn torch empty torch empty real = torch tensor dtype=torch float img = torch tensor dtype=torch float out = torch empty dtype=torch complex checkScript fn_out real img out real = torch tensor dtype=torch float img = torch tensor dtype=torch float out = torch empty dtype=torch complex checkScript fn_out real img out real = torch ones img = torch ones out = torch empty dtype=torch complex checkScript fn_out real img out real = torch ones img = torch ones out = torch empty dtype=torch complex checkScript fn_out real img out real = torch empty img = torch empty out = torch empty dtype=torch complex checkScript fn_out real img out real = torch zeros img = torch empty out = torch empty dtype=torch complex checkScript fn_out real img out real = torch ones img = torch empty out = torch empty dtype=torch complex checkScript fn_out real img out real = torch ones img = torch zeros out = torch empty dtype=torch complex checkScript fn_out real img out test_einsum check fn jitted args assertGraphContains jitted graph kind= aten einsum assertEqual fn args jitted args equation_format x y torch einsum i j- ij x y equation_format_varargs x y torch einsum i j- ij x y sublist_format x y torch einsum x y x = make_tensor dtype=torch float device= cpu y = make_tensor dtype=torch float device= cpu fn equation_format equation_format_varargs sublist_format check fn torch jit script fn x y check fn torch jit trace fn x y x y skipIfTorchDynamo TorchDynamo fails unknown reason test_python_ivalue Test pure python object can hold IValue conversion between IValue PyObject correct test numpy object py_array = np arange ret_py_obj = torch _C _ivalue_debug_python_object py_array assertEqual py_array ret_py_obj test function object ret_py_obj = torch _C _ivalue_debug_python_object F relu assertEqual F relu ret_py_obj test memory management we need ensure IValue correctly call incref decref avoid dangling behavior potential memory leaks during conversions test_func_scope_helper inp create scope do conversion - ivalue - pyobject func new pyobject refcount + inp_refcount = sys getrefcount inp ivalue_holder = torch _C _ivalue_debug_python_object inp assertEqual inp_refcount + sys getrefcount ivalue_holder ivalue_holder + test_input = before_count = sys getrefcount test_input test_func_scope_helper test_input after_count = sys getrefcount test_input after test_func_scope_helper_call refcount test_input should equal original refcount otherwise we get either dangling pointer memory leak assertEqual before_count after_count test_decompose_addmm does_decompose torch jit script addmm mat mat mat = mat addmm mat mat b = mat addmm mat mat alpha= beta= + b mat = torch randn mat = torch randn mat = torch randn out_ref = addmm mat mat mat run_pass decompose_ops addmm graph out_test = addmm mat mat mat assertEqual out_ref out_test FileCheck check_not addmm run str addmm graph doesnt_decompose torch jit script addmm mat mat mat alpha beta = mat addmm mat mat alpha= beta= b = mat addmm mat mat alpha=int alpha beta=int beta + b orig = str addmm graph run_pass decompose_ops addmm graph assertTrue orig == str addmm graph does_decompose doesnt_decompose suppress_warnings test_sparse_tensors torch jit ignore get_sparse torch sparse_coo_tensor dtype=torch float torch jit script test_is_sparse input type Tensor - bool input is_sparse script_out_is_sparse = test_is_sparse get_sparse script_out_is_dense = test_is_sparse torch randn assertEqual script_out_is_sparse True assertEqual script_out_is_dense False test_basic_sparse input output = get_sparse output input checkScript test_basic_sparse get_sparse checkScript test_basic_sparse torch tensor test_sparse_sum input torch sparse sum input checkScript test_sparse_sum get_sparse test_sparse_mm input input torch sparse mm input input checkScript test_sparse_mm get_sparse torch randn test_sparse_addmm input input input torch sparse addmm input input input test_sparse_addmm_alpha_beta input input input torch sparse addmm input input input alpha= beta= checkScript test_sparse_addmm torch randn get_sparse torch randn checkScript test_sparse_addmm_alpha_beta torch randn get_sparse torch randn suppress_warnings test_sparse_csr_tensors torch jit ignore get_sparse_csr torch randn to_sparse_csr torch jit script test_is_sparse_csr input type Tensor - bool input is_sparse_csr script_out_is_sparse_csr = test_is_sparse_csr get_sparse_csr script_out_is_dense_csr = test_is_sparse_csr torch randn assertEqual script_out_is_sparse_csr True assertEqual script_out_is_dense_csr False unittest skipIf RUN_CUDA requires CUDA test_device_not_equal compare_device x torch device x = torch device cuda compare_two_device x torch device y torch device x = y checkScript compare_device torch device cuda checkScript compare_two_device torch device cuda torch device cuda test_constant_prop_simple torch jit script constant_prop input_int type int - int = b = + b - input_int out_ref = constant_prop run_pass constant_propagation constant_prop graph out_test = constant_prop assertEqual out_ref out_test graph_str = str constant_prop graph assertTrue aten add graph_str aten mul graph_str const = constant_prop graph findNode prim Constant output toIValue assertEqual const test_constant_prop_nested torch jit script constant_prop b = + bool c = b + c = b - c out_ref = constant_prop torch tensor run_pass constant_propagation constant_prop graph out_test = constant_prop torch tensor assertEqual out_ref out_test if_node = constant_prop graph findNode prim If block if_node blocks node block nodes assertTrue node kind == prim Constant test_constant_prop_print torch jit script constant_prop input_tensor = print b = + b + input_tensor run_pass constant_propagation constant_prop graph graph = constant_prop graph print_node = graph findNode prim Print assertTrue print_node input toIValue == test_constant_prop_rand torch jit script constant_prop = torch randn b = + b run_pass constant_propagation constant_prop graph assertTrue aten randn str constant_prop graph test_constant_prop_none torch jit script typed_none type - Optional int None torch jit script constant_prop = typed_none b = typed_none None b None = = run_pass constant_propagation constant_prop graph FileCheck check prim Constant run constant_prop graph test_constant_prop_if_inline torch jit script constant_prop cond = True = cond = = testing error thrownn run_pass constant_propagation constant_prop graph test_constant_prop_exception checking y = does error constant propagation bad_index x type bool y = x = y = y checkScript bad_index False test_constant_prop_aliasing_type torch jit script foo len len torch tensor FileCheck check_dag aten tensor check_dag aten len run foo graph torch jit script fn == FileCheck check_not prim If run fn graph test_unchecked_cast test cond type bool = torch tensor cond b = None b = b None b = int checkScript test True checkScript test False test_constant_prop_if_constant torch jit script constant_prop b c = c = c = bool - c c bool b - c == - c c = c + == c = c + c = c + - c c c = c + == inlined c = c + dynamic c = c + set + c + c + c graph = constant_prop graph run_pass constant_propagation graph ifs = graph findAllNodes prim If recurse=False snd_if_inlined = len ifs == assertTrue snd_if_inlined first_if = ifs assertTrue first_if outputsSize == second_if = first_if findNode prim If recurse=False assertTrue second_if outputsSize == assertTrue second_if findNode prim If None test_constant_prop_loop_constant torch jit script constant_prop cond iter type bool int - int b = while True print stays _ range print stays _ range iter print stays while cond print stays while False print removed _ range print removed _ range - print removed b run_pass constant_propagation constant_prop graph graph = canonical constant_prop graph assertTrue graph count removed == assertTrue graph count stays == constant gets pooled assertTrue graph count prim Print == test_constant_prop_remove_output torch jit script constant_prop iter type int - None = b = c = i range iter == = i == b = c = print b c graph = constant_prop graph run_pass constant_propagation graph assertTrue graph findNode prim Loop outputsSize == TODO gmagogsfm Refactor test reduce complexity test_constant_insertion funcs_template = dedent func constant_constructor constants primitives int double bool str lists primitives tuples check_constant constant_constructor scope = funcs_str = funcs_template format constant_constructor=constant_constructor execWrapper funcs_str globals scope cu = torch jit CompilationUnit funcs_str f_script = cu func run_pass constant_propagation f_script graph FileCheck check_count prim Constant exactly=True run f_script graph assertEqual scope func f_script imported = getExportImportCopy f_script assertEqual imported f_script constants = None - True False b torch tensor True False torch tensor torch tensor True None None type Tensor str int float bool constants append torch jit annotate List + type + constant constants check_constant constant key_type str int float value_type Tensor bool str int float check_constant torch jit annotate Dict + key_type + + value_type + check_constant torch jit annotate Dict + key_type + Optional + value_type + i range len constants j range i + len constants tup_constant = constants i + + constants j check_constant tup_constant dict_constants = i range len constants check_constant constructs second dict another Tensor which fails comparison isinstance eval constants i str int float continue j range len constants dict_constant = + constants i + + constants j + check_constant dict_constant dict_constants append dict_constant constants = constants + dict_constants testing node hashing funcs_template = dedent func print constant_constructor single_elem_tuples = + x + x constants input_arg = join single_elem_tuples scope = funcs_str = funcs_template format constant_constructor=input_arg execWrapper funcs_str globals scope cu = torch jit CompilationUnit funcs_str f_script = cu func run_pass constant_propagation f_script graph prim None adds one constant assertEqual len constants + str f_script graph count prim Constant run_pass cse f_script graph node hashing correctly working no CSE occurs assertEqual len constants + str f_script graph count prim Constant funcs_template = dedent func = constant_constructor print b = constant_constructor print b generate dicts built-in types excluding torch Tensor xprod = itertools product constants constants test equal tuples dicts correctly work node hashing tup + x + x constants funcs_str = funcs_template format constant_constructor=tup scope = execWrapper funcs_str globals scope cu = torch jit CompilationUnit funcs_str f_script = cu func run_pass constant_propagation_immutable_types f_script graph num_constants = str f_script graph count prim Constant run_pass cse f_script graph FileCheck check_count prim Constant num_constants exactly=True run f_script graph unittest skipIf RUN_CUDA requires CUDA test_cuda_export_restore Sub torch jit ScriptModule __init__ - None super __init__ weight = nn Parameter torch randn torch jit script_method forward thing weight + thing M torch jit ScriptModule __init__ - None super __init__ mod = Sub torch jit script_method forward v mod v m = M m cuda m = getExportImportCopy m m cuda input = torch rand cuda assertEqual m input m input slowTest test_export_batchnorm mode eval train clazz torch nn BatchNorm d torch nn BatchNorm d affine=False torch nn BatchNorm d torch nn BatchNorm d affine=False getattr clazz mode input = torch randn isinstance clazz torch nn BatchNorm d \ torch randn traced = torch jit trace clazz input imported = getExportImportCopy traced x = torch randn isinstance clazz torch nn BatchNorm d \ torch randn assertEqual traced x imported x test_export_rnn clazz nn RNN nn GRU RNNTest torch nn Module __init__ - None super __init__ rnn = clazz forward x lengths h packed = torch nn utils rnn pack_padded_sequence x lengths out h = rnn packed h padded_outs _ = torch nn utils rnn pad_packed_sequence out padded_outs test = RNNTest traced = torch jit trace test torch randn torch LongTensor torch randn imported = getExportImportCopy traced NB We make sure pass batch different max sequence length ensure argument stashing pad_packed works properly x lengths h = torch randn torch LongTensor torch randn assertEqual traced x lengths h imported x lengths h test_export_lstm LSTMTest torch nn Module __init__ - None super __init__ rnn = nn LSTM forward x lengths hiddens h c = hiddens packed = torch nn utils rnn pack_padded_sequence x lengths out h c = rnn packed h c padded_outs _ = torch nn utils rnn pad_packed_sequence out padded_outs test = LSTMTest traced = torch jit trace test torch randn torch LongTensor torch randn torch randn imported = getExportImportCopy traced x lengths h c = \ torch randn torch LongTensor torch randn torch randn assertEqual traced x lengths h c imported x lengths h c test_unique_state_dict MyModule torch nn Module __init__ - None super __init__ shared_param = torch nn Parameter torch ones register_parameter w shared_param register_parameter w shared_param forward input input + w + w model = MyModule unittest TestCase assertEqual len torch jit _unique_state_dict model keep_vars=False unittest TestCase assertEqual len torch jit _unique_state_dict model keep_vars=True test_export_dropout test = torch nn Dropout test eval traced = torch jit trace test torch rand check_trace=False imported = getExportImportCopy traced x = torch randn assertEqual traced x imported x test_pretty_printer torch jit script if_test b FIXME use instead c = c = bool b c = b c = c torch jit script if_one b c = b bool b c = c torch jit script while_test i while bool i = i += torch jit script while_if_test b c = while bool = + b = b + bool b c = c = + + c torch jit script loop_use_test y x = y + z = x + while bool y y += z = x x z torch jit ignore python_fn x x + torch jit script python_op_name_test y python_fn y torch jit script empty_int_list_test y x = torch jit annotate List int x torch jit script empty_float_list_test y torch jit script print_weird_test y print hi\ assertExpected if_test code if_test assertExpected if_one code if_one assertExpected while_test code while_test assertExpected while_if_test code while_if_test assertExpected loop_use_test code loop_use_test assertExpected python_op_name_test code python_op_name_test assertExpected empty_int_list_test code empty_int_list_test assertExpected empty_float_list_test code empty_float_list_test assertExpected print_weird_test code print_weird_test test_cu_escaped_number cu = torch jit CompilationUnit foo print hi\ assertExpected cu foo code test_import_method torch _jit_internal _disable_emit_hooks Foo torch jit ScriptModule torch jit script_method forward x y x + y foo = Foo buffer = io BytesIO torch jit save foo buffer buffer seek foo_loaded = torch jit load buffer assertExpected foo_loaded forward code unittest skip temporarily disable test fwd compatibility test_non_ascii_string Foo torch jit ScriptModule __init__ - None super __init__ = Over \u e \u e torch jit script_method forward x y + hi\xA foo = Foo buffer = io BytesIO torch jit save foo buffer buffer seek foo_loaded = torch jit load buffer assertExpected foo_loaded forward code test_function_default_values outer_var = torch tensor outer_var = torch tensor = torch tensor b = torch tensor torch jit script simple_fn x a=a b=b c=outer_var + outer_var x + + b + c assertEqual simple_fn torch ones torch ones + + + + assertEqual simple_fn torch ones torch tensor torch tensor torch tensor torch ones + + + outer_c = torch tensor outer_flag = torch tensor False torch jit script bool_fn x a=outer_c flag=outer_flag bool flag result = x result = x + result assertEqual bool_fn torch ones torch ones + assertEqual bool_fn torch ones torch tensor torch tensor True torch ones torch jit script none_fn x=None type Optional int - Optional int x assertEqual none_fn None assertEqual none_fn torch jit script hints x a= b= type Tensor float int - Tensor x + + b assertEqual hints torch ones torch ones + + assertRaisesRegex RuntimeError Expected default value torch jit script hints_bad_types x a= b= noqa T type Tensor float int - Tensor x + + b assertRaisesRegex RuntimeError Expected default value torch jit script bad_no_optional x=None type Dict str int - Dict str int x test_module_default_values four = torch tensor Test torch jit ScriptModule torch jit script_method forward input other=four input + other t = Test assertEqual t torch ones torch ones + test_mutable_default_values assertRaisesRegex Exception Mutable default parameters torch jit script foo x= type Tuple int List Tensor x Test torch nn Module forward input= noqa B input assertRaisesRegex Exception Mutable default parameters torch jit script Test skipIfTorchDynamo TorchDynamo fails unknown reason test_warnings warnings fn x bool x warnings warn x less than x M torch nn Module forward x bool x warnings warn x less than x scripted_mod = torch jit script M scripted_fn = torch jit script fn warnings catch_warnings record=True warns fn torch ones warnings catch_warnings record=True script_warns scripted_fn torch ones warnings catch_warnings record=True script_mod_warns scripted_mod torch ones assertEqual str warns str script_warns assertEqual len script_mod_warns assertEqual str warns message str script_mod_warns message test_no_erroneous_warnings warnings fn x bool x warnings warn This should NOT printed x += x warnings catch_warnings record=True warns fn_script = torch jit script fn fn_script torch tensor warns = str w message w warns assertEqual len warns unittest skipIf True TODO re-enable https github com pytorch pytorch pull test_torch_load_error J torch jit ScriptModule torch jit script_method forward input input + j = J TemporaryFileName fname j save fname assertRaisesRegex RuntimeError zip torch load fname test_torch_load_zipfile_check torch jit script fn x x + TemporaryFileName fname fn save fname open fname rb f assertTrue torch serialization _is_zipfile f test_python_bindings lstm_cell = torch jit script LSTMCellS lstm x hx cx w_ih w_hh b_ih b_hh i range x size hx cx = lstm_cell x i hx cx w_ih w_hh b_ih b_hh hx slstm = torch jit script lstm inputs = get_lstm_inputs cpu training=True seq_length= slstm inputs sum backward global fw_graph fw_graph = slstm graph_for inputs nodes = list fw_graph nodes tested_blocks = False node nodes output node outputs assertTrue hasattr output type assertTrue output type None input node inputs assertTrue hasattr input type assertTrue input type None block node blocks tested_blocks = True assertTrue hasattr block inputs assertTrue hasattr block outputs output block outputs assertTrue hasattr output type assertTrue output type None input block inputs assertTrue hasattr input type assertTrue input type None assertTrue hasattr block returnNode assertTrue type block returnNode torch _C Node assertTrue hasattr block paramNode assertTrue type block paramNode torch _C Node assertTrue tested_blocks test_export_opnames Foo torch jit ScriptModule one x y type Tensor Tensor - Tensor x + y two x type Tensor - Tensor x torch jit script_method forward x type Tensor - Tensor one two x x Bar torch jit ScriptModule __init__ - None super __init__ sub = Foo torch jit script_method forward x type Tensor - Tensor sub forward x bar = Bar ops = torch jit export_opnames bar expected = aten add Tensor aten mul Scalar assertTrue set expected issubset set ops test_pytorch_jit_env_off subprocess env = os environ copy env PYTORCH_JIT = try subprocess check_output sys executable -c torch env=env except subprocess CalledProcessError e raise RuntimeError Could torch PYTORCH_JIT= e test_print_op_module Issue python python go through different paths python returns module torch ops built-in python uses __file__ module torch ops scratch ailzhang pytorch torch _ops py s = str torch ops assertRegex s r ops test_print_classes_module s = str torch classes assertRegex s r classes test_print_torch_ops_modules s = str torch _ops ops quantized assertRegex s r torch ops s = str torch _ops ops atan assertRegex s r torch ops test_hide_source_ranges_context_manager torch jit script foo x torch add x x graph = foo graph source_range_regex = \\ py assertRegex graph __repr__ source_range_regex torch jit _hide_source_ranges assertNotRegex graph __repr__ source_range_regex assertRegex graph str print_source_ranges=True source_range_regex assertRegex graph __repr__ source_range_regex TestFrontend JitTestCase test_instancing_error torch jit ignore MyScriptClass unscriptable + TestModule torch nn Module forward x MyScriptClass assertRaises torch jit frontend FrontendError cm torch jit script TestModule checker = FileCheck checker check Cannot instantiate checker check forward checker run str cm exception test_dictionary_as_example_inputs_for_jit_trace TestModule_v torch nn Module forward key =None key =None key =None key =None key =None key =None key + key + key TestModule_v torch nn Module forward x y x + y test_func x y x + y model_ = TestModule_v model_ = TestModule_v value = torch ones value = torch ones value = torch ones example_input_dict = key value key value key value example_input_dict_func = x value y value traced_model_ = torch jit trace model_ example_kwarg_inputs=example_input_dict strict=False traced_model_ _m = torch jit trace_module model_ forward example_input_dict example_inputs_is_kwarg=True strict=False traced_model_ = torch jit trace model_ example_kwarg_inputs= x torch rand y torch rand traced_func = torch jit trace test_func example_kwarg_inputs=example_input_dict_func strict=False res_ = traced_model_ example_input_dict res_ _m = traced_model_ _m example_input_dict assertEqual res_ torch ones assertEqual res_ _m torch ones res_func = traced_func example_input_dict_func assertEqual res_func torch ones assertRaisesRegex RuntimeError r forward\ \ missing value argument x res_ = traced_model_ z torch rand y torch rand noqa PIE assertRaisesRegex RuntimeError r forward\ \ missing value argument y res_ = traced_model_ x torch rand z torch rand noqa PIE TestScript JitTestCase Tests calling torch jit script repeated function allowed test_repeated_script_on_function torch jit script torch jit script fn x x torch jit script torch jit script fn test_pretty_print_function torch jit script foo x torch nn functional interpolate x FileCheck check interpolate run foo code test_inlined_graph Check ` inlined_graph ` property correctly returns inlined graph both through function calls method calls torch jit script foo x torch add x x MyNestedMod torch nn Module forward x torch sub x x MyMod torch nn Module __init__ - None super __init__ nested = MyNestedMod forward x x = nested x sub x = foo x add torch mul x x m = torch jit script MyMod FileCheck check aten sub \ check aten add \ check aten mul \ run m inlined_graph test_static_method_on_module Check ` staticmethod ` annotation function module works MyCell torch nn Module staticmethod do_it x h new_h = torch tanh x + h new_h new_h forward x h do_it x h my_cell = torch jit script MyCell x = torch rand h = torch rand jitted_cell = my_cell x h non_jitted_cell = MyCell do_it x h assertEqual jitted_cell non_jitted_cell test_code_with_constants Check ` code_with_constants ` property correctly returns graph CONSTANTS CONSTANTS cN format used output ` code ` property torch jit script foo x=torch ones x Moddy torch nn Module forward x foo m = torch jit script Moddy src CONSTANTS = m code_with_constants assertEqual CONSTANTS c torch ones assertEqual src m code test_code_with_constants_restore Check ` code_with_constants ` property correctly works restoration after save + load torch jit script foo x=torch ones x Moddy torch nn Module forward x foo m = torch jit script Moddy src CONSTANTS = m code_with_constants eic = getExportImportCopy m src_eic CONSTANTS_eic = eic code_with_constants assertEqual src src_eic assertEqual CONSTANTS c CONSTANTS_eic c test_oneline_func fn x x noqa E checkScript fn torch ones test_request_bailout enable_profiling_mode_for_profiling_tests fct_loop x _ range x = torch cat x x x x = torch ones dtype=torch float expected = fct_loop x jitted = torch jit script fct_loop profile jitted x optimize jitted x dstate = jitted get_debug_state eplan = get_execution_plan dstate num_bailouts = eplan code num_bailouts i range num_bailouts eplan code request_bailout i assertEqual jitted x expected unittest skip bailouts being deprecated test_dominated_bailout enable_profiling_mode_for_profiling_tests functional dominated guard torch jit script foo x dim = x dim dim == y = int x y = x size dim - y x = torch zeros assertEqual foo x assertEqual foo x g = torch jit last_executed_optimized_graph g_s = str g g_s = g_s g_s find FileCheck check_count prim BailOut exactly=True run g_s dominated guard non-functional value torch jit script foo x dim = x dim x add_ dim == x size dim - x = torch zeros assertEqual foo x assertEqual foo x g = torch jit last_executed_optimized_graph FileCheck check prim BailOut check aten add_ check_next prim BailOut check run g torch enable_grad torch jit ignore disable_grad torch set_grad_enabled False torch jit ignore enable_grad torch set_grad_enabled True torch jit script foo x x = x + dim = x dim disable_grad dim == y = int x y = x size dim - enable_grad y x = torch zeros requires_grad=True assertEqual foo x assertEqual foo x g = torch jit last_executed_optimized_graph there should still Bailout after disable_grad call FileCheck check disable_grad check BailOut check BailoutTemplate run g skipIfTorchDynamo Torchdynamo cannot correctly handle profiler profile calls unittest skipIf GRAPH_EXECUTOR = ProfilingMode PROFILING skip profiling isn t enabled test_profiling_merge torch jit script test_not_const x x size == enable_profiling_mode_for_profiling_tests num_profiled_runs test_not_const torch rand test_not_const torch rand graph_str = torch jit last_executed_optimized_graph FileCheck check profiled_type=Float strides= requires_grad= device=cpu run graph_str FileCheck check_not profiled_type=Float strides= requires_grad= device=cpu run graph_str test_nested_bailouts torch jit script fct_loop x _ range x = torch cat x x x x = torch ones dtype=torch float out = fct_loop x jit_trace = torch jit trace fct_loop x out_trace = jit_trace x test_no_self_arg_ignore_function MyModule nn Module torch jit ignore noqa B call_np noqa B type - int np random choice p= forward call_np assertRaisesRegex Exception does have argument torch jit script MyModule test_loop_liveness enable_profiling_mode_for_profiling_tests torch jit script f i type int - Tensor l = n l append torch zeros n i l f f test_bailout_loop_carried_deps_name_clash enable_profiling_mode_for_profiling_tests NUM_ITERATIONS = torch jit script fct_loop z size type int int - Tuple Tensor List int counters = torch jit annotate List int j = y = torch ones i range size counters append i + j y = torch cat y torch ones z j = j + y counters inputs = expected = x x range NUM_ITERATIONS inp inputs results = fct_loop inp NUM_ITERATIONS assertEqual results expected test_bailout_loop_counter_transition enable_profiling_mode_for_profiling_tests NUM_ITERATIONS = torch jit script fct_loop z size type int int - Tuple Tensor List int counters = torch jit annotate List int y = torch ones i range size counters append i y = torch cat y torch ones z y counters inputs = expected = list range NUM_ITERATIONS inp inputs results = fct_loop inp NUM_ITERATIONS assertEqual results expected test_ignored_method_binding Bar torch nn Module __init__ - None super __init__ x int = torch jit export setx x int x = x torch jit export getx x torch jit ignore ignored_getx x b = Bar b setx sb = torch jit script b assertEqual sb getx assertEqual sb ignored_getx sb setx assertEqual sb getx assertEqual sb ignored_getx test_set_attribute_through_optional A torch nn Module __annotations__ = x Optional torch Tensor __init__ - None super __init__ x = None torch jit ignore foo x None x = torch tensor x forward x = foo x + m = torch jit script A assertEqual m x None m torch rand assertEqual m x torch tensor test_mutate_constant M torch jit ScriptModule __constants__ = foo __init__ foo super __init__ foo = foo m = M m has constant attribute we can t assign assertRaises RuntimeError m foo = test_class_attribute M torch jit ScriptModule FOO = __init__ - None super __init__ foo = FOO m = M assertEqual m foo M FOO test_class_attribute_in_script M torch jit ScriptModule FOO = torch jit script_method forward FOO assertRaises RuntimeError M test_not_initialized_err M torch jit ScriptModule __init__ - None foo = torch rand assertRaises RuntimeError M test_attribute_in_init M torch jit ScriptModule __init__ - None super __init__ foo = torch jit Attribute float we should able use foo float here assert foo M test_scriptable_fn_as_attr M torch nn Module __init__ fn super __init__ fn = fn forward x fn x m = M torch sigmoid inp = torch rand checkModule m inp test_sequence_parsing tests = x x True x x expected x x True bar x x True bar Argument x provided b x x \n pass List iterables b = x x \n + b True exp result tests cu = torch jit CompilationUnit full = f bar x y x + y foo x exp isinstance result str assertRaisesRegex RuntimeError result cu define full cu define full test_namedtuple_python global MyTuple MyMod see local resolution python MyTuple = namedtuple MyTuple torch jit unused fn type - MyTuple MyTuple Only check compilation torch jit script fn type - MyTuple fn FileCheck check NamedTuple run fn graph MyMod torch nn Module torch jit unused fn type - MyTuple MyTuple forward x == MyTuple torch rand fn shouldn t throw type error torch jit script MyMod test_unused_decorator MyMod torch nn Module torch jit unused torch no_grad fn x type Tensor - int next x invalid should ignored forward x fn x torch jit script MyMod _inline_everything test_lazy_script untraceable x x ndim print hello print goodbye x + Non-working example fn x untraceable x capture_stdout traced_bad = torch jit trace fn torch ones FileCheck check_not goodbye check_not hello run traced_bad graph Working example untraceable = torch jit script_if_tracing untraceable fn x untraceable x capture_stdout traced = torch jit trace fn torch ones FileCheck check goodbye run traced graph foo x int x + torch jit script_if_tracing fee x int = foo + x test directly compiling function fee_compiled = torch jit script fee assertEqual fee_compiled fee test compiling within another function torch jit script hum fee x= assertEqual hum test_big_int_literals ok signed bit max = toobig = waytoobig = checkScript ok assertRaisesRegex RuntimeError out range torch jit script toobig assertRaisesRegex RuntimeError out range torch jit script waytoobig test_hex_literals test xaaaaaa test xaaaaaa test - xaaaaaa checkScript test checkScript test checkScript test ok = x FFFFFFFFFFFFFFF toobig = xFFFFFFFFFFFFFFFF waytoobig = xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF checkScript ok assertRaisesRegex RuntimeError out range torch jit script toobig assertRaisesRegex RuntimeError out range torch jit script waytoobig test_big_float_literals ok Python interprets inf = E check fn assertTrue fn == ok checkScript doesn t work since assertEqual doesn t consider ` inf ` == ` inf ` check torch jit script ok cu = torch jit CompilationUnit cu define dedent inspect getsource ok check cu ok _test_device_type dest fn x type Device - Tuple str Optional int x type x index device = torch ones dest device checkScript fn device test_device_type _test_device_type cpu unittest skipIf RUN_CUDA Requires CUDA test_device_type_cuda _test_device_type cuda test_string_device_implicit_conversion torch jit script fn x torch device x assertEqual fn cpu torch device cpu assertRaisesRegex RuntimeError Expected one fn invalid_device test_eval_python _test m assertTrue m torch ones assertTrue m training assertTrue m _c getattr training m eval assertFalse m training assertFalse m _c getattr training assertFalse m torch ones buffer = io BytesIO torch jit save m buffer buffer seek loaded = torch jit load buffer assertFalse loaded training assertFalse loaded _c getattr training M nn Module forward x training OldM torch jit ScriptModule torch jit script_method forward x training _test torch jit script M _test OldM test_inherit_method A torch jit ScriptModule torch jit script_method forward x x + bar x B A torch jit script_method bar x x x assertRaisesRegex RuntimeError attribute A cannot use because bar defined v = torch rand b = B assertEqual b v v + v v C torch jit ScriptModule torch jit script_method bar x x D C B __init__ - None super __init__ assertEqual D v v + v test_tensor_subclasses check_subclass x tensor template = dedent func input - torch zeros input shape dtype=input dtype _check_code template format x x func tensor check_subclass torch LongTensor torch LongTensor check_subclass torch DoubleTensor torch DoubleTensor check_subclass torch IntTensor torch IntTensor check_subclass torch BoolTensor torch BoolTensor False True True False check_subclass_warn input torch LongTensor - torch LongTensor torch zeros input shape dtype=input dtype warnings catch_warnings record=True warns scripted = torch jit script check_subclass_warn FileCheck check TorchScript will treat type annotations Tensor run str warns test_first_class_module Foo torch jit ScriptModule __init__ - None super __init__ foo = nn Parameter torch rand torch jit script_method forward input foo = input foo foo = Foo input = torch rand foo forward input assertEqual input foo foo _tmp_donotuse_dont_inline_everything test_first_class_calls torch jit script Foo __init__ x bar = x stuff x bar + x torch jit script foo x x x + Foo x stuff x torch jit script bar x foo x foo x x = torch rand assertEqual bar x x x + x x x + x test_static_methods M nn Module staticmethod my_method x x + forward x x + M my_method x N nn Module staticmethod my_method x x forward x x - M my_method x + N my_method x checkModule M torch ones checkModule N torch ones test_invalid_prefix_annotation assertRaisesRegex RuntimeError annotation prefix line capture_stdout captured torch jit script invalid_prefix_annotation #type Int - Int noqa E + assertRaisesRegex RuntimeError annotation prefix line capture_stdout captured torch jit script invalid_prefix_annotation #type Int - Int noqa E + assertRaisesRegex RuntimeError annotation prefix line capture_stdout captured torch jit script invalid_prefix_annotation type Int - Int + test_builtin_function_attributes Add nn Module __init__ - None super __init__ add = torch add forward input add input input checkModule Add torch randn test_pybind_type_comparisons torch jit script f None node = list f graph nodes t = node outputsAt type assertIsNotNone t unittest skipIf IS_WINDOWS TODO need fix test case test_unmatched_type_annotation message = re escape Number type annotations did match number function parameters message = invalid \\ a\\ \n\\s ~+\\ \\s+ --- HERE\n\\s+# type \\ Int Int\\ - Int\n\\s+return \\+ message = invalid \\ a\\ \n\\s ~+\\ \\s+ --- HERE\n\\s+# type \\ Int Int\\ - Int\n\\s+return \\+ assertRaisesRegex RuntimeError message torch jit script invalid type Int Int - Int + assertRaisesRegex RuntimeError message torch jit script invalid type Int Int - Int + assertRaisesRegex RuntimeError message invalid type Int Int - Int + torch jit script invalid assertRaisesRegex RuntimeError message invalid type Int Int - Int + torch jit script invalid test_calls_in_type_annotations assertRaisesRegex RuntimeError Type annotation should contain calls spooky type print Hello - Tensor noqa F + print torch __file__ torch jit annotations get_signature spooky None True test_is_optional ann = Union List int List float torch _jit_internal is_optional ann test_interpreter_fuzz builtins This test generates random tree-like programs fuzz test interpreter does have bug its stack manipulation code An assert code ensures individual operators reordered templates = torch rand + - torch tanh VAR gen_code src_lines = f exprs = n_variables = get_expr idx elem = exprs idx exprs idx = exprs - exprs pop elem select_expr_or_var idx = random randrange len exprs + n_variables idx len exprs get_expr idx f v idx - len exprs _ range n = None while n None n len exprs + n_variables template = random choice templates n = template count VAR template src_lines append f v n_variables = select_expr_or_var n_variables += exprs append template format select_expr_or_var _ range n src_lines append \n format join f v i i range n_variables \n join src_lines _ range g = torch torch code = gen_code builtins exec code g None cu = torch jit CompilationUnit code freeze_rng_state o = g f freeze_rng_state o = cu f assertEqual o o skipIfTorchDynamo TorchDynamo fails unknown reason test_cpp_module_iterator = nn Module name = p = nn Parameter torch rand foo = nn Module foo name = foo foo b = nn Buffer torch rand foo bar = nn Module foo bar name = bar foo bar an_int = another = nn Module another name = another sa = torch jit script result = torch _C _jit_debug_module_iterators sa _c replace e e p P e foo b B isinstance e torch _C ScriptModule e getattr name e v result values i range len v isinstance v i tuple n v = v i v i = n replace v v i = replace v i module type creation deterministic so we have sort result v sort expected = buffers buffers_r B children another foo modules another bar foo named_attributes _is_full_backward_hook None another another foo foo name p P training True named_attributes_r _is_full_backward_hook None another another another _is_full_backward_hook None another name another another training True foo foo foo _is_full_backward_hook None foo b B foo bar bar foo bar _is_full_backward_hook None foo bar an_int foo bar name bar foo bar training True foo name foo foo training True name p P training True named_buffers named_buffers_r foo b B named_children another another foo foo named_modules another another foo foo foo bar bar named_parameters p P named_parameters_r p P parameters P parameters_r P assertEqual expected result test_parameter_order m = nn Module i name enumerate string ascii_letters setattr m name nn Parameter torch tensor float i ms = torch jit script m print torch cat list m parameters print torch cat list ms parameters assertEqual list m parameters list ms parameters test_python_op_builtins torch jit unused fn x type List int - int sum x torch jit script script_fn x type List int - int fn x test_submodule_twice torch jit script foo x x x What torch jit ScriptModule __init__ x super __init__ foo = x = What foo c = What foo test_training_param What torch jit ScriptModule torch jit script_method forward x type int - int training r = x r = x + check double use training training r = r + r w = What assertEqual w w train False assertEqual w assertFalse training w state_dict test_class_as_attribute torch jit script Foo __init__ - None x = FooBar torch nn Module __init__ - None super __init__ f = Foo forward x x + f x scripted = torch jit script FooBar eic = getExportImportCopy scripted x = torch rand assertEqual scripted x eic x test_module_str Foo torch nn Module forward x torch relu x f = torch jit script Foo str_f = str f _c assertTrue str_f startswith ScriptObject assertTrue __torch__ str_f assertTrue Foo str_f test_jitter_bug torch jit script fn input kernel_size type Tensor List int - Tensor kernel_size _stride = _stride = kernel_size print _stride kernel_size input torch jit script fn input type Tensor - Tensor fn input test_parser_kwargonly cu = torch jit CompilationUnit foo x y - Tuple Tensor Tensor x x bar x foo x y=x assertTrue str cu foo schema assertRaisesRegex RuntimeError provided torch jit CompilationUnit foo x y - Tuple Tensor Tensor x x bar x foo x x test_annoying_doubles mod = types ModuleType temp mod inf = float inf mod ninf = float -inf mod nan = float nan torch _jit_internal _disable_emit_hooks Foo torch jit ScriptModule torch jit script_method forward math pi mod inf mod ninf e- mod nan foo = Foo buffer = io BytesIO torch jit save foo buffer buffer seek foo_loaded = torch jit load buffer r = foo r = foo_loaded use precise assert we checking floating point details assertTrue r - == r - assertTrue math isnan r - math isnan r - test_type_annotate foo torch jit annotate torch Tensor checkScript foo torch rand bar = torch jit annotate List int _ range append checkScript bar baz torch jit annotate float checkScript baz torch rand test annotate none types annotate_none torch jit annotate Optional torch Tensor None checkScript annotate_none test_robust_op_resolution neg = torch add misleading name make sure we resolve function stuff x neg x x = torch rand checkScript stuff test_nested_aug_assign torch jit script SomeClass __init__ - None num = __iadd__ x type int num += x __eq__ other type SomeClass - bool num == other num torch jit script SomeOutOfPlaceClass __init__ - None num = __add__ x type int num = x __eq__ other type SomeClass - bool num == other num Child nn Module __init__ - None super __init__ x = o = SomeClass oop = SomeOutOfPlaceClass list = A nn Module __init__ - None super __init__ child = Child forward child x += child o += child oop += some_list = child list += some_list child list = child x child o child list child oop = A sa = torch jit script A eager_result = script_result = sa assertEqual eager_result script_result assertEqual child x sa child x assertEqual child o sa child o assertEqual child list sa child list torch jit script SomeNonAddableClass __init__ - None num = __eq__ other type SomeClass - bool num == other num assertRaisesRegex RuntimeError A nn Module __init__ - None super __init__ x = SomeNonAddableClass forward x += SomeNonAddableClass x assertRaisesRegex RuntimeError Cannot emit inplace op torch jit script A test_var_aug_assign torch jit script SomeNonAddableClass __init__ - None num = __eq__ other type SomeNonAddableClass - bool num == other num assertRaisesRegex RuntimeError Cannot emit inplace op torch jit script fn = SomeNonAddableClass += SomeNonAddableClass torch jit script SomeClass __init__ - None num = __iadd__ x type int num += x __eq__ other type SomeClass - bool num == other num torch jit script SomeOutOfPlaceClass __init__ - None num = __add__ x type int num = x __eq__ other type SomeClass - bool num == other num fn = SomeClass a_copy = += assert a_copy b = SomeOutOfPlaceClass b_copy = b b += assert b b_copy c = c_copy = c c = assert c c_copy c += d = torch ones d_copy = d d += torch ones assert d d_copy b c d checkScript fn test_nested_list_construct foo + checkScript foo test_file_line_error foobar xyz torch blargh xyz _ lineno = inspect getsourcelines foobar assertRaisesRegex RuntimeError f test_jit py line lineno + scripted = torch jit script foobar test_file_line_error_class_defn FooBar baz xyz torch blargh xyz _ lineno = inspect getsourcelines FooBar assertRaisesRegex RuntimeError f test_jit py line lineno + torch jit script FooBar test_file_line_graph foobar xyz torch neg xyz scripted = torch jit script foobar _ lineno = inspect getsourcelines foobar fc = FileCheck check f test_jit py lineno + fc run scripted graph fc run str scripted graph test_file_line_save_load Scripted torch jit ScriptModule torch jit script_method forward xyz torch neg xyz scripted = Scripted NB using getExportImportCopy because takes different code path calls CompilationUnit _import rather than going through full save load pathway buffer = scripted save_to_buffer bytesio = io BytesIO buffer scripted = torch jit load bytesio _ lineno = inspect getsourcelines Scripted fc = FileCheck check f lineno + fc run scripted graph fc run str scripted graph test_file_line_string scripted = torch jit CompilationUnit foo xyz torch neg xyz fc = FileCheck check string fc run scripted foo graph fc run str scripted foo graph skipIfCrossRef test_file_line_trace foobar xyz torch neg xyz scripted = torch jit trace foobar torch rand _ lineno = inspect getsourcelines foobar fc = FileCheck check f test_jit py lineno + fc run scripted graph fc run str scripted graph test_serialized_source_ranges FooTest torch jit ScriptModule torch jit script_method forward x w torch mm x w t ft = FooTest loaded = getExportImportCopy ft _ lineno = inspect getsourcelines FooTest assertRaisesRegex RuntimeError f test_jit py line lineno + loaded torch rand torch rand test_serialized_source_ranges_graph FooTest torch jit ScriptModule torch jit script_method forward x w torch mm x w t ft = FooTest loaded = getExportImportCopy ft _ lineno = inspect getsourcelines FooTest fc = FileCheck check f test_jit py lineno + fc run loaded graph test_serialized_source_ranges FooTest torch jit ScriptModule torch jit script_method forward raise RuntimeError foo _ lineno = inspect getsourcelines FooTest assertRaisesRegex torch jit Error f test_jit py line lineno + ft = FooTest loaded = getExportImportCopy ft loaded test_serialized_source_ranges_dont_jitter FooTest torch jit ScriptModule torch jit script_method forward lim first = second = i = somenum = dontmutateme = third = while bool i lim third = first + second first = second second = third j = while j somenum = somenum j = j + i = i + j i = i + dontmutateme st = second + third fs = first + second third st fs ft = FooTest debug_records_from_mod mod buffer = io BytesIO torch jit save ft buffer buffer seek archive = zipfile ZipFile buffer files = filter lambda x x startswith archive code archive namelist debug_files = list filter lambda f f endswith debug_pkl files assertEqual len debug_files debug_file = archive open debug_files pickle load debug_file buffer records buffer = debug_records_from_mod ft buffer seek loaded = torch jit load buffer records buffer = debug_records_from_mod loaded buffer seek loaded = torch jit load buffer records _ = debug_records_from_mod loaded assertEqual records records assertEqual records records test_serialized_source_ranges_no_dups FooTest torch jit ScriptModule torch jit script_method forward lim first = second = i = somenum = dontmutateme = third = while bool i lim third = first + second first = second second = third j = while j somenum = somenum j = j + i = i + j i = i + dontmutateme st = second + third fs = first + second third st fs ft = FooTest debug_records_from_mod mod buffer = io BytesIO torch jit save ft buffer buffer seek archive = zipfile ZipFile buffer files = list filter lambda x x startswith archive code archive namelist debug_files = filter lambda f f endswith debug_pkl files debug_files = archive open f f debug_files debug_files = pickle load f f debug_files debug_files = f f debug_files list debug_files debug_files = debug_records_from_mod ft debug_file debug_files i range len debug_file - offset source_range_tag source_range = debug_file i offset source_range_tag source_range = debug_file i + assertNotEqual source_range source_range test_circular_dependency https github com pytorch pytorch issues A torch jit ScriptModule torch jit script_method forward x x B torch jit ScriptModule __init__ - None super __init__ foo = torch nn ModuleList A torch jit script_method forward x f foo x = f x x C torch jit ScriptModule __init__ - None super __init__ foo = torch nn Sequential B torch jit script_method forward x f foo x = f x x getExportImportCopy C test_serialize_long_lines OrderModuleLong torch nn Module forward long_arg_name List torch Tensor long_arg_name long_arg_name argmax src = str torch jit script OrderModuleLong code make long_arg_name does get reordered after argmax FileCheck check long_arg_name check argmax run src test_tensor_shape x = torch empty f x x shape checkScript f x test_block_input_grad_in_loop x = torch randn requires_grad=False y = torch randn requires_grad=True grad_in_loop x y _ range x = y x x scripted = torch jit script grad_in_loop outer = scripted graph_for x y loop = outer findNode prim Loop loop_block = next loop blocks param_node = loop_block paramNode x_value = list param_node outputs assertTrue x_value requires_grad test_tensor_grad x = torch randn requires_grad=True y = torch randn requires_grad=False f_requires_grad x x requires_grad checkScript f_requires_grad x checkScript f_requires_grad y f_grad x x grad x sum backward checkScript f_grad x checkScript f_grad y unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY shape analysis only enabled Legacy test_prim_grad_undefined x = torch ones f_grad x x grad scripted = checkScript f_grad x g = scripted graph_for x prim_grad_node = g findNode prim grad assertTrue next prim_grad_node outputs type undefined None test_tensor_data x = torch randn requires_grad=True y = torch randn f_data x x data scripted_f_data = torch jit script f_data scripted_x = scripted_f_data x assertEqual scripted_x f_data x assertEqual scripted_x requires_grad False scripted_y = scripted_f_data y assertEqual scripted_y f_data y assertEqual scripted_x requires_grad False test_tensor_dtype x_byte = torch empty dtype=torch uint x_long = torch empty dtype=torch long x_float = torch empty dtype=torch float torch jit script byte x x dtype == torch uint torch jit script long x x dtype == torch long torch jit script float x x dtype == torch float assertTrue byte x_byte assertFalse byte x_long assertFalse byte x_float assertFalse long x_byte assertTrue long x_long assertFalse long x_float assertFalse float x_byte assertFalse float x_long assertTrue float x_float unittest skipIf RUN_CUDA device tests require CUDA test_tensor_device cpu = torch empty device= cpu gpu = torch empty device= cuda torch jit script same_device x y x device == y device assertTrue same_device cpu cpu assertTrue same_device gpu gpu assertFalse same_device cpu gpu unittest skipIf RUN_CUDA device tests require CUDA test_tensor_to_device to_device x x device= cuda device=torch device cpu checkScript to_device torch ones test_tensor_to_cpu to_cpu x x cpu x = torch ones script_fn = torch jit script to_cpu assertEqual to_cpu x device script_fn x device checkScript to_cpu x unittest skipIf RUN_CUDA device tests require CUDA test_tensor_to_cuda to_cuda x x cuda x = torch ones script_fn = torch jit script to_cuda assertEqual to_cuda x device script_fn x device checkScript to_cuda x test_generic_list_errors assertRaisesRegex RuntimeError previously matched type torch jit script foo x x + test_script_cu cu = torch jit CompilationUnit foo b = b = Variable torch rand assertEqual cu foo because compilation unit ingests python strings use escape sequence escape backslash \\n = \n test_string_cu cu = torch jit CompilationUnit foo print a\\n\tb\\n a\ FileCheck check aa check a\\n\\tb\\n run str cu foo graph test_function_compilation_caching fun + fun_compiled = torch jit script fun python wrapper around script function different pointer underlying script function graph same assertIs fun_compiled graph torch jit script fun graph fun + num_ref_counts = sys getrefcount fun caching doesn t get tripped up same qualname fun_compiled_ = torch jit script fun assertIsNot fun_compiled fun_compiled_ assertEqual fun_compiled_ caching doesn t increase refcounts function holds weak reference assertTrue sys getrefcount fun num_ref_counts test_string_ops foo = + b + ab == b ab = b ab == ab ab = ab checkScript foo test_string_sorted foo strs List str sorted strs FileCheck \ check graph \ check_next str = aten sorted \ check_next \ run str torch jit script foo graph inputs = str str str checkScript foo inputs test_string_sort foo strs List str strs sort strs inputs = str str str checkScript foo inputs test_tuple_sorted foo tups List Tuple int int sorted tups inputs = checkScript foo inputs test_tuple_sort foo tups List Tuple int int tups sort tups inputs = checkScript foo inputs test_tuple_sort_reverse foo tups List Tuple int int tups sort reverse=True tups inputs = checkScript foo inputs test_tuple_unsortable_element_type torch jit script foo tups = tups sort tups assertRaisesRegexWithHighlight RuntimeError sortable tups sort foo test_tuple_unsortable_diff_type torch jit script foo inputs List Any inputs sort inputs inputs = foo bar assertRaisesRegexWithHighlight RuntimeError Only values same type can compared inputs sort foo inputs test_tuple_nested_sort foo inputs List Tuple int Tuple int str inputs sort inputs inputs = foo bar bar checkScript foo inputs test_tuple_unsortable_nested_diff_type torch jit script foo inputs List Any inputs sort inputs inputs = foo bar assertRaisesRegexWithHighlight RuntimeError Only values same type can compared inputs sort foo inputs test_string_new_line assertRaisesRegex RuntimeError expected valid token torch jit CompilationUnit test_while print test_string_single_escape assertRaisesRegex RuntimeError expected valid token torch jit CompilationUnit test_while print \\ test_script_annotation torch jit script foo + + s = Variable torch rand assertEqual s + s + s foo s test_torch_pow func b pow b func b c d pow pow c + b d func int b float type int float - float pow b func type - float pow - func x y pow x item y item func int b int type int int - float pow b = torch rand b = torch rand c = torch rand d = torch rand checkScript func b checkScript func b c d checkScript func - checkScript func checkScript func inputs = torch tensor torch tensor - torch tensor torch tensor x inputs y inputs x continue checkScript func x y unittest skipIf RUN_CUDA device tests require CUDA test_pow_scalar_backward_cuda see scalar exponent works cuda base enable_profiling_mode_for_profiling_tests dtype torch float torch double torch jit script func b type Tensor float - Tensor b = torch rand requires_grad=True device= cuda dtype=dtype func profile_and_replay=True backward torch jit script func b type float Tensor - Tensor b + = torch rand requires_grad=True device= cuda dtype=dtype func profile_and_replay=True backward _check_code code_str fn_name inputs scope = exec code_str globals scope cu = torch jit CompilationUnit code_str assertEqual cu func inputs scope fn_name inputs unittest skipIf RUN_CUDA no CUDA test_scriptmodule_releases_tensors_cuda enable_profiling_mode_for_profiling_tests torch jit script fn x y x sigmoid y tanh test backward=False x = torch randn dtype=torch double device= cuda requires_grad=True y = torch randn dtype=torch double device= cuda requires_grad=True out = fn x y profile_and_replay=True backward out sum backward assertLeaksNoCudaTensors test test test GRAPH_EXECUTOR = ProfilingMode SIMPLE assertLeaksNoCudaTensors test backward=True test backward=True test backward=True skipIfTorchDynamo Not TorchDynamo suitable test test_index consec size start= numel = torch tensor size prod item torch arange numel view size consec_list size list range size random_string size letters = string ascii_lowercase join random choice letters i range size check_indexing indexing tensor template = dedent func x x _check_code template format indexing func tensor check_dynamic_indexing indexing tensor value value value = torch tensor value value = torch tensor value template = dedent func x value value i = int value j = int value x _check_code template format indexing func tensor value value Torchscript assumes type Tensor default so we need explicit declaration check_indexing_list_int indexing list template = dedent func x type List int - Any x _check_code template format indexing func list check_indexing_str indexing str template = dedent func x type str - Any x _check_code template format indexing func str basic slices check_indexing consec check_indexing consec check_indexing consec check_indexing consec check_indexing - consec check_indexing consec check_indexing - consec check_indexing - - consec check_indexing consec check_indexing consec check_indexing consec multi-dim indexes check_indexing consec check_indexing consec check_indexing consec check_indexing - consec multi-dim mixed slicing indexing check_indexing consec check_indexing consec check_indexing consec check_indexing - consec check_indexing - consec check_indexing - consec check_indexing - consec check_indexing - consec zero-sized slices check_indexing consec check_indexing consec trivial expression usage check_indexing + consec check_indexing + consec None new dimensions check_indexing None consec check_indexing None consec check_indexing None None consec check_indexing None None consec check_indexing None consec check_indexing None - consec check_indexing None - - None consec check_indexing - None None consec check_indexing None - None None None consec dynamic expression usage check_dynamic_indexing i + j consec check_dynamic_indexing i j i consec positive striding check_indexing_list_int consec_list check_indexing_list_int consec_list check_indexing_list_int consec_list check_indexing_list_int consec_list check_indexing_list_int - consec_list check_indexing_list_int consec_list check_indexing_list_int - consec_list check_indexing_list_int - - consec_list check_indexing_list_int consec_list check_indexing_list_int consec_list check_indexing_list_int consec_list check_indexing_list_int consec_list check_indexing_list_int consec_list check_indexing_list_int consec_list negative striding check_indexing_list_int - consec_list check_indexing_list_int - consec_list check_indexing_list_int - consec_list check_indexing_list_int - consec_list check_indexing_list_int - consec_list check_indexing_list_int - - consec_list check_indexing_list_int - consec_list check_indexing_list_int - - consec_list only step specified check_indexing_list_int - consec_list check_indexing_list_int - consec_list check_indexing_list_int - consec_list check_indexing_list_int consec_list check_indexing_list_int consec_list check_indexing_list_int - consec_list check_indexing_list_int consec_list check_indexing_list_int - consec_list check_indexing_list_int consec_list check_indexing_list_int - consec_list assertRaisesRegex RuntimeError out bounds check_indexing_list_int - consec_list assertRaisesRegex RuntimeError should have non-zero step check_indexing_list_int consec_list striding strings check_indexing_str random_string check_indexing_str random_string check_indexing_str random_string check_indexing_str random_string check_indexing_str - random_string check_indexing_str random_string check_indexing_str - random_string check_indexing_str - - random_string check_indexing_str random_string check_indexing_str random_string check_indexing_str random_string check_indexing_str random_string check_indexing_str random_string check_indexing_str random_string check_indexing_str - random_string check_indexing_str - random_string check_indexing_str - random_string check_indexing_str - random_string check_indexing_str - random_string check_indexing_str - - random_string check_indexing_str - random_string check_indexing_str - - random_string check_indexing_str - random_string check_indexing_str - random_string check_indexing_str - random_string check_indexing_str random_string check_indexing_str random_string check_indexing_str - random_string check_indexing_str random_string check_indexing_str - random_string check_indexing_str random_string check_indexing_str - random_string assertRaisesRegex RuntimeError out bounds check_indexing_str - random_string assertRaisesRegex RuntimeError should have non-zero step check_indexing_str random_string test_module_copy_with_attributes Vocabulary torch jit ScriptModule __init__ vocab_list super __init__ _vocab = torch jit Attribute vocab_list List str some_idx = torch jit Attribute int idx = torch jit Attribute word i i word enumerate vocab_list Dict str int torch jit script_method lookup_indices_ d values type List str - List int result = torch jit annotate List int Direct list iteration supported i range len values value = values i result append idx get value some_idx result torch jit script_method forward values type List List str - List List int result = torch jit annotate List List int Direct list iteration supported i range len values result append lookup_indices_ d values i result v = Vocabulary list uabcdefg v __copy__ test_tuple_to_opt_list torch jit script foo x type Optional List int - int torch jit script tuple_call foo test_keyword torch jit script func x torch sum x dim= x = torch rand dtype=torch float requires_grad=True y = func x y = torch sum x dim= assertEqual y y test_constant_pooling_none torch jit script typed_nones a=None b=None c=None type Optional int Optional bool Optional Tensor - Tuple Optional int Optional bool Optional Tensor b c torch jit script test type bool - None print typed_nones print typed_nones graph_str = str test graph assertTrue graph_str count NoneType = prim Constant == test_constant_pooling_same_identity foo = torch tensor b = index = len - c = b index d = b index c d foo_script = torch jit script foo run_pass constant_propagation foo_script graph run_pass constant_pooling foo_script graph even though c d escape scope we still able pool them into one constant because they same object FileCheck check_count prim Constant exactly=True run foo_script graph assertEqual foo foo_script test_constant_pooling_introduce_aliasing torch jit script foo = torch tensor b = torch tensor b run_pass constant_propagation foo graph run_pass constant_pooling foo graph dont pool constants bc would introduce observable alias relationship changing b = foo assertIsNot b test_literal func b c = b d e = c d + e func b c = b d e = c f g = e d + f + g func b type float float - float c = x = True while x x = False c = b d e = c f g = e d + f + g = torch rand requires_grad=True b = torch rand requires_grad=True checkScript func b optimize=True checkScript func b optimize=True checkScript func item b item optimize=True test_expand torch jit script func x y x + y x = torch rand dtype=torch float requires_grad=True y = torch rand dtype=torch float requires_grad=True out = func x y assertEqual func x y x + y grad = torch randn dtype=torch float out backward grad assertEqual x grad grad assertEqual y grad grad sum dim= test_sum torch jit script func x x sum dim= torch jit script func x x sum dim= test shape analysis written correctly sum OptionalIntArrayRef dim argument run_pass constant_propagation func graph run_pass constant_propagation func graph g = _propagate_shapes func graph torch zeros False g = _propagate_shapes func graph torch zeros False test_cat enable_profiling_mode_for_profiling_tests torch jit script func x torch cat x x dim= x = torch rand dtype=torch float requires_grad=True assertEqual func x profile_and_replay=True torch cat x x dim= torch jit script func x y torch cat x x y disable_autodiff_subgraph_inlining sizes x = torch rand sizes requires_grad_ y = torch tensor output = func x y profile_and_replay=True output_ref = torch cat x x y assertEqual output output_ref GRAPH_EXECUTOR = ProfilingMode SIMPLE assertAutodiffNode func graph_for x y True aten cat grad = torch autograd grad output sum x grad_ref = torch autograd grad output_ref sum x assertEqual grad grad_ref test_cat_lifts torch jit script foo x torch cat x x dim= torch jit script foo x torch cat dim= torch jit script foo x torch cat x dim= g foo graph foo graph foo graph FileCheck check int = check ListConstruct check aten cat run str g test_stack enable_profiling_mode_for_profiling_tests torch jit script func x torch stack x x dim= x = torch rand assertEqual func x profile_and_replay=True torch stack x x dim= torch jit script func x y torch stack x y dim= disable_autodiff_subgraph_inlining x = torch randn requires_grad_ y = torch randn requires_grad_ output = func x y profile_and_replay=True output_ref = torch stack x y assertEqual output output_ref GRAPH_EXECUTOR = ProfilingMode SIMPLE assertAutodiffNode func graph_for x y True aten stack grads = torch autograd grad output sum x y grads_ref = torch autograd grad output_ref sum x y assertEqual grads grads_ref unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY Profiling executor will using different heuristics constructing differentiable graphs test_unbind enable_profiling_mode_for_profiling_tests torch jit script func x y type Tensor int - List Tensor torch unbind x y disable_autodiff_subgraph_inlining x = torch rand requires_grad_ y = outputs = func x y profile_and_replay=True outputs_ref = torch unbind x dim=y assertEqual outputs outputs_ref assertAutodiffNode func graph_for x y True grad = torch autograd grad _sum_of_list outputs x grad_ref = torch autograd grad _sum_of_list outputs_ref x assertEqual grad grad_ref unittest skipIf GRAPH_EXECUTOR == ProfilingMode PROFILING Profiling executor fails recognize tensors list require gradients test_meshgrid enable_profiling_mode_for_profiling_tests torch jit script func type List Tensor - List Tensor torch meshgrid disable_autodiff_subgraph_inlining = torch tensor requires_grad_ b = torch tensor requires_grad_ inputs = b outputs_ref = torch meshgrid inputs outputs = func inputs profile_and_replay=True assertEqual outputs outputs_ref GRAPH_EXECUTOR = ProfilingMode SIMPLE assertAutodiffNode func graph_for inputs True grads = torch autograd grad _sum_of_list outputs inputs grads_ref = torch autograd grad _sum_of_list outputs_ref inputs assertEqual grads grads_ref test_tensor_len func x len x checkScript func torch ones test_func_call add b + b mul x x func alpha beta x y add mul alpha x mul beta y alpha = torch rand dtype=torch float requires_grad=True beta = torch rand dtype=torch float requires_grad=True x = torch rand dtype=torch float requires_grad=True y = torch rand dtype=torch float requires_grad=True NOTE cannot optimize yet because broadcasts inserted before fuser runs checkScript func alpha beta x y optimize=False unittest skip bailouts being deprecated test_profiling_graph_executor torch jit script def_in_one_branch x z type Tensor bool - float y = x z False y = x + y sum = torch rand enable_profiling_mode_for_profiling_tests check prim profile inserted profiled_graph_str = str def_in_one_branch graph_for True FileCheck check_count prim profile run profiled_graph_str call optimized given shape def_in_one_branch False change shape so we go down bailout path = torch ones check prim BailOuts inserted bailout_graph_str = str def_in_one_branch graph_for True FileCheck check_count prim BailOut run bailout_graph_str triggers all bailouts assertEqual def_in_one_branch False triggers bailouts assertEqual def_in_one_branch True unittest skip bailouts being deprecated test_maxpool_guard_elimination torch jit script my_maxpool x F max_pool d x kernel_size= + torch ones = torch rand enable_profiling_mode_for_profiling_tests my_maxpool bailout_graph_str = str my_maxpool graph_for FileCheck check_count prim BailOut run bailout_graph_str unittest skip bailouts being deprecated test_slice_guard_elimination torch jit script my_slice x x + x = torch rand enable_profiling_mode_for_profiling_tests my_slice bailout_graph_str = str my_slice graph_for FileCheck check_count prim BailOut run bailout_graph_str unittest skip bailouts being deprecated test_unsqueeze_guard_elimination torch jit script my_unsqueeze x torch unsqueeze x + torch unsqueeze x = torch rand enable_profiling_mode_for_profiling_tests my_unsqueeze bailout_graph_str = str my_unsqueeze graph_for FileCheck check_count prim BailOut run bailout_graph_str test_resize_input_ops resize_ resize_as resize input tensor because our shape analysis flow invariant we set any Tensor can alias resized Tensor base Tensor Type without size information testing value which input graph gets handled out_op_graph_input torch jit script test x y z torch mul x y out=z z graph = _propagate_shapes test graph torch zeros torch zeros torch zeros False assertTrue next graph outputs type == TensorType get out_op_graph_input test_resize torch jit script test x after_resize_alias = torch zeros _ range b = x + f = before_resize_alias = b sub_ i range f append b resize_ f after_resize_alias = b add_ after_resize_alias run_pass constant_propagation test graph g = _propagate_shapes test graph torch zeros False resize_node = g findNode aten resize_ first input output b resize_ b assertTrue next resize_node inputs type == TensorType get assertTrue next resize_node outputs type == TensorType get correctly propagates b alias set before_resize = g findNode aten sub_ assertTrue next before_resize outputs type == TensorType get after_resize = g findNode aten add_ assertTrue next after_resize outputs type == TensorType get test_resize test_resize_as torch jit script test x b = torch zeros b resize_as_ x b g = test graph run_pass constant_propagation g g = _propagate_shapes test graph torch zeros False x doesn t alias resized op so shouldn t set base Tensor type assertTrue next g inputs type = TensorType get resized assertTrue next g outputs type == TensorType get test_resize_as test_uninitialized graph_str = graph int = prim Uninitialized int = prim Constant value= int = aten add g = parse_ir graph_str m = createFunctionFromGraph g getExportImportCopy m assertRaisesRegex RuntimeError expected int m unittest skipIf GRAPH_EXECUTOR == ProfilingMode SIMPLE Simple Executor doesn t use requires_grad information unittest skipIf GRAPH_EXECUTOR == ProfilingMode PROFILING Peeling now disabled test_requires_grad_loop torch jit script test x y z type Tensor Tensor int - Tensor _ range z x = y x x requires grad y does testing requires grad analysis correctly exits its input loop x requiring grad its output loop requiring grad output node conservatively setting grad true inps = torch tensor requires_grad=True torch tensor test inps profile_and_replay=True graph = test graph_for inps loop = graph findNode prim Loop loop_body = next loop blocks loop_inputs = list loop_body inputs loop_outputs = list loop_body outputs GRAPH_EXECUTOR == ProfilingMode PROFILING TODO simplify test s very sensitive optimized graph will have loops original loop peeled peeled loop also gets unrolled index_of_x_in_peeled_unrolled_loop = - assertTrue loop_inputs index_of_x_in_peeled_unrolled_loop requires_grad bailouts_in_outer_block = graph findAllNodes prim BailOut False last_bailout_index_on_loops_output = - assertFalse bailouts_in_outer_block last_bailout_index_on_loops_output output requires_grad assertTrue loop_inputs requires_grad assertTrue loop output requires_grad assertFalse loop_outputs requires_grad test_view_shape_prop cu = torch jit CompilationUnit test_view_shape_prop view size= - inputs = torch zeros outputs = torch zeros real_outs = cu test_view_shape_prop inputs assertEqual real_outs outputs skipIfTorchDynamo TorchDynamo fails unknown reason test_view_listconstruct_shape_prop fn x B = x size C = x size T = x size x view T B C x = torch randn requires_grad=True fn = torch jit script fn graph = _propagate_shapes fn graph x False assertTrue next graph outputs type scalarType == Float test_shape_prop_promotion torch jit script fn x y x + y x y = torch rand dtype=torch float torch rand dtype=torch double graph = _propagate_shapes fn graph x y False FileCheck check Double device=cpu = aten add run graph test_shape_prop_promote_scalar_arg torch jit script fn x math pi + x x = torch zeros dtype=torch long graph = _propagate_shapes fn graph x False default = torch get_default_dtype default == torch float FileCheck check Float requires_grad= device=cpu = aten add run graph FileCheck check Double requires_grad= device=cpu = aten add run graph test_integral_shape_inference cu = torch jit CompilationUnit test_integral_shape_inference inputs = torch ones dtype=torch long outputs = torch ones dtype=torch long assertEqual cu test_integral_shape_inference inputs outputs unittest skipIf RUN_CUDA This tests CPU fuser unittest skipIf IS_SANDCASTLE NYI fuser support Sandcastle enable_cpu_fuser test_batchnorm_fuser_cpu code = graph Tensor Tensor Float Tensor Tensor int = prim Constant value= float = prim Constant value= e- Tensor = aten sqrt Tensor = aten add Tensor = aten reciprocal norm_invstd Tensor = aten mul Tensor = aten sub Tensor = aten mul norm_invstd Tensor = aten mul Tensor = aten add Float = aten relu graph = parse_ir code inputs = torch rand dtype=torch float code = torch _C _jit_fuser_get_fused_kernel_code graph inputs FileCheck check sqrtf run code slowTest unittest skipIf RUN_CUDA This tests CPU fuser unittest skipIf IS_SANDCASTLE NYI fuser support Sandcastle enable_cpu_fuser test_fuser_double_float_codegen fns = log log log p log lgamma exp expm erf erfc cos acos cosh sin asin sinh tan atan tanh sqrt ceil floor round trunc frac lookup_c_equivalent_fn aten_fn aten_fn test_dispatch op expects dtype binary=False dtype == torch double dtype_str = Double dtype == torch float dtype_str = Float raise RuntimeError Unknown dtype binary code = f graph Tensor Tensor dtype_str = aten op dtype_str = aten relu code = f graph Tensor dtype_str = aten op dtype_str = aten relu graph = parse_ir code inputs = binary torch rand dtype=dtype code = torch _C _jit_fuser_get_fused_kernel_code graph inputs FileCheck check expects run code fn fns test_dispatch fn lookup_c_equivalent_fn fn + torch double test_dispatch fn lookup_c_equivalent_fn fn + f torch float min max previously tested now replaced ternary expressions instead fmin fmax binary_fns = pow fn binary_fns test_dispatch fn lookup_c_equivalent_fn fn + torch double binary=True test_dispatch fn lookup_c_equivalent_fn fn + f torch float binary=True unittest skipIf RUN_CUDA This tests CPU fuser unittest skipIf IS_SANDCASTLE NYI fuser support Sandcastle enable_cpu_fuser test_fuser_double_literal_precision code = graph Float int = prim Constant value= float = prim Constant value= Float = aten add Float = aten relu graph = parse_ir code code = torch _C _jit_fuser_get_fused_kernel_code graph torch rand FileCheck check run code test_fuser_multiple_blocks cu = torch jit CompilationUnit test_fuser_multiple_blocks theother meme i = while i = torch cat meme dim= = torch cat meme dim= theother = torch cat theother meme dim= i = i + theother inputs = torch ones inputs += torch ones outputs = torch ones assertEqual cu test_fuser_multiple_blocks inputs outputs unittest skip RuntimeError VariableType ID implemented test_cast script = to_int x int x x = Variable torch FloatTensor requires_grad=True out = Variable torch IntTensor requires_grad=True checkScript script x optimize=True outputs= out func= to_int test_str_cast torch jit script to_str x type int - str str x x assertEqual to_str test_int_cast torch jit script to_int x type str - int int x assertEqual to_int assertEqual - to_int - assertEqual to_int assertEqual - to_int - assertRaisesRegex RuntimeError invalid literal int to_int x assertRaisesRegex RuntimeError invalid literal int to_int b test_python_frontend fn x y z q = None q = x + y - z sigmoid print q w = -z x y z m = x z y while x y z q = x assert == hello x ast = torch jit frontend get_jit_def fn fn __name__ assertExpected str ast test_python_frontend_source_range fn raise Exception hello noqa TRY ast = torch jit frontend get_jit_def fn fn __name__ FileCheck check SourceRange \ check fn \ check ~~~~~~~~~ \ check raise Exception hello \ check ~~~~~~~~~~~~~~~~~ --- HERE \ run str ast range test_python_frontend_py fn raise Exception hello noqa TRY ast = torch jit frontend get_jit_def fn fn __name__ assertExpected str ast _make_scalar_vars arr dtype torch tensor val dtype=dtype val arr test_string_print func print b c d inputs = _make_scalar_vars torch int checkScript func inputs capture_output=True test_while func b max while bool max = + b = b + c = + b c inputs = _make_scalar_vars torch int checkScript func inputs optimize=True test_fibb func lim first = second = i = somenum = dontmutateme = third = while bool i lim third = first + second first = second second = third j = while j somenum = somenum j = j + i = i + j i = i + dontmutateme st = second + third fs = first + second third st fs inputs = _make_scalar_vars torch int checkScript func inputs optimize=True test_fibb_totally_better fib x type int - int prev = v = _ range x save = v v = v + prev prev = save v checkScript fib test_if func b type int int - int d = bool = + d b = + d d = c = + b c inputs = _make_scalar_vars - torch int checkScript func inputs optimize=True test_if_for_in_range func b type int int - int d = _ range bool = + d b = + d d = c = + b d inputs = _make_scalar_vars - torch int checkScript func inputs optimize=True test_if_noelse func b bool = + b c = + b c inputs = _make_scalar_vars - torch int checkScript func inputs optimize=True test_if_is_none_dispatch torch jit script test_lhs_none_rhs_none LHS RHS both alwaysNone dispatch always_none_branch only emit one prim Constant None None None None assertTrue str test_lhs_none_rhs_none graph count int = prim Constant == torch jit script test_lhs_opt_rhs_none lhs=None type Optional Tensor - int LHS maybeNone emit normal stmt contains constants lhs None lhs None assertTrue str test_lhs_opt_rhs_none graph count int = prim Constant == torch jit script test_lhs_none_rhs_opt rhs=None type Optional Tensor - int RHS maybeNone emit normal stmt contains constants None rhs None rhs assertTrue str test_lhs_opt_rhs_none graph count int = prim Constant == torch jit script test_lhs_never_rhs_none lhs LHS neverNone RHS alwaysNone dispatch never_none_branch only emit one prim Constant lhs None lhs None assertTrue str test_lhs_never_rhs_none graph count int = prim Constant == torch jit script test_lhs_none_rhs_never rhs LHS alwaysNone RHS neverNone dispatch never_none_branch only emit one prim Constant None rhs None rhs assertTrue str test_lhs_none_rhs_never graph count int = prim Constant == torch jit script test_bool_arith_and lhs lhs None lhs None assertEqual test_bool_arith_and torch zeros assertTrue str test_bool_arith_and graph count == torch jit script test_bool_arith_or lhs lhs None lhs None assertEqual test_bool_arith_or torch zeros assertTrue str test_bool_arith_or graph count == torch jit script test_bool_arith_not lhs lhs None assertEqual test_bool_arith_not torch zeros assertTrue str test_bool_arith_not graph count == test_conditional_casting test_bool_cast_tensor x x make_one_dim True False inp_val - - - inp_val = inp_val make_one_dim inp_val checkScript test_bool_cast_tensor torch tensor inp_val checkScriptRaisesRegex test_bool_cast_tensor torch tensor Exception Boolean value Tensor more than one value test_not_cast x x checkScript test_not_cast torch tensor checkScript test_not_cast torch tensor assertRaisesRegex RuntimeError r Could cast value type Tuple\ Tensor Tensor\ noqa W torch jit script test_mult x y x y test_cast_int x type int - int x checkScript test_cast_int checkScript test_cast_int checkScript test_cast_int - test_cast_float x type float - int x checkScript test_cast_float checkScript test_cast_float checkScript test_cast_float - assertRaisesRegex RuntimeError r Could cast value type Tuple\ int int\ bool noqa W torch jit script test_bad_conditional x noqa F test_while_nonexistent_value assertRaisesRegex RuntimeError undefined value x torch jit CompilationUnit test_while b while bool = + x b = b + + b test_while_nonexistent_cond_value assertRaisesRegex RuntimeError undefined value x torch jit CompilationUnit test_while b while x = + b = b + + b torch jit script test_ternary x type Optional int - int x = x x None x torch jit script test_not_none x type Optional int - None x None print x + torch jit script test_and x y type Optional int Optional int - None x None y None print x + y torch jit script test_not x y type Optional int Optional int - None x None y None pass print x + y torch jit script test_bool_expression x type Optional int - None x None x print x + torch jit script test_nested_bool_expression x y type Optional int Optional int - int x None x y None x = x + y x = x + torch jit script test_or x y type Optional int Optional int - None y None x None pass print x + y backwards compatibility torch jit script test_manual_unwrap_opt x type Optional int - int x None x = x = torch jit _unwrap_optional x x noqa T assertRaisesRegex RuntimeError Arguments call valid torch jit script or_error x y type Optional int Optional int - None x None y None print x + y noqa T assertRaisesRegex RuntimeError Arguments call valid torch jit script and_error x y type Optional int Optional int - None x None y None pass print x + y noqa T assertRaisesRegex RuntimeError Arguments call valid torch jit script named_var x type Optional int - None x_none = x None x_none print x + noqa T assertRaisesRegex RuntimeError Arguments call valid torch jit script named_var_and x y type Optional int Optional int - None x_none = x None y None x_none print x + y noqa T test_assertion_optional_refinement torch jit script test x y type Optional int Optional int - int assert x None y None x + y assertEqual test assertRaisesRegex Exception test None unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY current version Profiler doesn t profile specialize Optionals test_optional_tensor torch jit script fn x y type Optional Tensor int - int x None y res = fn None assertEqual res g = torch jit last_executed_optimized_graph first_input = next g inputs check input disconnected assertEqual first_input type kind OptionalType assertEqual first_input uses t = torch ones res = fn t assertEqual res g = torch jit last_executed_optimized_graph assertEqual next g inputs type kind TensorType torch jit script fn x y b type Optional Tensor Tensor bool - Tensor b res = y res = torch jit _unwrap_optional x res t = torch zeros res = fn t t True assertEqual res t assertRaisesRegex RuntimeError Unwrapping null optional res = fn None t False res = fn None t True g = torch jit last_executed_optimized_graph assertIn next g outputs type str Tensor Tensor requires_grad= unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY current version Profiler doesn t profile specialize Optionals test_optional_list torch jit script fn x y type Optional List int int - int x None y res = d x res += d res res = fn None assertEqual res g = torch jit last_executed_optimized_graph first_input = next g inputs check input disconnected assertEqual first_input type kind OptionalType assertEqual first_input uses l = res = fn l assertEqual res g = torch jit last_executed_optimized_graph assertEqual next g inputs type kind ListType torch jit script fn x y b type Optional List int List int bool - List int b l = torch jit _unwrap_optional x l = y l l = res = fn l l True assertEqual res l assertRaisesRegex RuntimeError Unwrapping null optional res = fn None l True res = fn None l False g = torch jit last_executed_optimized_graph assertEqual next g outputs type str int test_alias_covariant_type_containers torch jit script foo x type bool x = None = torch jit script foo x li type bool Tuple Optional List Tensor x li = None li test_while_write_outer_then_read func b while bool = + b = + + b inputs = _make_scalar_vars torch int checkScript func inputs optimize=True skipIfTorchDynamo TorchDynamo fails unknown reason test_while_nest_if func b type int int - int c = while = + b = b + b c = -a c = -b c + inputs = _make_scalar_vars - torch int checkScript func inputs optimize=True test_divmod func_int b type int int - Tuple int int divmod b func_float b type float float - Tuple float float divmod b func_int_float b type int float - Tuple float float divmod b func_float_int b type float int - Tuple float float divmod b divmod_test_iterator func num den i num j den checkScript func i j frames_up= num_int = - den_int = - num_float = - den_float = - divmod_test_iterator func_int num_int den_int divmod_test_iterator func_float num_float den_float divmod_test_iterator func_int_float num_int den_float divmod_test_iterator func_float_int num_float den_int assertRaisesRegex RuntimeError ZeroDivisionError integer division modulo zero cu = torch jit CompilationUnit dedent inspect getsource func_int cu func_int assertRaisesRegex RuntimeError ZeroDivisionError float divmod cu = torch jit CompilationUnit dedent inspect getsource func_float cu func_float assertRaisesRegex RuntimeError ZeroDivisionError float divmod cu = torch jit CompilationUnit dedent inspect getsource func_int_float cu func_int_float assertRaisesRegex RuntimeError ZeroDivisionError float divmod cu = torch jit CompilationUnit dedent inspect getsource func_float_int cu func_float_int skipIfTorchDynamo Not TorchDynamo suitable test test_math_ops checkMathWrap func_name num_args= is_float=True args is_float checkMath func_name num_args True args checkMath func_name num_args False args checkMath func_name num_args is_float args inf = float inf NaN = float nan mx_int = - mn_int = - float_vals = inf NaN - - - -inf + i i range + - i i range int_vals = list range - + mx_int + mx_int mn_int - mn_int checkMath func_name num_args is_float=True ret_type= float debug=False vals=None args_type=None funcs_template = dedent func b type args_type - ret_type math func args num_args == args = num_args == args = b raise RuntimeError Test doesn t support more than arguments args_type None args_type = float float is_float int int funcs_str = funcs_template format func=func_name args=args args_type=args_type ret_type=ret_type scope = execWrapper funcs_str globals scope cu = torch jit CompilationUnit funcs_str f_script = cu func f = scope func vals None vals = float_vals is_float int_vals vals = i j i vals j vals b vals res_python = None res_script = None try res_python = f b except Exception e res_python = e try res_script = f_script b except Exception e res_script = e debug print b print out res_python res_script We can t use assertEqual because couple differences nan == nan should true When python functions throw exception we usually want silently ignore them ie We want ` nan ` math sqrt - res_python = res_script isinstance res_python Exception continue type res_python type res_script isinstance res_python tuple math isnan res_python == math isnan res_script continue isinstance res_python float math isnan res_python math isnan res_script continue msg = f Failed func_name inputs b Python res_python Script res_script math pow behavior has changed see https docs python org library math html#math pow sys version_info = func_name == pow == b == -math inf assertTrue res_python == math inf type res_script RuntimeError assertEqual res_python res_script msg=msg atol= e- max abs res_python res_script rtol= unary_float_ops = log log p log exp sqrt gamma lgamma erf erfc expm fabs acos asin atan cos sin tan asinh atanh acosh sinh cosh tanh degrees radians binary_float_ops = atan fmod copysign op unary_float_ops checkMathWrap op op binary_float_ops checkMathWrap op checkMath modf ret_type= Tuple float float checkMath frexp ret_type= Tuple float int checkMath isnan ret_type= bool checkMath isinf ret_type= bool checkMath ldexp is_float=False ret_type= float args_type= float int vals= i j i float_vals j range - checkMath pow is_float=False ret_type= float checkMath pow is_float=True ret_type= float checkMathWrap floor ret_type= int checkMathWrap ceil ret_type= int checkMathWrap gcd is_float=False ret_type= int checkMath isfinite ret_type= bool checkMathWrap remainder checkMathWrap factorial is_float=False ret_type= int vals= i i range - skipIfTorchDynamo TorchDynamo fails unknown reason test_if_nest_while func b type int int - int c = b while b b = b + c = -b c inputs = _make_scalar_vars torch int checkScript func inputs test_script_optional_none none_stmt x output = None output = x output none_args x type Optional Tensor - Optional Tensor None checkScript none_stmt torch arange optimize=True checkScript none_args None optimize=True test undefined tensor None default param test_script_optional_tensor_none x=None type Optional Tensor - Tensor res = torch zeros dtype=torch int x None res = res + res = x res fn = test_script_optional_tensor_none scripted_fn = torch jit script fn assertEqual fn scripted_fn assertEqual fn torch zeros scripted_fn torch zeros test typical None default param test_script_optional_other_none x=None type Optional float - float res = x None res = res + res = x res fn = test_script_optional_other_none scripted_fn = torch jit script fn assertEqual fn scripted_fn assertEqual fn scripted_fn test_script_clamp_none test_script_clamp_max_none x torch clamp x min= max=None test_script_clamp_max x torch clamp x max= test_script_clamp_min_none x torch clamp x min=None max= test_script_clamp_min x torch clamp x min= input = torch arange checkScript test_script_clamp_max_none input optimize=True checkScript test_script_clamp_max input optimize=True checkScript test_script_clamp_min_none input optimize=True checkScript test_script_clamp_min input optimize=True test_script_bool_constant test_script_bool_constant = True checkScript test_script_bool_constant test_ternary func b c = c = + b bool b c inputs_true = _make_scalar_vars torch int inputs_false = _make_scalar_vars torch int checkScript func inputs_true optimize=True checkScript func inputs_false optimize=True test_ternary_module_type_hint M torch nn Module forward - Any out training M torch nn Module forward - Any out Any = out training out M torch nn Module forward - Optional int None training module M M M checkModule module train checkModule module eval test_ternary_static_if Test True branch when condition variable annotated Final M torch nn Module flag torch jit Final bool __init__ - None super __init__ flag = True forward - torch Tensor torch ones flag Test True branch when condition variable annotated Final M torch nn Module flag torch jit Final bool __init__ - None super __init__ flag = False forward - torch Tensor flag torch ones model = M model = M script_model_ = torch jit script model script_model_ = torch jit script model assertEqual model forward script_model_ forward assertEqual model forward script_model_ forward test_ternary_right_associative plus_ x int x + x == x + x == x + checkScript plus_ checkScript plus_ checkScript plus_ skipIfTorchDynamo TorchDynamo fails unknown reason test_print func x y q = x + y sigmoid print q w = -q w w x = torch arange requires_grad=True y = torch arange requires_grad=True checkScript func x y optimize=True capture_output=True test_format func x print I m format Hello test print format blank format print stuff before format hi print stuff after format hi x + x = torch arange requires_grad=True checkScript func x optimize=True capture_output=True test_logical_short_circuit torch jit script testNoThrows t c = False bool t True bool t noqa SIM SIM c = c FileCheck check_not prim If run testNoThrows graph assertEqual testNoThrows torch randn assertEqual testNoThrows torch randn torch jit script throwsOr t c = False bool t print c torch jit script throwsAnd t c = True bool t print c t = torch randn assertRaisesRegex RuntimeError index out range tensor size throwsOr t assertRaisesRegex RuntimeError index out range tensor size throwsAnd t test_type_cast template = dedent func v type from_type - to_type to_type v check_cast from_type to_type value raises=False code = template format from_type=from_type to_type=to_type checkScript code value check_cast int float check_cast int bool check_cast int bool check_cast float int check_cast float bool check_cast float bool check_cast bool int True check_cast bool float True test_multiple_assignment outer_func x x x + torch jit script func x y z = outer_func x y + z x = torch arange assertEqual func x x + x + test_literals func view size= = torch randn checkScript func optimize=True test_return no_return + void_return one_return + multiple_returns = torch randn dtype=torch float checkScript no_return optimize=True checkScript void_return optimize=True checkScript one_return optimize=True checkScript multiple_returns optimize=True assertRaisesRegex RuntimeError does along all paths torch jit CompilationUnit no_return_bad_annotation type Tensor - Tensor + test_error torch jit script foo t s = Variable torch rand XXX should stay quiet stay propagation only fail interpreter assertRaisesRegex RuntimeError failed TorchScript interpreter foo s torch jit script bar c b c + b assertRaisesRegex RuntimeError failed TorchScript interpreter bar Variable torch rand requires_grad=True Variable torch rand requires_grad=True test_error_stacktrace torch jit script baz c b c + b torch jit script foo c b baz c b torch jit script bar c b foo c b assertRaises RuntimeError cm bar torch rand torch rand FileCheck check The following operation failed TorchScript interpreter \ check Traceback \ check foo check baz run str cm exception test_error_stacktrace_interface torch jit script baz c b c + b torch jit script foo c b baz c b torch jit script bar c b foo c b torch jit script Bar one x y bar x y torch jit interface IFace one x y type Tensor Tensor - Tensor pass make_global IFace torch jit script as_interface x type IFace - IFace x f = as_interface Bar assertRaises RuntimeError cm x = f one torch rand torch rand bar torch rand torch rand FileCheck check The following operation failed TorchScript interpreter \ check Traceback \ check foo check baz run str cm exception test_operator_precedence double x type int - int x complicated_arithmetic_operation TODO we need test exponent operator bitwise operator ~ once they properly supported list = result = list + double + - + + &#124; + ^ result checkScript complicated_arithmetic_operation test_in_operator_with_two_strings fn - bool abcd checkScript fn test_bitwise_ops int_test ^ &#124; checkScript int_test bool_test x y type bool bool - Tuple bool bool bool x y x ^ y x &#124; y checkScript bool_test True False checkScript bool_test True True tensor_test x y x y x ^ y x &#124; y tensor_with_int_test x y type Tensor int - Tuple Tensor Tensor x y x y x = torch tensor y = torch tensor checkScript tensor_test x y checkScript tensor_with_int_test x not_test x ~x checkScript not_test torch tensor test_all torch jit script test_all_tensor x all x assertFalse test_all_tensor torch tensor dtype=torch uint assertTrue test_all_tensor torch tensor dtype=torch uint assertTrue test_all_tensor torch tensor True True dtype=torch uint assertFalse test_all_tensor torch tensor True False dtype=torch uint torch jit script test_all_bool_list x type List bool - bool all x assertTrue test_all_bool_list True True assertTrue test_all_bool_list True assertFalse test_all_bool_list True False assertFalse test_all_bool_list True assertFalse test_all_bool_list False assertTrue test_all_bool_list torch jit script test_all_int_list x type List int - bool all x assertTrue test_all_int_list assertFalse test_all_int_list torch jit script test_all_float_list x type List float - bool all x assertTrue test_all_float_list assertFalse test_all_float_list skipIfTorchDynamo Not TorchDynamo suitable test test_number_math ops_template = dedent func scalar op scalar ops = + - = = == = funcs_template = dedent func func scalar scalar funcs = min max scalars = - - - scalar_pairs = scalar scalar scalar scalars scalar scalars run_test code scope = execWrapper code globals scope cu = torch jit CompilationUnit code assertEqual cu func scope func scalar scalar scalar_pairs op ops code = ops_template format op=op scalar =scalar scalar =scalar run_test code func funcs code = funcs_template format func=func scalar =scalar scalar =scalar run_test code test Scalar overloads scalar scalar scalar_pairs item = torch tensor + scalar + item item = torch tensor + scalar + item op ops code = ops_template format op=op scalar =item scalar =scalar run_test code code = ops_template format op=op scalar =scalar scalar =item run_test code code = ops_template format op=op scalar =item scalar =item run_test code func funcs code = funcs_template format func=func scalar =item scalar =scalar run_test code code = funcs_template format func=func scalar =scalar scalar =item run_test code code = funcs_template format func=func scalar =item scalar =item run_test code test_number_abs func x type float - float abs x func x type int - int abs x func x abs x checkScript func - checkScript func checkScript func - checkScript func checkScript func torch tensor - - - checkScript func torch tensor checkScript func torch tensor - - test_number_div assertEqual div_int_future torch jit script div_int_future checkScript div_float_future checkScript div_int_nofuture checkScript div_float_nofuture Testing bitwise shorthand aug assignment test_bool_augassign_bitwise_or func bool b bool - bool &#124; = b checkScript func True False optimize=True checkScript func True True optimize=True checkScript func False False optimize=True checkScript func False True optimize=True test_bool_augassign_bitwise_and func bool b bool - bool = b checkScript func True False optimize=True checkScript func True True optimize=True checkScript func False False optimize=True checkScript func False True optimize=True test_bool_augassign_bitwise_xor func bool b bool - bool ^= b checkScript func True False optimize=True checkScript func True True optimize=True checkScript func False False optimize=True checkScript func False True optimize=True test_number_augassign_bitwise_lshift func - int z = z = z checkScript func optimize=True test_number_augassign_bitwise_rshift func - int z = z = z checkScript func optimize=True test_number_augassign_bitwise_pow func - float z = z = z checkScript func optimize=True test_number_augassign func z = z += z checkScript func optimize=True test_nested_select_assign SubSubModule torch nn Module __init__ - None super __init__ abc = forward x abc SubModule torch nn Module __init__ - None super __init__ = nested = SubSubModule forward x TestModule torch nn Module __init__ - None super __init__ sub = SubModule hi = forward hi = sub = sub nested abc = sub + sub nested abc + hi checkModule TestModule test_number_neg int - int func - float - float func - checkScript func optimize=True checkScript func optimize=True test_compare_two_bool_inputs compare_eq bool b bool == b compare_ne bool b bool = b scripted_fn_eq = torch jit script compare_eq scripted_fn_ne = torch jit script compare_ne assertEqual scripted_fn_eq True False compare_eq True False assertEqual scripted_fn_eq False True compare_eq False True assertEqual scripted_fn_eq True True compare_eq True True assertEqual scripted_fn_eq False False compare_eq False False assertEqual scripted_fn_ne True False compare_ne True False assertEqual scripted_fn_ne False True compare_ne False True assertEqual scripted_fn_ne True True compare_ne True True assertEqual scripted_fn_ne False False compare_ne False False _test_tensor_number_math device= cpu template = dedent func t lhs op rhs test op tensor const swap_args template=template args = t const swap_args args = const t code = template format lhs=args rhs=args op=op scope = execWrapper code globals scope cu = torch jit CompilationUnit code message = f code ` args op args ` t= tensor res = cu func tensor res = scope func tensor assertEqual res res msg=message + \nres = + str res + \nres = + str res assertEqual res dtype res dtype msg=message + \nres = + str res + \nres = + str res var_int = - var_float = - ops = + - = = == = float_tensor = torch randn device=device double_tensor = torch randn dtype=torch double device=device long_tensor = torch randint - dtype=torch long device=device long_tensor long_tensor == = tensors = float_tensor double_tensor long_tensor consts = var_int + var_float op tensor const swap_args product ops tensors consts True False FIXME things like long_tensor implemented correctly Look torch _tensor py see how pytorch implements op == tensor data_ptr == long_tensor data_ptr continue operator does take const tensor op == swap_args True continue test op tensor const swap_args skipIfTorchDynamo Not TorchDynamo suitable test test_tensor_number_math _test_tensor_number_math test_torch_tensor_bad_input assertRaisesRegex RuntimeError must ints floats bools got None torch jit script test torch tensor None test assertRaisesRegex RuntimeError r Empty lists default List\ Tensor\ torch jit script tmp torch tensor tmp torch jit script foo torch tensor assertRaisesRegex RuntimeError Expected sequence length foo suppress_warnings test_torch_tensor_as_tensor_empty_list tensor_template = dedent func empty_list = torch jit annotate List int ten = torch tensor_op input ten ops = tensor as_tensor inputs = empty_list empty_list empty_list empty_list op ops inp inputs code = tensor_template format tensor_op=op input=inp scope = exec code globals scope cu = torch jit CompilationUnit code t = cu func t = scope func inp == empty_list torchscript returns int tensor python returns float tensor assertNotEqual t dtype t dtype assertEqual t t exact_dtype=False assertEqual t device t device unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY Simple Executor doesn t have any shapes propagate test_tensor_as_tensor_shape_prop tensor_template = dedent func torch tensor_op input ops = tensor as_tensor inputs = False False torch jit annotate List List int expected_shape = Long device=cpu Bool device=cpu Float device=cpu Float device=cpu Long device=cpu Bool device=cpu Long device=cpu op ops inp expect zip inputs expected_shape code = tensor_template format tensor_op=op input=inp scope = exec code globals scope cu = torch jit CompilationUnit code torch _C _jit_pass_complete_shape_analysis cu func graph False FileCheck check expect check f aten op run cu func graph torch jit script test_dtype inp_dtype torch dtype = torch tensor dtype=torch float requires_grad=True torch tensor dtype=inp_dtype GRAPH_EXECUTOR == ProfilingMode PROFILING g = test_dtype graph_for profile_and_replay=True both should have completed shapes FileCheck check Tensor = aten tensor check Float device=cpu = prim BailOut \ check Tensor = aten tensor check Half device=cpu = prim BailOut run g g = test_dtype graph_for first should have type set second should FileCheck check Float requires_grad= device=cpu = aten tensor \ check Tensor requires_grad= = aten tensor run g torch jit script test_as_tensor_tensor_input input = torch as_tensor input dtype=input dtype torch as_tensor input dtype=torch float GRAPH_EXECUTOR == ProfilingMode PROFILING g = test_as_tensor_tensor_input graph_for torch ones profile_and_replay=True FileCheck check Tensor = aten as_tensor check Float = prim BailOut \ check Tensor = aten as_tensor check Float = prim BailOut run g g = test_as_tensor_tensor_input graph_for torch ones FileCheck check Tensor = aten as_tensor check Float requires_grad= device=cpu = aten as_tensor run g unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY testing legacy behavior test_tensor_requires_grad torch jit script test b type bool - Tuple Tensor Tensor Tensor = torch tensor requires_grad=b b = torch tensor requires_grad=True c = torch tensor requires_grad=False b c g = test graph_for True out = next g outputs out_inp = list out node inputs assertTrue out_inp requires_grad assertTrue out_inp requires_grad assertFalse out_inp requires_grad test_grad_from_script test = torch tensor requires_grad=True b = b b = test b backward a_script b_script = torch jit script test b_script backward assertEqual grad a_script grad test_torch_tensor_as_tensor tensor_template = dedent func li = list_create ten = torch tensor_op li options ten lists = True False - False True False torch jit annotate List List int torch jit annotate List int - False True dtypes = dtype=torch float dtype=torch double dtype=torch half dtype=torch uint dtype=torch int dtype=torch short dtype=torch int dtype=torch long dtype=torch cfloat dtype=torch cdouble ops = tensor as_tensor devices = device= cpu RUN_CUDA devices append device= cuda option_pairs = dtype + device dtype dtypes device devices op ops li lists option option_pairs tensor empty list type float python annotated type torchscript annotate li dtype option continue Skip unsigned tensor initialization signed values torch uint option - li continue code = tensor_template format list_create=li tensor_op=op options=option scope = exec code globals scope cu = torch jit CompilationUnit code t = cu func t = scope func t dtype == torch float equality NYI half tensor assertTrue str t == str t assertEqual t t assertEqual t dtype t dtype assertEqual t device t device test_as_tensor_tensor_input input type Tensor - Tuple Tensor Tensor Tensor torch as_tensor input dtype=torch cfloat torch as_tensor input dtype=torch float \ torch as_tensor input dtype=torch int inp = torch randn dtype=torch cfloat checkScript test_as_tensor_tensor_input inp test_torch_tensor_dtype foo s float torch tensor s torch tensor s s need clear function cache so we re run shape analysis set_default_dtype torch double assertEqual torch jit script foo foo exact_dtype=True GRAPH_EXECUTOR == ProfilingMode LEGACY FileCheck check Double check_same aten tensor run torch jit last_executed_optimized_graph set_default_dtype torch float del torch jit _state _jit_caching_layer foo assertEqual torch jit script foo foo exact_dtype=True GRAPH_EXECUTOR == ProfilingMode LEGACY FileCheck check Float check_same aten tensor run torch jit last_executed_optimized_graph set_default_dtype torch half del torch jit _state _jit_caching_layer foo assertEqual torch jit script foo foo exact_dtype=True GRAPH_EXECUTOR == ProfilingMode LEGACY FileCheck check Half check_same aten tensor run torch jit last_executed_optimized_graph test_shape_analysis_grad_property torch jit script foo x torch sub x torch tanh x torch _C _jit_pass_complete_shape_analysis foo graph torch tensor False requires_grad property shouldn t accidentally set shape analysis assertTrue foo graph findNode aten sub output requiresGrad None test_empty_like_memory_format_bc f x type Tensor - Tensor torch zeros_like x memory_format=None scripted_f = torch jit script f x = torch rand assertEqual scripted_f x f x test_multiline_string_dedents foo - None multiline_string_dedent_ = This string dedent multiline_string_dedent_ = This string dedent multiline_string_dedent_ = This string dedent multiline_string_dedent_ = This string dedent scripted_foo = torch jit script foo assertEqual scripted_foo foo test_class_with_comment_at_lower_indentation Foo torch nn Module forward x x = torch neg x This comment wrong indent x torch jit script Foo adapted test test_torch test_tensor_to template = dedent func t cuda = cuda device = device non_blocking = non_blocking to_str s t to_str non_blocking=None device=None cuda=None device = device device None str t device non_blocking = non_blocking non_blocking None False cuda = cuda cuda None cuda code = template format to_str=to_str device=device non_blocking=non_blocking cuda=cuda scope = cu = torch jit CompilationUnit code cu func t profile_and_replay=True test_copy_behavior t non_blocking=False assertIs t s t t t non_blocking=non_blocking non_blocking assertIs t s t t t dtype non_blocking=non_blocking non_blocking assertIs t s t t torch empty_like t non_blocking=non_blocking non_blocking assertIsNot t s t t t non_blocking=non_blocking copy=True non_blocking assertIsNot t s t t t dtype non_blocking=non_blocking copy=True non_blocking assertIsNot t s t t torch empty_like t non_blocking=non_blocking copy=True non_blocking devices = t device t device type == cuda t device index == - devices append f cuda torch cuda current_device t device index == torch cuda current_device devices append cuda device devices assertIs t s t t device non_blocking=non_blocking non_blocking device assertIs t s t t device t dtype non_blocking=non_blocking non_blocking device assertIsNot t s t t device non_blocking=non_blocking copy=True non_blocking device assertIsNot t s t t device t dtype non_blocking=non_blocking copy=True non_blocking device t = torch tensor test_copy_behavior t assertEqual t device s t t cpu device assertEqual t device s t t cpu dtype=torch float device assertIs torch float s t t cpu dtype=torch float dtype assertEqual t device s t t torch float device assertIs torch float s t t dtype=torch float dtype assertEqual t data_ptr s t t cpu data_ptr assertEqual t data_ptr s t t dtype=t dtype device=t device copy=False data_ptr assertEqual t data_ptr s t t cpu copy=False data_ptr assertNotEqual t data_ptr s t t cpu copy=True data_ptr = torch tensor torch cuda is_available non_blocking True False cuda cuda cuda torch cuda device_count == cuda b = torch tensor device=cuda test_copy_behavior b non_blocking assertEqual b device s b t cuda non_blocking=non_blocking device cuda=cuda assertEqual device s b t cpu non_blocking=non_blocking device assertEqual b device s b t cuda non_blocking=non_blocking device cuda=cuda assertIs torch int s b t cpu dtype=torch int non_blocking=non_blocking dtype assertEqual device s b t cpu dtype=torch int non_blocking=non_blocking device assertIs torch int s b t dtype=torch int dtype assertEqual b device s b t dtype=torch int device Test AD aten Tensor int dtype bool non_blocking bool copy - Tensor t = torch tensor float requires_grad_ out_ref = t torch float out = s t t torch float assertEqual out_ref out grad_ref = torch autograd grad out_ref sum t grad = torch autograd grad out sum t assertEqual grad_ref grad Test AD aten Tensor Device device int dtype bool non_blocking bool copy - Tensor out_ref = t cpu out = s t t cpu assertEqual out_ref out grad_ref = torch autograd grad out_ref sum t grad = torch autograd grad out sum t assertEqual grad_ref grad Test AD aten Tensor Tensor other bool non_blocking bool copy - Tensor torch jit script func t t_ref t t_ref disable_autodiff_subgraph_inlining t_ref = torch tensor double out_ref = t t_ref out = func t t_ref grad_ref = torch autograd grad out_ref sum t grad = torch autograd grad out sum t assertEqual grad_ref grad unittest skipIf RUN_CUDA No CUDA test_tensor_number_math_cuda _test_tensor_number_math device= cuda test_not test operator python TODO add more tests when bool conversions ready test_not_op bool checkScript test_not_op torch tensor optimize=True test_is_isnot test operator python template = dedent func type - bool lhs op rhs test op args code = template format lhs=args rhs=args op=op scope = execWrapper code globals scope cu = torch jit CompilationUnit code assertEqual cu func scope func msg=f Failed op op lhs args rhs args ops = type_literals = True False None do literals product try any types combinations op lhs rhs product ops type_literals type_literals test op lhs rhs test_isinstance_refinement torch jit script foo type Optional int - int isinstance int + assertEqual foo assertEqual foo None torch jit script foo b type Optional int Optional int - int isinstance int isinstance b int + b assertEqual foo assertEqual foo None assertEqual foo None torch jit script any_refinement b type Any Any - int isinstance int isinstance b int + b assertEqual any_refinement assertEqual any_refinement hi torch jit script any_refinement type Any - Tensor isinstance Tensor torch tensor assertEqual any_refinement torch tensor assertEqual any_refinement torch tensor torch tensor unittest skipIf GRAPH_EXECUTOR == ProfilingMode LEGACY bug persists deprecated executor test_unspecialized_any_binding any binding will infer type infers specialized tensor type ` x ` Dict type will fail isinstance check torch jit script foo x Any assert isinstance x Dict str torch Tensor foo torch tensor assertRaises Exception foo skipIfTorchDynamo Not TorchDynamo suitable test test_isinstance test isinstance operator static type checking template = dedent func x type type_hint - bool isinstance x typ test inp typ type_hint code = template format typ=typ type_hint=type_hint scope = execWrapper code globals scope cu = torch jit CompilationUnit code assertEqual cu func inp scope func inp msg=f Failed typ typ inputs = True torch tensor type_literals = bool int float torch Tensor list tuple list tuple int float bool type_annotations = bool int float Tensor List int Tuple float List int int do zipping try different types inp typ type_hint zip inputs type_literals type_annotations test inp typ type_hint test optional isinstance check torch jit script opt_func x type Optional int - bool isinstance x int assertTrue opt_func assertFalse opt_func None test_dropout_eval ScriptedConv d torch jit ScriptModule __init__ in_channels out_channels kwargs super __init__ conv = nn Conv d in_channels out_channels bias=False kwargs bn = nn BatchNorm d out_channels eps= torch jit script_method forward x x = conv x x = bn x F relu x inplace=True ScriptMod torch jit ScriptModule __init__ - None super __init__ Conv d_ a_ x = ScriptedConv d kernel_size= stride= torch jit script_method forward x x = Conv d_ a_ x x F dropout x training=self training EagerConv d torch nn Module __init__ in_channels out_channels kwargs super __init__ conv = nn Conv d in_channels out_channels bias=False kwargs bn = nn BatchNorm d out_channels eps= forward x x = conv x x = bn x F relu x inplace=True EagerMod torch nn Module __init__ - None super __init__ Conv d_ a_ x = EagerConv d kernel_size= stride= forward x x = Conv d_ a_ x x F dropout x training=self training script_input = torch rand eager_input = script_input clone freeze_rng_state script_mod = ScriptMod script_mod eval script_output = script_mod script_input freeze_rng_state eager_mod = EagerMod eager_mod eval eager_output = eager_mod eager_input assertEqual script_output eager_output freeze_rng_state script_mod = ScriptMod script_mod train script_output = script_mod script_input freeze_rng_state eager_mod = EagerMod eager_mod train eager_output = eager_mod eager_input assertEqual script_output eager_output test_nested_breaks no_bool_loop_outputs g testing did exit transform values loop block outputs thus affecting one loop another loops = g findAllNodes prim Loop loop loops out loop outputs assertTrue out type = BoolType get test y type int ret = tensor = torch tensor while int tensor add_ y == continue _ range y continue ret += ret += ret int tensor assertEqual torch jit script test test assertEqual torch jit script test test no_bool_loop_outputs torch jit script test graph foo y = torch tensor z = while int y add_ int y i range i == continue i break z += int y == break int y == continue z += int y z no_bool_loop_outputs torch jit script foo graph checkScript foo test_nested_two i = k = while i j range k += j == continue i += k += i == break i k checkScript test_nested_two no_bool_loop_outputs torch jit script test_nested_two graph test_breaks_continues foo_continue cond type int j = i range i == cond continue j += j foo_break cond type int j = i range i == cond break j += j i range checkScript foo_continue i checkScript foo_break i test_refine_outside_loop == x = None x = i = j = while x None torch jit _unwrap_optional x i i x = torch jit annotate Optional int None i += continue x = x = x None x x = x + j = x + x x j checkScript test_refine_outside_loop assign_after_break y type int x = i range y x = y + i break x = x checkScript assign_after_break checkScript assign_after_break checkScript assign_after_break assign_after_break_nested y type int x = _ range y y == x = break assert == x = x + break assert == x = - assert == x checkScript assign_after_break_nested checkScript assign_after_break_nested checkScript assign_after_break_nested may_break y type int x = _ range y y == x = x = x + break x = - x checkScript may_break checkScript may_break checkScript may_break test x y type int int = while x y == i range y += i + x -= x == = x break x x == -= x -= break -= x -= x checkScript test checkScript test checkScript test checkScript test checkScript test test_delete_after_break x type int = b = i range x = i break b = i b checkScript test_delete_after_break checkScript test_delete_after_break test_will_break_after_guard x type int = i range x i == = break -= break assert == -= - checkScript test_will_break_after_guard checkScript test_will_break_after_guard checkScript test_will_break_after_guard test_varexit cond type int m = _ range cond == cond == m = break k = k = m += k m use k tests pathway where we have insert uninitialized checkScript test_varexit checkScript test_varexit test_break_true i = while True i += i == break while False i += i checkScript test_break_true test_break_continue_error assertRaisesRegex RuntimeError Syntax cu = torch jit CompilationUnit other_func break assertRaisesRegex RuntimeError Syntax cu = torch jit CompilationUnit other_func i range foo break assertRaisesRegex RuntimeError do support break continue inside torch jit script foo x i = b = x break b test_python_call pyfunc cu = torch jit CompilationUnit other_func + test_call_python b = pyfunc b = other_func b i = step = while i b = pyfunc b bool b b = pyfunc b i = b inputs = _make_scalar_vars torch float outputs = _make_scalar_vars torch float assertEqual cu test_call_python inputs outputs test_python_call_failure assertRaisesRegex RuntimeError undefined value pyfunc pyfunc cu = torch jit CompilationUnit other_func + test_call_python b = pyfunc b = other_func b i = step = while i b = pyfunc b b b = pyfunc b i = b inputs = _make_scalar_vars torch float outputs = _make_scalar_vars torch float assertEqual cu test_call_python inputs outputs test_type_call_in_script torch jit script fn x type x assertRaisesRegex RuntimeError value type _TensorMeta fn torch tensor test_python_call_annotation pyfunc torch jit script foo pyfunc + pyfunc inputs = _make_scalar_vars torch float outputs = _make_scalar_vars torch float assertEqual foo inputs outputs test_python_call_annoytation_failure assertRaisesRegex RuntimeError undefined value pyfunc pyfunc torch jit script foo pyfunc + pyfunc noqa F inputs = _make_scalar_vars torch float outputs = _make_scalar_vars torch float assertEqual foo inputs outputs test_desugar_module torch nn functional F fn x slope = torch abs x b = torch nn functional prelu x slope c = F prelu x slope b c x = torch arange - slope = torch tensor checkScript fn x slope optimize=True test_script_docstring torch jit script with_docstring x test str y = x y same x y assertEqual with_docstring __doc__ test str test_script_method_docstring A torch jit ScriptModule torch jit script_method with_docstring x test str y = x y same x y = A assertEqual with_docstring __doc__ test str test_script_module M torch jit ScriptModule __init__ - None super __init__ weight = nn Parameter torch randn torch jit script_method forward thing weight + thing PModule nn Module __init__ - None super __init__ = nn Parameter torch randn forward mm M torch jit ScriptModule __init__ - None super __init__ test submodule sub = M sub = PModule test parameters weight = nn Parameter torch randn bias = nn Parameter torch randn test defining method string define hi weight mm test script methods torch jit script_method doit input test use parameter weight mm input torch jit script_method doit input weight mm input torch jit script_method forward input = doit input b = doit input c = hi input d = sub input + b + bias + sub + c + d torch jit optimized_execution False m = M input = torch randn = m weight mm input b = m weight mm input c = m weight mm input d = m sub mm input ref = + b + m bias + m sub weight + + c + d assertEqual ref m forward input m weight = nn Parameter torch zeros_like m weight m bias = nn Parameter torch zeros_like m bias m sub weight = nn Parameter torch zeros_like m sub weight m sub data zero_ assertEqual torch zeros m forward torch randn test_irparser graph_str = graph Double CHECK aten relu Double = aten relu FileCheck run graph_str parse_ir graph_str test_parse_tensor_constants foo torch zeros foo_s = torch jit script foo torch _C _jit_pass_constant_propagation foo_s graph g = str foo_s graph g_parsed = parse_ir g parse_tensor_constants=True assertEqual str canonical g_parsed str canonical foo_s graph func = torch _C _create_function_from_graph forward g_parsed out_parsed = func out_func = foo checking data just dtype size etc out_parsed = out_func = assertEqual out_func out_parsed assertRaises RuntimeError parse_ir g parse_tensor_constants=False test_parse_scalar_tensor_constants dtype_str dtype value Float torch float Double torch float BFloat torch bfloat Int torch int Long torch int Short torch int g_str = f graph dtype_str requires_grad= device=cpu = prim Constant value= value jit_graph = parse_ir g_str parse_tensor_constants=True node = next n n jit_graph nodes isinstance n output type torch TensorType assert isinstance node output type torch TensorType t = node t value assert isinstance t torch Tensor assertEqual t dtype dtype assertEqual t item value assertRaises RuntimeError g_str = graph Long requires_grad= device=cpu = prim Constant value= invalid jit_graph = parse_ir g_str parse_tensor_constants=True test_parse_nested_names g_str = graph x Tensor int = prim Constant value= int = prim Constant value= hi submod value Tensor = aten add x hi submod value g = parse_ir g_str round_trip_g = parse_ir str g assertEqual canonical g canonical round_trip_g func = torch _C _create_function_from_graph forward g func = torch _C _create_function_from_graph forward round_trip_g assertEqual func torch ones func torch ones test_is_after_use sorted_input_use g uses = list next g inputs uses sorted uses key=functools cmp_to_key type uses isAfter torch jit script foo x = x + x x uses_sorted = sorted_input_use foo graph sorts last use end assertFalse uses_sorted isAfter uses_sorted assertTrue uses_sorted user kind == aten add assertEqual uses_sorted offset torch jit script foo x cond bool cond x + x - uses_sorted = sorted_input_use foo graph assertTrue uses_sorted user kind == aten add assertTrue uses_sorted user kind == aten sub torch jit script foo x cond bool cond bool cond x + cond x - x graph = foo graph torch jit script foo x cond bool cond bool cond x + cond x - x graph = foo graph graph graph graph uses_sorted = sorted_input_use graph assertTrue uses_sorted user kind == aten add assertTrue uses_sorted user kind == aten sub assertTrue uses_sorted user kind == aten div test_canonicalize_control_outputs test_all_outputs g ifs = g findAllNodes prim If loops = g findAllNodes prim Loop contained_blocks node len node findAllNodes prim If + len node findAllNodes prim Loop node ifs + loops outs = list node outputs out_name = x debugName x outs len out_name == continue fc = FileCheck find last output then all subsequent uses fc check out_name - + skip past node body _ range contained_blocks node fc check - node kind == prim If fc check - check - check \n fc check - check \n canonical order same order first use appears text name out_name fc check name fc run g torch jit script test x type bool - Tuple int int b = = x = b = x = False x b = = b b test_all_outputs test graph torch jit script test x type bool - Tuple int int b = = x = b = x = False x print x print b b test_all_outputs test graph torch jit script test_loop x iter type bool int - None = b = c = _ range iter = b = c = x = True print c x print b test_all_outputs test_loop graph torch jit script loop_unused iter type int - None = b = c = _ range iter c = c + b = b + = + print b print c c used then unused should ordered alphabetical FileCheck check r c int int b int run loop_unused graph test_filecheck test_check file = FileCheck check check check run file FileCheck check run file assertRaisesRegex RuntimeError Expected find FileCheck check run file assertRaisesRegex RuntimeError CHECK FileCheck check check run file test_check test_check_count file = FileCheck check_count run file FileCheck check_count run file FileCheck check_count run file assertRaisesRegex RuntimeError Expected find FileCheck check_count exactly=True run file assertRaisesRegex RuntimeError Expected find FileCheck check_count run file assertRaisesRegex RuntimeError CHECK-COUNT- FileCheck check_count run file test_check_count test_check_same file = \n FileCheck check_same run file assertRaisesRegex RuntimeError Expected find FileCheck check_same run file file = FileCheck check check_same run file FileCheck check_count check_same run file test_check_same test_check_next file = \n \n \n FileCheck check check_next check_next run file FileCheck check_next check_next check_next run file assertRaisesRegex RuntimeError Expected find FileCheck check check_next run assertRaisesRegex RuntimeError Expected find FileCheck check check_next run \n\n test_check_next test_check_dag fc = FileCheck check_dag check_dag check_not fc run fc run fc = FileCheck fc check_not check_dag check_dag check_not fc run fc run fc = FileCheck check_dag check_dag check assertRaisesRegex RuntimeError Expected find did find fc run test_check_dag test_check_not FileCheck check_not check run FileCheck check check_not run assertRaisesRegex RuntimeError Expected find FileCheck check_not check run assertRaisesRegex RuntimeError Expected find FileCheck check check_not run checks distinct range matchings fb = FileCheck check_count check_count check_not assertRaisesRegex RuntimeError Expected find fb run fb = FileCheck check_count check_not check_count assertRaisesRegex RuntimeError Expected find fb run _dtype_to_jit_name dtype dtype == torch float Float dtype == torch float Double dtype == torch int Long dtype == torch int Int dtype == torch bool Bool raise RuntimeError dtype handled _dtype_to_expect dtype dim= param = join dim + device=cpu param = + param + jit_type = _dtype_to_jit_name dtype dim = jit_type + param special case representing wrapped number jit_type lower _test_dtype_op_shape ops args input_dims= input_dims raise RuntimeError input dims must least dtypes = torch float torch float torch int torch int str_args = join str arg arg args + len args tensor_data = input_dims + + input_dims template = dedent func return_line op ops dtype dtypes + None tensor_type dtypes couple ops aren t implemented non-floating types tensor_type is_floating_point dtype None dtype is_floating_point op mean softmax log_softmax continue return_line = f torch tensor tensor_data dtype= tensor_type op str_args dtype= dtype uncomment debugging failed test print testing format return_line code = template format return_line=return_line scope = exec code globals scope cu = torch jit CompilationUnit code graph = cu func graph torch _C _jit_pass_complete_shape_analysis graph False input_array = _ range input_dims input_array = input_array t = torch tensor input_array dtype=tensor_type attr = getattr t op kwargs = dtype dtype result = attr args kwargs expect = _dtype_to_expect result dtype result dim FileCheck check aten tensor check expect run graph test_dtype_op_shape ops = prod _test_dtype_op_shape ops args= _test_dtype_op_shape ops args= False _test_dtype_op_shape ops args= False _test_dtype_op_shape ops args= True test_dtype_op_shape ops = cumprod cumsum softmax log_softmax _test_dtype_op_shape ops args= _test_dtype_op_shape ops args= input_dims= _test_binary_op_shape ops input_dims= dtypes = torch float torch float torch int torch int torch bool input_dims == shape = shape = + + _ range input_dims shape = + join shape + template = dedent func arg = arg = torch arg arg args = dtype dtypes args = args + f torch tensor shape dtype= dtype args = args + isBool arg type arg bool type arg str torch bool arg op ops first_arg args second_arg args subtract supported bool op == sub op == div isBool first_arg isBool second_arg continue div implemented correctly mixed-type int params op == div type first_arg type second_arg isinstance first_arg int isinstance first_arg str int first_arg continue return_line = f torch op first_arg second_arg uncomment debugging failed test print testing format return_line code = template format first_arg second_arg op scope = exec code globals scope non_jit_result = scope func cu = torch jit CompilationUnit code graph = cu func graph torch _C _jit_pass_complete_shape_analysis graph False use dim=- represent python jit scalar dim = - type first_arg str type second_arg str non_jit_result dim dtype = non_jit_result dtype jit only supports int float scalars dim dtype == torch int dtype = torch int dtype == torch float dtype = torch float expect = _dtype_to_expect dtype dim jit_output = next graph outputs check = FileCheck check check expect run str jit_output test_binary_op_shape _test_binary_op_shape mul div add sub _test_binary_op_shape mul div add sub test_no_dtype_shape torch jit script foo x scalar_number = x item x add scalar_number torch jit script foo x scalar_number = x item torch tensor add scalar_number t = torch tensor g = foo graph_for t type = next g outputs assertTrue type type == torch _C TensorType get g = foo graph_for t type = next g outputs assertTrue type type == torch _C TensorType get test_filecheck_parse test_check file = CHECK CHECK CHECK FileCheck run checks_file=file test_file=file file = CHECK FileCheck run file assertRaisesRegex RuntimeError Expected find FileCheck run file assertRaisesRegex RuntimeError Expected find FileCheck run CHECK test_check test_check_count file = FileCheck run CHECK-COUNT- file FileCheck run CHECK-COUNT-EXACTLY- file FileCheck run CHECK-COUNT- file FileCheck run CHECK-COUNT- file assertRaisesRegex RuntimeError Expected find FileCheck run CHECK-COUNT-EXACTLY- file test_check_count test_check_same file = \n FileCheck run CHECK-SAME file assertRaisesRegex RuntimeError Expected find FileCheck run CHECK-SAME file file = FileCheck run CHECK \n CHECK-SAME file FileCheck run CHECK-COUNT- \n CHECK-SAME file test_check_same test_bad_input assertRaisesRegex RuntimeError Check bad input FileCheck run assertRaisesRegex RuntimeError Could parse check FileCheck run CHECK test_bad_input test_script_module_call_noscript M torch jit ScriptModule __init__ - None super __init__ value = torch jit ignore foo torch ones + value torch jit script_method forward input input + foo torch jit optimized_execution False m = M input = torch randn o = m input assertEqual o input + torch ones + check we can change python attributes those changes picked up script methods m value = o = m input assertEqual o input + torch ones + test_script_module_nochange_submodule M torch jit ScriptModule __init__ - None super __init__ sub = nn Linear torch jit script_method forward input sub input torch jit optimized_execution False m = M input = torch randn o = m input assertEqual o m sub input assertRaisesRegex RuntimeError Cannot re-assign m sub = nn Linear test_module_apis Sub torch nn Module forward thing thing - Double torch nn Module forward thing thing MyMod torch nn Module __init__ - None super __init__ mod = Sub mod = Sub mod = nn Sequential nn Sequential Sub mod = nn Sequential Sub Double torch jit export method x x y y mod_names = name mod named_modules mod_names = mod_names + + name x = mod x children_names = name mod named_children children_names = children_names + + name x = mod x mod modules y = mod y mod children y = mod y mod_names children_names x x y y forward x x + mod = torch jit script MyMod inps = tuple torch tensor i i range assertEqual mod method inps MyMod method inps test_script_module_const M torch jit ScriptModule __constants__ = b i c s __init__ - None super __init__ b = False i = c = s = hello torch jit script_method forward b i c torch jit optimized_execution False m = M o o o = m assertEqual o assertEqual o assertEqual o test_script_module_fail_exist M torch jit ScriptModule torch jit script_method forward x x + whatisgoingon assertRaisesRegex RuntimeError Module M has no attribute M unittest skip module dedupe currently NoneType refinement optional attributes doesn t work test_script_module_none_exist_fail M torch jit ScriptModule __init__ my_optional super __init__ my_optional = my_optional torch jit script_method forward x my_optional None torch neg x + my_optional torch neg x assertRaisesRegex RuntimeError has no attribute my_optional x = torch rand fb = M None fb x test_script_module_invalid_consts Foo torch jit ScriptModule __constants__ = invalid __init__ - None super __init__ invalid = nn Linear assertRaisesRegex TypeError Linear object attribute Foo invalid valid constant Foo Foo torch jit ScriptModule __constants__ = invalid __init__ - None super __init__ invalid = int assertRaisesRegex TypeError valid constant Foo Foo torch jit ScriptModule __constants__ = invalid __init__ - None super __init__ invalid = assertRaisesRegex TypeError valid constant Foo Foo torch jit ScriptModule __constants__ = invalid __init__ - None super __init__ invalid = np int verify we capture human understandable name assertRaisesRegex TypeError numpy int Foo test_script_module_param_buffer_mutation TODO add param mutation test case after JIT support ModuleBufferMutate torch jit ScriptModule __init__ - None super __init__ running_var = nn Buffer torch tensor dtype=torch long torch jit script_method forward training running_var += running_var torch jit optimized_execution False m = ModuleBufferMutate assertEqual m m eval assertEqual m test_script_module_for M torch jit ScriptModule __constants__ = b __init__ - None super __init__ b = torch jit script_method forward sum = i b sum += i sum torch jit optimized_execution False m = M assertEqual m test_override_magic OverrideMagic nn Module torch jit export __len__ mod = OverrideMagic assertEqual len mod len torch jit script mod OverrideMagicSeq nn Sequential torch jit export __len__ mod = OverrideMagicSeq assertEqual len mod len torch jit script mod assertTrue torch jit script mod test_script_module_for Sub torch jit ScriptModule __init__ - None super __init__ weight = nn Parameter torch randn torch jit script_method forward thing weight + thing M torch jit ScriptModule __init__ - None super __init__ mods = nn ModuleList Sub i range torch jit script_method forward v m mods v = m v v torch jit optimized_execution False i = torch empty m = M o = m i v = i sub m mods v = sub v assertEqual o v assertRaisesRegex Exception object iterable print list m test_attr_qscheme_script Foo torch nn Module __init__ - None super __init__ qscheme = torch per_tensor_affine forward qscheme == torch per_tensor_symmetric f = Foo scripted = torch jit script f assertEqual f scripted test_script_module_const_submodule_fail Sub torch jit ScriptModule __init__ - None super __init__ weight = nn Parameter torch randn torch jit script_method forward thing weight + thing M torch jit ScriptModule __init__ - None super __init__ mods = Sub _ range torch jit script_method forward _ mods print assertRaisesRegex RuntimeError has no attribute mods M DerivedStateModule torch jit ScriptModule __init__ - None super TestScript DerivedStateModule __init__ param = torch nn Parameter torch ones dtype=torch float derived = nn Buffer torch neg param detach clone This flag so we can test pack method called pack_called = nn Buffer torch zeros dtype=torch long This flag so we can test unpack method called unpack_called = nn Buffer torch zeros dtype=torch long torch jit script_method _pack pack_called set_ torch ones dtype=torch long derived set_ torch rand detach torch jit script_method _unpack unpack_called set_ torch ones dtype=torch long derived set_ torch neg param detach torch jit script_method forward x x + derived test_pack_unpack_state sm = TestScript DerivedStateModule x = torch rand torch testing assert_close sm x x + torch neg torch ones dtype=torch float Test save path assertFalse sm pack_called item assertFalse sm unpack_called item imported = getExportImportCopyWithPacking sm ensure pack called before serialization assertTrue sm pack_called item ensure unpack called after serialization so leave module initialized state assertTrue sm unpack_called item torch testing assert_close sm derived torch neg sm param Test load paths assertTrue imported unpack_called item torch testing assert_close imported x x + torch neg torch ones dtype=torch float unittest skipIf TEST_MKL PyTorch built without MKL support unittest skipIf True Skipping while landing PR stack test_torch_functional stft input n_fft type Tensor int - Tensor torch stft input n_fft return_complex=True inps = torch randn assertEqual stft inps torch jit script stft inps istft input n_fft type Tensor int - Tensor torch istft input n_fft inps = stft inps inps assertEqual istft inps torch jit script istft inps lu_unpack x A_LU pivots = torch linalg lu_factor x torch lu_unpack A_LU pivots shape = torch randn shape checkScript lu_unpack cdist_fn = torch tensor - - - b = torch tensor - - - torch cdist b compute_mode= use_mm_for_euclid_dist checkScript cdist_fn norm c = torch tensor - dtype=torch float torch norm c p= fro torch norm c p= nuc torch norm c torch norm c p= checkScript norm torch_unique dim Optional int ten = torch unique torch tensor dtype=torch long = torch unique ten dim=dim b = torch unique ten return_counts=True dim=dim c = torch unique ten return_inverse=True dim=dim d = torch unique ten return_counts=True return_inverse=True dim=dim b c d checkScript torch_unique None checkScript torch_unique torch_unique_consecutive dim Optional int ten = torch unique torch tensor dtype=torch long = torch unique_consecutive ten dim=dim b = torch unique_consecutive ten return_counts=True dim=dim c = torch unique_consecutive ten return_inverse=True dim=dim d = torch unique_consecutive ten return_counts=True return_inverse=True dim=dim b c d checkScript torch_unique_consecutive None checkScript torch_unique_consecutive test_torch_functional_tensordot_int tensordot_dims_int torch Tensor b torch Tensor dims int torch tensordot b dims=dims = torch arange reshape b = torch arange reshape dims = checkScript tensordot_dims_int b dims dims - try tensordot_dims_int b dims except RuntimeError error dims assertEqual str error tensordot expects dims = got dims= + str dims dims min dim b dim assertEqual str error tensordot expects dims ndim_a ndim_b got dims= + str dims test_torch_functional_tensordot_tensor tensordot_dims_tensor torch Tensor b torch Tensor dims torch Tensor torch tensordot b dims=dims = torch arange reshape b = torch arange reshape dims = torch tensor checkScript tensordot_dims_tensor b dims = torch arange reshape b = torch arange reshape dims = torch tensor dtype=torch long checkScript tensordot_dims_tensor b dims test_torch_functional_tensordot_list tensordot_dims_list torch Tensor b torch Tensor dims List List int torch tensordot b dims=dims = torch arange reshape b = torch arange reshape dims = checkScript tensordot_dims_list b dims test_torch_functional_tensordot_tuple tensordot_dims_tuple torch Tensor b torch Tensor dims Tuple List int List int torch tensordot b dims=dims = torch arange reshape b = torch arange reshape dims = checkScript tensordot_dims_tuple b dims test_missing_getstate Foo torch nn Module __init__ - None super __init__ x = forward x x x torch jit export __setstate__ state x = state training = state assertRaisesRegex RuntimeError getstate scripted = torch jit script Foo test_inlining_cleanup foo x F linear x x torch jit script fee x foo x inlining optimizations should have cleaned up linear statement run_pass inline fee graph FileCheck check_not prim If run fee graph skipIfTorchDynamo TorchDynamo fails unknown reason test_pack_unpack_nested SubSubMod torch jit ScriptModule __init__ - None super __init__ buf = nn Buffer torch ones torch jit script_method _pack buf set_ torch zeros torch jit script_method _unpack buf set_ torch ones torch jit script_method forward x x + buf SubMod torch jit ScriptModule __init__ - None super __init__ buf = nn Buffer torch ones ssm = SubSubMod torch jit script_method _pack buf set_ torch zeros torch jit script_method _unpack buf set_ torch ones torch jit script_method forward x ssm x + buf Mod torch jit ScriptModule __init__ - None super __init__ submod = SubMod buf = nn Buffer torch ones torch jit script_method _pack buf set_ torch zeros torch jit script_method _unpack buf set_ torch ones torch jit script_method forward x submod x + buf m = Mod torch testing assert_close m torch zeros torch ones m apply lambda s s _pack torch testing assert_close m torch zeros torch zeros m apply lambda s s _unpack torch testing assert_close m torch zeros torch ones test_torch_any fn x torch any x fn x dim int torch any x dim checkScript fn torch randn checkScript fn torch empty checkScript fn torch empty checkScript fn torch ones checkScript fn torch zeros checkScript fn torch empty - checkScript fn torch randn checkScript fn torch zeros - checkScript fn torch empty test_any fn x List int any x fn x List float any x fn x List bool any x fn x List str any x checkScript fn checkScript fn checkScript fn checkScript fn checkScript fn checkScript fn checkScript fn checkScript fn True False False checkScript fn False False False checkScript fn True True True True checkScript fn checkScript fn checkScript fn - checkScript fn test_script_module_not_tuple M torch jit ScriptModule __constants__ = mods __init__ - None super __init__ mods = torch jit script_method forward v m mods print m v assertRaisesRegex RuntimeError int object iterable M test_attr_module_constants M torch jit ScriptModule __init__ mod_list super __init__ mods = mod_list torch jit script_method forward x mods forward x torch jit optimized_execution False m = M nn Sequential nn ReLU assertExportImportModule m torch randn test_script_sequential_for Sub torch jit ScriptModule __init__ - None super __init__ weight = nn Parameter torch randn torch jit script_method forward thing weight + thing M torch jit ScriptModule __init__ - None super __init__ mods = nn Sequential Sub Sub Sub torch jit script_method forward v m mods v = m v v torch jit script_method forward v mods v torch jit optimized_execution False i = torch empty m = M o = m i v = i sub m mods _modules values v = sub v assertEqual o v o = m forward i assertEqual o v test_script_sequential_sliced_iteration seq_mod nn Module __init__ - None super __init__ layers = nn ReLU nn ReLU nn ReLU layers = nn Sequential layers forward input x = layers forward input layer layers x = layer forward x layer layers x = layer forward x x seq = seq_mod checkModule seq torch tensor - - test_script_sequential_orderdict M torch jit ScriptModule __init__ - None super __init__ mods = nn Sequential OrderedDict conv nn Conv d relu nn ReLU torch jit script_method forward input mods input m = M assertTrue mods conv weight m state_dict keys test_script_sequential_multi_output_fail Sub torch jit ScriptModule __init__ - None super __init__ weight = nn Parameter torch randn torch jit script_method forward thing weight + thing ReturnMulti torch jit ScriptModule torch jit script_method forward x x x x HaveSequential torch jit ScriptModule __init__ - None super __init__ someseq = nn Sequential Sub ReturnMulti Sub torch jit script_method forward x someseq x assertRaisesRegex RuntimeError Tensor Tensor Tensor torch jit optimized_execution False hs = HaveSequential i = torch empty hs i _tmp_donotuse_dont_inline_everything test_script_sequential_in_mod_list Sub torch jit ScriptModule __init__ - None super __init__ weight = nn Parameter torch randn torch jit script_method forward thing weight + thing M torch jit ScriptModule __init__ - None super __init__ mods = nn ModuleList Sub nn Sequential Sub nn Sequential Sub Sub Sub torch jit script_method forward v mod mods v = mod v v m = M graph = str m graph assertTrue graph count prim CallMethod == assertTrue python graph _tmp_donotuse_dont_inline_everything test_script_nested_mod_list Sub torch jit ScriptModule __init__ - None super __init__ weight = nn Parameter torch randn torch jit script_method forward thing weight + thing M torch jit ScriptModule __init__ - None super __init__ mods = nn ModuleList nn ModuleList Sub nn Sequential Sub nn ModuleList Sub Sub torch jit script_method forward v mod mods m mod v = m v v m = M graph = str m graph assertTrue graph count prim CallMethod == assertTrue python graph test_constant_as_attr M torch jit ScriptModule __constants__ = dim __init__ - None super __init__ dim = torch jit script_method forward v torch cat v v v dim=self dim v = torch zeros torch jit optimized_execution False assertEqual torch cat v v v dim= M v StarTestSumStarred torch nn Module __init__ - None super TestScript StarTestSumStarred __init__ forward inputs output = inputs i range len inputs output += inputs i output StarTestReturnThree torch nn Module __init__ - None super TestScript StarTestReturnThree __init__ forward rep rep rep rep test_script_star_expr M torch jit ScriptModule __init__ - None super __init__ m = torch jit trace TestScript StarTestSumStarred torch ones torch ones torch ones g = torch jit trace TestScript StarTestReturnThree torch ones torch jit script_method forward rep tup = g rep m tup m = M assertEqual m torch zeros torch zeros test_script_star_expr_string M torch jit ScriptModule __init__ - None super __init__ m = torch jit trace TestScript StarTestSumStarred torch ones torch ones torch ones g = torch jit trace TestScript StarTestReturnThree torch ones define forward rep tup = g rep m tup m = M assertEqual m torch zeros torch zeros StarTestSumAndReturnThree torch nn Module __init__ - None super TestScript StarTestSumAndReturnThree __init__ forward inputs output = inputs i range len inputs output += inputs i output output output test_script_star_assign M torch jit ScriptModule __init__ - None super __init__ g = torch jit trace TestScript StarTestSumAndReturnThree torch ones define forward rep head tail = g rep head m = M assertEqual m torch zeros torch zeros test_script_module_star_assign M torch jit ScriptModule __init__ - None super __init__ g = torch jit trace TestScript StarTestSumAndReturnThree torch ones torch ones torch ones _force_outplace=True define forward rep head tail = g rep rep rep tail m = M assertEqual m torch ones torch ones test_script_module_star_assign _inplace M torch jit ScriptModule __init__ - None super __init__ g = torch jit trace TestScript StarTestSumAndReturnThree torch ones torch ones torch ones _force_outplace=False define forward rep head tail = g rep rep rep tail m = M since forward makes three aliases input ` rep ` before passing StarTestSumAndReturnThree in-place behavior will different than above out place assertEqual m torch ones torch ones test_script_module_star_assign_fail_pythonop assertRaisesRegex RuntimeError cannot used tuple M torch jit ScriptModule __init__ - None super __init__ torch jit ignore myfunc torch zeros torch zeros define forward rep b = myfunc m = M m torch zeros test_script_module_star_assign_fail_builtin assertRaisesRegex RuntimeError cannot used tuple M torch jit ScriptModule __init__ - None super __init__ define forward rep b = torch neg rep m = M m torch zeros test_script_pack_padded_sequence torch nn utils rnn pack_padded_sequence pad_packed_sequence pack_padded_pad_packed_script x seq_lens x = pack_padded_sequence x seq_lens x lengths = pad_packed_sequence x x lengths T B C = x = torch ones T B C seq_lens = torch tensor set padding value so we can test equivalence b range B seq_lens b T x seq_lens b b = eager_seq eager_lengths = pack_padded_pad_packed_script x seq_lens torch _jit_internal _disable_emit_hooks scripted_pack_padded_seq = torch jit script pack_padded_pad_packed_script script_seq script_lengths = scripted_pack_padded_seq x seq_lens assertEqual eager_seq script_seq assertEqual eager_lengths script_lengths ExperimentalLSTM torch nn Module __init__ input_dim hidden_dim super __init__ forward input type Tensor packed = pack_padded_sequence input=input lengths=torch tensor enforce_sorted=False output lengths = pad_packed_sequence sequence=packed total_length= lengths flipped so output output lstm = ExperimentalLSTM input_dim= hidden_dim= torch _jit_internal _disable_emit_hooks checkModule lstm torch ones test_script_pad_sequence_pack_sequence torch nn utils rnn pad_sequence pack_sequence pad_packed_sequence pad_sequence_func tensor_list batch_first=False padding_value= padding_side= right type List Tensor bool float str - Tensor pad_sequence tensor_list batch_first padding_value padding_side pack_sequence_func tensor_list enforce_sorted=True type List Tensor bool - Tensor pad_packed_sequence pack_sequence tensor_list enforce_sorted ones = torch ones ones = torch ones ones = torch ones tensor = torch tensor tensor = torch tensor tensor = torch tensor torch _jit_internal _disable_emit_hooks checkScript pad_sequence_func ones ones ones checkScript pad_sequence_func ones ones ones True checkScript pad_sequence_func ones ones ones True checkScript pad_sequence_func ones ones ones True left checkScript pad_sequence_func ones ones ones False left checkScript pack_sequence_func tensor tensor tensor checkScript pack_sequence_func tensor tensor tensor False test_script_get_tracing_state test_if_tracing x torch _C _get_tracing_state x + x - inp = torch randn checkScript test_if_tracing inp test_script_is_tracing test_is_tracing x torch jit is_tracing x + x - inp = torch randn checkScript test_is_tracing inp test_is_scripting foo torch jit is_scripting assertFalse foo scripted = torch jit script foo assertTrue scripted test_comment_ignore_indent Model torch nn Module __init__ - None useless comment indented correctly noqa E super __init__ forward should compile without error checkModule Model test_script_outputs assertRaisesRegex RuntimeError cannot used tuple torch jit script foo c d = + c + d torch jit script assertRaisesRegex RuntimeError too many values unpack torch jit script bind b = print print b unittest skipIf RUN_CUDA requires CUDA test_script_get_device_cuda torch jit script foo get_device v = torch randn device= cuda assertEqual foo v test_script_chunk torch jit script foo b c = torch chunk dim= chunks= b v = torch rand assertEqual torch chunk v dim= chunks= foo v test_script_copy M torch nn Module __annotations__ = val Optional torch Tensor __init__ - None super __init__ val = None some_method forward x type Tensor - Tensor val = x + some_method x m = torch jit script M test copy copy copy m copy deepcopy m test_script_forward_method_replacement We want support use case attaching different ` forward ` method LowLevelModule torch nn Module forward input torch Tensor Generic forward dispatch forward_pytorch input TestModule LowLevelModule __init__ - None super __init__ Replace forward method forward = types MethodType LowLevelModule forward forward_pytorch input torch Tensor torch tensor forward input torch Tensor Should use forward method raise AssertionError This method should used forward_pytorch input m = TestModule assertEqual m torch tensor torch tensor m_scripted = torch jit script m assertEqual m_scripted torch tensor torch tensor test_python_call_non_tensor foo b c type Tensor int Tuple Tensor int - Tuple int Tensor d e = c b + e + d torch jit script bar x = torch ones b = foo x x b assertEqual torch ones + bar test_python_call_non_tensor_wrong assertRaisesRegex RuntimeError r instead got value type tuple torch jit ignore foo type - Tensor noqa T torch jit script bar foo bar test_if_different_type assertRaisesRegex RuntimeError c set type int true branch type float false branch torch jit script diff_type_used == c = c = c assertRaisesRegex RuntimeError Variable c previously had type float torch jit script diff_existing_type x c = == c = print x x torch jit script diff_type_unused == c = print c c = print c test_if_not_defined_error assertRaisesRegex RuntimeError c defined false branch torch jit script test == c = c assertRaisesRegex RuntimeError c defined true branch torch jit script test == pass c = c test_if_list_cat testing different length lists don t throw error cat shape prop torch jit script test_list x bool x sum c = x x c = x x x torch cat c b = torch zeros _propagate_shapes test_list graph b False test_if_supertype torch jit script tensor_unifying x y z testing dynamic appropriately set y z bool x x y z = x + y z noqa PLW x y z = x + x y x y z = torch zeros dtype=torch float b = torch zeros dtype=torch long c = torch zeros dtype=torch float graph = _propagate_shapes tensor_unifying graph b c False if_outputs = list graph findNode prim If outputs assertTrue if_outputs type str == Float requires_grad= device=cpu assertTrue if_outputs type str == Tensor requires_grad= device=cpu assertTrue if_outputs type str == Tensor requires_grad= device=cpu test_list_unify allowing unififed int would cause runtime error b c index operation expects int generic list true branch IValue will int list assertRaisesRegex RuntimeError int true branch type None torch jit script list_optional_fails x type bool - Optional int x y = y = None noqa T y torch jit script list_tensors x type bool - Tuple Tensor List Tensor x = torch zeros y = = torch zeros y = y run_pass constant_propagation list_tensors graph m = createFunctionFromGraph list_tensors graph testing tensor type lists unified getExportImportCopy m skipIfTorchDynamo Not TorchDynamo suitable test _inline_everything test_import_constants_not_specialized Mod torch nn Module forward x torch cat x dim= ScriptMod torch jit ScriptModule __init__ mod super __init__ x = torch zeros mod_fn = lambda mod x noqa E mod = torch jit trace mod_fn torch jit script_method forward mod cm = ScriptMod Mod specialized tensor graph FileCheck check Float strides= requires_grad= device=cpu run cm forward graph buffer = io BytesIO torch jit save cm buffer buffer seek when tensor loaded constant isn t specialized cm_load = torch jit load buffer FileCheck check_not Float run cm_load forward graph skipIfTorchDynamo TorchDynamo fails unknown reason test_type_annotations_repeated_list torch jit script float_fn x y type float BroadcastingList float - List float y assertEqual float_fn float_fn assertEqual float_fn float_fn torch jit script float_fn_call print float_fn print float_fn torch jit script int_fn x type BroadcastingList int - List int x assertEqual int_fn int_fn assertEqual int_fn int_fn torch jit script int_fn_call print int_fn print int_fn assertRaisesRegex RuntimeError must positive integer torch jit script noqa T fn x type BroadcastingListx int - List int noqa T x using CU so flake error int raised noqa working assertRaisesRegex RuntimeError Unknown type constructor cu = torch jit CompilationUnit nested x y type int Tuple int int - List int x noqa T torch jit script f x BroadcastingList int x out = f assertTrue isinstance out int assertEqual out test_ntuple_builtins torch nn modules utils _single _pair _triple _quadruple test_ints _single _pair _triple _quadruple test_floats _single _pair _triple _quadruple checkScript test_ints checkScript test_floats test_embedding_renorm_grad_error Testing builtin call embedding_renorm_ correctly throws Error when backward called its input embedding_norm input embedding_matrix max_norm F embedding input embedding_matrix max_norm= torch jit script embedding_norm_script input embedding_matrix max_norm type Tensor Tensor float - None F embedding input embedding_matrix max_norm= _ embedding_norm embedding_norm_script input = torch tensor embedding_matrix = torch randn var = torch randn requires_grad=True var = var detach requires_grad_ output = var embedding_matrix output = var embedding_matrix output sum backward ignore = F embedding input embedding_matrix max_norm= assertRaisesRegex RuntimeError modified output sum backward test_type_annotations fn x y type Tensor Tensor - Tuple Tensor Tensor Tensor x x x assertRaisesRegex RuntimeError r need values found only torch jit script script_fn x x y z w = fn x x assertRaisesRegex RuntimeError r too many values need found torch jit script script_fn x x y = fn x x fn_unpack x y z w = fn x x y fn_index x q = fn x x x fn_string str strpair type str Tuple str str - Tuple str int str str str str = strpair str str str x = torch ones checkScript fn_unpack x optimize=True checkScript fn_index x optimize=True checkScript fn_string optimize=True test_type_annotations_varargs torch jit ignore fn_varargs x args args args x fn x y z fn_varargs x fn x y z fn_varargs x y fn x y z fn_varargs x y z x y z = torch randn _ range checkScript fn x y z optimize=True checkScript fn x y z optimize=True checkScript fn x y z optimize=True test_type_annotation_py code = dedent torch torch Tensor typing Tuple fn x torch Tensor y Tensor z - Tuple Tensor Tensor Tensor x y + z z tempfile TemporaryDirectory tmp_dir script_path = os path join tmp_dir script py open script_path w f f write code fn = get_fn test_type_annotation_py script_path fn = torch jit ignore fn assertRaisesRegex RuntimeError r Expected value type Tensor argument r x instead found type Tuple\ Tensor torch jit script bad_fn x x y = fn x x x x y assertRaisesRegex RuntimeError r too many values need found torch jit script bad_fn x x y = fn x x x y assertRaisesRegex RuntimeError r need values found only torch jit script bad_fn x x y z w = fn x x x y good_fn x y z w = fn x x x y z w checkScript good_fn torch ones optimize=True test_type_annotation_module BaseModule torch jit ScriptModule torch jit ignore foo x type Tensor - Tensor x + torch jit ignore bar x y type Tensor Tensor - Tuple Tensor Tensor x + y y torch jit ignore baz x y x ModuleTooMany BaseModule torch jit script_method method x foo x x ModuleTooFew BaseModule torch jit script_method method x bar x ModuleTooManyAssign BaseModule torch jit script_method method x y z w = bar x x x ModuleDefault BaseModule torch jit script_method method x y = baz x x assertRaisesRegex RuntimeError Expected most arguments found ModuleTooMany assertRaisesRegex RuntimeError Argument y provided ModuleTooFew assertRaisesRegex RuntimeError need values found only ModuleTooManyAssign assertRaisesRegex RuntimeError Argument y provided ModuleDefault test_type_inferred_from_empty_annotation Test type inferred empty missing annotation Torch Tensor ` inferred=true ` torch jit script fn x x graph = fn graph n = next graph inputs assertTrue n type == torch _C TensorType getInferred assertRaisesRegex RuntimeError Inferred x type Tensor fn test_script_define_order M torch jit ScriptModule torch jit script_method call_foo input foo input torch jit script_method foo input input + m = M assertEqual m call_foo torch ones dtype=torch int test_script_define_order_recursive_fail M torch jit ScriptModule torch jit script_method call_foo input foo input torch jit script_method foo input call_foo input assertRaisesRegex RuntimeError called recursively M test_script_kwargs_fn_call M torch jit ScriptModule torch jit script_method call_foo input foo input=input bar= torch jit script_method foo bar input type int Tensor - Tensor input + bar m = M assertEqual m call_foo torch ones dtype=torch int test_if_define torch jit script foo bool == b = b = b + torch jit script foo b = bool == b = b + torch jit script foo b = bool == c = b = b + = torch ones dtype=torch long b = torch zeros dtype=torch long assertEqual foo assertEqual foo b assertEqual foo assertEqual foo b assertEqual foo assertEqual foo b test_script_module_export_submodule M torch jit ScriptModule __init__ - None super __init__ weight = nn Parameter torch randn torch jit script_method forward thing weight + thing M torch jit ScriptModule __init__ - None super __init__ test submodule sub = M weight = nn Parameter torch randn bias = nn Parameter torch randn define hi weight mm torch jit script_method doit input weight mm input torch jit script_method doit input weight mm input torch jit script_method doit input input + torch ones dtype=torch double torch jit script_method forward input = doit input b = doit input c = hi input + b + bias + c torch jit optimized_execution False m_orig = M m_import = getExportImportCopy m_orig input = torch randn assertEqual m_orig doit input m_import doit input assertEqual m_orig hi input m_import hi input assertEqual m_orig doit input m_import doit input assertEqual m_orig forward input m_import forward input slowTest test_compile_module_with_constant Double nn Module __init__ downsample=None super __init__ forward input input Mod nn Module __constants__ = downsample __init__ downsample=None super __init__ downsample = downsample forward input downsample None downsample input input none_mod = torch jit script Mod None double_mod = torch jit script Mod Double assertEqual none_mod torch tensor torch tensor assertEqual double_mod torch tensor torch tensor test_device_kwarg torch device f device type= cuda torch device type= cpu checkScript f test_script_module_export_tensor_type M torch jit ScriptModule __init__ type super __init__ param = torch nn Parameter torch zeros dtype=type random_ torch jit script_method foo param torch jit optimized_execution False type torch float torch double m_orig = M type m_import = getExportImportCopy m_orig check make sure storage wasn t resized assertTrue m_orig param storage size == assertEqual m_orig foo m_import foo assertTrue m_orig foo dtype == m_import foo dtype unittest skipIf RUN_CUDA testing cuda tensors require CUDA test_script_module_export_tensor_cuda M torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch zeros device= cuda random_ torch jit script_method foo param m_orig = M m_import = getExportImportCopy m_orig check make sure storage wasn t resized assertTrue m_orig param storage size == assertTrue m_import foo device == torch device cuda assertEqual m_orig foo m_import foo assertTrue m_orig foo dtype == m_import foo dtype test_script_module_export_blocks M torch jit ScriptModule __init__ n m super __init__ weight = torch nn Parameter torch rand n m torch jit script_method forward input bool input sum output = weight mv input output = weight + input output m_orig = M m_import = getExportImportCopy m_orig t = torch rand assertEqual m_orig t m_import t test_script_module_export_shared_storage M torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand param = torch nn Parameter param param = torch nn Parameter torch rand param = torch nn Parameter torch rand torch jit script_method foo param + param + param + param torch jit optimized_execution False m_orig = M m_import = getExportImportCopy m_orig assertEqual m_orig foo m_import foo assertTrue m_import param storage data_ptr == m_import param storage data_ptr assertTrue m_import param storage data_ptr = m_import param storage data_ptr test_sequential_intermediary_types A torch nn Module forward x x + B torch nn Module forward x x C torch nn Module __init__ - None super __init__ foo = torch nn Sequential A B forward x foo x checkModule C torch tensor test_ellipsis_const_mid ellipsize x type Tensor - List int x Ellipsis size dummy = torch zeros checkScript ellipsize dummy optimize=True test_ellipsis_const_mid_select ellipsize x type Tensor - List int x Ellipsis size dummy = torch zeros checkScript ellipsize dummy optimize=True test_ellipsis_const_start ellipsize x type Tensor - List int x Ellipsis size dummy = torch zeros checkScript ellipsize dummy optimize=True test_ellipsis_const_end ellipsize x type Tensor - List int x Ellipsis size dummy = torch zeros checkScript ellipsize dummy optimize=True test_ellipsis_mid ellipsize x type Tensor - List int x size dummy = torch zeros checkScript ellipsize dummy optimize=True test_ellipsis_mid_select ellipsize x type Tensor - List int x size dummy = torch zeros checkScript ellipsize dummy optimize=True test_ellipsis_start ellipsize x type Tensor - List int x size dummy = torch zeros checkScript ellipsize dummy optimize=True test_ellipsis_end ellipsize x type Tensor - List int x size dummy = torch zeros checkScript ellipsize dummy optimize=True test_torch_manual_seed freeze_rng_state test torch manual_seed torch rand script = torch jit script test assertEqual test script graph = script graph_for FileCheck check aten manual_seed run graph skipIfTorchDynamo Not TorchDynamo suitable test test_index_select_shape_prop torch jit script foo x y torch index_select x index=y dim= = torch zeros b = torch zeros dtype=torch long torch _C _jit_pass_complete_shape_analysis foo graph b False FileCheck check Float strides= requires_grad= device=cpu run str foo graph test_shape_analysis_loop foo b x c = first iteration loop appears c should have expand size b second+ iterations there no broadcast sizes different previously would cause compiler enter infinite loop trying compute shape insert invalid broadcasts test ensure we don t regress these issues _ range = c + b c = x b = x checkScript foo torch zeros torch zeros torch zeros optimize=False test_intlist_args func_ x torch nn functional adaptive_avg_pool d x func_ x torch nn functional adaptive_avg_pool d x output_size= func_ x torch nn functional adaptive_avg_pool d x output_size= x = torch randn checkScript func_ x optimize=True checkScript func_ x optimize=True checkScript func_ x optimize=True test_wrong_implicit_expand _trace torch zeros torch zeros foo b + b = torch rand b = torch rand assertEqual + b foo b test_builtin_args_fails assertRaisesRegex RuntimeError Argument provided torch jit script f torch sum foo= assertRaisesRegex RuntimeError specified twice torch jit script f torch sum self=a assertRaisesRegex RuntimeError provided torch jit script f torch sum dim= assertRaisesRegex RuntimeError argument \ tensors\ instead found type \ Tensor torch jit script f torch cat assertRaisesRegex RuntimeError r argument \ tensors\ instead found type \ List\ int\ torch jit script f torch cat assertRaisesRegex RuntimeError r Expected value r type \ List\ int\ \ argument r \ size\ instead found type r \ List\ Union\ List\ int\ int\ \ torch jit script f expand size= test_builtin_args t default arg dim torch cat checkScript t torch zeros t keywords out order torch cat dim= tensors= checkScript t torch zeros t mix const non-const attributes == b = b = torch sum dim=b keepdim=False checkScript t torch zeros test_parser_type_annotations cu = torch jit CompilationUnit foo x Tensor y Tuple Tuple Tensor Tensor Tensor - Tuple Tensor Tensor x x assertExpected str cu foo schema test_parser_type_annotations_comment cu = torch jit CompilationUnit foo x y type Tensor Tuple Tuple Tensor Tensor Tensor - Tuple Tensor Tensor x x assertExpected str cu foo schema test_parser_type_annotations_unknown_type assertRaisesRegex RuntimeError Unknown type name Foo cu = torch jit CompilationUnit foo x Tensor y Tuple Tuple Foo Tensor Tensor - Tuple Tensor Tensor x x test_parser_type_annotations_subscript_non_ident assertRaisesRegex RuntimeError r Subscripted type must type identifier cu = torch jit CompilationUnit foo x Tensor y Tuple Tensor Tensor Tensor - Tuple Tensor Tensor x x test_parser_type_annotations_subscript_tensor assertRaisesRegex RuntimeError r Unknown type constructor Tensor cu = torch jit CompilationUnit foo x Tensor y Tensor Tensor Tensor - Tuple Tensor Tensor x x test_parser_type_annotations_incompatible_expression assertRaisesRegex RuntimeError r Expression type \+ cannot used type expression cu = torch jit CompilationUnit foo x Tensor y Tuple + Tensor - Tuple Tensor Tensor x x test_gather_dynamic_index t x gather = x idx = + gather = x idx gather + gather checkScript t torch zeros test_torch_ignore_conversion_to_none A torch nn Module torch jit ignore ignored int - None l int = len i range i forward - int int = b int = ignored + b B torch nn Module torch jit ignore ignored int l int = len i range i forward - int int = b int = ignored + b modelA = torch jit script A assertEqual modelA modelB = torch jit script B assertEqual modelB test_addmm_grad This test checks several things An expand node inserted before addmm operating bias term The fused form addmm appears ultimate graph s executed A sum op emitted accumulating gradients along th expanded dimension bias term The correct symbolic representation backward pass mm operator emitted x t - mm TODO we should actually check these conditions once we have way dump GraphExecutor state Namely processed forward graph backward graph torch jit script addmm_grad_test b x w torch addmm b x w Initialize param input values w_init = torch rand b_init = torch rand x = torch rand Clone trainable params b = b_init clone b requires_grad_ w = w_init clone w requires_grad_ Test symbolic differentiation y = addmm_grad_test b x w y sum backward clone params autograd reference b_ref = b_init clone b_ref requires_grad_ w_ref = w_init clone w_ref requires_grad_ y_ref = torch addmm b_ref x w_ref y_ref sum backward assertEqual w grad w_ref grad assertEqual b grad b_ref grad unittest skipIf RUN_CUDA running tests cuda verify cudnn fix test_batch_norm_inference_backward_cuda enable_profiling_mode_for_profiling_tests MyBatchNorm torch nn Module __init__ num_features affine track_running_stats super __init__ bn = torch nn BatchNorm d num_features e- affine=affine track_running_stats=track_running_stats float forward x torch Tensor o = bn x o = torch nn functional relu o o batch = c = hw = Initialize param input values x_init = torch randn batch c hw hw dtype=torch float cuda grad = torch randn batch c hw hw dtype=torch float cuda training = False affine = True track_running_stats = True module = torch jit script MyBatchNorm c affine track_running_stats cuda ref_module = MyBatchNorm c affine track_running_stats cuda module eval ref_module eval jit_module = torch jit script module ref_module load_state_dict module state_dict x = x_init detach clone x requires_grad_ x_ref = x_init detach clone x_ref requires_grad_ Test symbolic differentiation Run Forward Backward thrice trigger autodiff graph _ range y = jit_module x y backward grad x grad zero_ module bn running_mean zero_ module bn running_var fill_ ref_module bn running_mean zero_ ref_module bn running_var fill_ run jitted module y = jit_module x y backward grad reference computation y_ref = ref_module x_ref y_ref backward grad assertEqual y_ref y assertEqual x grad x_ref grad assertEqual module bn running_mean ref_module bn running_mean assertEqual module bn running_var ref_module bn running_var test_zeros M torch jit ScriptModule __constants__ = d __init__ - None super __init__ d = torch device cpu torch jit script_method create torch zeros dtype=torch float device=self d layout=torch strided r = M create assertEqual r dtype torch float assertEqual torch zeros dtype=torch float r fn torch zeros checkScript fn test_vararg_zeros foo torch zeros dtype=torch int checkScript foo unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY original version test_rand test_rand test_rand = torch rand + - checkScript test_rand fn = torch jit script test_rand out = fn assertEqual out dtype torch get_default_dtype g = fn graph_for Testing shape analysis correctly setting type GRAPH_EXECUTOR = ProfilingMode SIMPLE FileCheck check Double requires_grad= device=cpu \ check_not Float requires_grad= device=cpu run g torch jit script randint torch randint out = randint assertEqual out dtype torch int GRAPH_EXECUTOR = ProfilingMode SIMPLE FileCheck check Long requires_grad= device=cpu \ check_not Float requires_grad= device=cpu \ check_not Double requires_grad= device=cpu \ run randint graph_for unittest skipIf RUN_CUDA no CUDA unittest skipIf GRAPH_EXECUTOR = ProfilingMode PROFILING skip profiling isn t enabled test_autodiff_complex foo x torch Tensor y torch Tensor W torch Tensor torch exp torch mm torch complex x y W cfloat torch jit script jitted_foo x torch Tensor y torch Tensor W torch Tensor torch exp torch mm torch complex x y W cfloat x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda W = torch randn dtype=torch float device= cuda requires_grad=True W data = enable_profiling_mode_for_profiling_tests _ range assertTrue foo x y W grad_fn None == jitted_foo x y W grad_fn None test_linear_grad enable_profiling_mode_for_profiling_tests t x torch Tensor w torch Tensor b Optional torch Tensor torch nn functional linear x w b x_init = torch randn w_init = torch randn b_init = torch randn grad = torch randn disable_autodiff_subgraph_inlining script module jit_t = torch jit script t x = x_init detach requires_grad_ w = w_init detach requires_grad_ b = b_init detach requires_grad_ x_ref = x_init detach requires_grad_ w_ref = w_init detach requires_grad_ b_ref = b_init detach requires_grad_ profiling optimization runs jit_o = jit_t x w b jit_o backward grad jit_o = jit_t x w b jit_o backward grad x grad zero_ w grad zero_ b grad zero_ jit_o = jit_t x w b jit_o backward grad o = t x_ref w_ref b_ref o backward grad assertEqual jit_o o assertEqual x grad x_ref grad assertEqual w grad w_ref grad assertEqual b grad b_ref grad x grad zero_ w grad zero_ x_ref grad zero_ w_ref grad zero_ jit_o = jit_t x w None jit_o backward grad o = t x_ref w_ref None o backward grad assertEqual jit_o o assertEqual x grad x_ref grad assertEqual w grad w_ref grad skipIfTorchDynamo TorchDynamo doesn t support profile unittest skipIf GRAPH_EXECUTOR = ProfilingMode PROFILING profiling version test_rand test_rand_profiling test_rand = torch rand + - Testing shape analysis correctly setting type enable_profiling_mode_for_profiling_tests num_profiled_runs fn = torch jit script test_rand out = fn graph_str = torch jit last_executed_optimized_graph assertEqual out dtype torch float FileCheck check Float strides= requires_grad= device=cpu \ check_not Double strides= requires_grad= device=cpu run graph_str fn = checkScript test_rand out = fn assertEqual out dtype torch float torch jit script randint torch randint enable_profiling_mode_for_profiling_tests num_profiled_runs out = randint graph_str = torch jit last_executed_optimized_graph assertEqual out dtype torch int FileCheck check profiled_type=Long strides= requires_grad= device=cpu run graph_str test_erase_number_types func b = + + c = + b c += b c graph = torch jit script func graph FileCheck check int = prim Constant check aten add_ run str graph run_pass erase_number_types graph FileCheck check_not int = prim Constant run str graph test_refine_tuple_types TupleConstruct output type correct here graph_str = graph Float b Float c Tensor Tensor = prim TupleConstruct b c graph = parse_ir graph_str torch _C _jit_pass_refine_tuple_types graph After pass output type should ve been updated assertTrue Float Float str graph findNode prim TupleConstruct output TODO henrytu Add test RefineTypes NamedTuple when s supported IR parser test_remove_dropout weight_ _shape = weight_ _shape = input_shape = M torch nn Module __init__ - None super __init__ weight_ = torch nn Parameter torch rand weight_ _shape weight_ = torch nn Parameter torch rand weight_ _shape forward x o = F linear x weight_ o = F dropout o training=self training o = F linear o weight_ o data = torch rand input_shape m = M m = torch jit script m assertRaisesRegex RuntimeError r Dropout removal module training mode yet supported torch _C _jit_pass_remove_dropout m _c m eval ref_res = m data Need inline otherwise we see instances Function We would have use torch linear dropout get around otherwise torch jit _recursive wrap_cpp_module m = wrap_cpp_module torch _C _freeze_module m _c torch _C _jit_pass_remove_dropout m _c res = m data FileCheck check_not aten dropout run str m graph torch testing assert_close ref_res res rtol= e- atol= e- test_unfold_zero_dim fn x x unfold graph = torch jit script fn graph torch _C _jit_pass_complete_shape_analysis graph torch tensor False out_dims = fn torch tensor ndim assertEqual graph findNode aten unfold output type dim out_dims test_mm_batching enable_profiling_mode_for_profiling_tests lstm_cell = torch jit script LSTMCellS lstm x hx cx w_ih w_hh b_ih b_hh i range x size hx cx = lstm_cell x i hx cx w_ih w_hh b_ih b_hh hx slstm = torch jit script lstm inputs = get_lstm_inputs cpu training=True seq_length= slstm inputs profile_and_replay=True sum backward retain_graph=True GRAPH_EXECUTOR == ProfilingMode PROFILING slstm inputs profile_and_replay=True sum backward fw_graph = slstm graph_for inputs GRAPH_EXECUTOR == ProfilingMode LEGACY bw_graph = backward_graph slstm diff_graph_idx= assertTrue prim MMBatchSide str fw_graph assertTrue prim MMTreeReduce str bw_graph sout = slstm inputs out = lstm inputs assertEqual sout out assertEqual torch autograd grad sout sum inputs torch autograd grad out sum inputs test_loop_unrolling fn x y = i range int x y -= i y graph = torch jit script fn graph run_pass loop_unrolling graph unroll_factor = FileCheck check prim Loop check_count aten sub unroll_factor \ check prim Loop check aten sub run str graph checkScript fn torch tensor test_loop_unrolling_const fn y = _ range y -= y fn y = i range y -= i y check fn name graph = torch jit script fn graph run_pass loop_unrolling graph entirely unrolled FileCheck check_not prim Loop run str graph checkScript fn check fn add_const check fn add_iter test_loop_unrolling_nested fn x y = _ range j range int x y -= j y graph = torch jit script fn graph run_pass loop_unrolling graph inner loop subs followed loop epilogue unroll_factor = FileCheck check prim Loop check prim Loop check_count aten sub unroll_factor \ check prim Loop check aten sub run str graph checkScript fn torch tensor test_loop_unroll_unused_counter fn x y = _ range int x y -= y graph = torch jit script fn graph run_pass loop_unrolling graph FileCheck check prim Loop check_not aten add check \ run str graph test_loop_unroll_negative fn x y = _ range int x y += y checkScript fn torch tensor - checkScript fn torch tensor - checkScript fn torch tensor - checkScript fn torch tensor checkScript fn torch tensor checkScript fn torch tensor test_where fn x y torch where x x y checkScript fn torch randn dtype=torch float torch ones dtype=torch float test_where_method fn x y x where x y checkScript fn torch randn dtype=torch float torch ones dtype=torch float test_union_to_number torch jit script fn x Union int complex float y Union int complex float x + y FileCheck check Scalar run fn graph test_reassign_module_lhs assertRaisesRegex RuntimeError Cannot re-assign \ self\ ReassignSelfLHS torch jit ScriptModule torch jit script_method forward x _ range = x ReassignSelfLHS test_reassign_module_rhs assertRaisesRegex RuntimeError Cannot re-assign \ x\ value type module ReassignSelfRHS torch jit ScriptModule torch jit script_method forward x _ range x = ReassignSelfRHS test_unknown_builtin assertRaisesRegex RuntimeError object has no attribute method torch jit script unknown_builtin x x splork test_return_tuple return_tuple x = x x x checkScript return_tuple torch rand test_add_tuple_optional foo input Tuple torch Tensor Optional torch Tensor Optional torch Tensor - Optional torch Tensor changed_input = input + value Tuple torch Tensor Optional torch Tensor Optional torch Tensor = changed_input + input value inp Tuple torch Tensor Optional torch Tensor Optional torch Tensor = torch rand None None checkScript foo inp test_add_tuple_non_optional foo input Tuple torch Tensor torch Tensor torch Tensor - torch Tensor changed_input = input + value Tuple torch Tensor torch Tensor torch Tensor = changed_input + input torch sum value + inp Tuple torch Tensor torch Tensor torch Tensor = torch rand torch rand torch rand checkScript foo inp test_add_tuple_different_types foo Tuple int float b Tuple int - int c Tuple int float int = + b d Tuple int float int int = c + b d + = b = checkScript foo b test_add_tuple_same_types foo Tuple int int b Tuple int int int - int c Tuple int int int int int = + b d Tuple int int int int int int int int = c + b d - = b = checkScript foo b test_method_no_self assertRaisesRegex RuntimeError methods must have argument MethodNoSelf torch jit ScriptModule torch jit script_method noqa B forward noqa B torch zeros MethodNoSelf test_return_stmt_not_at_end return_stmt x bool x x + x checkScript return_stmt torch rand test_for_in_range fn c = i range c += i c checkScript fn test_for_in_range_dynamic fn c = i range acc = j range i acc += j c += acc c checkScript fn optimize=False test_for_in_range_ast test_script_for_in_range_ast c = i range acc = j range i acc += j c += acc c checkScript test_script_for_in_range_ast test_for_in_range_if_ast torch jit script test_script_for_in_range_if_ast x output = x i range i == output = x unsqueeze output = torch cat output x unsqueeze dim= output inputs = _make_scalar_vars torch int assertEqual test_script_for_in_range_if_ast inputs shape test_for_in_range_start_end fn x = i range x += i x checkScript fn test_for_in_range_start_end_step fn start end step type int int int - int x = i range start end step x += i x checkScript fn checkScript fn - checkScript fn - - checkScript fn - checkScript fn checkScript fn - - - test_for_in_range_zero_step torch jit script fn x = i range - x += i x assertRaisesRegex RuntimeError must zero fn test_range_args assertRaisesRegex RuntimeError r range expected least arguments got torch jit script range_no_arg x _ range x += x assertRaisesRegex RuntimeError r found float torch jit script range_non_float i range print i test_parse_empty_tuple_annotation cu = torch jit CompilationUnit foo x Tuple - Tuple x foo_code = cu find_function foo code FileCheck check Tuple check Tuple run foo_code test_parse_empty_tuple_annotation_element_error assertRaisesRegex RuntimeError Tuple literal Tuple type annotation must have any elements cu = torch jit CompilationUnit foo x Tuple int - Tuple int x test_parse_none_type_annotation cu = torch jit CompilationUnit foo x NoneType - NoneType x foo_code = cu find_function foo code FileCheck check NoneType check - NoneType run foo_code test_empty_tuple_str empty_tuple_type = torch _C TupleType g = Tuple typing Tuple python_type = eval empty_tuple_type annotation_str g assert python_type typing Tuple test_tuple_str tuple _type = torch _C TupleType torch _C StringType get assertEqual tuple _type annotation_str Tuple str tuple _type = torch _C TupleType torch _C StringType get torch _C StringType get assertEqual tuple _type annotation_str Tuple str str test_dict_str dict_type = torch _C DictType torch _C StringType get torch _C StringType get assertEqual dict_type annotation_str Dict str str test_none_type_str none_type = torch _C NoneType get g = NoneType type None python_type = eval none_type annotation_str g assert python_type type None skipIfTorchDynamo TorchDynamo fails unknown reason test_zip_enumerate_modulelist Sub torch nn Module forward thing thing - Double torch nn Module forward thing thing zipping over two ZipModLists torch nn Module __init__ mods mods super __init__ mods = mods mods = mods forward x iter = mod mod zip mods mods x = mod mod x iter += x iter ZipWithValues torch nn Module __constants__ = tup_larger tup_smaller __init__ mods mods super __init__ mods = mods mods = mods tup_larger = list range len mods + tup_smaller = list range max len mods + forward x iter = x = x val mod mod zip tup_larger mods mods x = mod mod x + val iter += val mod mod zip tup_smaller mods mods x = mod mod x + val iter += x iter mods = nn ModuleList Double nn ModuleList Double Sub Sub nn ModuleList Sub Double i range len mods j range len mods mod = ZipModLists mods i mods j checkModule mod torch tensor mod = ZipWithValues mods i mods j checkModule mod torch tensor test_enumerate_modlist_range Double torch nn Module forward thing thing Mod torch nn Module __init__ - None super __init__ mods = nn ModuleList Double Double forward x x = x iter = val mod enumerate mods x = mod x val iter += iter x x checkModule Mod torch tensor variable length modulelist Mod Mod forward x val mod zip range int x mods x = mod x val x assertRaisesRegex Exception does have statically determinable length torch jit script Mod modulelist variable length Mod Mod forward x val mod zip mods range int x x = mod x val x assertRaisesRegex Exception does have statically determinable length torch jit script Mod test_for_in_enumerate fn x type List int - int sum = i v enumerate x sum += i v sum checkScript fn fn_enumerate_start_arg x type List int - int sum = i v enumerate x sum += i v sum checkScript fn_enumerate_start_arg fn_enumerate_start_kwarg x type List int - int sum = i v enumerate x start= sum += i v sum checkScript fn_enumerate_start_kwarg fn_nested_enumerate x type List int - int sum = i j v enumerate enumerate x sum += i j v sum checkScript fn_nested_enumerate assertRaisesRegex RuntimeError r enumerate expected least arguments got torch jit script enumerate_no_arg x type List int - int sum = _ enumerate sum += sum assertRaisesRegex RuntimeError r enumerate expected most arguments got torch jit script enumerate_too_many_args x type List int - int sum = _ enumerate x x x sum += sum test_list_comprehension_modulelist Inner torch nn Module forward x x + M torch nn Module __init__ mod_list super __init__ module_list = mod_list forward x out = torch jit annotate List Tensor mod x mod module_list out mod = M nn ModuleList Inner Inner checkModule mod torch tensor mod = M nn ModuleList torch jit script mod M M __init__ mod_list super __init__ mod_list forward x out = mod x mod module_list out mod = M nn ModuleList Inner Inner checkModule mod torch tensor mod = M nn ModuleList defaults List Tensor empty modulelist assertEqual torch jit script mod torch tensor bad_type_annotation out = torch jit annotate int x x noqa C out assertRaisesRegex Exception Expected annotation type List torch jit script bad_type_annotation test_list_comprehension_variable_write i comprehension doesn t write function scope foo i = x = i i = i range noqa C i x assertEqual foo torch jit script foo test_for_in_zip fn x y type List int List int - int sum = i j zip x y sum += i j sum checkScript fn fn_multi_inputs x y z type List int List int List int - int sum = i j k zip x y z sum += i j k sum checkScript fn_multi_inputs fn_nested_zip x y z type List int List int List int - int sum = i j k zip x zip y z sum += i j k sum checkScript fn_multi_inputs assertRaisesRegex RuntimeError r zip expected least arguments got torch jit script zip_no_arg x type List int - int sum = _ zip sum += sum assertRaisesRegex RuntimeError r too many values unpack need found torch jit script fn_nested_zip_wrong_target_assign x y z type List int List int List int - int sum = i j k zip x y z sum += i j k sum test_for_in_zip_enumerate fn_zip_enumerate x y type List int List int - int sum = i j v k zip x enumerate y range sum += i j v k sum checkScript fn_zip_enumerate fn_enumerate_zip x y type List int List int - int sum = i j v enumerate zip x y sum += i j v sum checkScript fn_enumerate_zip test_for_in_tensors test_sizes x sumz = _ x sumz += sumz checkScript test_sizes torch rand checkScript test_sizes torch rand checkScript test_sizes torch rand test_for_in_tensors_rank assertRaisesRegex RuntimeError -d tensor torch jit script test_sizes x sumz = _ x sumz += sumz test_sizes torch tensor test_for_in_tensors_fail_scalar assertRaisesRegex RuntimeError float object iterable torch jit script test_sizes x type float - int sumz = _ x sumz += sumz test_sizes test_for_in_tensors_nested test_sizes x sumz = n x _ n sumz += sumz checkScript test_sizes torch rand avoid defining sum_list multiple tests get_sum_list_fn sum_list type List int - int sum = i sum += i sum sum_list test_sum_list_diff_elms checkScript get_sum_list_fn test_sum_list_empty checkScript get_sum_list_fn test_sum_list_one checkScript get_sum_list_fn test_sum_list_literal sum_list type - int sum = i sum += i sum checkScript sum_list test_sum_list_wrong_type assertRaisesRegex RuntimeError int object iterable torch jit script sum_list type int - int sum = i noqa T sum += i sum sum_list test_list_iterables assertRaisesRegex RuntimeError List iterables supported currently cu = torch jit CompilationUnit list_iterables x i j x += i x += j x test_for_in_string test_strings x type str - str reverse = c x reverse = c + reverse reverse checkScript test_strings hello checkScript test_strings test_list_strings x type List str - str result = sub_str x result += sub_str result checkScript test_list_strings hello world checkScript test_list_strings hello world test_for_in_dict test_dicts x type Dict str int - int sum = key x sum += x key sum checkScript test_dicts b c test_dict_keys_values x type Dict str int - Tuple str int key_str = sum = key x keys key_str += key val x values sum += val key_str sum checkScript test_dicts b c test_for_tuple_unpack for_tuple_unpack x y i j x += i y += j x y checkScript for_tuple_unpack torch tensor torch tensor nested_tuple_unpack x y type List int List int - int sum = i j k v zip x enumerate x y sum += i + j + k + v sum checkScript nested_tuple_unpack test_for_tuple_assign test_simple_assign x type Tuple int float - float sum = x sum += float sum checkScript test_simple_assign test_tuple_assign x type Tuple Tuple int int Tuple int int - int sum = x sum += sum += sum checkScript test_tuple_assign test_single_starred_lhs assertRaisesRegex RuntimeError A Starred expression may only appear lhs within presence another non-starred expression cu = torch jit CompilationUnit single_starred_lhs x = x x x b = b test_singleton_tuple_unpack foo b = b + checkScript foo torch rand test_tuple_assignments var_tuple_assign x y type Tuple Tensor Tensor Tensor - Tensor b c = x y + b + c tuple_inputs = torch randn torch randn checkScript var_tuple_assign tuple_inputs torch randn nested_tuple_assign x y z type int Tuple int Tuple int int Tuple int int - int b c d e f = x y z + b + c + d + e + f checkScript nested_tuple_assign subscript_tuple_assign x i type List int Tensor int - Tuple int Tensor int i x i b = i + x + b checkScript subscript_tuple_assign torch tensor star_tuple_assign type - Tuple int int Tuple int int Tuple int int b c d = b c d checkScript star_tuple_assign subscript_tuple_augmented_assign type Tuple int int - Tuple int int += assertRaisesRegex RuntimeError does support augmented assign scripted_aug_assign = torch jit script subscript_tuple_augmented_assign AttrTupleAssignmentTestClass __init__ int b int = b = b set_ab int b int b = b get - Tuple int int b make_global AttrTupleAssignmentTestClass torch jit script attr_tuple_assignment o AttrTupleAssignmentTestClass int b int o set_ab b o o = AttrTupleAssignmentTestClass assertEqual attr_tuple_assignment o get test_multiple_assign test = b c = d f = side effect ten = torch tensor ten = ten = ten add_ ordering x = y = x y = y x + y b c d f ten ten ten x y checkScript test test_multi_reduction assertRaisesRegex RuntimeError augmented assignment can only have one LHS expression cu = torch jit CompilationUnit multi_reduction x b += x b test_invalid_call_arguments assertRaisesRegex RuntimeError instead found type torch jit script invalid_call_arguments x torch unsqueeze test_invalid_lhs_assignment assertRaisesRegex RuntimeError unexpected expression cu = torch jit CompilationUnit invalid_lhs_assignment x x + = x x test_multi_starred_expr_lhs assertRaisesRegex RuntimeError Only one starred expression allowed lhs cu = torch jit CompilationUnit multi_starred_expr_lhs b c = test_pack_tuple_into_non_var assertRaisesRegex RuntimeError Cannot pack tuple into non-variable cu = torch jit CompilationUnit pack_tuple_into_non_var x = x test_print_kwargs assertRaisesRegex RuntimeError print doesn\ t accept any keyword arguments cu = torch jit CompilationUnit print_kwargs x print x flush=True x test_builtin_use_as_value assertRaisesRegex RuntimeError builtin cannot used value torch jit script builtin_use_as_value x x unsqueeze test_wrong_use_as_tuple assertRaisesRegex RuntimeError cannot used tuple test_fn torch jit script wrong_use_as_tuple b = test_fn test_wrong_attr_lookup assertRaisesRegex RuntimeError attribute lookup defined builtin torch jit script wrong_attr_lookup x = x unsqueeze myattr test_wrong_use_as_callable assertRaisesRegex RuntimeError cannot call value torch jit script wrong_use_as_callable x x test_python_val_doesnt_have_attr assertRaisesRegex RuntimeError object has no attribute abcd torch jit script python_val_doesnt_have_attr has module otherwise attr lookup would allowed first place shutil abcd test_wrong_module_attr_lookup assertRaisesRegex RuntimeError python value type \ type\ cannot used value io torch jit script wrong_module_attr_lookup io BytesIO test_wrong_method_call_inputs assertRaisesRegex RuntimeError Argument y provided SomeModule torch jit ScriptModule torch jit script_method foo x y x torch jit script_method forward x y foo x SomeModule test_single_starred_expr_for_loop assertRaisesRegex RuntimeError A Starred expression may only appear cu = torch jit CompilationUnit test x = x = x + x test_call_ge assertRaisesRegex RuntimeError Expected most arguments found _trace torch zeros foo x x torch jit script test_fn foo torch full torch full torch full test_wrong_return_type assertRaisesRegex RuntimeError instead got value type tuple torch jit ignore somefunc type - Tuple Tuple Tensor Tensor torch zeros torch zeros noqa T torch jit script wrong_return_type somefunc wrong_return_type Tests calling between different front-end modes test_call_python_fn_from_tracing_fn python_fn x torch neg x _trace torch rand traced_fn x python_fn x + The neg op python function should properly inlined graph FileCheck check aten neg run str traced_fn graph test_call_python_mod_from_tracing_fn PythonMod torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand requires_grad=False forward x torch mm x param pm = PythonMod _trace torch rand traced_fn x pm x + Note parameter param Python module inlined into graph assertTrue len list traced_fn graph inputs == FileCheck check aten mm check aten add run str traced_fn graph _tmp_donotuse_dont_inline_everything test_call_traced_fn_from_tracing_fn _trace torch rand traced_fn x torch neg x _trace torch rand traced_fn x traced_fn x + FileCheck check traced_fn check prim CallFunction check aten add \ run str traced_fn graph unittest skip error first mode test_call_traced_mod_from_tracing_fn TracedModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand requires_grad=False forward x torch mm x param tm = torch jit trace TracedModule torch rand assertRaisesRegex RuntimeError must registered submodules _trace torch rand traced_fn x tm x + _tmp_donotuse_dont_inline_everything test_call_script_fn_from_tracing_fn torch jit script script_fn x torch neg x _trace torch rand traced_fn x script_fn x + FileCheck check prim CallFunction check aten add run str traced_fn graph unittest skip error first mode test_call_script_mod_from_tracing_fn assertRaisesRegex RuntimeError must registered submodules ScriptMod torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand requires_grad=False torch jit script_method forward x _ range x += param x sm = ScriptMod _trace torch rand traced_fn x sm x + test_call_python_fn_from_traced_module python_fn x torch neg x TracedModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand forward x torch mm python_fn x param tm = torch jit trace TracedModule torch rand Note parameter param traced module should appear input graph neg op Python function should properly inlined assertTrue len list tm graph inputs == FileCheck check aten neg check aten mm run str tm graph test_call_python_mod_from_traced_module PythonModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand forward x torch mm x param TracedModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand mod = PythonModule forward x mod torch mm x param + tm = torch jit trace TracedModule torch rand FileCheck check_not value= Tensor check aten mm \ check prim CallMethod name= forward check aten add \ run str tm graph FileCheck check aten mm run str tm mod graph test_op_dtype check_equal_and_dtype b assertEqual b assertEqual dtype b dtype fn = torch arange b = torch arange dtype=torch float c = torch arange d = torch arange dtype=torch float e = torch arange f = torch arange dtype=torch float b c d e f scripted_fn = torch jit script fn eager_out = fn script_out = scripted_fn b zip eager_out script_out check_equal_and_dtype b test_floor_div torch jit script foo b type int int - int b i range - j range - j = assertEqual foo i j i j test_floordiv funcs_template = dedent fn ten = a_construct ten_or_scalar = b_construct ten ten_or_scalar torch floor_divide ten ten_or_scalar lhs = torch tensor torch tensor torch tensor rhs = + lhs tensor lhs tensor_or_scalar rhs funcs_str = funcs_template format a_construct=tensor b_construct=tensor_or_scalar scope = execWrapper funcs_str globals scope cu = torch jit CompilationUnit funcs_str f_script = cu fn f = scope fn assertEqual f_script f test_call_python_fn_from_script_fn torch jit ignore python_fn x torch neg x torch jit script script_fn x python_fn x + Note call python_fn appears ` ^python_fn ` called PythonOp interpreter = torch tensor assertEqual script_fn torch tensor FileCheck check python_fn run str script_fn graph test_call_python_mod_from_script_fn PythonModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand forward x torch mm x param pm = PythonModule torch jit script script_fn x pm x + Note call pm x appears ^ python_value trace Parameters NOT inlined FileCheck check python_value check aten add run str script_fn graph _tmp_donotuse_dont_inline_everything test_call_script_fn_from_script_fn torch jit script script_fn x torch neg x torch jit script script_fn x script_fn x + FileCheck check prim CallFunction run str script_fn graph test_call_script_mod_from_script_fn assertRaisesRegex RuntimeError Cannot call ScriptModule submodule caller ScriptMod torch jit ScriptModule torch jit script_method forward x torch mm x torch zeros sm = ScriptMod torch jit script script_fn x sm x + test_call_python_fn_from_script_module torch jit ignore python_fn x torch neg x ScriptMod torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand torch jit script_method forward x python_fn torch mm x param sm = ScriptMod FileCheck check aten mm check python_fn \ run str sm forward graph test_call_python_mod_from_script_module PythonMod torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand torch jit ignore forward x torch mm x param ScriptMod torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand pm = PythonMod torch jit script_method forward x pm torch mm x param sm = ScriptMod Note call into PythonMod appears ^forward Parameters NOT inlined FileCheck check aten mm check forward run str sm graph _tmp_donotuse_dont_inline_everything test_call_script_fn_from_script_module torch jit script script_fn x torch neg x ScriptMod torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand torch jit script_method forward x script_fn torch mm x param sm = ScriptMod graph = sm forward graph FileCheck check aten mm check prim CallFunction run str graph _tmp_donotuse_dont_inline_everything test_call_script_mod_from_script_module ScriptMod torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand torch jit script_method forward x torch mm x param ScriptMod torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand tm = ScriptMod torch jit script_method forward x tm torch mm x param sm = ScriptMod Note parameters both modules should appear flattened input list graph The mm op ScriptMod should properly inlined values graph input lists two mms body FileCheck check_count check check_count mm check prim CallMethod run str sm graph test_module_with_params_called_fails assertRaisesRegex RuntimeError Cannot call ScriptModule submodule caller ScriptMod torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand torch jit script_method forward x torch mm x param sm = ScriptMod torch jit script some_func x sm x test_tuple_index_to_list test_non_constant_input type bool - int b = b = c = c b checkScript test_non_constant_input True checkScript test_non_constant_input False assertRaisesRegex RuntimeError because we cannot resolve output type torch jit script test_non_constant_input type bool - None b = b = c = print c b test_tuple_indexing tuple_index bool b = b = b - b checkScript tuple_index torch tensor checkScript tuple_index torch tensor checkScript tuple_index torch tensor optimize=True tuple_comp = torch jit script tuple_index FileCheck check_count TupleIndex exactly=True run str tuple_comp graph assertRaisesRegex RuntimeError index must integer torch jit script test_indexing_float c = c test_indexing_out_of_bounds_pos c = c checkScriptRaisesRegex test_indexing_out_of_bounds_pos Exception out range test_indexing_out_of_bounds_neg c = c - checkScriptRaisesRegex test_indexing_out_of_bounds_pos Exception out range negative_index tup = tup - checkScript negative_index really_negative_index tup = tup - checkScriptRaisesRegex really_negative_index Exception index out range negative_slice tup = tup - checkScript negative_slice really_slice_out_of_bounds tup = tup - checkScript really_slice_out_of_bounds test_namedtuple_attr f x x max dim= indices + torch max x dim= indices checkScript f torch rand optimize=True assertRaisesRegex RuntimeError object has no attribute method torch jit script g x x max dim= unknown_symbol assertRaisesRegex RuntimeError object has no attribute method torch jit script g x print x x x __doc__ x test_tuple_len torch jit script foo len str None assertEqual foo torch jit script test_indexing_end_out_of_bounds c = c assertEqual test_indexing_end_out_of_bounds test_lower_nested_tuples torch jit script test run_pass constant_propagation test graph FileCheck check prim Constant check_not TupleConstruct run test graph fails tuple can t lowered run_pass lower_all_tuples test graph test_unwrap_optional_builtin test x type Optional int - int x = torch jit _unwrap_optional x x = x + x noqa T x checkScript test assertRaisesRegex AssertionError Unwrapping null optional test None test_script = torch jit script test assertRaisesRegex RuntimeError Unwrapping null optional test_script None torch jit script test_test torch jit _unwrap_optional assertRaisesRegex RuntimeError r could inferred actual type None torch jit script test_no_type type - int torch jit _unwrap_optional None test_indexing_error assertRaisesRegex RuntimeError int object subscriptable torch jit script test_wrong_type = test_unsupported_builtin_error assertRaisesRegex RuntimeError Python builtin built-in function hypot currently torch jit script test_unsupported math hypot test_annotated_script_fn torch jit script foo x y z type Tensor Tuple Tensor Tensor Tensor Tuple Tensor Tuple Tensor Tensor - Tensor x assertExpected str foo schema test_annotated_script_method SM torch jit ScriptModule torch jit script_method forward x y type Tuple Tensor Tensor Tensor - Tuple Tensor Tensor Tensor y y y sm = SM assertExpectedStripMangled str sm forward schema test_annotated_script_fn_return_mismatch assertRaisesRegex RuntimeError actually type torch jit script return_tup x type Tensor - Tuple Tuple Tensor Tensor Tensor x x noqa T test_annotated_script_fn_arg_mismatch assertRaisesRegex RuntimeError r Arguments call valid torch jit script tuple_arg x type Tuple Tensor Tensor - Tensor x + noqa T test_script_non_tensor_args_outputs torch jit script fn x y type Tensor float - float float x + y sum x = torch ones z = fn x assertIsInstance z float assertEqual z unittest skip https github com pytorch pytorch issues test_inline_and_run_annotated_script_fn torch jit script to_inline x y type Tuple Tensor Tensor Tensor - Tensor y torch jit script some_func x to_inline x x x x = torch rand assertEqual some_func x x _make_filereader_test_file filename = tempfile mktemp writer = torch _C PyTorchFileWriter filename buffers = os urandom size size random randint i range offsets = i buf enumerate buffers writer write_record str i buf len buf offsets append i serialized_offsets = pickle dumps offsets writer write_record meta serialized_offsets len serialized_offsets writer write_end_of_file filename buffers serialized_offsets test_file_format_serialization filename buffers serialized_offsets = _make_filereader_test_file reader = torch _C PyTorchFileReader filename serialized_offsets_read = reader get_record meta parsed_serialized_offsets = pickle loads serialized_offsets i offset enumerate parsed_serialized_offsets data = reader get_record str offset assert data == buffers i test_file_reader_no_memory_leak num_iters = filename _ _ = _make_filereader_test_file Load filename tracemalloc start _ range num_iters torch _C PyTorchFileReader filename _ peak_from_string = tracemalloc get_traced_memory tracemalloc stop Load stream tracemalloc start open filename rb f _ range num_iters f seek torch _C PyTorchFileReader f _ peak_from_file = tracemalloc get_traced_memory tracemalloc stop Check peak sizes most differ empirically obtained factor assertLess peak_from_file peak_from_string each type input type annotation corresponding type annotation type_input_return_pairs Tensor Tensor torch Tensor Tensor str str int int bool bool BroadcastingList float List float BroadcastingList int List int List int List int Optional int Optional int replacing code input type pair format_code code pair code format input=pair output=pair Type annotation tests Test combinations String frontend Python AST Frontend Python -style type annotations MyPy-style type comments Script method Script function String frontend Python -style type annotations Script function test_annot_string_py _fn code = foo x input y Tuple Tensor Tensor - Tuple output output x x test_str = pair type_input_return_pairs cu = torch jit CompilationUnit format_code code pair test_str append str cu foo schema assertExpected \n join test_str + \n String frontend Python -style type annotations Script method test_annot_string_py _method TestModule torch jit ScriptModule __init__ - None super __init__ code = foo x input y Tuple Tensor Tensor - Tuple output output x x test_str = pair type_input_return_pairs clear registry we will defining foo multiple times jit_utils clear_class_registry tm = TestModule tm define format_code code pair test_str append str tm foo schema assertExpectedStripMangled \n join test_str + \n String frontend MyPy-style type comments Script function test_annot_string_mypy_fn code = foo x y type input Tuple Tensor Tensor - Tuple output output x x test_str = pair type_input_return_pairs cu = torch jit CompilationUnit format_code code pair test_str append str cu foo schema assertExpectedStripMangled \n join test_str + \n String frontend MyPy-style type comments Script method test_annot_string_mypy_method TestModule torch jit ScriptModule __init__ - None super __init__ code = foo x y type input Tuple Tensor Tensor - Tuple output output x x test_str = pair type_input_return_pairs clear registry we will defining foo multiple times jit_utils clear_class_registry tm = TestModule tm define format_code code pair test_str append str tm foo schema assertExpectedStripMangled \n join test_str + \n Python AST Frontend Python -style type annotations Script function test_annot_ast_py _fn code = dedent typing Tuple List Optional torch Tensor torch jit annotations BroadcastingList BroadcastingList torch torch jit script foo x input y Tuple Tensor Tensor - Tuple output output x x test_str = pair type_input_return_pairs fn = jit_utils _get_py _code format_code code pair foo test_str append str fn schema assertExpectedStripMangled \n join test_str + \n test_multiline_annot_ast_py _fn code = dedent typing Tuple List Optional torch Tensor torch jit annotations BroadcastingList BroadcastingList torch torch jit script foo x type input y type Tuple Tensor Tensor type - Tuple output output x x test_str = pair type_input_return_pairs fn = jit_utils _get_py _code format_code code pair foo args = fn schema arguments returns = fn schema returns assertEqual str args type pair assertEqual str args type Tuple Tensor Tensor assertEqual str returns type f Tuple pair pair test_bad_multiline_annotations assertRaisesRegex RuntimeError Return type line torch jit script bad_type_line type Tensor b type Tensor c type Tensor type int int int - Tensor type bad type line noqa F + b + c assertRaisesRegex RuntimeError Return type line torch jit script bad_return_line type Tensor b c type Tensor type int int int - Tensor + b + c TODO should supported difficult parse assertRaisesRegex RuntimeError Number type annotations torch jit script missing_type type Tensor b c type Tensor type - Tensor + b + c Python AST Frontend Python -style type annotations Script method test_annot_ast_py _method code = dedent typing Tuple List Optional torch Tensor torch jit annotations BroadcastingList \\ BroadcastingList torch FooModule torch jit ScriptModule torch jit script_method foo x input y Tuple Tensor Tensor - Tuple output output x x instance = FooModule test_str = pair type_input_return_pairs fn = jit_utils _get_py _code format_code code pair instance test_str append str fn foo schema assertExpectedStripMangled \n join test_str + \n Python AST Frontend MyPy-style type comments Script function test_annot_ast_mypy_fn code = dedent torch torch jit script foo x y type input Tuple Tensor Tensor - Tuple output output x x test_str = pair type_input_return_pairs fn = jit_utils _get_py _code format_code code pair foo test_str append str fn schema assertExpected \n join test_str + \n Python AST Frontend MyPy-style type comments Script method test_annot_ast_mypy_method code = dedent torch FooModule torch jit ScriptModule torch jit script_method foo x y type input Tuple Tensor Tensor - Tuple output output x x instance = FooModule test_str = pair type_input_return_pairs fn = jit_utils _get_py _code format_code code pair instance test_str append str fn foo schema assertExpectedStripMangled \n join test_str + \n Tests type ignore supported type lines properly ignored test_mypy_type_ignore torch jit script foo x type ignore x torch jit script bar x type ignore no-redef x test_method_casts_script cast_types = byte char double float int long short cast_type cast_types cu = torch jit CompilationUnit f cast_to x x cast_type x = torch rand cu_result = cu cast_to x reference = getattr x cast_type assertEqual cu_result reference test_string_frontend_elif code = func niter type int rv = i range niter i == i == rv += i == rv += i == rv += rv += i rv checkScript dedent code test_module_parameters_and_buffers weights = torch randn bias = torch randn weights = torch randn bias = torch randn TestLinear torch nn Module __init__ in_features out_features super __init__ in_features = in_features out_features = out_features weight = torch nn Parameter torch empty out_features in_features bias = torch nn Parameter torch empty out_features counter = nn Buffer torch ones out_features reset_parameters reset_parameters torch nn init kaiming_uniform_ weight a=math sqrt bias None fan_in _ = torch nn init _calculate_fan_in_and_fan_out weight bound = math sqrt fan_in torch nn init uniform_ bias -bound bound forward input F linear input weight bias + counter Initialize ScriptModule uses weak module above multiple times Strong torch jit ScriptModule __init__ - None super __init__ fc = TestLinear fc weight = torch nn Parameter weights fc bias = torch nn Parameter bias fc = TestLinear fc weight = torch nn Parameter weights fc bias = torch nn Parameter bias torch jit script_method forward x x + fc x + fc x + fc x strong_mod = Strong Run same calculation module inp = torch ones lin = torch nn Linear lin weight = torch nn Parameter weights lin bias = torch nn Parameter bias lin = torch nn Linear lin weight = torch nn Parameter weights lin bias = torch nn Parameter bias expected_result = inp + lin inp + torch ones + lin inp + torch ones assertEqual strong_mod inp expected_result assertExportImportModule strong_mod inp test_module_copying Submodule torch nn Module forward x x + Weak torch nn Module __init__ in_features out_features super __init__ weight = torch nn Parameter torch ones out_features in_features bias = torch nn Parameter torch ones out_features buffer = nn Buffer torch ones out_features submodule = Submodule forward x F linear x weight bias \ + buffer + submodule x Strong torch jit ScriptModule __init__ weak super __init__ weak = weak torch jit script_method forward x weak x inp = torch ones weak_mod = Weak strong_mod = Strong weak_mod assertTrue isinstance strong_mod weak torch jit ScriptModule assertFalse isinstance weak_mod torch jit ScriptModule assertIs strong_mod weak weight weak_mod weight assertIs strong_mod weak buffer weak_mod buffer strong_mod weak submodule has been recursively scripted assertIsNot strong_mod weak submodule weak_mod submodule weak_mod weight data += torch ones assertTrue strong_mod inp allclose weak_mod inp Re-assignment tracked weak_mod weight = torch nn Parameter torch ones assertFalse strong_mod inp allclose weak_mod inp test_backend_cudnn_enabled Only test compiles torch jit script fn x torch backends cudnn enabled x = x + x = x + x test_inplace_add foo b c = + b c add_ b c checkScript foo torch rand torch rand test_add_out foo b c = + b e = torch add c b out=e e checkScript foo torch rand torch rand test_tuple_error_msg fn t Any isinstance t tuple b = t + b assertRaisesRegexWithHighlight RuntimeError Provided tuple fully defined refined t s = torch jit script fn test_augmented_assign foo b += b -= b = b = b b checkScript foo torch rand torch rand test_ignored_props A nn Module __jit_ignored_attributes__ = ignored ignored_return_val property ignored raise ValueError shouldn t called property ignored_return_val torch jit ignore call ignored_return_val f = torch jit script A jank way test there no error assertTrue isinstance f torch jit ScriptModule assertTrue isinstance f call property test_pass foo x type bool - int _ range pass x pass pass checkScript foo True test_lhs_indexing foo b = clone = b checkScript foo torch rand torch rand test_lhs_advanced_indexing_assignment foo x y = torch exp x b = x == b = y b checkScript foo torch ones torch ones test_lhs_advanced_indexing_augmented_assignment foo x y = torch exp x b = x == b += y b checkScript foo torch ones torch ones test_lhs_indexing_list foo b ls = ls = b ls checkScript foo torch rand torch rand test_inplace_copy_script foo x = torch rand copy_ x checkScript foo torch rand test_lhs_indexing_increment foo b += b checkScript foo torch rand torch rand test_lhs_indexing_increment_list foo b = clone ls = b ls += b ls checkScript foo torch rand torch rand test_lhs_indexing_increment_list_prim foo ls = ls += ls checkScript foo test_lhs_indexing_multi foo b = clone foo bar = b foo bar checkScript foo torch rand torch rand test_bool_dispatch torch _jit_internal _disable_emit_hooks TODO Python print broadcasting list kwarg_false x type Tensor - Tensor F max_pool d x return_indices=False checkScript kwarg_false torch randn kwarg_true x type Tensor - Tuple Tensor Tensor F max_pool d x return_indices=True checkScript kwarg_true torch randn full_kwarg_false x type Tensor - Tensor F max_pool d x ceil_mode=False return_indices=False checkScript full_kwarg_false torch randn full_kwarg_true x type Tensor - Tuple Tensor Tensor F max_pool d x ceil_mode=False return_indices=True checkScript full_kwarg_true torch randn use_default x type Tensor - Tensor F max_pool d x checkScript use_default torch randn arg_false x type Tensor - Tensor F max_pool d x False False checkScript arg_false torch randn arg_true x type Tensor - Tuple Tensor Tensor F max_pool d x False True checkScript arg_true torch randn test_infer_size torch _C _infer_size fn x y type Tensor Tensor - List int _infer_size x size y size checkScript fn torch ones torch ones test_hash tester fn inputs x inputs y inputs x == y assertEqual fn x fn y assertNotEqual fn x fn y torch jit script int_hash x type int - int hash x torch jit script float_hash x type float - int hash x torch jit script str_hash x type str - int hash x tester int_hash tester float_hash tester str_hash hello test_id assertRaisesRegex RuntimeError Expected value torch jit script test_id_scalars id == id None torch jit script FooTest __init__ x foo = x getFooTest foo torch jit script test_id_class_types obj = FooTest torch tensor obj = FooTest torch tensor assert obj obj assert id obj = id obj assert id obj = id None True assertTrue test_id_class_types test_mutable_dce torch jit script foo = torch rand += torch rand b = torch rand b += torch rand b should cleaned up FileCheck check_count aten rand exactly=True \ check_count aten add exactly=True run str foo graph test_mutable_dce_block torch jit script foo = torch rand += torch rand b = torch rand bool torch zeros b += torch rand += torch rand should cleaned up b b FileCheck check prim If check_count aten rand exactly=True \ run str foo graph test_mutable_dce_graph_input torch jit script foo += torch rand shouldn t clean up ` ` even though s used output FileCheck check aten rand check aten add run str foo graph test_mutable_dce_list torch jit script foo l = l append c = l b = torch rand c += torch rand b c does get cleaned up because there wildcard + mutation FileCheck check_count aten rand exactly=True run str foo graph test_mutable_dce_loop torch jit script foo l = l append i = b = torch rand while i dead = torch rand c = l c += torch rand i += b FileCheck check prim Loop check_not aten rand check aten __getitem__ \ check_count aten rand exactly=True run str foo graph test_mutable_dce_indirect_wildcards fn x = torch ones x_ = x view - l = l append x_ x_view = l x add_ torch ones x_view checkScript fn test_mutable_dce_indirect_wildcard_write fn indexes = torch jit annotate List Tensor word_ids = torch zeros dtype=torch int word_ids = indexes append word_ids word_ids checkScript fn test_mutable_dce_wildcards fn x = torch ones l = l append x x_view = l x add_ torch ones x_view checkScript fn profiling=ProfilingMode SIMPLE test_cpp_function_tensor_str x = torch randn scale = torch randn requires_grad=True shift = torch randn requires_grad=True torch jit script fn x scale shift scale x + shift capture_stdout captured print fn x scale shift test_string_index fn x type str x x - checkScript fn abcde test_ord fn x type str - int ord x checkScript fn h checkScript fn y index_str_to_tensor s type str - Tensor torch tensor ord s noqa T s = \u encode checkScript index_str_to_tensor s test_chr fn x type int - str chr x checkScript fn checkScript fn test_round round_float x type float - float round x round_int x type int - float round x checkScript round_float checkScript round_int test_convert_base test_hex x type int - str hex x test_oct x type int - str oct x test_bin x type int - str bin x numbers = - - n numbers checkScript test_bin n checkScript test_oct n checkScript test_hex n unittest skipIf IS_SANDCASTLE NYI TemporaryFileName support Sandcastle test_get_set_state Root torch jit ScriptModule __constants__ = number __init__ number super __init__ buffer = nn Buffer torch ones buffer = nn Buffer torch ones number = number torch jit script_method __getstate__ buffer buffer training torch jit script_method __setstate__ state buffer = state + buffer = state + training = state M torch jit ScriptModule __constants__ = number __init__ number submodule super __init__ buffer = nn Buffer torch ones buffer = nn Buffer torch ones number = number submodule = submodule torch jit script_method __getstate__ buffer buffer submodule training torch jit script_method __setstate__ state buffer = state + buffer = state + submodule = state training = state TemporaryFileName fname m = M submodule=Root m save fname loaded = torch jit load fname Check original module assertEqual m buffer torch ones assertEqual m buffer torch ones Check top level module assertEqual loaded buffer torch ones + assertEqual loaded buffer torch ones + Check submodule assertEqual loaded submodule buffer torch ones + assertEqual loaded submodule buffer torch ones + Check simpler module NoArgState torch nn Module __init__ - None super __init__ buffer = nn Buffer torch ones buffer = nn Buffer torch ones forward pass torch jit export __getstate__ training torch jit export __setstate__ state buffer = torch ones + state buffer = torch ones + training = state TemporaryFileName fname m = torch jit script NoArgState m save fname loaded = torch jit load fname assertEqual loaded buffer torch ones + assertEqual loaded buffer torch ones + test_string_slicing fn x type str - str x fn x type str - str x - fn x type str - str x fn x type str - str x checkScript fn abcdefghi checkScript fn abcdefghi checkScript fn abcdefghi checkScript fn abcdefghi test_early_return_closure code = dedent tanh output = torch tanh backward grad_output pass output backward cu = torch jit CompilationUnit code g = cu tanh graph FileCheck check_count prim Closure_ check NoneType = prim Constant \ check_next run g code = dedent tanh output = torch tanh backward grad_output = output = output backward cu = torch jit CompilationUnit code g = cu tanh graph FileCheck check_count prim Closure_ check int = prim If \ run g code = dedent loop_in_closure output = torch tanh backward grad_output i range output backward cu = torch jit CompilationUnit code fc = FileCheck fc check prim Closure check Tensor NoneType = prim TupleConstruct Loop then two s added exit transform fc check prim Closure check prim Loop check_count prim If fc run cu loop_in_closure graph code = dedent tanh output = torch tanh backward grad_output == output backward assertRaisesRegex RuntimeError returned value type int cu = torch jit CompilationUnit code _inline_everything test_early_return_fork_join torch jit script foo x x dim == torch neg x x torch neg x x + x = torch rand torch jit script wait_script x fut = torch jit _fork foo x y_hat = foo x y = torch jit _wait fut y y_hat FileCheck check prim fork check prim If check \ run wait_script graph test_early_return_type_refinement torch jit script test x type Optional int - int x None x assertEqual test None assertEqual test test_exceptions_with_control_flow test_num_ifs func num_ifs g = torch jit script func graph FileCheck check_count prim If num_ifs exactly=True run g no_guard_ifs_added x type int - int x == x == raise RuntimeError hi raise RuntimeError hi checkScript no_guard_ifs_added checkScriptRaisesRegex no_guard_ifs_added Exception test_num_ifs no_guard_ifs_added FUNCTION LOOKS LIKE graph x int str = prim Constant value= Exception int = prim Constant value= int = prim Constant value= int = prim Uninitialized bool = aten eq x int = prim If block - block bool = aten eq x = prim If block = prim RaiseException - block = prim RaiseException - - no_ifs_added x type int - int x raise RuntimeError hi x checkScript no_ifs_added checkScriptRaisesRegex no_ifs_added - Exception test_num_ifs no_ifs_added test_if_might x type int x x == = raise RuntimeError hi + checkScript test_if_might checkScript test_if_might checkScriptRaisesRegex no_ifs_added - Exception test_num_ifs test_if_might one added guard + test_loop_no_escape x type int x = _ range x raise RuntimeError hi x + checkScript test_loop_no_escape checkScript test_loop_no_escape - checkScriptRaisesRegex test_loop_no_escape Exception guard gets optimized away test_num_ifs test_loop_no_escape test_loop_exception_with_continue x type int i = i range i == x raise RuntimeError hi continue print i i + checkScript test_loop_exception_with_continue - checkScriptRaisesRegex test_loop_exception_with_continue Exception test_num_ifs test_loop_exception_with_continue no ifs added guard print test_exception_exits_closure code = dedent no_return_func type Tensor - Tensor output = torch tanh backward grad_output raise RuntimeError Hi assertRaisesRegex RuntimeError does along all cu = torch jit CompilationUnit code code = dedent test_exit_pair_reset x type int - int x = backward grad_output raise RuntimeError Hi = + x + func = torch jit CompilationUnit code test_exit_pair_reset assertEqual func assertEqual func - - final + gets inlined into first branch optimized away FileCheck check_count prim If exactly=True run func graph test_non_final_return simple x bool x x + x + raise RuntimeError nope nest x x = x + bool x bool x x += x + x + early_ret x x = x + bool x x + x = x + x + nest_early_ret x x = x + bool x bool x x + x + x = x + x + not_early_ret x s = bool x bool x s s += foo s += s += hi s not_total_ret x s = bool x bool x s s s += s i range func simple nest early_ret nest_early_ret not_early_ret not_total_ret checkScript func torch tensor + i vars_used_after_ret x type int - int x == x y = z = x + y z checkScript vars_used_after_ret checkScript vars_used_after_ret complicated x type int - int x x == assert == x == assert == = b = = b = + b assert == i range checkScript complicated i test_partial_returns assertRaisesRegex RuntimeError does along all torch jit script no_ret type - int pass assertRaisesRegex RuntimeError does along all torch jit script partial x type Tensor - int x assertRaisesRegex RuntimeError does along all torch jit script typed_none type - Optional int pass torch jit script none_ret pass assertIs none_ret None FileCheck check None run none_ret graph test_early_returns_loops nest_while_ret x type int - int y = while x x y y = y + break y = y + y = y + y checkScript nest_while_ret checkScript nest_while_ret checkScript nest_while_ret loop_ret x y type int int - int i = i range x x == y x + y i = i + y i = i - i checkScript loop_ret checkScript loop_ret checkScript loop_ret test_will_ret y type int - int _ range y checkScript test_will_ret checkScript test_will_ret test_loop_nest_ret y type int - int _ range y _ range y - checkScript test_loop_nest_ret checkScript test_loop_nest_ret checkScript test_loop_nest_ret test_nn_init tests = constant_ lambda torch ones Tensor float ones_ lambda torch ones Tensor zeros_ lambda torch ones Tensor uniform_ lambda torch ones Tensor normal_ lambda torch ones Tensor xavier_normal_ lambda torch ones Tensor xavier_uniform_ lambda torch ones Tensor name args_fn type_str tests Build test code arg_str = join chr i + ord i range len args_fn code = dedent test arg_str type type_str torch nn init name arg_str format arg_str=arg_str type_str=type_str name=name cu = torch jit CompilationUnit code Compare functions init_fn = getattr torch nn init name script_out = runAndSaveRNG cu test args_fn eager_out = runAndSaveRNG init_fn args_fn assertEqual script_out eager_out FileCheck check_not prim PythonOp run cu test graph test_nn_init_generator init_fns = uniform_ normal_ xavier_normal_ xavier_uniform_ name init_fns Build test code code = dedent test tensor generator type Tensor Generator torch nn init name tensor generator=generator format name=name cu = torch jit CompilationUnit code Compare functions init_fn = getattr torch nn init name torch manual_seed g = torch Generator g manual_seed script_out = cu test torch ones g Change seed default generator make sure we re using provided generator torch manual_seed g = torch Generator g manual_seed eager_out = init_fn torch ones generator=g assertEqual script_out eager_out FileCheck check_not prim PythonOp run cu test graph test_parse_generator _test_parse_generator seed jit_graph = parse_ir f graph float = prim Constant value=- float = prim Constant value= Generator = prim Constant value=torch Generator device= cpu seed= seed NoneType = prim Constant int = prim Constant value= int = prim Constant value= Device = prim Constant value= cpu Tensor = aten empty Float = aten uniform node = next n n jit_graph nodes isinstance n output type torch _C _GeneratorType assert isinstance node output type torch _C _GeneratorType g = node ival value assert isinstance g torch Generator assertEqual g initial_seed seed _test_parse_generator _test_parse_generator - assertRaisesRegex RuntimeError Seed must non-negative integer _test_parse_generator - assertRaisesRegex RuntimeError Number too big _test_parse_generator test_early_return_rewrite test_foo x bool x checkScript test_foo True checkScript test_foo False FileCheck check_count prim If exactly=True run torch jit script test_foo graph test_multiple x int x == x x y = x z = y z == z = z = z - abc = z = z abc z z z checkScript test_multiple checkScript test_multiple checkScript test_multiple checkScript test_multiple checkScript test_multiple graph = torch jit script test_multiple graph FileCheck check_count prim If exactly=True run graph test_is_scripting_metacompile torch jit script foo torch jit is_scripting print hello + will compiled assertEqual foo test_boolean_literal_constant_metacompile Mod torch nn Module __constants__ = val __init__ val super __init__ val = val forward val checkModule Mod True checkModule Mod False torch jit script foo True assertEqual foo test_assert_is_scripting_metacompile foo assert torch jit is_scripting TestErrorMsg print hello + will compiled f = torch jit script foo assertRaisesRegex torch jit Error TestErrorMsg f test_isinstance_metacompile torch jit script test_primitive_type x type int - int isinstance x int x + x - assertEqual test_primitive_type assertRaisesRegex Exception Expected value type test_primitive_type _MyNamedTuple = namedtuple _MyNamedTuple value torch jit script test_non_primitive_types x type _MyNamedTuple - Tensor isinstance _MyNamedTuple isinstance x _MyNamedTuple x value + out = test_non_primitive_types _MyNamedTuple value=torch tensor assertEqual out torch tensor test_namedtuple_type_inference _AnnotatedNamedTuple = NamedTuple _NamedTupleAnnotated value int noqa UP _UnannotatedNamedTuple = namedtuple _NamedTupleUnAnnotated value test_check_named_tuple_value named_tuple = _AnnotatedNamedTuple named_tuple value checkScript test_check_named_tuple_value test_error _UnannotatedNamedTuple assertRaisesRegex RuntimeError r Expected value type \ Tensor \ inferred\ \ r argument \ value\ instead found type \ int\ torch jit script test_error test_namedtuple_default_values_simple_type Point NamedTuple x Optional int = None y int = make_global Point M torch nn Module forward point Point point p = Point x= y= checkModule M p checkModule M Point m = torch jit script M FileCheck check r NamedTuple x int = None y int = \ run m graph test_namedtuple_default_values_missing Point NamedTuple x Optional int y int z int = make_global Point M torch nn Module forward point Point point p = Point x= y= p = Point x= y= z= checkModule M p checkModule M p m = torch jit script M FileCheck check r NamedTuple x int y int z int = \ run m graph test_namedtuple_default_values_container_type Point NamedTuple x Optional List int = None y List int = z Optional Dict str int = make_global Point M torch nn Module forward point Point point p = Point x= y= z= b checkModule M p checkModule M Point m = torch jit script M first_line = r NamedTuple x int = None y int = \ r z Dict str int = FileCheck check first_line \ run m graph test_namedtuple_default_values_Tensor_type Point NamedTuple x torch Tensor = torch rand make_global Point M torch nn Module forward point Point point p = Point x=torch rand assertRaisesRegex RuntimeError Tensors supported default NamedTuple fields m = torch jit script M m p test_namedtuple_default_values_using_factory_constructor Pair = namedtuple Pair x y defaults= make_global Pair torch jit script fn x Pair - Pair x TODO We can t use ` checkScript ` NamedTuple factory constructor Using factory constructor TorchScript TorchScript creates anonymous ` NamedTuple ` instead preserving actual name For example actual generated signature case graph x NamedTuple x Tensor y Tensor It looks like similar test cases have had issue well see ` test_namedtuple_python ` FileCheck check r NamedTuple x Tensor = y Tensor = \ check_next r x \ run fn graph test_isinstance_dynamic torch jit script foo type Optional List int - int b = isinstance int float list str b += isinstance int str b += isinstance List int b += b assertEqual foo assertEqual foo None test_function_overloads TODO pyflakes currently does compose overload annotation other decorators This fixed master version Next version update remove noqa add typing overload annotation torch jit _overload noqa F test_simple x noqa F type int - int pass torch jit _overload noqa F test_simple x noqa F type float - float pass test_simple x noqa F x invoke_function test_simple test_simple checkScript invoke_function testing functions cached compiled_fns_ = torch jit _script _get_overloads test_simple compiled_fns_ = torch jit _script _get_overloads test_simple b zip compiled_fns_ compiled_fns_ assertIs graph b graph old_func = test_simple testing new functions added work caching torch jit _overload noqa F test_simple x noqa F type str - str pass torch jit script my_func old_func hi testing new function same qualified name torch jit _overload noqa F test_simple b noqa F type int int - int pass test_simple b + b torch jit script fn test_simple assertEqual fn currently we take default values have specified overload well - TODO take them implementation apply where type valid torch jit _overload noqa F identity x noqa F type str - str pass torch jit _overload noqa F identity x noqa F type float - float pass identity x = noqa F x invoke identity identity identity hi checkScript invoke schema_match_failure identity thrown = False try torch jit script schema_match_failure except Exception e thrown = True assertTrue r type str str e r type float str e assertTrue thrown assertRaisesRegex Exception cannot directly compiled torch jit script identity torch jit _overload noqa F impl_compile_failure x y noqa F type str str - str pass torch jit _overload noqa F impl_compile_failure x y noqa F type int int - int pass impl_compile_failure x y noqa F x - y test impl_compile_failure one two assertRaisesRegex Exception Arguments call valid torch jit script test torch jit _overload noqa F good_overload x= noqa F type int - int pass good_overload x= noqa F x torch jit script foo good_overload assertEqual foo assertRaisesRegex Exception must equal default parameter torch jit _overload noqa F bad_default_on_overload x y= noqa F type int int - int pass bad_default_on_overload x y= noqa F type int int - int pass torch jit script test bad_default_on_overload torch jit _overload noqa F diff_default x noqa F type int - int pass torch jit _overload noqa F diff_default x noqa F type str - str pass diff_default x= hi noqa F x test diff_default diff_default diff_default abc assertEqual test torch jit script test torch jit _overload noqa F diff_num_params x noqa F type float - float pass torch jit _overload noqa F diff_num_params x y noqa F type int int - int pass diff_num_params x y= z= noqa F type Union float int int int x + y + z test diff_num_params diff_num_params diff_num_params diff_num_params assertEqual test torch jit script test torch jit _overload noqa F diff_num_params_no_annot type - int pass diff_num_params_no_annot x= noqa F x test diff_num_params_no_annot assertRaisesRegex Exception Parameters specified torch jit script test test_function_overload_misuse assertRaisesRegex RuntimeError Only ` pass ` statement ` ` can body torch jit _overload wrong_decl_body x str - str x + assertRaisesRegex RuntimeError Only ` pass ` statement ` ` can body MyClass torch jit _overload_method method torch jit _overload null_overload x int - int noqa E torch jit _overload noqa F null_overload x str - str noqa F pass null_overload_driver null_overload assertRaisesRegex RuntimeError Implementation function + missing torch jit script null_overload_driver OverloadMisuse torch nn Module torch jit _overload_method forward x int pass torch jit _overload_method noqa F forward x Tensor noqa F pass assertRaisesRegex RuntimeError Implementation method + missing m = torch jit script OverloadMisuse test_script_method_torch_function_overload MyCustomTensor torch Tensor pass MyCustomModule torch nn Module forward x torch relu x scripted_mod = torch jit script MyCustomModule t = torch tensor ref_out = scripted_mod t t_custom = MyCustomTensor out = scripted_mod t_custom assertEqual out ref_out out = scripted_mod forward t_custom assertEqual out ref_out test_function_overloading_isinstance torch jit _overload noqa F my_conv x y noqa F type float str - float pass torch jit _overload noqa F my_conv x y noqa F type float float - float pass my_conv x y= noqa F isinstance y str y == hi - x - x + x test_uses my_conv my_conv hi my_conv checkScript test_uses test_method_overloading Over torch nn Module torch jit _overload_method noqa F forward x noqa F type Tuple Tensor Tensor - Tensor pass torch jit _overload_method noqa F forward x noqa F type Tensor - Tensor pass forward x noqa F isinstance x Tensor x + x + S torch jit ScriptModule __init__ - None super __init__ weak = Over torch jit script_method forward x weak x + weak x x s_mod = S x = torch ones assertEqual s_mod x x + + + x over = Over assertEqual over x x x + assertEqual over x x + Unannotated torch nn Module torch jit _overload_method noqa F hello x noqa F pass torch jit _overload_method noqa F hello x noqa F type int - int pass hello x noqa F x + forward hello hello w = Unannotated assertRaisesRegex Exception explicitly add type annotations overloaded functions torch jit script w CompileOverloadError torch nn Module torch jit _overload_method noqa F hello x noqa F type str - int pass torch jit _overload_method noqa F hello x noqa F type int - int pass hello x noqa F x + forward hello hi hello w = CompileOverloadError assertRaisesRegex Exception instead found type str torch jit script w testing overload declared first then non-overload sys version_info test broken assertRaisesRegex Exception Overloads usable when module W torch nn Module torch jit _overload_method noqa F forward x noqa F type int - int pass torch jit _overload_method noqa F forward x noqa F type Tensor - Tensor pass forward x noqa F x + = W b = torch jit script W torch nn Module forward x noqa F x + + = W b = torch jit script testing non-overload declared first then overload W torch nn Module hello x x x + x forward x hello x x = torch jit script W assertEqual torch tensor torch tensor W torch nn Module torch jit _overload_method noqa F hello x noqa F pass torch jit _overload_method noqa F hello x noqa F type int - int pass hello x noqa F x + + forward x hello hello x sys version_info test broken assertRaisesRegex Exception Overloads usable when module = torch jit script W test_narrow_copy foo narrow_copy checkScript foo torch rand test_select_after_chunk foo x chunked = torch chunk x foo = chunked foo add_ x checkScript foo torch rand test_nn_LSTM_with_layers M torch jit ScriptModule __init__ - None super __init__ rnn = nn LSTM dropout= torch jit script_method forward x lengths h c rnn x h c Eager torch nn Module __init__ - None super __init__ rnn = nn LSTM dropout= forward x lengths h c rnn x h c inputs = torch randn torch LongTensor torch randn torch randn eager_out = runAndSaveRNG lambda Eager inputs script_out = runAndSaveRNG lambda M inputs assertEqual eager_out script_out test_nn_LSTM input = torch nn utils rnn pack_sequence torch randn S torch jit ScriptModule __init__ - None super __init__ x = torch nn LSTM torch jit script_method forward input PackedSequence - Tuple PackedSequence Tuple torch Tensor torch Tensor x input eager_out = runAndSaveRNG lambda x torch nn LSTM x input script_out = runAndSaveRNG lambda x S x input assertEqual eager_out script_out test_nn_GRU seq_input = torch nn utils rnn pack_sequence torch randn tensor_input = torch randn SeqLengthGRU torch jit ScriptModule __init__ - None super __init__ x = torch nn GRU torch jit script_method forward input PackedSequence - Tuple PackedSequence torch Tensor x input TensorGRU torch jit ScriptModule __init__ - None super __init__ x = torch nn GRU torch jit script_method forward input torch Tensor - Tuple torch Tensor torch Tensor x input seq_eager_out = runAndSaveRNG lambda x torch nn GRU x seq_input seq_script_out = runAndSaveRNG lambda x SeqLengthGRU x seq_input tensor_eager_out = runAndSaveRNG lambda x torch nn GRU x tensor_input tensor_script_out = runAndSaveRNG lambda x TensorGRU x tensor_input assertEqual seq_eager_out seq_script_out assertEqual tensor_eager_out tensor_script_out test_torchscript_memoryformat torch jit script fn x x contiguous memory_format=torch channels_last x = torch randn y = fn x assertTrue y is_contiguous memory_format=torch channels_last test_torchscript_multi_head_attn torch jit script jit_multihead_attn_forward query type Tensor key type Tensor value type Tensor embed_dim_to_check type int num_heads type int in_proj_weight type Tensor in_proj_bias type Tensor bias_k type Optional Tensor bias_v type Optional Tensor add_zero_attn type bool dropout type float out_proj_weight type Tensor out_proj_bias type Tensor training=True type bool key_padding_mask=None type Optional Tensor need_weights=True type bool attn_mask=None type Optional Tensor type - Tuple Tensor Optional Tensor torch nn functional multi_head_attention_forward query key value embed_dim_to_check num_heads in_proj_weight in_proj_bias bias_k bias_v add_zero_attn dropout out_proj_weight out_proj_bias training key_padding_mask need_weights attn_mask src_l = bsz = embed_size = nhead = multi_head_attn = torch nn MultiheadAttention embed_size nhead query = torch rand src_l bsz embed_size key = torch rand src_l bsz embed_size value = torch rand src_l bsz embed_size mask = torch triu torch ones src_l src_l == transpose mask = mask float masked_fill mask == float -inf masked_fill mask == torch get_default_dtype jit_out = jit_multihead_attn_forward query key value embed_size nhead multi_head_attn in_proj_weight multi_head_attn in_proj_bias multi_head_attn bias_k multi_head_attn bias_v multi_head_attn add_zero_attn multi_head_attn dropout multi_head_attn out_proj weight multi_head_attn out_proj bias attn_mask=mask py_out = torch nn functional multi_head_attention_forward query key value embed_size nhead multi_head_attn in_proj_weight multi_head_attn in_proj_bias multi_head_attn bias_k multi_head_attn bias_v multi_head_attn add_zero_attn multi_head_attn dropout multi_head_attn out_proj weight multi_head_attn out_proj bias attn_mask=mask print rel error print jit_out py_out - assertEqual jit_out py_out atol= e- rtol= e- test_torchscript_multi_head_attn_fast_path src_l = bsz = embed_size = nhead = multi_head_attn = torch nn MultiheadAttention embed_size nhead batch_first=True multi_head_attn = multi_head_attn eval query = key = value = torch rand bsz src_l embed_size torch no_grad py_out = multi_head_attn query key value mha = torch jit script multi_head_attn jit_out = mha query key value torch testing assert_close jit_out py_out unittest skipIf RUN_CUDA no CUDA test_scriptmodule_multi_head_attn_cuda MyModule torch jit ScriptModule __init__ embed_dim num_heads super __init__ sample_q = torch randn embed_dim sample_kv = torch randn embed_dim attention = nn MultiheadAttention embed_dim num_heads attention eval mod = torch jit trace attention sample_q sample_kv sample_kv torch jit script_method forward q k v mod q k v embed_dim = num_heads = sl = bs = model = MyModule embed_dim num_heads cuda q = torch randn sl bs embed_dim device= cuda kv = torch randn sl bs embed_dim device= cuda jit_out = model q kv kv py_out = torch nn functional multi_head_attention_forward q kv kv embed_dim num_heads model mod in_proj_weight model mod in_proj_bias None None None model mod out_proj weight model mod out_proj bias assertEqual jit_out py_out atol= e- rtol= e- unittest skipIf RUN_CUDA no CUDA test_scriptmodule_transformer_cuda MyModule torch jit ScriptModule __init__ transformer sample_q sample_kv super __init__ transformer eval mod = torch jit trace transformer sample_q sample_kv torch jit script_method forward q k mod q k d_model = nhead = num_encoder_layers = num_decoder_layers = dim_feedforward = bsz = seq_length = tgt_length = torch no_grad src = torch randn seq_length bsz d_model tgt = torch randn tgt_length bsz d_model transformer = nn Transformer d_model nhead num_encoder_layers num_decoder_layers dim_feedforward dropout= model = MyModule transformer tgt src src = torch randn seq_length bsz d_model tgt = torch randn tgt_length bsz d_model jit_out = model tgt src py_out = transformer tgt src print jit_out py_out- print torch allclose jit_out py_out atol= e- rtol= e- assertEqual jit_out py_out atol= e- rtol= e- test_list_python_op python_list_op lst type List Tensor - Tensor lst fn lst type List Tensor - Tensor python_list_op lst checkScript fn torch ones + torch ones unittest skipIf RUN_CUDA no CUDA test_weak_cuda M torch jit ScriptModule __init__ - None super __init__ lstm = torch nn LSTM lstm cuda torch jit script_method forward x lstm x m = M m cuda out = m torch ones cuda assertTrue out is_cuda test_ignore_decorator warnings catch_warnings record=True warns M torch jit ScriptModule __init__ - None super __init__ tensor = torch zeros requires_grad=False some_state = nn Buffer torch nn Parameter tensor torch jit script_method forward x ignored_code x x torch jit ignore drop_on_export=True ignored_code x some_state = torch tensor FileCheck check TorchScript will now drop function run str warns Assert ignored code run m = M m = getExportImportCopy m pp = str m forward code assertNotIn ignored_code pp assertRaisesRegex torch jit Error annotated ignored cannot run m forward torch ones test_ignored_as_value Model nn Module torch jit unused tuple_ignored x type Tensor - Tuple Tensor Tensor x x torch jit unused single_val_ignored x y type Tensor Tensor - Tensor x forward x use_ignore_path type Tensor bool - Tuple Tensor Tensor == tuple_ignored x use_ignore_path single_val_ignored x x single_val_ignored x x x x original = Model scripted = torch jit script original assertEqual scripted torch tensor False torch tensor torch tensor buffer = io BytesIO torch jit save scripted buffer buffer seek loaded = torch jit load buffer assertRaisesRegex torch jit Error annotated ignored cannot run loaded torch tensor True test_module_error MyModule torch nn Module forward foo foo assertRaisesRegex RuntimeError cannot compiled since inherits nn Module torch jit script MyModule test_view_write fn x y l = l append x x_view = l = x + x x_view add_ y b = x + x == b checkScript fn torch rand torch rand test_module_attrs M torch jit ScriptModule __init__ table super __init__ table = torch jit Attribute table Dict str torch Tensor x = torch nn Parameter torch tensor torch jit script_method forward key type str - Tensor table key + x torch _jit_internal _disable_emit_hooks TODO re-enable module hook when Python printing attributes supported m = M char torch ones + ord char - ord char abcdefg assertEqual m c torch tensor test_module_none_attrs MyMod torch jit ScriptModule __init__ - None super __init__ optional_value = None torch jit script_method forward optional_value graph = MyMod forward graph FileCheck check prim GetAttr run graph run_pass peephole graph FileCheck check_not prim GetAttr run graph test_tensor_import_export torch jit script foo x = torch tensor b = torch tensor c = b c run_pass constant_propagation foo graph m = createFunctionFromGraph foo graph getExportImportCopy m get_pickle_values dict I am test test Dict str str float float int int bool False bool tuple Tuple int int int int list List Tuple int int tensor torch randn torch Tensor int_list List int tensor_list torch ones + i i range List torch Tensor bool_list True True False True List bool float_list List float str_list hello bye List str none None Optional int a_device torch device cpu torch device another_device torch device cuda torch device test_attribute_serialization tester = M torch jit ScriptModule __init__ - None super __init__ name value the_type tester get_pickle_values setattr name torch jit Attribute value the_type torch jit script_method forward dict float int bool tuple list int_list tensor_list bool_list float_list str_list none m = M imported_m = getExportImportCopy m assertEqual m imported_m test_string_len fn x type str - int len x checkScript fn checkScript fn h checkScript fn hello test_multiline_optional_future_refinement torch jit script fun - int future Optional torch jit Future Tuple torch Tensor = None assertEqual fun unittest skipIf IS_SANDCASTLE NYI TemporaryFileName support Sandcastle test_attribute_unpickling tensor = torch randn tester = M torch jit ScriptModule __init__ - None super __init__ name value the_type tester get_pickle_values setattr _ + name torch jit Attribute value the_type torch jit script_method forward _dict _float _int _bool _tuple _list _int_list _tensor_list _bool_list _float_list _str_list _none TemporaryFileName fname M save fname loaded = torch jit load fname is_tensor_value item isinstance item torch Tensor True isinstance item list is_tensor_value item False name value _the_type get_pickle_values is_tensor_value value continue assertEqual value getattr loaded _ + name test_submodule_attribute_serialization S torch jit ScriptModule __init__ list_data super __init__ table = torch jit Attribute I am test test Dict str str list = torch jit Attribute list_data List Tuple int int torch jit script_method forward table list M torch jit ScriptModule __init__ - None super __init__ table = torch jit Attribute different dict Dict str str tensor = torch jit Attribute torch randn torch Tensor s = S s = S torch jit script_method forward table tensor s table s list s list m = M imported_m = getExportImportCopy m assertEqual m imported_m test_serialization_big_ints M torch jit ScriptModule __init__ - None super __init__ int _max = torch jit Attribute - int int _min = torch jit Attribute - int uint _max = torch jit Attribute int int _max = torch jit Attribute - int int _min = torch jit Attribute - int tensor = torch nn Parameter torch ones torch jit script_method forward x type int - int x + int _max + int _min + int _max + int _min m = M imported = getExportImportCopy m assertEqual m imported assertEqual m int _max imported int _max assertEqual m int _min imported int _min assertEqual m uint _max imported uint _max assertEqual m int _max imported int _max assertEqual m int _min imported int _min test_script_scope scripted = torch jit script torch nn functional triplet_margin_loss test_serialization_sharing M torch jit ScriptModule __init__ - None super __init__ list = torch jit Attribute List str torch jit script_method forward key type str - List str list append key list append key list append key list text string should only appear once pickling m = M s = long string s = different even longer string assertEqual m s s assertEqual m s s + s TemporaryFileName fname m save fname archive_name = os path basename os path normpath fname archive = zipfile ZipFile fname r pickled_data = archive read f archive_name data pkl out = io StringIO pickletools dis pickled_data out=out disassembled = out getvalue archive close FileCheck check_count s exactly=True \ check_count BINGET exactly=True \ check_count s exactly=True \ check_count BINGET exactly=True run disassembled test_sys_stdout_override torch jit script foo print foo Redirect __init__ - None s = write s s += s old_stdout = sys stdout redirect = Redirect try sys stdout = redirect foo finally sys stdout = old_stdout FileCheck check foo run redirect s test_dtype_attr Foo torch nn Module __init__ - None super __init__ dtype = torch zeros dtype forward torch zeros dtype=self dtype f = Foo torch jit script f test_named_buffers_are_iterable MyMod torch nn Module __init__ - None super __init__ mod = torch nn ReLU mod = torch nn ReLU mod = torch nn Sequential torch nn Sequential torch nn ReLU x = nn Buffer torch zeros y = nn Buffer torch zeros z = torch zeros bleh z + torch jit export method names = vals = name buffer named_buffers names append name vals append buffer + names vals forward x x model = MyMod x = torch jit script model z = getExportImportCopy x assertEqual z method x method assertEqual z method model method assertEqual x method model method names = x method name names assertNotEqual z name test_static_if_prop MaybeHasAttr torch nn Module __init__ add_attr super __init__ add_attr maybe_attr = forward hasattr maybe_attr True maybe_attr MaybeHasAttr torch nn Module __init__ add_attr super __init__ add_attr maybe_attr = forward hasattr maybe_attr False maybe_attr torch jit script MaybeHasAttr True torch jit script MaybeHasAttr False torch jit script MaybeHasAttr True torch jit script MaybeHasAttr False MyMod torch nn Module forward hasattr foo torch jit export fee checkModule MyMod HasAttrMod torch nn Module __constants__ = fee __init__ - None super __init__ fee = forward = hasattr fee b = hasattr foo c = hasattr hi d = hasattr nonexistent b c d foo torch jit _overload_method hi x Tensor noqa E hi x noqa F checkModule HasAttrMod torch jit script FooTest __init__ - None x = foo y x + y foo = FooTest val = hasattr foo hasattr x hasattr bla val = hasattr FooTest foo hasattr FooTest val val assertEqual foo torch jit script foo _test_pickle_checkpoint device TemporaryFileName fname M torch jit ScriptModule __constants__ = fname __init__ tensor super __init__ fname = fname tensor = torch nn Parameter tensor torch jit script_method forward x y = tensor + x torch save y fname y param = torch randn device input = torch randn device m = M param m input open fname rb handle loaded_tensor = torch load fname assertEqual loaded_tensor input + param _test_pickle_checkpoint_views device TemporaryFileName fname M torch jit ScriptModule __constants__ = fname __init__ tensor super __init__ fname = fname tensor = torch nn Parameter tensor torch jit script_method forward x y = tensor + x y_view = y view torch save y y_view y fname y param = torch randn device input = torch randn device m = M param m input open fname rb handle loaded_y loaded_y_view loaded_y_ = torch load fname assertEqual loaded_y input + param torch no_grad loaded_y_view += assert loaded_y changed well assertEqual loaded_y view loaded_y_view assertEqual loaded_y_ view loaded_y_view unittest skipIf RUN_CUDA no CUDA test_pickle_checkpoint_cuda _test_pickle_checkpoint cuda _test_pickle_checkpoint_views cuda test_pickle_checkpoint _test_pickle_checkpoint cpu _test_pickle_checkpoint_views cpu test_pickle_checkpoint_tup torch jit script foo fname type str - None torch save fname TemporaryFileName name foo name assertEqual torch load name test_string_list fn string type str - List str list string checkScript fn abcdefgh test_unicode_comments torch jit script test  torch nn functional relu test_get_set_state_with_tensors M torch nn Module __init__ - None super __init__ tensor = torch randn torch jit export __getstate__ tensor training torch jit export __setstate__ state tensor = state training = state forward x x + tensor TemporaryFileName fname m = torch jit script M m save fname loaded = torch jit load fname assertEqual loaded tensor m tensor test_in_for_and_comp_expr fn d type Dict str int - List int out = i range d get hi out append i noqa PERF out checkScript fn hi bye checkScript fn bye test_for_else fn c = _ range c += print In block assertRaisesRegex torch jit frontend NotSupportedError branches loops aren t supported torch jit script fn test_split split_two tensor b c = torch split tensor dim= b c x = torch randn y = torch randn checkScript split_two x + y test_conv_error torch jit script fn x y F conv d x y try fn torch ones torch ones except RuntimeError e assertFalse frame str e test_python_op_name random assertRaisesRegex RuntimeError randint torch jit script fn random randint test_dir M torch jit ScriptModule forward t t assertTrue forward dir M test_kwarg_expansion_error torch jit ignore something_else h i pass fn x something_else x assertRaisesRegex torch jit frontend NotSupportedError keyword-arg expansion supported torch jit script fn test_kwargs_error_msg other kwargs print kwargs fn other assertRaisesRegex torch jit frontend NotSupportedError variable number torch jit script fn another_other args print args another_fn another_other assertRaisesRegex torch jit frontend NotSupportedError variable number torch jit script another_fn test_inferred_error_msg Test when we get type mismatch function where we inferred type tensor good error message given torch jit script foo assertRaisesRegex RuntimeError r Expected value type \ Tensor \ inferred\ \ r \S\s Inferred \ a\ type \ Tensor\ foo test_type_comments_in_body torch jit script foo type int b type int type - int type int + b M torch nn Module __init__ type int b type int type - None super __init__ = type int b = b type int torch jit script M test_input_keyword_in_schema f x torch ceil input=x inp = torch randn checkScript f inp test_module_method_reassignment Foo torch nn Module _forward x x forward = _forward sm = torch jit script Foo input = torch ones assertEqual input sm input Tests case where torch Tensor subclass like Parameter used input test_script_module_tensor_subclass_argument torch jit script parameter_script x torch nn Parameter x input = torch ones assertEqual input parameter_script input test_save_load_attr_error Inner nn Module forward x x Wrapper nn Module __init__ inner super __init__ inner = inner forward x attribute doesn t exist ` Inner ` inner b x inner_module = torch jit script Inner inner_module = getExportImportCopy inner_module wrapped = Wrapper inner_module This should properly complain ` inner ` doesn t have attribute ` b ` assertRaisesRegex RuntimeError has no attribute torch jit script wrapped test_rescripting_loaded_modules InnerSubmod nn Module __constants__ = my_constant __init__ - None super __init__ foo = torch nn Buffer torch ones register_parameter bar torch nn Parameter torch ones baz = torch ones my_constant = forward x x + x Inner nn Module __init__ - None super __init__ submod = InnerSubmod forward x submod x Wrapper nn Module __init__ inner super __init__ inner = inner forward x access inner elements ret = inner submod x + inner submod foo + inner submod bar + inner submod baz ret = ret + inner submod my_constant ret inner_module = torch jit script Inner wrapped = Wrapper inner_module checkModule wrapped torch ones inner_module_loaded = getExportImportCopy inner_module wrapped_loaded = Wrapper inner_module_loaded assertEqual wrapped torch ones wrapped_loaded torch ones test_interpret_graph fn x x unfold graph_str = graph Tensor b Tensor c Tensor = aten mul b c graph = parse_ir graph_str = torch rand b = torch rand test = torch _C _jit_interpret_graph graph b ref = b assertEqual test ref test_signed_float_zero MyModule torch nn Module forward x torch div x - inp = torch ones checkModule MyModule inp test_index_with_tuple MyModule torch nn Module forward x x checkModule MyModule torch ones test_context_manager MyModule torch nn Module forward x y p = x + y q = p + q x = torch randn dtype=torch float y = torch randn dtype=torch float fuser_name fuser fuser none torch jit fuser fuser_name checkModule MyModule x y test_zero_dimension_tensor_trace f x x x jf = torch jit trace f torch tensor device= cpu known failing tracer EXCLUDE_TRACED = The following fail due A prim ListConstruct involved indices get traced TensorType which always require_grad This causes crash autodiff test___getitem___adv_index test___getitem___adv_index_beg test___getitem___adv_index_comb test___getitem___adv_index_dup test___getitem___adv_index_sub test___getitem___adv_index_sub_ test___getitem___adv_index_sub_ test___getitem___adv_index_var jit doesn t support sparse tensors test_to_sparse test_to_sparse_dim EXCLUDE_TYPE_CHECK = slogdet tests use itemgetter select its only differentiable output happens outside graph we handle so there fewer reference outputs than graph outputs test_slogdet_ x _neg_det test_slogdet_ x _pos_det test_slogdet_distinct_singular_values test_slogdet_neg_det test_slogdet_pos_det test_slogdet_symmetric test_slogdet_symmetric_pd test_slogdet_batched_ x _neg_det test_slogdet_batched_pos_det test_slogdet_batched_symmetric test_slogdet_batched_symmetric_pd test_slogdet_batched_distinct_singular_values chunk returns list scripting we don t unpack list Thus won t replaced ConstantChunk run AD It s explicitly checked test_chunk_constant_script_ad Similarly split s replaced split_with_sizes tracing we don t have AD formula aten split Tensor int int op registered JIT so AD triggered scripting EXCLUDE_SCRIPT_AD_CHECK = test_chunk test_chunk_dim test_chunk_dim_neg test_split_size_list test_split_size_list_dim test_split_size_list_dim_neg test_tensor_indices_sections test_tensor_indices_sections_dim test_tensor_indices_sections_dim_neg test_tensor_split_sections test_tensor_split_sections_dim test_tensor_split_sections_dim_neg EXCLUDE_PYTHON_PRINT = no support BroadcastingList python printer test_nn_max_unpool d test_nn_max_unpool d test_nn_max_unpool d test_nn_max_pool d test_nn_max_pool d test_nn_max_pool d test_nn_max_pool d_with_indices EXCLUDE_ALIAS = aliases which may appear method_tests tested elsewhere true_divide Disable tests lu common_methods_invocations py TODO nikitaved Enable jit tests once autograd Function does support scripting lu TestJitGeneratedModule JitTestCase pass TestJitGeneratedFunctional JitTestCase pass L = M = S = add_nn_module_test args kwargs no_grad = kwargs get no_grad False desc kwargs eval kwargs desc eval supported so skip these tests test_name = get_nn_mod_test_name kwargs suppress_warnings do_test test_name EXCLUDE_SCRIPT_MODULES kwargs get check_jit True raise unittest SkipTest module test skipped JIT default_dtype = torch get_default_dtype default_dtype kwargs kwargs default_dtype None default_dtype = kwargs default_dtype module_name = get_nn_module_name_from_kwargs kwargs constructor kwargs nn_module = kwargs constructor nn_module = getattr torch nn module_name FunctionalModule str nn_module set_default_dtype default_dtype constructor_args_fn kwargs constructor_args = kwargs constructor_args_fn constructor_args = kwargs get constructor_args create_script_module args kwargs Construct script module passes arguments through submodule formals tensors actuals = get_script_args args method_args = join + actuals call_args_str = join actuals call = f submodule call_args_str script = script_method_template format method_args call submodule_constants = kwargs get is_constant submodule_constants = submodule Create module use script method TheModule torch jit ScriptModule __constants__ = submodule_constants __init__ - None super __init__ submodule = nn_module constructor_args make_module script module = TheModule check __repr__ str module module define script module module = make_module script assertExportImportModule module tensors create_script_module last_graph = module graph mod = module args mod Construct normal nn module stay consistent create_script_module make use single global rng_state module initialization create_nn_module args kwargs module = nn_module constructor_args module args Set up inputs tuple sizes constructor fn dtype = torch get_default_dtype input_fn kwargs input = kwargs input_fn isinstance input Tensor input = input all tensor is_complex tensor input dtype == torch float dtype = torch cfloat dtype == torch double dtype = torch cdouble raise AssertionError f default_dtype default_dtype supported input = kwargs input_size target_size kwargs input = input + kwargs target_size target_fn kwargs torch is_tensor input input = input input = input + kwargs target_fn target kwargs input = input + kwargs target Extra parameters forward extra_args kwargs input = input + kwargs extra_args args_variable kwargs_variable = create_input input dtype=dtype f_args_variable = deepcopy unpack_variables args_variable TODO issue# Neither nor no_grad should required check_against_reference updated check gradients w r t weights then only check w r t inputs any inputs require any_requires_grad = any input requires_grad input f_args_variable Check against Python module reference check_against_reference create_script_module create_nn_module lambda x x f_args_variable no_grad=no_grad any_requires_grad slowTest kwargs do_test = slowTest do_test post_add_test test_name do_test TestJitGeneratedModule post_add_test test_name skipTestIf do_test test_class assert hasattr test_class test_name Two tests have same name + test_name skip skipTestIf do_test = skip do_test setattr test_class test_name do_test normalize_check_ad check_ad name normalized check_ad -element tuple bool List str List str len check_ad == check_ad = False aten + name len check_ad == check_ad = check_ad aten + name len check_ad == check_ad = check_ad check_ad len check_ad == check_ad = list check_ad raise Exception Invalid check_ad requires bool str &#124; List str str &#124; List str noqa TRY check_ad = t isinstance t str t t check_ad check_ad TestProducerVersion TestCase test_version issue gh- assertTrue torch __version__ startswith torch onnx producer_version test get_all_nn_module_tests add_nn_module_test test test criterion_tests test no_grad = True add_nn_module_test test __name__ == __main__ TestCase _default_dtype_check_enabled = True run_tests jit test_module_interface suite = unittest findTestCases jit test_module_interface unittest TextTestRunner run suite