Owner s oncall speech_infra copy torch torch nn nn torch ao quantization experimental adaround_optimization AdaptiveRoundingOptimizer torch nn functional F torch quantization observer MinMaxObserver torch testing _internal common_quantization QuantizationTestCase forward_wrapper fetcher forward module input output fetcher append input detach fetcher append output detach forward TestAdaround QuantizationTestCase feedforawrd_callback model data - None model data feedforawrd_callback_with_wrapper model data wrapper - None wrapper model data run_adaround model img_data wrapper=None adaround_optimizer = AdaptiveRoundingOptimizer model feedforawrd_callback wrapper None feedforawrd_callback_with_wrapper forward_wrapper img_data max_iter= batch_size= feed_forward_wrapper=wrapper adarounded_model = adaround_optimizer run_adaround adarounded_model get_fake_quant model hard_fake_quant_model = copy deepcopy model _ module hard_fake_quant_model named_modules isinstance module torch nn Linear torch nn Conv d weight_observer = MinMaxObserver quant_min=- quant_max= dtype=torch qint qscheme=torch per_tensor_symmetric weight_observer module weight scale zero_point = weight_observer calculate_qparams fake_quant_module = torch fake_quantize_per_tensor_affine module weight scale=scale zero_point=zero_point quant_min=- quant_max= module weight data copy_ fake_quant_module hard_fake_quant_model get_feed_forward_wrapper FeedForwardWrapper nn Module __init__ - None super __init__ forward model sample model sample wrapper_module = FeedForwardWrapper wrapper_module test_linear_chain LinearChain nn Module __init__ - None super __init__ linear = nn Linear linear = nn Linear linear = nn Linear forward x x = linear x x = linear x x = linear x x float_model = LinearChain img_data = torch rand dtype=torch float _ range adarounded_model = run_adaround float_model img_data get_feed_forward_wrapper fq_model = get_fake_quant float_model rand_input = torch rand torch no_grad ada_out = adarounded_model rand_input fq_out = fq_model rand_input float_out = float_model rand_input ada_loss = F mse_loss ada_out float_out fq_loss = F mse_loss fq_out float_out assertTrue ada_loss item fq_loss item test_conv_chain ConvChain nn Module __init__ - None super __init__ conv d = nn Conv d conv d = nn Conv d conv d = nn Conv d forward x x = conv d x x = conv d x x = conv d x x float_model = ConvChain img_data = torch rand dtype=torch float _ range adarounded_model = run_adaround float_model img_data fq_model = get_fake_quant float_model rand_input = torch rand torch no_grad ada_out = adarounded_model rand_input fq_out = fq_model rand_input float_out = float_model rand_input ada_loss = F mse_loss ada_out float_out fq_loss = F mse_loss fq_out float_out assertTrue ada_loss item fq_loss item __name__ == __main__ raise RuntimeError This test currently used should enabled discover_tests py required