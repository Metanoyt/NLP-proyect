mypy allow-untyped-defs __future__ annotations dataclasses dataclass enum Enum typing Any Optional TYPE_CHECKING Union torch TYPE_CHECKING collections abc Callable torch ao quantization utils Pattern __all__ = BackendConfig BackendPatternConfig DTypeConfig DTypeWithConstraints ObservationType DTypeConfig dict keys INPUT_DTYPE_DICT_KEY = input_dtype OUTPUT_DTYPE_DICT_KEY = output_dtype WEIGHT_DTYPE_DICT_KEY = weight_dtype BIAS_DTYPE_DICT_KEY = bias_dtype IS_DYNAMIC_DICT_KEY = is_dynamic BackendConfig dict keys NAME_DICT_KEY = name CONFIGS_DICT_KEY = configs BackendPatternConfig dict keys PATTERN_DICT_KEY = pattern PATTERN_COMPLEX_FORMAT_DICT_KEY = pattern_complex_format OBSERVATION_TYPE_DICT_KEY = observation_type DTYPE_CONFIGS_DICT_KEY = dtype_configs ROOT_MODULE_DICT_KEY = root_module QAT_MODULE_DICT_KEY = qat_module REFERENCE_QUANTIZED_MODULE_DICT_KEY = reference_quantized_module_for_root FUSED_MODULE_DICT_KEY = fused_module FUSER_METHOD_DICT_KEY = fuser_method ROOT_NODE_GETTER_DICT_KEY = root_node_getter EXTRA_INPUTS_GETTER_DICT_KEY = extra_inputs_getter NUM_TENSOR_ARGS_TO_OBSERVATION_TYPE_DICT_KEY = num_tensor_args_to_observation_type INPUT_TYPE_TO_INDEX_DICT_KEY = input_type_to_index TODO maybe rename something s related observer e g QParamsType ObservationType Enum An enum represents different ways how operator operator pattern should observed OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT = means input output observed different observers based qconfig activation example conv linear softmax OUTPUT_SHARE_OBSERVER_WITH_INPUT = means output will use same observer instance input based qconfig activation example torch cat maxpool INPUT_OUTPUT_NOT_OBSERVED = means input output never observed example x shape x size dataclass DTypeWithConstraints Config specifying additional constraints given dtype such quantization value ranges scale value ranges fixed quantization params used ` ~torch ao quantization backend_config DTypeConfig ` The constraints currently supported ` quant_min_lower_bound ` ` quant_max_upper_bound ` Lower upper bounds minimum maximum quantized values respectively If QConfig s ` quant_min ` ` quant_max ` fall outside range then QConfig will ignored ` scale_min_lower_bound ` ` scale_max_upper_bound ` Lower upper bounds minimum maximum scale values respectively If QConfig s minimum scale value currently exposed ` eps ` falls below lower bound then QConfig will ignored Note upper bound currently enforced ` scale_exact_match ` ` zero_point_exact_match ` Exact match requirements scale zero point used operators fixed quantization parameters such sigmoid tanh If observer specified QConfig neither ` FixedQParamsObserver ` nor ` FixedQParamsFakeQuantize ` quantization parameters don t match then QConfig will ignored dtype Optional torch dtype = None quant_min_lower_bound Union int float None = None quant_max_upper_bound Union int float None = None scale_min_lower_bound Union int float None = None scale_max_upper_bound Union int float None = None scale_exact_match Optional float = None zero_point_exact_match Optional int = None dataclass DTypeConfig Config object specifies supported data types passed arguments quantize ops reference model spec input output activations weights biases For example consider following reference model quant - dequant - fp _linear - quant - dequant The pattern square brackets refers reference pattern statically quantized linear Setting input dtype ` torch quint ` DTypeConfig means we pass ` torch quint ` dtype argument first quantize op quant Similarly setting output dtype ` torch quint ` means we pass ` torch quint ` dtype argument second quantize op quant Note dtype here does refer interface dtypes op For example input dtype here dtype input tensor passed quantized linear op Though can still same interface dtype always case e g interface dtype fp dynamic quantization input dtype specified DTypeConfig would still quint The semantics dtypes here same semantics dtypes specified observers These dtypes matched against ones specified user s QConfig If there match QConfig satisfies constraints specified DTypeConfig any then we will quantize given pattern using DTypeConfig Otherwise QConfig ignored pattern will quantized Example usage xdoctest +SKIP failing dtype_config = DTypeConfig input_dtype=torch quint output_dtype=torch quint weight_dtype=torch qint bias_dtype=torch float dtype_config = DTypeConfig input_dtype=DTypeWithConstraints dtype=torch quint quant_min_lower_bound= quant_max_upper_bound= output_dtype=DTypeWithConstraints dtype=torch quint quant_min_lower_bound= quant_max_upper_bound= weight_dtype=DTypeWithConstraints dtype=torch qint quant_min_lower_bound=- quant_max_upper_bound= bias_dtype=torch float dtype_config input_dtype torch quint dtype_config input_dtype torch quint dtype_config input_dtype_with_constraints DTypeWithConstraints dtype=torch quint quant_min_lower_bound= quant_max_upper_bound= \ scale_min_lower_bound=None scale_max_upper_bound=None input_dtype_with_constraints DTypeWithConstraints output_dtype_with_constraints DTypeWithConstraints weight_dtype_with_constraints DTypeWithConstraints bias_dtype Optional torch dtype is_dynamic Optional bool __init__ input_dtype Union torch dtype DTypeWithConstraints None = None output_dtype Union torch dtype DTypeWithConstraints None = None weight_dtype Union torch dtype DTypeWithConstraints None = None bias_dtype Optional torch dtype = None is_dynamic Optional bool = None isinstance input_dtype DTypeWithConstraints input_dtype_with_constraints = input_dtype input_dtype_with_constraints = DTypeWithConstraints dtype=input_dtype isinstance output_dtype DTypeWithConstraints output_dtype_with_constraints = output_dtype output_dtype_with_constraints = DTypeWithConstraints dtype=output_dtype isinstance weight_dtype DTypeWithConstraints weight_dtype_with_constraints = weight_dtype weight_dtype_with_constraints = DTypeWithConstraints dtype=weight_dtype bias_dtype = bias_dtype is_dynamic = is_dynamic property input_dtype - Optional torch dtype input_dtype_with_constraints dtype property output_dtype - Optional torch dtype output_dtype_with_constraints dtype property weight_dtype - Optional torch dtype weight_dtype_with_constraints dtype classmethod from_dict cls dtype_config_dict dict str Any - DTypeConfig Create ` ` DTypeConfig ` ` dictionary following items all optional input_dtype torch dtype ` ` DTypeWithConstraints ` ` output_dtype torch dtype ` ` DTypeWithConstraints ` ` weight_dtype torch dtype ` ` DTypeWithConstraints ` ` bias_type torch dtype is_dynamic bool input_dtype = dtype_config_dict get INPUT_DTYPE_DICT_KEY input_dtype None isinstance input_dtype torch dtype DTypeWithConstraints raise ValueError Expected input_dtype torch dtype DTypeWithConstraints output_dtype = dtype_config_dict get OUTPUT_DTYPE_DICT_KEY output_dtype None isinstance output_dtype torch dtype DTypeWithConstraints raise ValueError Expected output_dtype torch dtype DTypeWithConstraints weight_dtype = dtype_config_dict get WEIGHT_DTYPE_DICT_KEY weight_dtype None isinstance weight_dtype torch dtype DTypeWithConstraints raise ValueError Expected weight_dtype torch dtype DTypeWithConstraints bias_dtype = dtype_config_dict get BIAS_DTYPE_DICT_KEY is_dynamic = dtype_config_dict get IS_DYNAMIC_DICT_KEY cls input_dtype output_dtype weight_dtype bias_dtype is_dynamic to_dict - dict str Any Convert ` ` DTypeConfig ` ` dictionary items described func ` ~torch ao quantization backend_config DTypeConfig from_dict ` dtype_config_dict dict str Any = input_dtype None dtype_config_dict INPUT_DTYPE_DICT_KEY = input_dtype_with_constraints output_dtype None dtype_config_dict OUTPUT_DTYPE_DICT_KEY = output_dtype_with_constraints weight_dtype None dtype_config_dict WEIGHT_DTYPE_DICT_KEY = weight_dtype_with_constraints bias_dtype None dtype_config_dict BIAS_DTYPE_DICT_KEY = bias_dtype is_dynamic None dtype_config_dict IS_DYNAMIC_DICT_KEY = is_dynamic dtype_config_dict BackendConfig TODO refer NativeBackendConfig once implemented Config defines set patterns can quantized given backend how reference quantized models can produced these patterns A pattern context refers module functional operator directed acyclic graph above Each pattern supported target backend can individually configured through ` ~torch ao quantization backend_config BackendPatternConfig ` terms The supported input output activation weight bias data types How observers quant dequant ops inserted order construct reference pattern Optionally Fusion QAT reference module mappings The format patterns described https github com pytorch pytorch blob master torch ao quantization backend_config README md Example usage torch torch ao quantization backend_config BackendConfig BackendPatternConfig DTypeConfig ObservationType weighted_int _dtype_config = DTypeConfig input_dtype=torch quint output_dtype=torch quint weight_dtype=torch qint bias_dtype=torch float fuse_conv d_relu is_qat conv relu torch ao nn intrinsic ConvReLU d conv relu For quantizing Linear linear_config = BackendPatternConfig torch nn Linear \ set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT \ add_dtype_config weighted_int _dtype_config \ set_root_module torch nn Linear \ set_qat_module torch ao nn qat Linear \ set_reference_quantized_module torch ao nn quantized reference Linear For fusing Conv d + ReLU into ConvReLU d conv_relu_config = BackendPatternConfig torch nn Conv d torch nn ReLU \ set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT \ add_dtype_config weighted_int _dtype_config \ set_fused_module torch ao nn intrinsic ConvReLU d \ set_fuser_method fuse_conv d_relu For quantizing ConvReLU d fused_conv_relu_config = BackendPatternConfig torch ao nn intrinsic ConvReLU d \ set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT \ add_dtype_config weighted_int _dtype_config \ set_root_module torch nn Conv d \ set_qat_module torch ao nn intrinsic qat ConvReLU d \ set_reference_quantized_module torch ao nn quantized reference Conv d backend_config = BackendConfig my_backend \ set_backend_pattern_config linear_config \ set_backend_pattern_config conv_relu_config \ set_backend_pattern_config fused_conv_relu_config __init__ name str = name = name Store all BackendPatternConfigs map handle duplicates Note key map uses complex reversed tuple format This intended only internal use users who wish access original patterns should go through ` configs ` instead _pattern_complex_format_to_config dict Pattern BackendPatternConfig = __repr__ f BackendConfig __dict__ set_name name str - BackendConfig Set name target backend name = name set_backend_pattern_config config BackendPatternConfig - BackendConfig Set config pattern can run target backend This overrides any existing config given pattern Avoid circular dependencies pattern_complex_format = torch ao quantization backend_config utils _get_pattern_in_reversed_nested_tuple_format config type ignore attr-defined _pattern_complex_format_to_config pattern_complex_format = config set_backend_pattern_configs configs list BackendPatternConfig - BackendConfig Set configs patterns can run target backend This overrides any existing config given pattern previously registered already conf configs set_backend_pattern_config conf property configs - list BackendPatternConfig Return copy list configs set ` BackendConfig ` list _pattern_complex_format_to_config values classmethod from_dict cls backend_config_dict dict str Any - BackendConfig Create ` ` BackendConfig ` ` dictionary following items name name target backend configs list dictionaries each represents ` BackendPatternConfig ` conf = cls backend_config_dict get NAME_DICT_KEY d backend_config_dict get CONFIGS_DICT_KEY isinstance d BackendPatternConfig conf set_backend_pattern_config d isinstance d dict conf set_backend_pattern_config BackendPatternConfig from_dict d raise ValueError f Expected backend_config_dict CONFIGS_DICT_KEY dictionary conf to_dict - dict str Any Convert ` ` BackendConfig ` ` dictionary items described func ` ~torch ao quantization backend_config BackendConfig from_dict ` NAME_DICT_KEY name CONFIGS_DICT_KEY c to_dict c configs BackendPatternConfig Config object specifies quantization behavior given operator pattern For detailed example usage see ` ~torch ao quantization backend_config BackendConfig ` __init__ pattern Optional Pattern = None pattern Optional Pattern = pattern observation_type = ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT dtype_configs list DTypeConfig = root_module Optional type torch nn Module = None qat_module Optional type torch nn Module = None reference_quantized_module Optional type torch nn Module = None fused_module Optional type torch nn Module = None fuser_method Optional Callable = None Temporary internal configs _root_node_getter Optional Callable = None _extra_inputs_getter Optional Callable = None _num_tensor_args_to_observation_type dict int ObservationType = _input_type_to_index dict str int = _pattern_complex_format Optional Pattern = None __repr__ dict_nonempty = k v k v __dict__ items isinstance v list dict v None isinstance v list dict len v f BackendPatternConfig dict_nonempty set_pattern pattern Pattern - BackendPatternConfig Set pattern configure The pattern can float module functional operator pytorch operator tuple combination above Tuple patterns treated sequential patterns currently only tuples elements supported _pattern_complex_format None raise ValueError Only one pattern pattern_complex_format can set pattern = pattern set_observation_type observation_type ObservationType - BackendPatternConfig Set how observers should inserted graph pattern Observation type here refers how observers quant-dequant ops will placed graph This used produce desired reference patterns understood backend Weighted ops such linear conv require different observers quantization parameters passed quantize ops reference model input output There two observation types ` OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT ` default output observer instance will different input This most common observation type ` OUTPUT_SHARE_OBSERVER_WITH_INPUT ` output observer instance will same input This useful operators like ` cat ` Note This will renamed near future since we will soon insert QuantDeQuantStubs observers fake quantizes attached instead observers themselves observation_type = observation_type add_dtype_config dtype_config DTypeConfig - BackendPatternConfig Add set supported data types passed arguments quantize ops reference model spec dtype_configs append dtype_config set_dtype_configs dtype_configs list DTypeConfig - BackendPatternConfig Set supported data types passed arguments quantize ops reference model spec overriding all previously registered data types dtype_configs = dtype_configs set_root_module root_module type torch nn Module - BackendPatternConfig Set module represents root pattern When we construct reference quantized model during convert phase root modules e g torch nn Linear torch ao nn intrinsic LinearReLU will swapped corresponding reference quantized modules e g torch ao nn reference quantized Linear This allows custom backends specify custom reference quantized module implementations match numerics their lowered operators Since one-to-one mapping both root module reference quantized module must specified same BackendPatternConfig order conversion take place root_module = root_module set_qat_module qat_module type torch nn Module - BackendPatternConfig Set module represents QAT implementation pattern qat_module = qat_module set_reference_quantized_module reference_quantized_module type torch nn Module - BackendPatternConfig Set module represents reference quantized implementation pattern s root module For more detail see func ` ~torch ao quantization backend_config BackendPatternConfig set_root_module ` reference_quantized_module = reference_quantized_module set_fused_module fused_module type torch nn Module - BackendPatternConfig Set module represents fused implementation pattern fused_module = fused_module set_fuser_method fuser_method Callable - BackendPatternConfig Set function specifies how fuse BackendPatternConfig s pattern The first argument function should ` is_qat ` rest arguments should items tuple pattern The value function should resulting fused module For example fuser method pattern ` torch nn Linear torch nn ReLU ` can fuse_linear_relu is_qat linear relu torch ao nn intrinsic LinearReLU linear relu For more complicated example see https gist github com jerryzh bea ba c f c b f fuser_method = fuser_method _set_root_node_getter root_node_getter Callable - BackendPatternConfig _root_node_getter = root_node_getter _set_extra_inputs_getter extra_inputs_getter Callable - BackendPatternConfig _extra_inputs_getter = extra_inputs_getter _set_num_tensor_args_to_observation_type num_tensor_args_to_observation_type dict int ObservationType - BackendPatternConfig _num_tensor_args_to_observation_type = num_tensor_args_to_observation_type _set_input_type_to_index input_type_to_index dict str int - BackendPatternConfig _input_type_to_index = input_type_to_index _set_pattern_complex_format pattern Pattern - BackendPatternConfig Set pattern configure using reversed nested tuple format See BackendConfig README more detail https github com pytorch pytorch blob master torch ao quantization backend_config README md#advanced-pattern-specification pattern None raise ValueError Only one pattern pattern_complex_format can set _pattern_complex_format = pattern classmethod from_dict cls backend_pattern_config_dict dict str Any - BackendPatternConfig Create ` ` BackendPatternConfig ` ` dictionary following items pattern pattern being configured observation_type ` ~torch ao quantization backend_config ObservationType ` specifies how observers should inserted pattern dtype_configs list dictionaries represents ` ~torch ao quantization backend_config DTypeConfig ` s root_module ` torch nn Module ` represents root pattern qat_module ` torch nn Module ` represents QAT implementation pattern reference_quantized_module ` torch nn Module ` represents reference quantized implementation pattern s root module fused_module ` torch nn Module ` represents fused implementation pattern fuser_method function specifies how fuse pattern pattern pattern_complex_format pattern specified reversed nested tuple format deprecated _get_dtype_config obj Any - DTypeConfig Convert given object into ` ` DTypeConfig ` ` possible throw exception isinstance obj DTypeConfig obj isinstance obj dict DTypeConfig from_dict obj raise ValueError f Expected list DTypeConfigs f backend_pattern_config_dict \ DTYPE_CONFIGS_DICT_KEY \ got type obj conf = cls PATTERN_DICT_KEY backend_pattern_config_dict conf set_pattern backend_pattern_config_dict PATTERN_DICT_KEY OBSERVATION_TYPE_DICT_KEY backend_pattern_config_dict conf set_observation_type backend_pattern_config_dict OBSERVATION_TYPE_DICT_KEY d backend_pattern_config_dict get DTYPE_CONFIGS_DICT_KEY conf add_dtype_config _get_dtype_config d conf set_root_module backend_pattern_config_dict get ROOT_MODULE_DICT_KEY type ignore arg-type conf set_qat_module backend_pattern_config_dict get QAT_MODULE_DICT_KEY type ignore arg-type conf set_reference_quantized_module backend_pattern_config_dict get REFERENCE_QUANTIZED_MODULE_DICT_KEY type ignore arg-type conf set_fused_module backend_pattern_config_dict get FUSED_MODULE_DICT_KEY type ignore arg-type conf set_fuser_method backend_pattern_config_dict get FUSER_METHOD_DICT_KEY type ignore arg-type conf _set_root_node_getter backend_pattern_config_dict get ROOT_NODE_GETTER_DICT_KEY type ignore arg-type conf _set_extra_inputs_getter backend_pattern_config_dict get EXTRA_INPUTS_GETTER_DICT_KEY type ignore arg-type conf _set_num_tensor_args_to_observation_type backend_pattern_config_dict get NUM_TENSOR_ARGS_TO_OBSERVATION_TYPE_DICT_KEY conf _set_input_type_to_index backend_pattern_config_dict get INPUT_TYPE_TO_INDEX_DICT_KEY PATTERN_COMPLEX_FORMAT_DICT_KEY backend_pattern_config_dict conf _set_pattern_complex_format backend_pattern_config_dict PATTERN_COMPLEX_FORMAT_DICT_KEY conf to_dict - dict str Any Convert ` ` BackendPatternConfig ` ` dictionary items described func ` ~torch ao quantization backend_config BackendPatternConfig from_dict ` backend_pattern_config_dict dict str Any = OBSERVATION_TYPE_DICT_KEY observation_type DTYPE_CONFIGS_DICT_KEY c to_dict c dtype_configs pattern None backend_pattern_config_dict PATTERN_DICT_KEY = pattern root_module None backend_pattern_config_dict ROOT_MODULE_DICT_KEY = root_module qat_module None backend_pattern_config_dict QAT_MODULE_DICT_KEY = qat_module reference_quantized_module None backend_pattern_config_dict REFERENCE_QUANTIZED_MODULE_DICT_KEY = reference_quantized_module fused_module None backend_pattern_config_dict FUSED_MODULE_DICT_KEY = fused_module fuser_method None backend_pattern_config_dict FUSER_METHOD_DICT_KEY = fuser_method _root_node_getter None backend_pattern_config_dict ROOT_NODE_GETTER_DICT_KEY = _root_node_getter _extra_inputs_getter None backend_pattern_config_dict EXTRA_INPUTS_GETTER_DICT_KEY = _extra_inputs_getter len _num_tensor_args_to_observation_type backend_pattern_config_dict NUM_TENSOR_ARGS_TO_OBSERVATION_TYPE_DICT_KEY = _num_tensor_args_to_observation_type len _input_type_to_index backend_pattern_config_dict INPUT_TYPE_TO_INDEX_DICT_KEY = _input_type_to_index _pattern_complex_format None backend_pattern_config_dict PATTERN_COMPLEX_FORMAT_DICT_KEY = _pattern_complex_format backend_pattern_config_dict