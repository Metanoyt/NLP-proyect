mypy allow-untyped-defs functools hashlib itertools dataclasses dataclass typing Any Optional TYPE_CHECKING Union typing_extensions override unittest mock patch sympy torch torch _inductor config torch _inductor utils clear_on_fresh_cache Placeholder torch _logging getArtifactLogger autotune_process CUDABenchmarkRequest TensorMeta ir Buffer CUDATemplateBuffer IRNode Layout utils IndentedBuffer unique virtualized V common KernelTemplate cuda_kernel CUDATemplateCaller CUDATemplateKernel cutlass_utils DTYPE_TO_CUTLASS_TYPE TYPE_CHECKING scheduler BaseSchedulerNode noqa TC BaseSchedulerNode = Any GemmOperation = Any autotuning_log = getArtifactLogger __name__ autotuning dataclass frozen=True ArgInfo name str ty str clear_on_fresh_cache CUDATemplate KernelTemplate index_counter = itertools count dict cache key code size_args code_cache dict str tuple str tuple int tuple int = cache_clear = staticmethod code_cache clear __init__ name str input_nodes list Buffer layout Layout input_reorder Optional list int = None - None Baseclass CUDA C++ Templates derived KernelTemplate Not instantiated directly Args name str The name CUDATemplate object input_nodes List IRNode A list input IRNodes layout Layout The layout output buffer tensor input_reorder Optional List int An optional list specifies order input nodes super __init__ name input_nodes = input_nodes output_node Buffer = Buffer name= buf_out layout=layout input_reorder = input_reorder layout = layout classmethod functools lru_cache None pyrefly ignore bad-override _template_from_string cls source str - Any KernelTemplate _template_from_string source staticmethod supports_epilogue_fusion op GemmOperation - bool False make_key name str input_key str layout_repr str - str Make key code cache The idea method cache everything matters doesn t include runtime param values i e get_runtime_arg_values Args kwargs Additional keyword arguments Including op GemmOperation hashlib sha str input_key input_reorder output layout same output_node get_layout layout_repr get_runtime_arg_info name encode utf- hexdigest generate_code_and_args name str input_key str layout_repr str kwargs - tuple str tuple int Generate code args caching We cache code even runtime args different key Optional str = None config cuda enable_caching_codegen key = make_key name=name input_key=input_key layout_repr=layout_repr key None key code_cache code size_args offset_args = code_cache key extra_args = tuple list size_args + list offset_args + list get_runtime_arg_values kwargs code extra_args kernel_name = str Placeholder KERNEL_NAME kernel = CUDATemplateKernel kernel_name=kernel_name runtime_arg_info=self get_runtime_arg_info runtime_arg_values=self get_runtime_arg_values kwargs patch object V graph get_dtype _fake_get_dtype output_node code = render kernel=kernel kwargs _ call_args _ _ = kernel args python_argdefs autotuning_log debug Generated Code \n s code autotuning_log debug Args cpp_argdefs s python_argdefs s kernel args cpp_argdefs DTYPE_TO_CUTLASS_TYPE kernel args python_argdefs input_reorder = input_reorder input_reorder None list range len input_nodes expected_args = list unique input_nodes idx get_name idx input_reorder expected_args extend output_node get_name assert list call_args len expected_args == expected_args call_args expected_args V graph sizevars size_hints map sympy expand call_args len expected_args size_args = V graph sizevars size_hints kernel get_dynamic_shape_args offset_args = V graph sizevars size_hints kernel get_offset_args key None code_cache key = code size_args offset_args extra args has runtime params which shouldn t cached extra_args = tuple list size_args + list offset_args + get_runtime_arg_values kwargs code extra_args generate type ignore override name str description str input_key str layout_repr str input_tensor_meta Union TensorMeta list TensorMeta output_tensor_meta Union TensorMeta list TensorMeta kwargs - CUDATemplateCaller Generates CUDA template caller object given GEMM template operation This CUDATemplateCaller may used call benchmark generated CUDA kernel standalone manner enable Autotuning Args description op name followed swizzle kwargs Additional keyword arguments Returns A CUDATemplateCaller object representing generated CUDA template caller code extra_args = generate_code_and_args name=name input_key=input_key layout_repr=layout_repr kwargs caching since kernel name needed below kernel_hash = hashlib sha code encode utf- hexdigest kernel_name = f cutlass_ kernel_hash code = code replace name kernel_name create BenchmarkRequest bmreq = CUDABenchmarkRequest kernel_name=kernel_name input_tensor_meta=input_tensor_meta output_tensor_meta=output_tensor_meta extra_args=extra_args source_code=code kwargs has op argument case CUTLASSGemmTemplate op = kwargs op op supports_epilogue_fusion = False epilogue fusion only supported TMA kernels supports_epilogue_fusion = supports_epilogue_fusion op make_kernel_render template_node CUDATemplateBuffer epilogue_nodes Optional list BaseSchedulerNode = None - tuple CUDATemplateKernel functools partial str assert supports_epilogue_fusion epilogue_nodes epilogue fusion supported kernel kernel = CUDATemplateKernel kernel_name=str Placeholder KERNEL_NAME runtime_arg_info=self get_runtime_arg_info runtime_arg_values=self get_runtime_arg_values kwargs render = functools partial render kernel=kernel template_buffer_node=template_node epilogue_nodes=epilogue_nodes kwargs includes op argument case CUTLASSGemmTemplate kernel render CUDATemplateCaller kernel_name cutlass_gemm input_nodes output_node get_layout make_kernel_render bmreq supports_epilogue_fusion kwargs description header - IndentedBuffer res = IndentedBuffer res splice #include exception #include iostream #include memory #include random #include vector res globals - IndentedBuffer res = IndentedBuffer res splice We compile all models -fvisibility=hidden Any symbols need exposed final shared library must declared PT_EXPORT make them visible #ifdef __GNUC__ Applies any compiler GNU extensions clang g++ #define PT_EXPORT __attribute__ __visibility__ default #else #ifdef _WIN #define PT_EXPORT __declspec dllexport #else #define PT_EXPORT #endif #endif res render kwargs - str raise NotImplementedError get_runtime_arg_info - list ArgInfo get_runtime_arg_values kwargs - list Any CUTLASSTemplate CUDATemplate CUTLASSTemplate provides template generating CUTLASS Templates Used baseclass CUTLASSGemmTemplate providing functionality might also relevant non-GEMM CUTLASS Kernels header - IndentedBuffer res = super header res splice #include cute tensor hpp #include cutlass cutlass h #include cutlass numeric_types h #include cutlass tensor_ref h #include cutlass util host_tensor h #include cutlass util reference host tensor_fill h #include cutlass util reference device tensor_fill h #include cutlass util device_memory h res globals - IndentedBuffer res = super globals res splice using namespace cute #define CUTLASS_CHECK status \\ \\ cutlass Status error = status \\ error = cutlass Status kSuccess \\ auto msg = std string + __FILE__ + Got cutlass error + \\ cutlassGetStatusString error + + std to_string __LINE__ \\ throw std runtime_error msg \\ \\ Used pass-through functor EVT just type casting rounding template typename T struct identity_op CUTLASS_HOST_DEVICE T operator T val const val res cute_int int_str str var_name str - str res = int_str L res = cute Int res = int_str f res var_name _DTYPE_TO_CUTLASS = torch float float torch float double torch float cutlass half_t torch int int _t torch int int _t torch int int _t torch uint uint _t torch bool bool torch bfloat cutlass bfloat _t torch float _e m fn cutlass float_e m _t _DTYPE_TO_CUTLASS_SPARSE_META = torch int uint _t torch int uint _t cutlass_type_cast node IRNode ptr str - str node None ptr f _DTYPE_TO_CUTLASS get node get_dtype ptr cutlass_sparse_meta_type_cast node IRNode ptr str - str node None ptr f _DTYPE_TO_CUTLASS_SPARSE_META get node get_dtype ptr override get_runtime_arg_info - list ArgInfo ArgInfo swizzle const uint _t override get_runtime_arg_values kwargs - list Any Helper method retrieve runtime args generate kwargs kwargs arg name arg get_runtime_arg_info