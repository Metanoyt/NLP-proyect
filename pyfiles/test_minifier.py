Owner s module inductor unittest unittest mock patch torch _dynamo config dynamo_config torch _inductor config inductor_config torch _dynamo test_minifier_common MinifierTestBase torch _inductor config torch export load export_load torch testing _internal common_utils IS_JETSON IS_MACOS skipIfXpu TEST_WITH_ASAN torch testing _internal inductor_utils GPU_TYPE torch testing _internal triton_utils requires_gpu MinifierTests MinifierTestBase Test compile accuracy errors after aot can repro d both CPU CUDA _test_after_aot device expected_error NB The program intentionally quite simple just enough trigger one minification step no more dedicated minifier tests should exercise minifier only run_code = f \ torch compile inner x x = torch relu x x = torch cos x x inner torch randn device _run_full_test run_code aot expected_error isolate=False unittest skipIf IS_JETSON Fails Jetson inductor_config patch cpp inject_relu_bug_TESTING_ONLY compile_error test_after_aot_cpu_compile_error _test_after_aot cpu CppCompileError unittest skipIf IS_JETSON Fails Jetson inductor_config patch cpp inject_relu_bug_TESTING_ONLY accuracy test_after_aot_cpu_accuracy_error _test_after_aot cpu AccuracyError requires_gpu inductor_config patch triton inject_relu_bug_TESTING_ONLY compile_error test_after_aot_gpu_compile_error _test_after_aot GPU_TYPE SyntaxError requires_gpu inductor_config patch triton inject_relu_bug_TESTING_ONLY accuracy test_after_aot_gpu_accuracy_error _test_after_aot GPU_TYPE AccuracyError inductor_config patch cpp inject_relu_bug_TESTING_ONLY accuracy test_constant_in_graph run_code = \ torch compile inner x torch tensor + torch relu x inner torch randn _run_full_test run_code aot AccuracyError isolate=False requires_gpu patch object config joint_graph_constant_folding False test_rmse_improves_over_atol From https twitter com itsclivetime status s= run_code = torch compile inner x x - torch tensor dtype=torch half device= GPU_TYPE inner torch tensor dtype=torch half device= GPU_TYPE replace GPU_TYPE GPU_TYPE If we disable RMSE against fp triggers accuracy error increased precision torch compile changes result dynamo_config patch same_two_models_use_fp False _run_full_test run_code aot AccuracyError isolate=False NB need avoid refusing minify when fp doesn t work which doesn t due config patch above minifier_args= -- strict-accuracy But using fp we see intended semantics increased precision so we report no problem _run_full_test run_code aot None isolate=False inductor_config patch cpp inject_relu_bug_TESTING_ONLY accuracy inductor_config patch cpp inject_log p_bug_TESTING_ONLY accuracy test_accuracy_vs_strict_accuracy run_code = torch compile inner x y = torch log p x b = y Need ensure suffix removal hits boolean output b = torch logical_not b b = torch logical_not b x = torch relu x torch where b x x inner torch randn Strict accuracy gets hung up boolean mask difference which will localize error sigmoid even though doesn t actually matter end result res = _run_full_test run_code aot AccuracyError isolate=False minifier_args= -- strict-accuracy assertExpectedInline res repro_module \ Repro torch nn Module __init__ - None super __init__ forward arg _ log p = torch ops aten log p default arg _ arg _ = None log p FP accuracy will refuse promote logical_not outputs so you ll get relu unless minifier somehow tries removing entire suffix except log p first res = _run_full_test run_code aot AccuracyError isolate=False assertExpectedInline res repro_module \ Repro torch nn Module __init__ - None super __init__ forward arg _ relu = torch ops aten relu default arg _ arg _ = None relu inductor_config patch cpp inject_relu_bug_TESTING_ONLY accuracy test_offload_to_disk Just smoketest doesn t actually test memory usage went down Test case carefully constructed hit delta debugging run_code = \ torch compile inner x x = torch sin x x = torch sin x x = torch cos x x = torch relu x x inner torch randn _run_full_test run_code aot AccuracyError isolate=False minifier_args= -- offload-to-disk Test compile errors AOTInductor can repro d both CPU CUDA _test_aoti device expected_error NB The program intentionally quite simple just enough trigger one minification step no more dedicated minifier tests should exercise minifier only run_code = f \ Model torch nn Module __init__ super __init__ fc = torch nn Linear relu = torch nn ReLU sigmoid = torch nn Sigmoid forward x x = fc x x = relu x x = sigmoid x x torch no_grad model = Model device example_inputs = torch randn device ep = torch export export model example_inputs torch _inductor aoti_compile_and_package ep _run_full_test run_code aot_inductor expected_error isolate=False Test compile errors AOTInductor can repro d both CPU CUDA _test_aoti_unflattened_inputs device expected_error NB The program intentionally quite simple just enough trigger one minification step no more dedicated minifier tests should exercise minifier only It tests minifier can handle unflattened inputs kwargs run_code = f \ Model torch nn Module __init__ super __init__ fc = torch nn Linear relu = torch nn ReLU sigmoid = torch nn Sigmoid forward inp k x = inp x y = inp y x = fc x y = fc y k = fc k x = relu x x = sigmoid x x + y + k torch no_grad model = Model device val = torch randn device example_inputs = x val clone y val clone kwargs = k val clone ep = torch export export model example_inputs kwargs torch _inductor aoti_compile_and_package ep _run_full_test run_code aot_inductor expected_error isolate=False _aoti_check_relu_repro res assert res None ep_file_path = res get_exported_program_path assert ep_file_path None gm = export_load ep_file_path module check_guards=False assertExpectedInline str gm code strip \ forward linear linear = fx_pytree tree_flatten_spec linear _in_spec relu = torch ops aten relu default linear linear = None pytree tree_unflatten relu _out_spec unittest skipIf IS_JETSON Fails Jetson inductor_config patch cpp inject_relu_bug_TESTING_ONLY compile_error test_aoti_cpu_compile_error res = _test_aoti cpu CppCompileError _aoti_check_relu_repro res unittest skipIf IS_JETSON Fails Jetson inductor_config patch cpp inject_relu_bug_TESTING_ONLY compile_error test_aoti_cpu_compile_error_unflatten res = _test_aoti_unflattened_inputs cpu CppCompileError _aoti_check_relu_repro res requires_gpu skipIfXpu msg= AOTI XPU enabled yet inductor_config patch triton inject_relu_bug_TESTING_ONLY compile_error test_aoti_gpu_compile_error res = _test_aoti GPU_TYPE SyntaxError _aoti_check_relu_repro res requires_gpu skipIfXpu msg= AOTI XPU enabled yet inductor_config patch triton inject_relu_bug_TESTING_ONLY compile_error test_aoti_gpu_compile_error_unflatten res = _test_aoti_unflattened_inputs GPU_TYPE SyntaxError _aoti_check_relu_repro res unittest skipIf IS_JETSON Fails Jetson inductor_config patch cpp inject_relu_bug_TESTING_ONLY accuracy test_aoti_cpu_accuracy_error res = _test_aoti cpu AccuracyError _aoti_check_relu_repro res requires_gpu skipIfXpu msg= AOTI XPU enabled yet inductor_config patch triton inject_relu_bug_TESTING_ONLY accuracy test_aoti_gpu_accuracy_error res = _test_aoti GPU_TYPE AccuracyError _aoti_check_relu_repro res __name__ == __main__ torch _dynamo test_case run_tests Skip CI tests mac since CPU inductor does seem work due C++ compile errors also skip ASAN due https github com pytorch pytorch issues IS_MACOS TEST_WITH_ASAN run_tests