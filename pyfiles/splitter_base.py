mypy allow-untyped-defs argparse copy json logging os collections defaultdict collections abc Iterable Sequence dataclasses dataclass typing Any Literal NamedTuple Optional torch torch _logging trace_structured torch fx _compatibility compatibility torch fx node map_arg torch fx passes graph_manipulation get_size_of_node graph_drawer FxGraphDrawer operator_support get_node_target OperatorSupportBase shape_prop ShapeProp split_utils split_by_tags tools_common CALLABLE_NODE_OPS FxNetAccFusionsFinder is_node_output_tensor NodeList NodeSet Tensors __all__ = FxNetAccNodesFinder FxNetSplitterInternalError Subgraph SplitResult generate_inputs_for_submodules NodeEvent NodeEventTracker _LOGGER = logging getLogger __name__ DEFAULT_MIN_ACC_MODULE_SIZE = DEFAULT_SKIP_FUSION = False DEFAULT_ALLOW_NON_TENSOR = False ENV var constants node tracker TRACKER_DUMP_PATH = _fx_net_tracker NODES_SUFFIX = _nodes txt ALL_SUFFIX = _all txt ENV_FX_NET_ACC_SPLITTER_TRACKER_MODE = FX_NET_ACC_SPLITTER_TRACKER_MODE ENV_FX_NET_ACC_SPLITTER_TRACKER_DUMP_PATH = FX_NET_ACC_SPLITTER_TRACKER_DUMP_PATH ENV_FX_NET_ACC_SPLITTER_TRACKER_TRACKED_NODES = FX_NET_ACC_SPLITTER_TRACKER_TRACKED_NODES DUMP_PREFIX = os environ get ENV_FX_NET_ACC_SPLITTER_TRACKER_DUMP_PATH TRACKER_DUMP_PATH Different modes event tracker local debugging No local dumps Information available setting breakpoints visually inspect pdb Dump all events DUMP_PREFIX_all txt In addition events dump track nodes specified ENV_FX_NET_ACC_SPLITTER_TRACKER_TRACKED_NODES recursively dump DUMP_PREFIX_nodex txt In addition events dump track all nodes more than event recursively dump DUMP_PREFIX_nodex txt In addition above local dumps tracker always enabled dumps via trace_structured TRACKER_MODE Literal = os environ get ENV_FX_NET_ACC_SPLITTER_TRACKER_MODE type ignore assignment _SplitterSettingBase __init__ min_acc_module_size=DEFAULT_MIN_ACC_MODULE_SIZE skip_fusion=DEFAULT_SKIP_FUSION allow_non_tensor=DEFAULT_ALLOW_NON_TENSOR max_acc_splits int = - parser = argparse ArgumentParser parser add_argument -- min-acc-module-size -- min_acc_module_size required=False type=int help= Minimum size limit accelerator subgraph parser add_argument -- max-acc-splits -- max_acc_splits required=False type=int help= Enforce maximum number split subgraphs parser add_argument -- skip-fusion -- skip_fusion default=False action= store_true help= If true then no fusion groups Fusion group used enforce no non-tensor data flow between submodules If we don t have constrain setting false recommended can reduce overhead parser add_argument -- allow-non-tensor -- allow_non_tensor default=False action= store_true help= For some backends non-tensor data flow between cpu them allowed Therefore node supported accelerator has non-tensor inputs outputs cpu node we would want consider cpu node during splitting However some backends we might care about non-tensor data flow we can set option true disable functionality prevent non-tensor data flow args _unknown = parser parse_known_args min_acc_module_size int = args min_acc_module_size args min_acc_module_size min_acc_module_size skip_fusion bool = args skip_fusion args skip_fusion skip_fusion allow_non_tensor bool = args allow_non_tensor args allow_non_tensor allow_non_tensor max_acc_splits int = max_acc_splits compatibility is_backward_compatible=False NodeEvent An event graph split happened node source Subject event desc readable description dep Optional dependency usually node caused event __init__ source torch fx Node desc str dep Optional torch fx Node = None source = source desc = desc dep = dep to_str source The name subject event desc description event format event_type &#124; explanation dep The name cause event which another node s caused subject node f source name desc dep name dep compatibility is_backward_compatible=False NodeEventTracker Tracks node events during splitter execution __init__ tracker_mode dump_prefix tracker_mode = tracker_mode dump_prefix = dump_prefix list events events = dict node name event index node_events = writer = print add node torch fx Node desc str dep Optional torch fx Node = None Add new event tracker event = NodeEvent node desc dep events append event node name node_events node_events node name = node_events node name append len events - print_node node_name recursive=False tab= writer=None Print node its events param recursive True print nodes caused events current node param tab Indentation dependencies param writer function write file If None use print writer writer = writer idx node_events get node_name event = events idx writer tab + event to_str recursive event dep None print_node event dep name recursive=True tab= &#124; + tab writer=writer to_dict Create dict dump all events ret dict str list str = name node_events keys ret name = idx node_events get name event = events idx ret name append event to_str ret print_all writer=None Print all nodes list param writer function write file If None use print writer writer = writer name node_events keys writer f Node name print_node name recursive=False tab= writer=writer dump Function invoked end finder execution printout tracked events specified mode dump via trace_structured trace_structured artifact metadata_fn=lambda name fx_net_acc_splitter_finder_events encoding json payload_fn=lambda json dumps to_dict writeln f fn x f write x + \n fn Mode no local dump Mode = Dump all events file tracker_mode = open dump_prefix + ALL_SUFFIX w f print_all writeln f dump_selected_nodes nodes open dump_prefix + NODES_SUFFIX w f node_name nodes writeln f ===== Tracking node node_name ===== print_node node_name recursive=True tab= &#124; - writer=writeln f writeln f ===== End tracking node node_name ===== Mode Dump specific nodes recursive manner Mode Dump all nodes more than event recursive manner tracker_mode == tracker_mode == nodes = os environ get ENV_FX_NET_ACC_SPLITTER_TRACKER_TRACKED_NODES split tracker_mode == name name events node_events items len events dump_selected_nodes nodes compatibility is_backward_compatible=False FxNetAccNodesFinder Finds set nodes can supported ACC excluding nodes have non-tensor input output cpu nodes prevent non-tensor data flow between backends cpu I e we have chain ACC_NODE_ - ACC_NODE_ - ACC_NODE_ - CPU_NODE_ where every ACC node produces non-tensor output then they all should treated CPU nodes This behavior can turned off passing allow_non_tensor=True __init__ module torch fx GraphModule operator_support OperatorSupportBase allow_non_tensor bool module = module operator_support = operator_support allow_non_tensor = allow_non_tensor acc_nodes NodeSet = set tracker = NodeEventTracker int TRACKER_MODE DUMP_PREFIX reduce_acc_nodes_non_tensor_input_helper cpu_worklist NodeList Transitively excludes nodes ACC supported set For every node worklist - removes its downstream ACC nodes ACC supported set - any downstream ACC node produces non-tensor output then gets added into worklist while cpu_worklist node = cpu_worklist pop user node users user acc_nodes acc_nodes remove user tracker add user acc_del &#124; user_of_new_cpu_node node is_node_output_tensor user tracker add user new_cpu_node &#124; non_tensor_output cpu_worklist append user reduce_acc_nodes_non_tensor_input Excludes nodes ACC supported set have direct upstream CPU nodes produce non-tensor outputs non_tensor_cpu_nodes NodeList = node module graph nodes node op CALLABLE_NODE_OPS continue node acc_nodes continue is_node_output_tensor node continue tracker add node new_cpu_node &#124; callable_non_tensor_input non_tensor_cpu_nodes append node reduce_acc_nodes_non_tensor_input_helper non_tensor_cpu_nodes reduce_acc_nodes_non_tensor_output Excludes nodes ACC supported set produce non-tensor outputs have downstream CPU nodes while True new_cpu_nodes NodeList = acc_node acc_nodes is_node_output_tensor acc_node continue user acc_node users user acc_nodes new_cpu_nodes append acc_node tracker add acc_node acc_del &#124; non_tensor_output_with_cpu_user user break new_cpu_nodes break new_cpu_node new_cpu_nodes acc_nodes remove new_cpu_node reduce_acc_nodes_non_tensor_input_helper new_cpu_nodes __call__ - NodeSet submodules = dict module named_modules acc_nodes = set n module graph nodes n op CALLABLE_NODE_OPS tracker add n init_cpu &#124; not_callable continue operator_support is_node_supported submodules n tracker add n init_cpu &#124; operator_support continue tracker add n init_acc &#124; callable_and_operator_supported acc_nodes add n allow_non_tensor reduce_acc_nodes_non_tensor_input reduce_acc_nodes_non_tensor_output tracker dump acc_nodes compatibility is_backward_compatible=False FxNetSplitterInternalError Exception pass compatibility is_backward_compatible=False dataclass Subgraph is_acc bool nodes NodeList device_ordinal Optional int = None compatibility is_backward_compatible=False SplitResult NamedTuple Stores results splitter Attributes split_module root module after splitting submodule_inputs dict maps submodule name its inputs non_acc_submodule_prefix prefix non acc submodules For acc submodule prefix always _run_on_acc_ split_module torch fx GraphModule submodule_inputs dict str Any non_acc_submodule_prefix str compatibility is_backward_compatible=False generate_inputs_for_submodules model torch nn Module inputs Sequence Any target_submodules Iterable str deepcopy bool = False - dict str Any Generate inputs targeting submdoules given model Note two submodules refer same obj function doesn t work Args model root model inputs inputs root model target_submodules submodules we want generate inputs Returns A dict maps submodule name its inputs handles = results = submodule_to_names = mod name name mod model named_modules pre_forward module module_inputs results submodule_to_names module = copy deepcopy module_inputs deepcopy module_inputs name mod model named_modules name target_submodules isinstance mod torch jit ScriptModule handles append mod register_forward_pre_hook pre_forward clean_up_handles h handles h remove try torch no_grad model inputs except Exception e clean_up_handles raise e clean_up_handles results _SplitterBase Splits GraphModule into sub-GraphModules execution CPU accelerator Output GraphModule supported unsupported operators grouped into few sub-GraphModules possible Assumes only call_module call_function call_method FX IR can potentially executed accelerator Given following graph == b == \\ d \\ == c == SimpleModule torch nn Module forward b = torch sin c = torch cos d = b + c d providing operator_support indicates b c can executed accelerator we will get following split result main forward run_on_acc_ _ = _run_on_acc_ _ getitem = run_on_acc_ _ getitem_ = run_on_acc_ _ run_on_cpu_ _ = _run_on_cpu_ _ getitem getitem_ run_on_cpu_ _ _run_on_acc_ _ forward sin_ = torch sin cos_ = torch cos sin_ cos_ _run_on_cpu_ _ forward sin_ cos_ add_ = sin_ + cos_ add_ PCIe bandwidth backend default GB s PCIe_BW = __init__ module torch fx GraphModule sample_input Sequence Any operator_support OperatorSupportBase settings _SplitterSettingBase non_acc_submodule_name str = _run_on_cpu_ return_tuple bool = False nodes_finder Optional FxNetAccNodesFinder = None Preprocesses graph before splitting - finds nodes supported ACC - finds fusion groups ACC nodes having non-tensor IO - builds graph direct dependencies - builds map fused nodes their fusions As result we get acc_nodes deps fusions assert isinstance module torch fx GraphModule module = module ShapeProp module propagate sample_input settings = settings operator_support = operator_support sample_input = sample_input nodes_finder None nodes_finder = FxNetAccNodesFinder module operator_support settings allow_non_tensor acc_nodes = nodes_finder settings skip_fusion fusions = fusions = FxNetAccFusionsFinder module acc_nodes Modify deps add more deps fused nodes deps = find_deps update_deps_for_fusions non_acc_submodule_name = non_acc_submodule_name _node_submodule_map dict str str = _return_tuple = return_tuple tags list str = =============================================================== Helpers ctor initial state =============================================================== get_node_submodule_map - dict str str Returns map node name submodule name e g node main_module_impl_impl_over_arch_unary_multiple_embedding _pooling_embedding_pooling_sparse_entity_equivalence_key _proxy_embedding_bag maps submodule name _run_on_acc_ _node_submodule_map find_deps - dict torch fx Node NodeSet Builds graph node dependencies Leaf nodes don t have any dependencies output node doesn t have nodes depending Resulting graph has only direct dependencies i e there no transitive dependencies deps dict torch fx Node NodeSet = defaultdict set node module graph nodes node op CALLABLE_NODE_OPS continue user node users user op = output deps user add node deps update_deps_for_fusions Updates graph dependencies so - nodes same fusion depend same set outer nodes - outer nodes depending fusion depend all nodes fusion node fusions fusion = fusions node fused_neighbor fusion deps node update deps fused_neighbor - fusion user fused_neighbor users user fusion deps user add node =============================================================== Helpers preview =============================================================== _lower_model_to_backend mod torch fx GraphModule inputs Tensors - torch nn Module Lower model backend mod _find_culprit mod torch fx GraphModule inputs Tensors - str When error occurs during lowering running lowered mod we use function find culprits ` mod ` causes error Unable find culprit because _find_culprit function implemented _draw_graph_based_on_node_support mod torch fx GraphModule supported_nodes NodeList color_map = default AliceBlue supported chartreuse unsupported crimson CustomDrawer FxGraphDrawer _get_node_style node template = super _get_node_style node node supported_nodes template fillcolor = color_map supported node op CALLABLE_NODE_OPS template fillcolor = color_map unsupported template fillcolor = color_map default template drawer = CustomDrawer mod node_support ignore_getattr=True dot_graph = drawer get_main_dot_graph pyre-fixme ` pydot Dot ` has no attribute ` write_raw ` dot_graph write_raw node_support dot type ignore attr-defined node_support_preview dump_graph bool = False submodules = dict module named_modules supported_nodes NodeList = supported_node_types = defaultdict set unsupported_node_types = defaultdict set get_dtype arg tensor_meta = arg meta get tensor_meta getattr tensor_meta dtype None node module graph nodes node op CALLABLE_NODE_OPS continue target = get_node_target submodules node Store dtype arg node args If arg doesn t have dtype i e tensor we ll store None arg_dtypes = get_dtype arg isinstance arg torch fx Node None arg node args Find last non-None element If all elements None max_len last_index = len arg_dtypes - next i i dtype enumerate reversed arg_dtypes dtype None len arg_dtypes Strip None elements end arg_dtypes_tuple = tuple arg_dtypes last_index kwarg_dtypes_tuple = tuple k get_dtype arg k arg node kwargs items isinstance arg torch fx Node operator_support is_node_supported submodules node supported_nodes append node supported_node_types target add arg_dtypes_tuple kwarg_dtypes_tuple unsupported_node_types target add arg_dtypes_tuple kwarg_dtypes_tuple dump_graph _draw_graph_based_on_node_support module supported_nodes reports = \nSupported node types model \n t dtypes supported_node_types items arg_dtypes_tuple kwarg_dtypes_tuple dtypes reports += f t arg_dtypes_tuple dict kwarg_dtypes_tuple \n reports += \nUnsupported node types model \n t dtypes unsupported_node_types items arg_dtypes_tuple kwarg_dtypes_tuple dtypes reports += f t arg_dtypes_tuple dict kwarg_dtypes_tuple \n print reports Return reports testing purpose reports split_preview dump_graph bool = False reports = subgraphs = put_nodes_into_subgraphs acc_subgraphs_num = len g g subgraphs g is_acc cpu_subgraphs_num = len subgraphs - acc_subgraphs_num reports += f Before removing small acc subgraphs total len subgraphs subgraphs created reports += f acc_subgraphs_num acc subgraphs cpu_subgraphs_num cpu subgraphs \n subgraphs = remove_small_acc_subgraphs subgraphs acc_subgraphs_num = len g g subgraphs g is_acc cpu_subgraphs_num = len subgraphs - acc_subgraphs_num reports += f After removing small acc subgraphs total len subgraphs subgraphs created reports += f acc_subgraphs_num acc subgraphs cpu_subgraphs_num cpu subgraphs \n i subgraph enumerate subgraphs reports += f _run_on_acc_ i subgraph is_acc f non_acc_submodule_name i reports += f len subgraph nodes node s \n tag subgraphs split_mod = split remove_tag=True split_mod eval dump_graph drawer = FxGraphDrawer split_mod preview ignore_getattr=True dot_graphs = drawer get_all_dot_graphs name dot_graph dot_graphs items pyre-fixme ` pydot Dot ` has no attribute ` write_raw ` dot_graph write_raw f name dot type ignore attr-defined max_qps float = PCIe_BW bottleneck_module = node split_mod graph nodes node op == call_module acc node target reports += f \nProcessing acc submodule node target \n submod = getattr split_mod node target get_submod_inputs main_mod submod example_inputs sub_inputs = None get_inputs inputs nonlocal sub_inputs sub_inputs = inputs handle = submod register_forward_pre_hook get_inputs main_mod example_inputs handle remove sub_inputs submod_inputs = get_submod_inputs split_mod submod sample_input ShapeProp submod propagate submod_inputs total_input_bytes = total_output_bytes = reports += Checking inputs \n n submod graph nodes n op == placeholder is_node_output_tensor n reports += f Input n name tensor might cause problems during lowering \n total_input_bytes += get_size_of_node submod n n op == output output_node = n reports += Checking outputs \n get_bytes node torch fx Node nonlocal total_output_bytes nonlocal reports is_node_output_tensor node reports += f Output node name tensor might cause problems during lowering \n total_output_bytes += get_size_of_node submod node map_arg output_node args get_bytes type ignore possibly-undefined qps = PCIe_BW max total_input_bytes total_output_bytes reports += f Total input size bytes total_input_bytes total output size bytes total_output_bytes reports += f theoretical max qps bounds PCIe bandwidth submodule qps \n qps max_qps max_qps = qps bottleneck_module = node target try lowered_submod = _lower_model_to_backend submod submod_inputs except RuntimeError reports += Run into error during lowering \n reports += _find_culprit submod submod_inputs continue try lowered_submod submod_inputs except RuntimeError reports += Run into error during inference \n reports += _find_culprit submod submod_inputs reports += Lowering running succeed \n reports += f \nTheoretical max qps bounds PCIe bandwidth model max_qps reports += f bottleneck submodule bottleneck_module print reports reports testing purposes reports =============================================================== Helpers extend_acc_subgraph method =============================================================== find_reverse_deps tag_id Optional int = None - dict torch fx Node NodeSet Builds reversed topological node dependencies tag_id specified we ignore nodes later subgraph i e nodes have greater tag_id result dict torch fx Node NodeSet = defaultdict set node module graph nodes node op CALLABLE_NODE_OPS continue user node users user op CALLABLE_NODE_OPS continue tag_id None int user tag split _ - tag_id result node add user result update_reverse_deps_for_fusions deps dict torch fx Node NodeSet processed_node = set node fusion fusions items node processed_node continue new_dep = set Create new dependency set which include all dependencies nodes fusion group n fusion new_dep update deps n Exclude nodes fusion new_dep difference_update fusion Update dependency n fusion deps n = new_dep arg n all_input_nodes arg fusion deps arg update fusion processed_node add n find_parent_nodes_of_subgraph tag str - NodeSet Finds parent nodes ` tag ` subgraph Traverse inputs nodes subgraph input doesn t belong subgraph placeholder we consider parent node subgraph parent_nodes = set node module graph nodes node op CALLABLE_NODE_OPS node tag == tag arg node all_input_nodes arg op CALLABLE_NODE_OPS arg tag = tag parent_nodes add arg parent_nodes extend_acc_subgraph tag str Extend acc subgraph ` tag ` going reversed topological direction Dict maps node its users ignore users subgraph has greater tag deps = find_reverse_deps tag_id=int tag rsplit _ maxsplit= - update_reverse_deps_for_fusions deps Parent nodes subgraph parent_nodes = find_parent_nodes_of_subgraph tag visited_nodes NodeSet = set while parent_nodes node = None Find acc node depends visited nodes only n parent_nodes deps n = visited_nodes n acc_nodes node = n break node None break Put node into ` tag ` subgraph node tag = tag type ignore attr-defined parent_nodes remove node visited_nodes add node If node fusion group add all fusion buddies parent nodes node fusions fusion_node fusions node fusion_node visited_nodes parent_nodes add fusion_node Add inputs node parent nodes arg node all_input_nodes arg op CALLABLE_NODE_OPS arg visited_nodes parent_nodes add arg =============================================================== Helpers split method =============================================================== starter_nodes - tuple NodeSet NodeSet Finds nodes consume module inputs get_attr nodes starter_cpu_nodes NodeSet = set starter_acc_nodes NodeSet = set node module graph nodes edge case call_function no dependencies node op == call_function len node all_input_nodes == node acc_nodes starter_acc_nodes add node starter_cpu_nodes add node node op placeholder get_attr continue user node users user acc_nodes starter_acc_nodes add user starter_cpu_nodes add user starter_cpu_nodes starter_acc_nodes put_nodes_into_subgraphs - list Subgraph We start graph traversal leaf nodes current_cpu_nodes current_acc_nodes = starter_nodes visited_nodes NodeSet = set Determine which subgraph start based which subgraph has -dep node acc_subgraph bool = any len deps n == n current_cpu_nodes current_subgraph_nodes NodeList = Result accumulator subgraphs list Subgraph = while current_cpu_nodes current_acc_nodes Find first node should belong current subgraph has all dependencies resolved current_nodes = current_acc_nodes acc_subgraph current_cpu_nodes node = next n n current_nodes deps n = visited_nodes None If nothing found then s time flip mode start new subgraph node None current_subgraph_nodes raise FxNetSplitterInternalError Subgraph can t empty subgraphs append Subgraph is_acc=acc_subgraph nodes=current_subgraph_nodes acc_subgraph = acc_subgraph current_subgraph_nodes = continue current_nodes remove node visited_nodes add node current_subgraph_nodes append node Add fusion buddies node fusions node acc_nodes current_acc_nodes update fusions node - visited_nodes current_cpu_nodes update fusions node - visited_nodes Put depending nodes into queue user node users user op CALLABLE_NODE_OPS continue Add downstream nodes user acc_nodes current_acc_nodes add user current_cpu_nodes add user Check last subgraph created current_subgraph_nodes subgraphs append Subgraph is_acc=acc_subgraph nodes=current_subgraph_nodes subgraphs raise FxNetSplitterInternalError Couldn t create subgraphs subgraphs remove_small_acc_subgraphs subgraphs list Subgraph - list Subgraph This pass finds ACC submodules less than specified size merges them adjacent CPU submodules result list Subgraph = subgraph subgraphs subgraph is_acc len subgraph nodes = settings min_acc_module_size result append subgraph print Eliminating acc subgraph because s smaller than threshold f len subgraph nodes settings min_acc_module_size result result - nodes extend subgraph nodes subgraph is_acc = False result append subgraph result result - is_acc result - nodes extend subgraph nodes result append subgraph result tag subgraphs list Subgraph tags = subgraph subgraphs tag = f _run_on_acc_ len tags subgraph is_acc f non_acc_submodule_name len tags tags append tag node subgraph nodes hasattr node tag raise FxNetSplitterInternalError f Node node already tagged node tag = tag type ignore attr-defined _node_submodule_map node name = tag split remove_tag bool = False - torch fx GraphModule split_module = split_by_tags module tags return_tuple=self _return_tuple remove_tag node module graph nodes hasattr node tag del node tag split_module type ignore return-value __call__ - torch fx GraphModule subgraphs = put_nodes_into_subgraphs subgraphs = remove_small_acc_subgraphs subgraphs acc_subgraphs_count = len s s subgraphs s is_acc non_acc_subgraphs_count = len subgraphs - acc_subgraphs_count print f Got acc_subgraphs_count acc subgraphs non_acc_subgraphs_count non-acc subgraphs tag subgraphs split generate_split_results - SplitResult split_module = submodule_names = name _mod split_module named_children submodule_names append name settings max_acc_splits len submodule_names settings max_acc_splits raise ValueError Cannot fulfill max_acc_splits limit This may cause split fragmentation result performance issues submodule_inputs = generate_inputs_for_submodules split_module sample_input submodule_names SplitResult split_module submodule_inputs non_acc_submodule_name