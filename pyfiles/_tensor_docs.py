mypy allow-untyped-defs Adds docstrings Tensor functions torch _C torch _C _add_docstr add_docstr torch _torch_docs parse_kwargs reproducibility_notes add_docstr_all method str docstr str - None add_docstr getattr torch _C TensorBase method docstr common_args = parse_kwargs memory_format ` torch memory_format ` optional desired memory format returned Tensor Default ` ` torch preserve_format ` ` new_common_args = parse_kwargs size int list tuple ` torch Size ` integers defining shape output tensor dtype ` torch dtype ` optional desired type returned tensor Default None same ` torch dtype ` tensor device ` torch device ` optional desired device returned tensor Default None same ` torch device ` tensor requires_grad bool optional If autograd should record operations returned tensor Default ` ` False ` ` pin_memory bool optional If set returned tensor would allocated pinned memory Works only CPU tensors Default ` ` False ` ` layout ` torch layout ` optional desired layout returned Tensor Default ` ` torch strided ` ` add_docstr_all new_tensor new_tensor data dtype=None device=None requires_grad=False layout=torch strided \ pin_memory=False - Tensor + r Returns new Tensor attr ` data ` tensor data By default returned Tensor has same ` torch dtype ` ` torch device ` tensor warning func ` new_tensor ` always copies attr ` data ` If you have Tensor ` ` data ` ` want avoid copy use func ` torch Tensor requires_grad_ ` func ` torch Tensor detach ` If you have numpy array want avoid copy use func ` torch from_numpy ` warning When data tensor ` x ` func ` new_tensor ` reads out data whatever passed constructs leaf variable Therefore ` ` tensor new_tensor x ` ` equivalent ` ` x detach clone ` ` ` ` tensor new_tensor x requires_grad=True ` ` equivalent ` ` x detach clone requires_grad_ True ` ` The equivalents using ` ` detach ` ` ` ` clone ` ` recommended Args data array_like The returned Tensor copies attr ` data ` Keyword args dtype device requires_grad layout pin_memory Example tensor = torch ones dtype=torch int data = tensor new_tensor data tensor dtype=torch int format new_common_args add_docstr_all new_full new_full size fill_value dtype=None device=None requires_grad=False layout=torch strided \ pin_memory=False - Tensor + r Returns Tensor size attr ` size ` filled attr ` fill_value ` By default returned Tensor has same ` torch dtype ` ` torch device ` tensor Args fill_value scalar number fill output tensor Keyword args dtype device requires_grad layout pin_memory Example tensor = torch ones dtype=torch float tensor new_full tensor dtype=torch float format new_common_args add_docstr_all new_empty new_empty size dtype=None device=None requires_grad=False layout=torch strided \ pin_memory=False - Tensor + r Returns Tensor size attr ` size ` filled uninitialized data By default returned Tensor has same ` torch dtype ` ` torch device ` tensor Args size int list tuple ` torch Size ` integers defining shape output tensor Keyword args dtype device requires_grad layout pin_memory Example tensor = torch ones tensor new_empty tensor e- e- - e+ e- e- e+ format new_common_args add_docstr_all new_empty_strided new_empty_strided size stride dtype=None device=None requires_grad=False layout=torch strided \ pin_memory=False - Tensor + r Returns Tensor size attr ` size ` strides attr ` stride ` filled uninitialized data By default returned Tensor has same ` torch dtype ` ` torch device ` tensor Args size int list tuple ` torch Size ` integers defining shape output tensor Keyword args dtype device requires_grad layout pin_memory Example tensor = torch ones tensor new_empty_strided tensor e- e- - e+ e- e- e+ format new_common_args add_docstr_all new_ones new_ones size dtype=None device=None requires_grad=False layout=torch strided \ pin_memory=False - Tensor + r Returns Tensor size attr ` size ` filled ` ` ` ` By default returned Tensor has same ` torch dtype ` ` torch device ` tensor Args size int list tuple ` torch Size ` integers defining shape output tensor Keyword args dtype device requires_grad layout pin_memory Example tensor = torch tensor dtype=torch int tensor new_ones tensor dtype=torch int format new_common_args add_docstr_all new_zeros new_zeros size dtype=None device=None requires_grad=False layout=torch strided \ pin_memory=False - Tensor + r Returns Tensor size attr ` size ` filled ` ` ` ` By default returned Tensor has same ` torch dtype ` ` torch device ` tensor Args size int list tuple ` torch Size ` integers defining shape output tensor Keyword args dtype device requires_grad layout pin_memory Example tensor = torch tensor dtype=torch float tensor new_zeros tensor dtype=torch float format new_common_args add_docstr_all abs r abs - Tensor See func ` torch abs ` add_docstr_all abs_ r abs_ - Tensor In-place version meth ` ~Tensor abs ` add_docstr_all absolute r absolute - Tensor Alias func ` abs ` add_docstr_all absolute_ r absolute_ - Tensor In-place version meth ` ~Tensor absolute ` Alias func ` abs_ ` add_docstr_all acos r acos - Tensor See func ` torch acos ` add_docstr_all acos_ r acos_ - Tensor In-place version meth ` ~Tensor acos ` add_docstr_all arccos r arccos - Tensor See func ` torch arccos ` add_docstr_all arccos_ r arccos_ - Tensor In-place version meth ` ~Tensor arccos ` add_docstr_all acosh r acosh - Tensor See func ` torch acosh ` add_docstr_all acosh_ r acosh_ - Tensor In-place version meth ` ~Tensor acosh ` add_docstr_all arccosh r acosh - Tensor See func ` torch arccosh ` add_docstr_all arccosh_ r acosh_ - Tensor In-place version meth ` ~Tensor arccosh ` add_docstr_all add r add other alpha= - Tensor Add scalar tensor attr ` ` tensor If both attr ` alpha ` attr ` other ` specified each element attr ` other ` scaled attr ` alpha ` before being used When attr ` other ` tensor shape attr ` other ` must ref ` broadcastable broadcasting-semantics ` shape underlying tensor See func ` torch add ` add_docstr_all add_ r add_ other alpha= - Tensor In-place version meth ` ~Tensor add ` add_docstr_all addbmm r addbmm batch batch beta= alpha= - Tensor See func ` torch addbmm ` add_docstr_all addbmm_ r addbmm_ batch batch beta= alpha= - Tensor In-place version meth ` ~Tensor addbmm ` add_docstr_all addcdiv r addcdiv tensor tensor value= - Tensor See func ` torch addcdiv ` add_docstr_all addcdiv_ r addcdiv_ tensor tensor value= - Tensor In-place version meth ` ~Tensor addcdiv ` add_docstr_all addcmul r addcmul tensor tensor value= - Tensor See func ` torch addcmul ` add_docstr_all addcmul_ r addcmul_ tensor tensor value= - Tensor In-place version meth ` ~Tensor addcmul ` add_docstr_all addmm r addmm mat mat beta= alpha= - Tensor See func ` torch addmm ` add_docstr_all addmm_ r addmm_ mat mat beta= alpha= - Tensor In-place version meth ` ~Tensor addmm ` add_docstr_all addmv r addmv mat vec beta= alpha= - Tensor See func ` torch addmv ` add_docstr_all addmv_ r addmv_ mat vec beta= alpha= - Tensor In-place version meth ` ~Tensor addmv ` add_docstr_all sspaddmm r sspaddmm mat mat beta= alpha= - Tensor See func ` torch sspaddmm ` add_docstr_all smm r smm mat - Tensor See func ` torch smm ` add_docstr_all addr r addr vec vec beta= alpha= - Tensor See func ` torch addr ` add_docstr_all addr_ r addr_ vec vec beta= alpha= - Tensor In-place version meth ` ~Tensor addr ` add_docstr_all align_as r align_as other - Tensor Permutes dimensions attr ` ` tensor match dimension order attr ` other ` tensor adding size-one dims any new names This operation useful explicit broadcasting names see examples All dims attr ` ` must named order use method The resulting tensor view original tensor All dimension names attr ` ` must present ` ` other names ` ` attr ` other ` may contain named dimensions ` ` names ` ` output tensor has size-one dimension each those new names To align tensor specific order use meth ` ~Tensor align_to ` Examples Example Applying mask mask = torch randint dtype=torch bool refine_names W H imgs = torch randn names= N H W C imgs masked_fill_ mask align_as imgs Example Applying per-channel-scale scale_channels input scale scale = scale refine_names C input scale align_as input num_channels = scale = torch randn num_channels names= C imgs = torch rand num_channels names= N H W C more_imgs = torch rand num_channels names= N C H W videos = torch randn num_channels names= N C H W D scale_channels agnostic dimension order input scale_channels imgs scale scale_channels more_imgs scale scale_channels videos scale warning The named tensor API experimental subject change add_docstr_all all r all dim=None keepdim=False - Tensor See func ` torch all ` add_docstr_all allclose r allclose other rtol= e- atol= e- equal_nan=False - Tensor See func ` torch allclose ` add_docstr_all angle r angle - Tensor See func ` torch angle ` add_docstr_all any r any dim=None keepdim=False - Tensor See func ` torch any ` add_docstr_all apply_ r apply_ callable - Tensor Applies function attr ` callable ` each element tensor replacing each element value returned attr ` callable ` note This function only works CPU tensors should used code sections require high performance add_docstr_all asin r asin - Tensor See func ` torch asin ` add_docstr_all asin_ r asin_ - Tensor In-place version meth ` ~Tensor asin ` add_docstr_all arcsin r arcsin - Tensor See func ` torch arcsin ` add_docstr_all arcsin_ r arcsin_ - Tensor In-place version meth ` ~Tensor arcsin ` add_docstr_all asinh r asinh - Tensor See func ` torch asinh ` add_docstr_all asinh_ r asinh_ - Tensor In-place version meth ` ~Tensor asinh ` add_docstr_all arcsinh r arcsinh - Tensor See func ` torch arcsinh ` add_docstr_all arcsinh_ r arcsinh_ - Tensor In-place version meth ` ~Tensor arcsinh ` add_docstr_all as_strided r as_strided size stride storage_offset=None - Tensor See func ` torch as_strided ` add_docstr_all as_strided_ r as_strided_ size stride storage_offset=None - Tensor In-place version meth ` ~Tensor as_strided ` add_docstr_all atan r atan - Tensor See func ` torch atan ` add_docstr_all atan_ r atan_ - Tensor In-place version meth ` ~Tensor atan ` add_docstr_all arctan r arctan - Tensor See func ` torch arctan ` add_docstr_all arctan_ r arctan_ - Tensor In-place version meth ` ~Tensor arctan ` add_docstr_all atan r atan other - Tensor See func ` torch atan ` add_docstr_all atan _ r atan _ other - Tensor In-place version meth ` ~Tensor atan ` add_docstr_all arctan r arctan other - Tensor See func ` torch arctan ` add_docstr_all arctan _ r atan _ other - Tensor In-place version meth ` ~Tensor arctan ` add_docstr_all atanh r atanh - Tensor See func ` torch atanh ` add_docstr_all atanh_ r atanh_ other - Tensor In-place version meth ` ~Tensor atanh ` add_docstr_all arctanh r arctanh - Tensor See func ` torch arctanh ` add_docstr_all arctanh_ r arctanh_ other - Tensor In-place version meth ` ~Tensor arctanh ` add_docstr_all baddbmm r baddbmm batch batch beta= alpha= - Tensor See func ` torch baddbmm ` add_docstr_all baddbmm_ r baddbmm_ batch batch beta= alpha= - Tensor In-place version meth ` ~Tensor baddbmm ` add_docstr_all bernoulli r bernoulli generator=None - Tensor Returns result tensor where each math ` \texttt result i ` independently sampled math ` \text Bernoulli \texttt i ` attr ` ` must have floating point ` ` dtype ` ` result will have same ` ` dtype ` ` See func ` torch bernoulli ` add_docstr_all bernoulli_ r bernoulli_ p= generator=None - Tensor Fills each location attr ` ` independent sample math ` \text Bernoulli \texttt p ` attr ` ` can have integral ` ` dtype ` ` attr ` p ` should either scalar tensor containing probabilities used drawing binary random number If tensor math ` \text i ^ th ` element attr ` ` tensor will set value sampled math ` \text Bernoulli \texttt p\_tensor i ` In case ` p ` must have floating point ` ` dtype ` ` See also meth ` ~Tensor bernoulli ` func ` torch bernoulli ` add_docstr_all bincount r bincount weights=None minlength= - Tensor See func ` torch bincount ` add_docstr_all bitwise_not r bitwise_not - Tensor See func ` torch bitwise_not ` add_docstr_all bitwise_not_ r bitwise_not_ - Tensor In-place version meth ` ~Tensor bitwise_not ` add_docstr_all bitwise_and r bitwise_and - Tensor See func ` torch bitwise_and ` add_docstr_all bitwise_and_ r bitwise_and_ - Tensor In-place version meth ` ~Tensor bitwise_and ` add_docstr_all bitwise_or r bitwise_or - Tensor See func ` torch bitwise_or ` add_docstr_all bitwise_or_ r bitwise_or_ - Tensor In-place version meth ` ~Tensor bitwise_or ` add_docstr_all bitwise_xor r bitwise_xor - Tensor See func ` torch bitwise_xor ` add_docstr_all bitwise_xor_ r bitwise_xor_ - Tensor In-place version meth ` ~Tensor bitwise_xor ` add_docstr_all bitwise_left_shift r bitwise_left_shift other - Tensor See func ` torch bitwise_left_shift ` add_docstr_all bitwise_left_shift_ r bitwise_left_shift_ other - Tensor In-place version meth ` ~Tensor bitwise_left_shift ` add_docstr_all bitwise_right_shift r bitwise_right_shift other - Tensor See func ` torch bitwise_right_shift ` add_docstr_all bitwise_right_shift_ r bitwise_right_shift_ other - Tensor In-place version meth ` ~Tensor bitwise_right_shift ` add_docstr_all broadcast_to r broadcast_to shape - Tensor See func ` torch broadcast_to ` add_docstr_all logical_and r logical_and - Tensor See func ` torch logical_and ` add_docstr_all logical_and_ r logical_and_ - Tensor In-place version meth ` ~Tensor logical_and ` add_docstr_all logical_not r logical_not - Tensor See func ` torch logical_not ` add_docstr_all logical_not_ r logical_not_ - Tensor In-place version meth ` ~Tensor logical_not ` add_docstr_all logical_or r logical_or - Tensor See func ` torch logical_or ` add_docstr_all logical_or_ r logical_or_ - Tensor In-place version meth ` ~Tensor logical_or ` add_docstr_all logical_xor r logical_xor - Tensor See func ` torch logical_xor ` add_docstr_all logical_xor_ r logical_xor_ - Tensor In-place version meth ` ~Tensor logical_xor ` add_docstr_all bmm r bmm batch - Tensor See func ` torch bmm ` add_docstr_all cauchy_ r cauchy_ median= sigma= generator=None - Tensor Fills tensor numbers drawn Cauchy distribution math f x = \dfrac \pi \dfrac \sigma x - \text median ^ + \sigma^ note Sigma math ` \sigma ` used denote scale parameter Cauchy distribution add_docstr_all ceil r ceil - Tensor See func ` torch ceil ` add_docstr_all ceil_ r ceil_ - Tensor In-place version meth ` ~Tensor ceil ` add_docstr_all cholesky r cholesky upper=False - Tensor See func ` torch cholesky ` add_docstr_all cholesky_solve r cholesky_solve input upper=False - Tensor See func ` torch cholesky_solve ` add_docstr_all cholesky_inverse r cholesky_inverse upper=False - Tensor See func ` torch cholesky_inverse ` add_docstr_all clamp r clamp min=None max=None - Tensor See func ` torch clamp ` add_docstr_all clamp_ r clamp_ min=None max=None - Tensor In-place version meth ` ~Tensor clamp ` add_docstr_all clip r clip min=None max=None - Tensor Alias meth ` ~Tensor clamp ` add_docstr_all clip_ r clip_ min=None max=None - Tensor Alias meth ` ~Tensor clamp_ ` add_docstr_all clone r clone memory_format=torch preserve_format - Tensor See func ` torch clone ` format common_args add_docstr_all coalesce r coalesce - Tensor Returns coalesced copy attr ` ` attr ` ` ref ` uncoalesced tensor sparse-uncoalesced-coo-docs ` Returns attr ` ` attr ` ` coalesced tensor warning Throws error attr ` ` sparse COO tensor add_docstr_all contiguous r contiguous memory_format=torch contiguous_format - Tensor Returns contiguous memory tensor containing same data attr ` ` tensor If attr ` ` tensor already specified memory format function returns attr ` ` tensor Args memory_format ` torch memory_format ` optional desired memory format returned Tensor Default ` ` torch contiguous_format ` ` add_docstr_all copy_ r copy_ src non_blocking=False - Tensor Copies elements attr ` src ` into attr ` ` tensor returns attr ` ` The attr ` src ` tensor must ref ` broadcastable broadcasting-semantics ` attr ` ` tensor It may different data type reside different device Args src Tensor source tensor copy non_blocking bool optional ` ` True ` ` copy between CPU GPU copy may occur asynchronously respect host For other cases argument has no effect Default ` ` False ` ` add_docstr_all conj r conj - Tensor See func ` torch conj ` add_docstr_all conj_physical r conj_physical - Tensor See func ` torch conj_physical ` add_docstr_all conj_physical_ r conj_physical_ - Tensor In-place version meth ` ~Tensor conj_physical ` add_docstr_all resolve_conj r resolve_conj - Tensor See func ` torch resolve_conj ` add_docstr_all resolve_neg r resolve_neg - Tensor See func ` torch resolve_neg ` add_docstr_all copysign r copysign other - Tensor See func ` torch copysign ` add_docstr_all copysign_ r copysign_ other - Tensor In-place version meth ` ~Tensor copysign ` add_docstr_all cos r cos - Tensor See func ` torch cos ` add_docstr_all cos_ r cos_ - Tensor In-place version meth ` ~Tensor cos ` add_docstr_all cosh r cosh - Tensor See func ` torch cosh ` add_docstr_all cosh_ r cosh_ - Tensor In-place version meth ` ~Tensor cosh ` add_docstr_all cpu r cpu memory_format=torch preserve_format - Tensor Returns copy object CPU memory If object already CPU memory then no copy performed original object returned Args memory_format format common_args add_docstr_all count_nonzero r count_nonzero dim=None - Tensor See func ` torch count_nonzero ` add_docstr_all cov r cov correction= fweights=None aweights=None - Tensor See func ` torch cov ` add_docstr_all corrcoef r corrcoef - Tensor See func ` torch corrcoef ` add_docstr_all cross r cross other dim=None - Tensor See func ` torch cross ` add_docstr_all cuda r cuda device=None non_blocking=False memory_format=torch preserve_format - Tensor Returns copy object CUDA memory If object already CUDA memory correct device then no copy performed original object returned Args device ` torch device ` optional The destination GPU device Defaults current CUDA device non_blocking bool optional If ` ` True ` ` source pinned memory copy will asynchronous respect host Otherwise argument has no effect Default ` ` False ` ` memory_format format common_args add_docstr_all mtia r mtia device=None non_blocking=False memory_format=torch preserve_format - Tensor Returns copy object MTIA memory If object already MTIA memory correct device then no copy performed original object returned Args device ` torch device ` optional The destination MTIA device Defaults current MTIA device non_blocking bool optional If ` ` True ` ` source pinned memory copy will asynchronous respect host Otherwise argument has no effect Default ` ` False ` ` memory_format format common_args add_docstr_all ipu r ipu device=None non_blocking=False memory_format=torch preserve_format - Tensor Returns copy object IPU memory If object already IPU memory correct device then no copy performed original object returned Args device ` torch device ` optional The destination IPU device Defaults current IPU device non_blocking bool optional If ` ` True ` ` source pinned memory copy will asynchronous respect host Otherwise argument has no effect Default ` ` False ` ` memory_format format common_args add_docstr_all xpu r xpu device=None non_blocking=False memory_format=torch preserve_format - Tensor Returns copy object XPU memory If object already XPU memory correct device then no copy performed original object returned Args device ` torch device ` optional The destination XPU device Defaults current XPU device non_blocking bool optional If ` ` True ` ` source pinned memory copy will asynchronous respect host Otherwise argument has no effect Default ` ` False ` ` memory_format format common_args add_docstr_all logcumsumexp r logcumsumexp dim - Tensor See func ` torch logcumsumexp ` add_docstr_all cummax r cummax dim - Tensor Tensor See func ` torch cummax ` add_docstr_all cummin r cummin dim - Tensor Tensor See func ` torch cummin ` add_docstr_all cumprod r cumprod dim dtype=None - Tensor See func ` torch cumprod ` add_docstr_all cumprod_ r cumprod_ dim dtype=None - Tensor In-place version meth ` ~Tensor cumprod ` add_docstr_all cumsum r cumsum dim dtype=None - Tensor See func ` torch cumsum ` add_docstr_all cumsum_ r cumsum_ dim dtype=None - Tensor In-place version meth ` ~Tensor cumsum ` add_docstr_all data_ptr r data_ptr - int Returns address first element attr ` ` tensor add_docstr_all dequantize r dequantize - Tensor Given quantized Tensor dequantize dequantized float Tensor add_docstr_all dense_dim r dense_dim - int Return number dense dimensions ref ` sparse tensor sparse-docs ` attr ` ` note Returns ` ` len shape ` ` attr ` ` sparse tensor See also meth ` Tensor sparse_dim ` ref ` hybrid tensors sparse-hybrid-coo-docs ` add_docstr_all diag r diag diagonal= - Tensor See func ` torch diag ` add_docstr_all diag_embed r diag_embed offset= dim =- dim =- - Tensor See func ` torch diag_embed ` add_docstr_all diagflat r diagflat offset= - Tensor See func ` torch diagflat ` add_docstr_all diagonal r diagonal offset= dim = dim = - Tensor See func ` torch diagonal ` add_docstr_all diagonal_scatter r diagonal_scatter src offset= dim = dim = - Tensor See func ` torch diagonal_scatter ` add_docstr_all as_strided_scatter r as_strided_scatter src size stride storage_offset=None - Tensor See func ` torch as_strided_scatter ` add_docstr_all fill_diagonal_ r fill_diagonal_ fill_value wrap=False - Tensor Fill main diagonal tensor has least -dimensions When dims all dimensions input must equal length This function modifies input tensor in-place returns input tensor Arguments fill_value Scalar fill value wrap bool optional diagonal wrapped after N columns tall matrices Default ` ` False ` ` Example = torch zeros fill_diagonal_ tensor b = torch zeros b fill_diagonal_ tensor c = torch zeros c fill_diagonal_ wrap=True tensor add_docstr_all floor_divide r floor_divide value - Tensor See func ` torch floor_divide ` add_docstr_all floor_divide_ r floor_divide_ value - Tensor In-place version meth ` ~Tensor floor_divide ` add_docstr_all diff r diff n= dim=- prepend=None append=None - Tensor See func ` torch diff ` add_docstr_all digamma r digamma - Tensor See func ` torch digamma ` add_docstr_all digamma_ r digamma_ - Tensor In-place version meth ` ~Tensor digamma ` add_docstr_all dim r dim - int Returns number dimensions attr ` ` tensor add_docstr_all dist r dist other p= - Tensor See func ` torch dist ` add_docstr_all div r div value rounding_mode=None - Tensor See func ` torch div ` add_docstr_all div_ r div_ value rounding_mode=None - Tensor In-place version meth ` ~Tensor div ` add_docstr_all divide r divide value rounding_mode=None - Tensor See func ` torch divide ` add_docstr_all divide_ r divide_ value rounding_mode=None - Tensor In-place version meth ` ~Tensor divide ` add_docstr_all dot r dot other - Tensor See func ` torch dot ` add_docstr_all element_size r element_size - int Returns size bytes individual element Example torch tensor element_size torch tensor dtype=torch uint element_size add_docstr_all eq r eq other - Tensor See func ` torch eq ` add_docstr_all eq_ r eq_ other - Tensor In-place version meth ` ~Tensor eq ` add_docstr_all equal r equal other - bool See func ` torch equal ` add_docstr_all erf r erf - Tensor See func ` torch erf ` add_docstr_all erf_ r erf_ - Tensor In-place version meth ` ~Tensor erf ` add_docstr_all erfc r erfc - Tensor See func ` torch erfc ` add_docstr_all erfc_ r erfc_ - Tensor In-place version meth ` ~Tensor erfc ` add_docstr_all erfinv r erfinv - Tensor See func ` torch erfinv ` add_docstr_all erfinv_ r erfinv_ - Tensor In-place version meth ` ~Tensor erfinv ` add_docstr_all exp r exp - Tensor See func ` torch exp ` add_docstr_all exp_ r exp_ - Tensor In-place version meth ` ~Tensor exp ` add_docstr_all exp r exp - Tensor See func ` torch exp ` add_docstr_all exp _ r exp _ - Tensor In-place version meth ` ~Tensor exp ` add_docstr_all expm r expm - Tensor See func ` torch expm ` add_docstr_all expm _ r expm _ - Tensor In-place version meth ` ~Tensor expm ` add_docstr_all exponential_ r exponential_ lambd= generator=None - Tensor Fills attr ` ` tensor elements drawn PDF probability density function math f x = \lambda e^ -\lambda x x note In probability theory exponential distribution supported interval math ` \inf ` i e math ` x = ` implying zero can sampled exponential distribution However func ` torch Tensor exponential_ ` does sample zero which means its actual support interval math ` \inf ` Note func ` torch distributions exponential Exponential ` supported interval math ` \inf ` can sample zero add_docstr_all fill_ r fill_ value - Tensor Fills attr ` ` tensor specified value add_docstr_all floor r floor - Tensor See func ` torch floor ` add_docstr_all flip r flip dims - Tensor See func ` torch flip ` add_docstr_all fliplr r fliplr - Tensor See func ` torch fliplr ` add_docstr_all flipud r flipud - Tensor See func ` torch flipud ` add_docstr_all roll r roll shifts dims - Tensor See func ` torch roll ` add_docstr_all floor_ r floor_ - Tensor In-place version meth ` ~Tensor floor ` add_docstr_all fmod r fmod divisor - Tensor See func ` torch fmod ` add_docstr_all fmod_ r fmod_ divisor - Tensor In-place version meth ` ~Tensor fmod ` add_docstr_all frac r frac - Tensor See func ` torch frac ` add_docstr_all frac_ r frac_ - Tensor In-place version meth ` ~Tensor frac ` add_docstr_all frexp r frexp input - Tensor mantissa Tensor exponent See func ` torch frexp ` add_docstr_all flatten r flatten start_dim= end_dim=- - Tensor See func ` torch flatten ` add_docstr_all gather r gather dim index - Tensor See func ` torch gather ` add_docstr_all gcd r gcd other - Tensor See func ` torch gcd ` add_docstr_all gcd_ r gcd_ other - Tensor In-place version meth ` ~Tensor gcd ` add_docstr_all ge r ge other - Tensor See func ` torch ge ` add_docstr_all ge_ r ge_ other - Tensor In-place version meth ` ~Tensor ge ` add_docstr_all greater_equal r greater_equal other - Tensor See func ` torch greater_equal ` add_docstr_all greater_equal_ r greater_equal_ other - Tensor In-place version meth ` ~Tensor greater_equal ` add_docstr_all geometric_ r geometric_ p generator=None - Tensor Fills attr ` ` tensor elements drawn geometric distribution math P X=k = - p ^ k - p k = note func ` torch Tensor geometric_ ` ` k ` -th trial first success hence draws samples math ` \ \ldots\ ` whereas func ` torch distributions geometric Geometric ` math ` k+ ` -th trial first success hence draws samples math ` \ \ldots\ ` add_docstr_all geqrf r geqrf - Tensor Tensor See func ` torch geqrf ` add_docstr_all ger r ger vec - Tensor See func ` torch ger ` add_docstr_all inner r inner other - Tensor See func ` torch inner ` add_docstr_all outer r outer vec - Tensor See func ` torch outer ` add_docstr_all hypot r hypot other - Tensor See func ` torch hypot ` add_docstr_all hypot_ r hypot_ other - Tensor In-place version meth ` ~Tensor hypot ` add_docstr_all i r i - Tensor See func ` torch i ` add_docstr_all i _ r i _ - Tensor In-place version meth ` ~Tensor i ` add_docstr_all igamma r igamma other - Tensor See func ` torch igamma ` add_docstr_all igamma_ r igamma_ other - Tensor In-place version meth ` ~Tensor igamma ` add_docstr_all igammac r igammac other - Tensor See func ` torch igammac ` add_docstr_all igammac_ r igammac_ other - Tensor In-place version meth ` ~Tensor igammac ` add_docstr_all indices r indices - Tensor Return indices tensor ref ` sparse COO tensor sparse-coo-docs ` warning Throws error attr ` ` sparse COO tensor See also meth ` Tensor values ` note This method can only called coalesced sparse tensor See meth ` Tensor coalesce ` details add_docstr_all get_device r get_device - Device ordinal Integer For CUDA tensors function returns device ordinal GPU which tensor resides For CPU tensors function returns ` - ` Example x = torch randn device= cuda x get_device x cpu get_device - add_docstr_all values r values - Tensor Return values tensor ref ` sparse COO tensor sparse-coo-docs ` warning Throws error attr ` ` sparse COO tensor See also meth ` Tensor indices ` note This method can only called coalesced sparse tensor See meth ` Tensor coalesce ` details add_docstr_all gt r gt other - Tensor See func ` torch gt ` add_docstr_all gt_ r gt_ other - Tensor In-place version meth ` ~Tensor gt ` add_docstr_all greater r greater other - Tensor See func ` torch greater ` add_docstr_all greater_ r greater_ other - Tensor In-place version meth ` ~Tensor greater ` add_docstr_all has_names r Is ` ` True ` ` any tensor s dimensions named Otherwise ` ` False ` ` add_docstr_all hardshrink r hardshrink lambd= - Tensor See func ` torch nn functional hardshrink ` add_docstr_all heaviside r heaviside values - Tensor See func ` torch heaviside ` add_docstr_all heaviside_ r heaviside_ values - Tensor In-place version meth ` ~Tensor heaviside ` add_docstr_all histc r histc bins= min= max= - Tensor See func ` torch histc ` add_docstr_all histogram r histogram input bins range=None weight=None density=False - Tensor Tensor See func ` torch histogram ` add_docstr_all index_add_ r index_add_ dim index source alpha= - Tensor Accumulate elements attr ` alpha ` times ` ` source ` ` into attr ` ` tensor adding indices order given attr ` index ` For example ` ` dim == ` ` ` ` index i == j ` ` ` ` alpha=- ` ` then ` ` i ` ` \ th row ` ` source ` ` subtracted ` ` j ` ` \ th row attr ` ` The attr ` dim ` \ th dimension ` ` source ` ` must have same size length attr ` index ` which must vector all other dimensions must match attr ` ` error will raised For -D tensor output given index i += alpha src i dim == index i += alpha src i dim == index i += alpha src i dim == Note forward_reproducibility_note Args dim int dimension along which index index Tensor indices ` ` source ` ` select should have dtype either ` torch int ` ` torch int ` source Tensor tensor containing values add Keyword args alpha Number scalar multiplier ` ` source ` ` Example x = torch ones t = torch tensor dtype=torch float index = torch tensor x index_add_ index t tensor x index_add_ index t alpha=- tensor format reproducibility_notes add_docstr_all index_copy_ r index_copy_ dim index tensor - Tensor Copies elements attr ` tensor ` into attr ` ` tensor selecting indices order given attr ` index ` For example ` ` dim == ` ` ` ` index i == j ` ` then ` ` i ` ` \ th row attr ` tensor ` copied ` ` j ` ` \ th row attr ` ` The attr ` dim ` \ th dimension attr ` tensor ` must have same size length attr ` index ` which must vector all other dimensions must match attr ` ` error will raised note If attr ` index ` contains duplicate entries multiple elements attr ` tensor ` will copied same index attr ` ` The result nondeterministic since depends which copy occurs last Args dim int dimension along which index index LongTensor indices attr ` tensor ` select tensor Tensor tensor containing values copy Example x = torch zeros t = torch tensor dtype=torch float index = torch tensor x index_copy_ index t tensor add_docstr_all index_fill_ r index_fill_ dim index value - Tensor Fills elements attr ` ` tensor value attr ` value ` selecting indices order given attr ` index ` Args dim int dimension along which index index LongTensor indices attr ` ` tensor fill value float value fill Example x = torch tensor dtype=torch float index = torch tensor x index_fill_ index - tensor - - - - - - add_docstr_all index_put_ r index_put_ indices values accumulate=False - Tensor Puts values tensor attr ` values ` into tensor attr ` ` using indices specified attr ` indices ` which tuple Tensors The expression ` ` tensor index_put_ indices values ` ` equivalent ` ` tensor indices = values ` ` Returns attr ` ` If attr ` accumulate ` ` ` True ` ` elements attr ` values ` added attr ` ` If accumulate ` ` False ` ` behavior undefined indices contain duplicate elements Args indices tuple LongTensor tensors used index into ` ` values Tensor tensor same dtype ` ` accumulate bool whether accumulate into add_docstr_all index_put r index_put indices values accumulate=False - Tensor Out-place version meth ` ~Tensor index_put_ ` add_docstr_all index_reduce_ r index_reduce_ dim index source reduce include_self=True - Tensor Accumulate elements ` ` source ` ` into attr ` ` tensor accumulating indices order given attr ` index ` using reduction given ` ` reduce ` ` argument For example ` ` dim == ` ` ` ` index i == j ` ` ` ` reduce == prod ` ` ` ` include_self == True ` ` then ` ` i ` ` \ th row ` ` source ` ` multiplied ` ` j ` ` \ th row attr ` ` If obj ` include_self= True ` values attr ` ` tensor included reduction otherwise rows attr ` ` tensor accumulated treated they filled reduction identities The attr ` dim ` \ th dimension ` ` source ` ` must have same size length attr ` index ` which must vector all other dimensions must match attr ` ` error will raised For -D tensor obj ` reduce= prod ` obj ` include_self=True ` output given index i = src i dim == index i = src i dim == index i = src i dim == Note forward_reproducibility_note note This function only supports floating point tensors warning This function beta may change near future Args dim int dimension along which index index Tensor indices ` ` source ` ` select should have dtype either ` torch int ` ` torch int ` source FloatTensor tensor containing values accumulate reduce str reduction operation apply obj ` prod ` obj ` mean ` obj ` amax ` obj ` amin ` Keyword args include_self bool whether elements ` ` ` ` tensor included reduction Example x = torch empty fill_ t = torch tensor dtype=torch float index = torch tensor x index_reduce_ index t prod tensor x = torch empty fill_ x index_reduce_ index t prod include_self=False tensor format reproducibility_notes add_docstr_all index_select r index_select dim index - Tensor See func ` torch index_select ` add_docstr_all sparse_mask r sparse_mask mask - Tensor Returns new ref ` sparse tensor sparse-docs ` values strided tensor attr ` ` filtered indices sparse tensor attr ` mask ` The values attr ` mask ` sparse tensor ignored attr ` ` attr ` mask ` tensors must have same shape note The returned sparse tensor might contain duplicate values attr ` mask ` coalesced It therefore advisable pass ` ` mask coalesce ` ` such behavior desired note The returned sparse tensor has same indices sparse tensor attr ` mask ` even when corresponding values attr ` ` zeros Args mask Tensor sparse tensor whose indices used filter Example nse = dims = I = torch cat torch randint dims size= nse torch randint dims size= nse reshape nse V = torch randn nse dims dims S = torch sparse_coo_tensor I V dims coalesce D = torch randn dims D sparse_mask S tensor indices=tensor values=tensor - - - - - - - size= nnz= layout=torch sparse_coo add_docstr_all inverse r inverse - Tensor See func ` torch inverse ` add_docstr_all isnan r isnan - Tensor See func ` torch isnan ` add_docstr_all isinf r isinf - Tensor See func ` torch isinf ` add_docstr_all isposinf r isposinf - Tensor See func ` torch isposinf ` add_docstr_all isneginf r isneginf - Tensor See func ` torch isneginf ` add_docstr_all isfinite r isfinite - Tensor See func ` torch isfinite ` add_docstr_all isclose r isclose other rtol= e- atol= e- equal_nan=False - Tensor See func ` torch isclose ` add_docstr_all isreal r isreal - Tensor See func ` torch isreal ` add_docstr_all is_coalesced r is_coalesced - bool Returns ` ` True ` ` attr ` ` ref ` sparse COO tensor sparse-coo-docs ` coalesced ` ` False ` ` otherwise warning Throws error attr ` ` sparse COO tensor See meth ` coalesce ` ref ` uncoalesced tensors sparse-uncoalesced-coo-docs ` add_docstr_all is_contiguous r is_contiguous memory_format=torch contiguous_format - bool Returns True attr ` ` tensor contiguous memory order specified memory format Args memory_format ` torch memory_format ` optional Specifies memory allocation order Default ` ` torch contiguous_format ` ` add_docstr_all is_pinned r Returns true tensor resides pinned memory By default device pinned memory will current ref ` accelerator accelerators ` add_docstr_all is_floating_point r is_floating_point - bool Returns True data type attr ` ` floating point data type add_docstr_all is_complex r is_complex - bool Returns True data type attr ` ` complex data type add_docstr_all is_inference r is_inference - bool See func ` torch is_inference ` add_docstr_all is_conj r is_conj - bool Returns True conjugate bit attr ` ` set true add_docstr_all is_neg r is_neg - bool Returns True negative bit attr ` ` set true add_docstr_all is_signed r is_signed - bool Returns True data type attr ` ` signed data type add_docstr_all is_set_to r is_set_to tensor - bool Returns True both tensors pointing exact same memory same storage offset size stride add_docstr_all item r item - number Returns value tensor standard Python number This only works tensors one element For other cases see meth ` ~Tensor tolist ` This operation differentiable Example x = torch tensor x item add_docstr_all kron r kron other - Tensor See func ` torch kron ` add_docstr_all kthvalue r kthvalue k dim=None keepdim=False - Tensor LongTensor See func ` torch kthvalue ` add_docstr_all ldexp r ldexp other - Tensor See func ` torch ldexp ` add_docstr_all ldexp_ r ldexp_ other - Tensor In-place version meth ` ~Tensor ldexp ` add_docstr_all lcm r lcm other - Tensor See func ` torch lcm ` add_docstr_all lcm_ r lcm_ other - Tensor In-place version meth ` ~Tensor lcm ` add_docstr_all le r le other - Tensor See func ` torch le ` add_docstr_all le_ r le_ other - Tensor In-place version meth ` ~Tensor le ` add_docstr_all less_equal r less_equal other - Tensor See func ` torch less_equal ` add_docstr_all less_equal_ r less_equal_ other - Tensor In-place version meth ` ~Tensor less_equal ` add_docstr_all lerp r lerp end weight - Tensor See func ` torch lerp ` add_docstr_all lerp_ r lerp_ end weight - Tensor In-place version meth ` ~Tensor lerp ` add_docstr_all lgamma r lgamma - Tensor See func ` torch lgamma ` add_docstr_all lgamma_ r lgamma_ - Tensor In-place version meth ` ~Tensor lgamma ` add_docstr_all log r log - Tensor See func ` torch log ` add_docstr_all log_ r log_ - Tensor In-place version meth ` ~Tensor log ` add_docstr_all log r log - Tensor See func ` torch log ` add_docstr_all log _ r log _ - Tensor In-place version meth ` ~Tensor log ` add_docstr_all log p r log p - Tensor See func ` torch log p ` add_docstr_all log p_ r log p_ - Tensor In-place version meth ` ~Tensor log p ` add_docstr_all log r log - Tensor See func ` torch log ` add_docstr_all log _ r log _ - Tensor In-place version meth ` ~Tensor log ` add_docstr_all logaddexp r logaddexp other - Tensor See func ` torch logaddexp ` add_docstr_all logaddexp r logaddexp other - Tensor See func ` torch logaddexp ` add_docstr_all log_normal_ r log_normal_ mean= std= generator=None Fills attr ` ` tensor numbers samples log-normal distribution parameterized given mean math ` \mu ` standard deviation math ` \sigma ` Note attr ` mean ` attr ` std ` mean standard deviation underlying normal distribution returned distribution math f x = \dfrac x \sigma \sqrt \pi \ e^ -\frac \ln x - \mu ^ \sigma^ add_docstr_all logsumexp r logsumexp dim keepdim=False - Tensor See func ` torch logsumexp ` add_docstr_all lt r lt other - Tensor See func ` torch lt ` add_docstr_all lt_ r lt_ other - Tensor In-place version meth ` ~Tensor lt ` add_docstr_all less r lt other - Tensor See func ` torch less ` add_docstr_all less_ r less_ other - Tensor In-place version meth ` ~Tensor less ` add_docstr_all lu_solve r lu_solve LU_data LU_pivots - Tensor See func ` torch lu_solve ` add_docstr_all map_ r map_ tensor callable Applies attr ` callable ` each element attr ` ` tensor given attr ` tensor ` stores results attr ` ` tensor attr ` ` tensor given attr ` tensor ` must ref ` broadcastable broadcasting-semantics ` The attr ` callable ` should have signature callable b - number add_docstr_all masked_scatter_ r masked_scatter_ mask source Copies elements attr ` source ` into attr ` ` tensor positions where attr ` mask ` True Elements attr ` source ` copied into attr ` ` starting position attr ` source ` continuing order one-by-one each occurrence attr ` mask ` being True The shape attr ` mask ` must ref ` broadcastable broadcasting-semantics ` shape underlying tensor The attr ` source ` should have least many elements number ones attr ` mask ` Args mask BoolTensor boolean mask source Tensor tensor copy note The attr ` mask ` operates attr ` ` tensor given attr ` source ` tensor Example = torch tensor mask = torch tensor dtype=torch bool source = torch tensor masked_scatter_ mask source tensor add_docstr_all masked_fill_ r masked_fill_ mask value Fills elements attr ` ` tensor attr ` value ` where attr ` mask ` True The shape attr ` mask ` must ref ` broadcastable broadcasting-semantics ` shape underlying tensor Args mask BoolTensor boolean mask value float value fill add_docstr_all masked_select r masked_select mask - Tensor See func ` torch masked_select ` add_docstr_all matrix_power r matrix_power n - Tensor note meth ` ~Tensor matrix_power ` deprecated use func ` torch linalg matrix_power ` instead Alias func ` torch linalg matrix_power ` add_docstr_all matrix_exp r matrix_exp - Tensor See func ` torch matrix_exp ` add_docstr_all max r max dim=None keepdim=False - Tensor Tensor Tensor See func ` torch max ` add_docstr_all amax r amax dim=None keepdim=False - Tensor See func ` torch amax ` add_docstr_all maximum r maximum other - Tensor See func ` torch maximum ` add_docstr_all fmax r fmax other - Tensor See func ` torch fmax ` add_docstr_all argmax r argmax dim=None keepdim=False - LongTensor See func ` torch argmax ` add_docstr_all argwhere r argwhere - Tensor See func ` torch argwhere ` add_docstr_all mean r mean dim=None keepdim=False dtype=None - Tensor See func ` torch mean ` add_docstr_all nanmean r nanmean dim=None keepdim=False dtype=None - Tensor See func ` torch nanmean ` add_docstr_all median r median dim=None keepdim=False - Tensor LongTensor See func ` torch median ` add_docstr_all nanmedian r nanmedian dim=None keepdim=False - Tensor LongTensor See func ` torch nanmedian ` add_docstr_all min r min dim=None keepdim=False - Tensor Tensor Tensor See func ` torch min ` add_docstr_all amin r amin dim=None keepdim=False - Tensor See func ` torch amin ` add_docstr_all minimum r minimum other - Tensor See func ` torch minimum ` add_docstr_all aminmax r aminmax dim=None keepdim=False - Tensor min Tensor max See func ` torch aminmax ` add_docstr_all fmin r fmin other - Tensor See func ` torch fmin ` add_docstr_all argmin r argmin dim=None keepdim=False - LongTensor See func ` torch argmin ` add_docstr_all mm r mm mat - Tensor See func ` torch mm ` add_docstr_all mode r mode dim=None keepdim=False - Tensor LongTensor See func ` torch mode ` add_docstr_all movedim r movedim source destination - Tensor See func ` torch movedim ` add_docstr_all moveaxis r moveaxis source destination - Tensor See func ` torch moveaxis ` add_docstr_all mul r mul value - Tensor See func ` torch mul ` add_docstr_all mul_ r mul_ value - Tensor In-place version meth ` ~Tensor mul ` add_docstr_all multiply r multiply value - Tensor See func ` torch multiply ` add_docstr_all multiply_ r multiply_ value - Tensor In-place version meth ` ~Tensor multiply ` add_docstr_all multinomial r multinomial num_samples replacement=False generator=None - Tensor See func ` torch multinomial ` add_docstr_all mv r mv vec - Tensor See func ` torch mv ` add_docstr_all mvlgamma r mvlgamma p - Tensor See func ` torch mvlgamma ` add_docstr_all mvlgamma_ r mvlgamma_ p - Tensor In-place version meth ` ~Tensor mvlgamma ` add_docstr_all narrow r narrow dimension start length - Tensor See func ` torch narrow ` add_docstr_all narrow_copy r narrow_copy dimension start length - Tensor See func ` torch narrow_copy ` add_docstr_all ndimension r ndimension - int Alias meth ` ~Tensor dim ` add_docstr_all nan_to_num r nan_to_num nan= posinf=None neginf=None - Tensor See func ` torch nan_to_num ` add_docstr_all nan_to_num_ r nan_to_num_ nan= posinf=None neginf=None - Tensor In-place version meth ` ~Tensor nan_to_num ` add_docstr_all ne r ne other - Tensor See func ` torch ne ` add_docstr_all ne_ r ne_ other - Tensor In-place version meth ` ~Tensor ne ` add_docstr_all not_equal r not_equal other - Tensor See func ` torch not_equal ` add_docstr_all not_equal_ r not_equal_ other - Tensor In-place version meth ` ~Tensor not_equal ` add_docstr_all neg r neg - Tensor See func ` torch neg ` add_docstr_all negative r negative - Tensor See func ` torch negative ` add_docstr_all neg_ r neg_ - Tensor In-place version meth ` ~Tensor neg ` add_docstr_all negative_ r negative_ - Tensor In-place version meth ` ~Tensor negative ` add_docstr_all nelement r nelement - int Alias meth ` ~Tensor numel ` add_docstr_all nextafter r nextafter other - Tensor See func ` torch nextafter ` add_docstr_all nextafter_ r nextafter_ other - Tensor In-place version meth ` ~Tensor nextafter ` add_docstr_all nonzero r nonzero - LongTensor See func ` torch nonzero ` add_docstr_all nonzero_static r nonzero_static input size fill_value=- - Tensor Returns -D tensor where each row index non-zero value The returned Tensor has same ` torch dtype ` ` torch nonzero ` Args input Tensor input tensor count non-zero elements Keyword args size int size non-zero elements expected included out tensor Pad out tensor ` fill_value ` ` size ` larger than total number non-zero elements truncate out tensor ` size ` smaller The size must non-negative integer fill_value int optional value fill output tensor when ` size ` larger than total number non-zero elements Default ` - ` represent invalid index Example Example Padding input_tensor = torch tensor static_size = t = torch nonzero_static input_tensor size=static_size tensor - - dtype=torch int Example Truncating input_tensor = torch tensor static_size = t = torch nonzero_static input_tensor size=static_size tensor dtype=torch int Example size input_tensor = torch tensor static_size = t = torch nonzero_static input_tensor size=static_size tensor size= dtype=torch int Example rank input input_tensor = torch tensor static_size = t = torch nonzero_static input_tensor size=static_size tensor size= dtype=torch int add_docstr_all norm r norm p= dim=None keepdim=False - Tensor See func ` torch norm ` add_docstr_all normal_ r normal_ mean= std= generator=None - Tensor Fills attr ` ` tensor elements samples normal distribution parameterized attr ` mean ` attr ` std ` add_docstr_all numel r numel - int See func ` torch numel ` add_docstr_all numpy r numpy force=False - numpy ndarray Returns tensor NumPy ` ndarray ` If attr ` force ` ` ` False ` ` default conversion performed only tensor CPU does require grad does have its conjugate bit set dtype layout NumPy supports The returned ndarray tensor will share their storage so changes tensor will reflected ndarray vice versa If attr ` force ` ` ` True ` ` equivalent calling ` ` t detach cpu resolve_conj resolve_neg numpy ` ` If tensor isn t CPU conjugate negative bit set tensor won t share its storage returned ndarray Setting attr ` force ` ` ` True ` ` can useful shorthand Args force bool ` ` True ` ` ndarray may copy tensor instead always sharing memory defaults ` ` False ` ` add_docstr_all orgqr r orgqr input - Tensor See func ` torch orgqr ` add_docstr_all ormqr r ormqr input input left=True transpose=False - Tensor See func ` torch ormqr ` add_docstr_all permute r permute dims - Tensor See func ` torch permute ` add_docstr_all polygamma r polygamma n - Tensor See func ` torch polygamma ` add_docstr_all polygamma_ r polygamma_ n - Tensor In-place version meth ` ~Tensor polygamma ` add_docstr_all positive r positive - Tensor See func ` torch positive ` add_docstr_all pow r pow exponent - Tensor See func ` torch pow ` add_docstr_all pow_ r pow_ exponent - Tensor In-place version meth ` ~Tensor pow ` add_docstr_all float_power r float_power exponent - Tensor See func ` torch float_power ` add_docstr_all float_power_ r float_power_ exponent - Tensor In-place version meth ` ~Tensor float_power ` add_docstr_all prod r prod dim=None keepdim=False dtype=None - Tensor See func ` torch prod ` add_docstr_all put_ r put_ index source accumulate=False - Tensor Copies elements attr ` source ` into positions specified attr ` index ` For purpose indexing attr ` ` tensor treated -D tensor attr ` index ` attr ` source ` need have same number elements necessarily same shape If attr ` accumulate ` ` ` True ` ` elements attr ` source ` added attr ` ` If accumulate ` ` False ` ` behavior undefined attr ` index ` contain duplicate elements Args index LongTensor indices into source Tensor tensor containing values copy accumulate bool optional whether accumulate into Default ` ` False ` ` Example src = torch tensor src put_ torch tensor torch tensor tensor add_docstr_all put r put input index source accumulate=False - Tensor Out-of-place version meth ` torch Tensor put_ ` ` input ` corresponds ` ` meth ` torch Tensor put_ ` add_docstr_all qr r qr some=True - Tensor Tensor See func ` torch qr ` add_docstr_all qscheme r qscheme - torch qscheme Returns quantization scheme given QTensor add_docstr_all quantile r quantile q dim=None keepdim=False interpolation= linear - Tensor See func ` torch quantile ` add_docstr_all nanquantile r nanquantile q dim=None keepdim=False interpolation= linear - Tensor See func ` torch nanquantile ` add_docstr_all q_scale r q_scale - float Given Tensor quantized linear affine quantization returns scale underlying quantizer add_docstr_all q_zero_point r q_zero_point - int Given Tensor quantized linear affine quantization returns zero_point underlying quantizer add_docstr_all q_per_channel_scales r q_per_channel_scales - Tensor Given Tensor quantized linear affine per-channel quantization returns Tensor scales underlying quantizer It has number elements matches corresponding dimensions q_per_channel_axis tensor add_docstr_all q_per_channel_zero_points r q_per_channel_zero_points - Tensor Given Tensor quantized linear affine per-channel quantization returns tensor zero_points underlying quantizer It has number elements matches corresponding dimensions q_per_channel_axis tensor add_docstr_all q_per_channel_axis r q_per_channel_axis - int Given Tensor quantized linear affine per-channel quantization returns index dimension which per-channel quantization applied add_docstr_all random_ r random_ from= to=None generator=None - Tensor Fills attr ` ` tensor numbers sampled discrete uniform distribution over ` ` - ` ` If specified values usually only bounded attr ` ` tensor s data type However floating point types unspecified range will ` ` ^mantissa ` ` ensure every value representable For example ` torch tensor dtype=torch double random_ ` will uniform ` ` ^ ` ` add_docstr_all rad deg r rad deg - Tensor See func ` torch rad deg ` add_docstr_all rad deg_ r rad deg_ - Tensor In-place version meth ` ~Tensor rad deg ` add_docstr_all deg rad r deg rad - Tensor See func ` torch deg rad ` add_docstr_all deg rad_ r deg rad_ - Tensor In-place version meth ` ~Tensor deg rad ` add_docstr_all ravel r ravel - Tensor see func ` torch ravel ` add_docstr_all reciprocal r reciprocal - Tensor See func ` torch reciprocal ` add_docstr_all reciprocal_ r reciprocal_ - Tensor In-place version meth ` ~Tensor reciprocal ` add_docstr_all record_stream r record_stream stream Marks tensor having been used stream When tensor deallocated ensure tensor memory reused another tensor until all work queued attr ` stream ` time deallocation complete note The caching allocator aware only stream where tensor allocated Due awareness already correctly manages life cycle tensors only one stream But tensor used stream different stream origin allocator might reuse memory unexpectedly Calling method lets allocator know which streams have used tensor warning This method most suitable use cases where you providing function created tensor side stream want users able make use tensor without having think carefully about stream safety when making use them These safety guarantees come some performance predictability cost analogous tradeoff between GC manual memory management so you situation where you manage full lifetime your tensors you may consider instead manually managing CUDA events so calling method necessary In particular when you call method later allocations allocator will poll recorded stream see all operations have completed yet you can potentially race side stream computation non-deterministically reuse fail reuse memory allocation You can safely use tensors allocated side streams without meth ` ~Tensor record_stream ` you must manually ensure any non-creation stream uses tensor synced back creation stream before you deallocate tensor As CUDA caching allocator guarantees memory will only reused same creation stream sufficient ensure writes future reallocations memory will delayed until non-creation stream uses done Counterintuitively you may observe CPU side we have already reallocated tensor even though CUDA kernels old tensor still progress This fine because CUDA operations new tensor will appropriately wait old operations complete they all same stream Concretely looks like torch cuda stream s x = torch zeros N s wait_stream s torch cuda stream s y = some_comm_op x some compute s synchronize creation stream s side stream s before deallocating x s wait_stream s del x Note some discretion required when deciding when perform ` ` s wait_stream s ` ` In particular we wait immediately after ` ` some_comm_op ` ` there wouldn t any point having side stream would equivalent have run ` ` some_comm_op ` ` ` ` s ` ` Instead synchronization must placed some appropriate later point time where you expect side stream ` ` s ` ` have finished work This location typically identified via profiling e g using Chrome traces produced meth ` torch autograd profiler profile export_chrome_trace ` If you place wait too early work s will block until ` ` s ` ` has finished preventing further overlapping communication computation If you place wait too late you will use more memory than strictly necessary you keeping ` ` x ` ` live longer For concrete example how guidance can applied practice see post ` FSDP CUDACachingAllocator https dev-discuss pytorch org t fsdp-cudacachingallocator-an-outsider-newb-perspective ` _ add_docstr_all remainder r remainder divisor - Tensor See func ` torch remainder ` add_docstr_all remainder_ r remainder_ divisor - Tensor In-place version meth ` ~Tensor remainder ` add_docstr_all renorm r renorm p dim maxnorm - Tensor See func ` torch renorm ` add_docstr_all renorm_ r renorm_ p dim maxnorm - Tensor In-place version meth ` ~Tensor renorm ` add_docstr_all repeat r repeat repeats - Tensor Repeats tensor along specified dimensions Unlike meth ` ~Tensor expand ` function copies tensor s data warning meth ` ~Tensor repeat ` behaves differently ` numpy repeat https numpy org doc stable reference generated numpy repeat html ` _ more similar ` numpy tile https numpy org doc stable reference generated numpy tile html ` _ For operator similar ` numpy repeat ` see func ` torch repeat_interleave ` Args repeat torch Size int tuple int list int The number times repeat tensor along each dimension Example x = torch tensor x repeat tensor x repeat size torch Size add_docstr_all repeat_interleave r repeat_interleave repeats dim=None output_size=None - Tensor See func ` torch repeat_interleave ` add_docstr_all requires_grad_ r requires_grad_ requires_grad=True - Tensor Change autograd should record operations tensor sets tensor s attr ` requires_grad ` attribute in-place Returns tensor func ` requires_grad_ ` s main use case tell autograd begin recording operations Tensor ` ` tensor ` ` If ` ` tensor ` ` has ` ` requires_grad=False ` ` because obtained through DataLoader required preprocessing initialization ` ` tensor requires_grad_ ` ` makes so autograd will begin record operations ` ` tensor ` ` Args requires_grad bool If autograd should record operations tensor Default ` ` True ` ` Example Let s say we want preprocess some saved weights use result new weights saved_weights = loaded_weights = torch tensor saved_weights weights = preprocess loaded_weights some function weights tensor - - - Now start record operations done weights weights requires_grad_ out = weights pow sum out backward weights grad tensor - - - add_docstr_all reshape r reshape shape - Tensor Returns tensor same data number elements attr ` ` specified shape This method returns view attr ` shape ` compatible current shape See meth ` torch Tensor view ` when possible view See func ` torch reshape ` Args shape tuple ints int desired shape add_docstr_all reshape_as r reshape_as other - Tensor Returns tensor same shape attr ` other ` ` ` reshape_as other ` ` equivalent ` ` reshape other sizes ` ` This method returns view ` ` other sizes ` ` compatible current shape See meth ` torch Tensor view ` when possible view Please see meth ` reshape ` more information about ` ` reshape ` ` Args other ` torch Tensor ` The result tensor has same shape attr ` other ` add_docstr_all resize_ r resize_ sizes memory_format=torch contiguous_format - Tensor Resizes attr ` ` tensor specified size If number elements larger than current storage size then underlying storage resized fit new number elements If number elements smaller underlying storage changed Existing elements preserved any new memory uninitialized warning This low-level method The storage reinterpreted C-contiguous ignoring current strides unless target size equals current size which case tensor left unchanged For most purposes you will instead want use meth ` ~Tensor view ` which checks contiguity meth ` ~Tensor reshape ` which copies data needed To change size in-place custom strides see meth ` ~Tensor set_ ` note If func ` torch use_deterministic_algorithms ` attr ` torch utils deterministic fill_uninitialized_memory ` both set ` ` True ` ` new elements initialized prevent nondeterministic behavior using result input operation Floating point complex values set NaN integer values set maximum value Args sizes torch Size int desired size memory_format ` torch memory_format ` optional desired memory format Tensor Default ` ` torch contiguous_format ` ` Note memory format attr ` ` going unaffected ` ` size ` ` matches ` ` sizes ` ` Example x = torch tensor x resize_ tensor add_docstr_all resize_as_ r resize_as_ tensor memory_format=torch contiguous_format - Tensor Resizes attr ` ` tensor same size specified attr ` tensor ` This equivalent ` ` resize_ tensor size ` ` Args memory_format ` torch memory_format ` optional desired memory format Tensor Default ` ` torch contiguous_format ` ` Note memory format attr ` ` going unaffected ` ` size ` ` matches ` ` tensor size ` ` add_docstr_all rot r rot k dims - Tensor See func ` torch rot ` add_docstr_all round r round decimals= - Tensor See func ` torch round ` add_docstr_all round_ r round_ decimals= - Tensor In-place version meth ` ~Tensor round ` add_docstr_all rsqrt r rsqrt - Tensor See func ` torch rsqrt ` add_docstr_all rsqrt_ r rsqrt_ - Tensor In-place version meth ` ~Tensor rsqrt ` add_docstr_all scatter_ r scatter_ dim index src reduce=None - Tensor Writes all values tensor attr ` src ` into attr ` ` indices specified attr ` index ` tensor For each value attr ` src ` its output index specified its index attr ` src ` ` ` dimension = dim ` ` corresponding value attr ` index ` ` ` dimension = dim ` ` For -D tensor attr ` ` updated index i j k j k = src i j k dim == i index i j k k = src i j k dim == i j index i j k = src i j k dim == This reverse operation manner described meth ` ~Tensor gather ` It also required ` ` index size d = src size d ` ` all dimensions ` ` d ` ` ` ` index size d = size d ` ` all dimensions ` ` d = dim ` ` Note ` ` input ` ` ` ` index ` ` do broadcast against each other NPUs so when running NPUs attr ` input ` attr ` index ` must have same number dimensions Standard broadcasting occurs all other cases Moreover meth ` ~Tensor gather ` values attr ` index ` must between ` ` ` ` ` ` size dim - ` ` inclusive warning When indices unique behavior non-deterministic one values ` ` src ` ` will picked arbitrarily gradient will incorrect will propagated all locations source correspond same index note The backward pass implemented only ` ` src shape == index shape ` ` Additionally accepts optional attr ` reduce ` argument allows specification optional reduction operation which applied all values tensor attr ` src ` into attr ` ` indices specified attr ` index ` For each value attr ` src ` reduction operation applied index attr ` ` which specified its index attr ` src ` ` ` dimension = dim ` ` corresponding value attr ` index ` ` ` dimension = dim ` ` Given -D tensor reduction using multiplication operation attr ` ` updated index i j k j k = src i j k dim == i index i j k k = src i j k dim == i j index i j k = src i j k dim == Reducing addition operation same using meth ` ~torch Tensor scatter_add_ ` warning The reduce argument Tensor ` ` src ` ` deprecated will removed future PyTorch release Please use meth ` ~torch Tensor scatter_reduce_ ` instead more reduction options Args dim int axis along which index index LongTensor indices elements scatter can either empty same dimensionality ` ` src ` ` When empty operation returns ` ` ` ` unchanged src Tensor source element s scatter Keyword args reduce str optional reduction operation apply can either ` ` add ` ` ` ` multiply ` ` Example src = torch arange reshape src tensor index = torch tensor torch zeros dtype=src dtype scatter_ index src tensor index = torch tensor torch zeros dtype=src dtype scatter_ index src tensor torch full scatter_ torch tensor reduce= multiply tensor torch full scatter_ torch tensor reduce= add tensor function scatter_ dim index value reduce=None - Tensor noindex Writes value attr ` value ` into attr ` ` indices specified attr ` index ` tensor This operation equivalent previous version attr ` src ` tensor filled entirely attr ` value ` Args dim int axis along which index index LongTensor indices elements scatter can either empty same dimensionality ` ` src ` ` When empty operation returns ` ` ` ` unchanged value Scalar value scatter Keyword args reduce str optional reduction operation apply can either ` ` add ` ` ` ` multiply ` ` Example index = torch tensor value = torch zeros scatter_ index value tensor add_docstr_all scatter_add_ r scatter_add_ dim index src - Tensor Adds all values tensor attr ` src ` into attr ` ` indices specified attr ` index ` tensor similar fashion meth ` ~torch Tensor scatter_ ` For each value attr ` src ` added index attr ` ` which specified its index attr ` src ` ` ` dimension = dim ` ` corresponding value attr ` index ` ` ` dimension = dim ` ` For -D tensor attr ` ` updated index i j k j k += src i j k dim == i index i j k k += src i j k dim == i j index i j k += src i j k dim == attr ` ` attr ` index ` attr ` src ` should have same number dimensions It also required ` ` index size d = src size d ` ` all dimensions ` ` d ` ` ` ` index size d = size d ` ` all dimensions ` ` d = dim ` ` Note ` ` index ` ` ` ` src ` ` do broadcast When attr ` index ` empty we always original tensor without further error checking Note forward_reproducibility_note note The backward pass implemented only ` ` src shape == index shape ` ` Args dim int axis along which index index LongTensor indices elements scatter add can either empty same dimensionality ` ` src ` ` When empty operation returns ` ` ` ` unchanged src Tensor source elements scatter add Example src = torch ones index = torch tensor torch zeros dtype=src dtype scatter_add_ index src tensor index = torch tensor torch zeros dtype=src dtype scatter_add_ index src tensor format reproducibility_notes add_docstr_all scatter_reduce_ r scatter_reduce_ dim index src reduce include_self=True - Tensor Reduces all values attr ` src ` tensor indices specified attr ` index ` tensor attr ` ` tensor using applied reduction defined via attr ` reduce ` argument obj ` sum ` obj ` prod ` obj ` mean ` obj ` amax ` obj ` amin ` For each value attr ` src ` reduced index attr ` ` which specified its index attr ` src ` ` ` dimension = dim ` ` corresponding value attr ` index ` ` ` dimension = dim ` ` If obj ` include_self= True ` values attr ` ` tensor included reduction attr ` ` attr ` index ` attr ` src ` should all have same number dimensions It also required ` ` index size d = src size d ` ` all dimensions ` ` d ` ` ` ` index size d = size d ` ` all dimensions ` ` d = dim ` ` Note ` ` index ` ` ` ` src ` ` do broadcast For -D tensor obj ` reduce= sum ` obj ` include_self=True ` output given index i j k j k += src i j k dim == i index i j k k += src i j k dim == i j index i j k += src i j k dim == Note forward_reproducibility_note note The backward pass implemented only ` ` src shape == index shape ` ` warning This function beta may change near future Args dim int axis along which index index LongTensor indices elements scatter reduce src Tensor source elements scatter reduce reduce str reduction operation apply non-unique indices obj ` sum ` obj ` prod ` obj ` mean ` obj ` amax ` obj ` amin ` include_self bool whether elements attr ` ` tensor included reduction Example src = torch tensor index = torch tensor input = torch tensor input scatter_reduce index src reduce= sum tensor input scatter_reduce index src reduce= sum include_self=False tensor input = torch tensor input scatter_reduce index src reduce= amax tensor input scatter_reduce index src reduce= amax include_self=False tensor format reproducibility_notes add_docstr_all select r select dim index - Tensor See func ` torch select ` add_docstr_all select_scatter r select_scatter src dim index - Tensor See func ` torch select_scatter ` add_docstr_all slice_scatter r slice_scatter src dim= start=None end=None step= - Tensor See func ` torch slice_scatter ` add_docstr_all set_ r set_ source=None storage_offset= size=None stride=None - Tensor Sets underlying storage size strides If attr ` source ` tensor attr ` ` tensor will share same storage have same size strides attr ` source ` Changes elements one tensor will reflected other If attr ` source ` ` ~torch Storage ` method sets underlying storage offset size stride Args source Tensor Storage tensor storage use storage_offset int optional offset storage size torch Size optional desired size Defaults size source stride tuple optional desired stride Defaults C-contiguous strides add_docstr_all sigmoid r sigmoid - Tensor See func ` torch sigmoid ` add_docstr_all sigmoid_ r sigmoid_ - Tensor In-place version meth ` ~Tensor sigmoid ` add_docstr_all logit r logit - Tensor See func ` torch logit ` add_docstr_all logit_ r logit_ - Tensor In-place version meth ` ~Tensor logit ` add_docstr_all sign r sign - Tensor See func ` torch sign ` add_docstr_all sign_ r sign_ - Tensor In-place version meth ` ~Tensor sign ` add_docstr_all signbit r signbit - Tensor See func ` torch signbit ` add_docstr_all sgn r sgn - Tensor See func ` torch sgn ` add_docstr_all sgn_ r sgn_ - Tensor In-place version meth ` ~Tensor sgn ` add_docstr_all sin r sin - Tensor See func ` torch sin ` add_docstr_all sin_ r sin_ - Tensor In-place version meth ` ~Tensor sin ` add_docstr_all sinc r sinc - Tensor See func ` torch sinc ` add_docstr_all sinc_ r sinc_ - Tensor In-place version meth ` ~Tensor sinc ` add_docstr_all sinh r sinh - Tensor See func ` torch sinh ` add_docstr_all sinh_ r sinh_ - Tensor In-place version meth ` ~Tensor sinh ` add_docstr_all size r size dim=None - torch Size int Returns size attr ` ` tensor If ` ` dim ` ` specified returned value ` torch Size ` subclass ` tuple ` If ` ` dim ` ` specified returns int holding size dimension Args dim int optional The dimension which retrieve size Example t = torch empty t size torch Size t size dim= add_docstr_all shape r shape - torch Size Returns size attr ` ` tensor Alias attr ` size ` See also meth ` Tensor size ` Example t = torch empty t size torch Size t shape torch Size add_docstr_all sort r sort dim=- descending=False - Tensor LongTensor See func ` torch sort ` add_docstr_all msort r msort - Tensor See func ` torch msort ` add_docstr_all argsort r argsort dim=- descending=False - LongTensor See func ` torch argsort ` add_docstr_all sparse_dim r sparse_dim - int Return number sparse dimensions ref ` sparse tensor sparse-docs ` attr ` ` note Returns ` ` ` ` attr ` ` sparse tensor See also meth ` Tensor dense_dim ` ref ` hybrid tensors sparse-hybrid-coo-docs ` add_docstr_all sparse_resize_ r sparse_resize_ size sparse_dim dense_dim - Tensor Resizes attr ` ` ref ` sparse tensor sparse-docs ` desired size number sparse dense dimensions note If number specified elements attr ` ` zero then attr ` size ` attr ` sparse_dim ` attr ` dense_dim ` can any size positive integers such ` ` len size == sparse_dim + dense_dim ` ` If attr ` ` specifies one more elements however then each dimension attr ` size ` must smaller than corresponding dimension attr ` ` attr ` sparse_dim ` must equal number sparse dimensions attr ` ` attr ` dense_dim ` must equal number dense dimensions attr ` ` warning Throws error attr ` ` sparse tensor Args size torch Size desired size If attr ` ` non-empty sparse tensor desired size cannot smaller than original size sparse_dim int number sparse dimensions dense_dim int number dense dimensions add_docstr_all sparse_resize_and_clear_ r sparse_resize_and_clear_ size sparse_dim dense_dim - Tensor Removes all specified elements ref ` sparse tensor sparse-docs ` attr ` ` resizes attr ` ` desired size number sparse dense dimensions warning Throws error attr ` ` sparse tensor Args size torch Size desired size sparse_dim int number sparse dimensions dense_dim int number dense dimensions add_docstr_all sqrt r sqrt - Tensor See func ` torch sqrt ` add_docstr_all sqrt_ r sqrt_ - Tensor In-place version meth ` ~Tensor sqrt ` add_docstr_all square r square - Tensor See func ` torch square ` add_docstr_all square_ r square_ - Tensor In-place version meth ` ~Tensor square ` add_docstr_all squeeze r squeeze dim=None - Tensor See func ` torch squeeze ` add_docstr_all squeeze_ r squeeze_ dim=None - Tensor In-place version meth ` ~Tensor squeeze ` add_docstr_all std r std dim=None correction= keepdim=False - Tensor See func ` torch std ` add_docstr_all storage_offset r storage_offset - int Returns attr ` ` tensor s offset underlying storage terms number storage elements bytes Example x = torch tensor x storage_offset x storage_offset add_docstr_all untyped_storage r untyped_storage - torch UntypedStorage Returns underlying ` UntypedStorage ` add_docstr_all stride r stride dim - tuple int Returns stride attr ` ` tensor Stride jump necessary go one element next one specified dimension attr ` dim ` A tuple all strides returned when no argument passed Otherwise integer value returned stride particular dimension attr ` dim ` Args dim int optional desired dimension which stride required Example x = torch tensor x stride x stride x stride - add_docstr_all sub r sub other alpha= - Tensor See func ` torch sub ` add_docstr_all sub_ r sub_ other alpha= - Tensor In-place version meth ` ~Tensor sub ` add_docstr_all subtract r subtract other alpha= - Tensor See func ` torch subtract ` add_docstr_all subtract_ r subtract_ other alpha= - Tensor In-place version meth ` ~Tensor subtract ` add_docstr_all sum r sum dim=None keepdim=False dtype=None - Tensor See func ` torch sum ` add_docstr_all nansum r nansum dim=None keepdim=False dtype=None - Tensor See func ` torch nansum ` add_docstr_all svd r svd some=True compute_uv=True - Tensor Tensor Tensor See func ` torch svd ` add_docstr_all swapdims r swapdims dim dim - Tensor See func ` torch swapdims ` add_docstr_all swapdims_ r swapdims_ dim dim - Tensor In-place version meth ` ~Tensor swapdims ` add_docstr_all swapaxes r swapaxes axis axis - Tensor See func ` torch swapaxes ` add_docstr_all swapaxes_ r swapaxes_ axis axis - Tensor In-place version meth ` ~Tensor swapaxes ` add_docstr_all t r t - Tensor See func ` torch t ` add_docstr_all t_ r t_ - Tensor In-place version meth ` ~Tensor t ` add_docstr_all tile r tile dims - Tensor See func ` torch tile ` add_docstr_all r args kwargs - Tensor Performs Tensor dtype device conversion A ` torch dtype ` ` torch device ` inferred arguments ` ` args kwargs ` ` note If ` ` ` ` Tensor already has correct ` torch dtype ` ` torch device ` then ` ` ` ` returned Otherwise returned tensor copy ` ` ` ` desired ` torch dtype ` ` torch device ` note If ` ` ` ` requires gradients ` ` requires_grad=True ` ` target ` ` dtype ` ` specified integer type returned tensor will implicitly set ` ` requires_grad=False ` ` This because only tensors floating-point complex dtypes can require gradients Here ways call ` ` ` ` method dtype non_blocking=False copy=False memory_format=torch preserve_format - Tensor noindex Returns Tensor specified attr ` dtype ` Args memory_format note According ` C++ type conversion rules https en cppreference com w cpp language implicit_conversion html ` _ converting floating point value integer type will truncate fractional part If truncated value cannot fit into target type e g casting ` ` torch inf ` ` ` ` torch long ` ` behavior undefined result may vary across platforms method device=None dtype=None non_blocking=False copy=False memory_format=torch preserve_format - Tensor noindex Returns Tensor specified attr ` device ` optional attr ` dtype ` If attr ` dtype ` ` ` None ` ` inferred ` ` dtype ` ` When attr ` non_blocking ` set ` ` True ` ` function attempts perform conversion asynchronously respect host possible This asynchronous behavior applies both pinned pageable memory However caution advised when using feature For more information refer ` tutorial good usage non_blocking pin_memory https pytorch org tutorials intermediate pinmem_nonblock html ` __ When attr ` copy ` set new Tensor created even when Tensor already matches desired conversion Args memory_format method other non_blocking=False copy=False - Tensor noindex Returns Tensor same ` torch dtype ` ` torch device ` Tensor attr ` other ` When attr ` non_blocking ` set ` ` True ` ` function attempts perform conversion asynchronously respect host possible This asynchronous behavior applies both pinned pageable memory However caution advised when using feature For more information refer ` tutorial good usage non_blocking pin_memory https pytorch org tutorials intermediate pinmem_nonblock html ` __ When attr ` copy ` set new Tensor created even when Tensor already matches desired conversion Example tensor = torch randn Initially dtype=float device=cpu tensor torch float tensor - - dtype=torch float cuda = torch device cuda tensor cuda tensor - - device= cuda tensor cuda dtype=torch float tensor - - dtype=torch float device= cuda other = torch randn dtype=torch float device=cuda tensor other non_blocking=True tensor - - dtype=torch float device= cuda format common_args add_docstr_all byte r byte memory_format=torch preserve_format - Tensor ` ` byte ` ` equivalent ` ` torch uint ` ` See func ` ` Args memory_format format common_args add_docstr_all bool r bool memory_format=torch preserve_format - Tensor ` ` bool ` ` equivalent ` ` torch bool ` ` See func ` ` Args memory_format format common_args add_docstr_all char r char memory_format=torch preserve_format - Tensor ` ` char ` ` equivalent ` ` torch int ` ` See func ` ` Args memory_format format common_args add_docstr_all bfloat r bfloat memory_format=torch preserve_format - Tensor ` ` bfloat ` ` equivalent ` ` torch bfloat ` ` See func ` ` Args memory_format format common_args add_docstr_all double r double memory_format=torch preserve_format - Tensor ` ` double ` ` equivalent ` ` torch float ` ` See func ` ` Args memory_format format common_args add_docstr_all float r float memory_format=torch preserve_format - Tensor ` ` float ` ` equivalent ` ` torch float ` ` See func ` ` Args memory_format format common_args add_docstr_all cdouble r cdouble memory_format=torch preserve_format - Tensor ` ` cdouble ` ` equivalent ` ` torch complex ` ` See func ` ` Args memory_format format common_args add_docstr_all cfloat r cfloat memory_format=torch preserve_format - Tensor ` ` cfloat ` ` equivalent ` ` torch complex ` ` See func ` ` Args memory_format format common_args add_docstr_all chalf r chalf memory_format=torch preserve_format - Tensor ` ` chalf ` ` equivalent ` ` torch complex ` ` See func ` ` Args memory_format format common_args add_docstr_all half r half memory_format=torch preserve_format - Tensor ` ` half ` ` equivalent ` ` torch float ` ` See func ` ` Args memory_format format common_args add_docstr_all int r int memory_format=torch preserve_format - Tensor ` ` int ` ` equivalent ` ` torch int ` ` See func ` ` Args memory_format format common_args add_docstr_all int_repr r int_repr - Tensor Given quantized Tensor ` ` int_repr ` ` returns CPU Tensor uint _t data type stores underlying uint _t values given Tensor add_docstr_all long r long memory_format=torch preserve_format - Tensor ` ` long ` ` equivalent ` ` torch int ` ` See func ` ` Args memory_format format common_args add_docstr_all short r short memory_format=torch preserve_format - Tensor ` ` short ` ` equivalent ` ` torch int ` ` See func ` ` Args memory_format format common_args add_docstr_all take r take indices - Tensor See func ` torch take ` add_docstr_all take_along_dim r take_along_dim indices dim - Tensor See func ` torch take_along_dim ` add_docstr_all tan r tan - Tensor See func ` torch tan ` add_docstr_all tan_ r tan_ - Tensor In-place version meth ` ~Tensor tan ` add_docstr_all tanh r tanh - Tensor See func ` torch tanh ` add_docstr_all softmax r softmax dim - Tensor Alias func ` torch nn functional softmax ` add_docstr_all tanh_ r tanh_ - Tensor In-place version meth ` ~Tensor tanh ` add_docstr_all tolist r tolist - list number Returns tensor nested list For scalars standard Python number returned just like meth ` ~Tensor item ` Tensors automatically moved CPU first necessary This operation differentiable Examples = torch randn tolist - tolist add_docstr_all topk r topk k dim=None largest=True sorted=True - Tensor LongTensor See func ` torch topk ` add_docstr_all to_dense r to_dense dtype=None masked_grad=True - Tensor Creates strided copy attr ` ` attr ` ` strided tensor otherwise returns attr ` ` Keyword args dtype masked_grad bool optional If set ` ` True ` ` default attr ` ` has sparse layout then backward meth ` to_dense ` returns ` ` grad sparse_mask ` ` Example s = torch sparse_coo_tensor torch tensor torch tensor size= s to_dense tensor add_docstr_all to_sparse r to_sparse sparseDims - Tensor Returns sparse copy tensor PyTorch supports sparse tensors ref ` coordinate format sparse-coo-docs ` Args sparseDims int optional number sparse dimensions include new sparse tensor Example d = torch tensor d tensor d to_sparse tensor indices=tensor values=tensor size= nnz= layout=torch sparse_coo d to_sparse tensor indices=tensor values=tensor size= nnz= layout=torch sparse_coo method to_sparse layout=None blocksize=None dense_dim=None - Tensor noindex Returns sparse tensor specified layout blocksize If attr ` ` strided number dense dimensions could specified hybrid sparse tensor will created ` dense_dim ` dense dimensions ` dim - - dense_dim ` batch dimension note If attr ` ` layout blocksize parameters match specified layout blocksize attr ` ` Otherwise sparse tensor copy attr ` ` Args layout ` torch layout ` optional The desired sparse layout One ` ` torch sparse_coo ` ` ` ` torch sparse_csr ` ` ` ` torch sparse_csc ` ` ` ` torch sparse_bsr ` ` ` ` torch sparse_bsc ` ` Default ` ` None ` ` ` ` torch sparse_coo ` ` blocksize list tuple ` torch Size ` optional Block size resulting BSR BSC tensor For other layouts specifying block size ` ` None ` ` will result RuntimeError exception A block size must tuple length two such its items evenly divide two sparse dimensions dense_dim int optional Number dense dimensions resulting CSR CSC BSR BSC tensor This argument should used only attr ` ` strided tensor must value between dimension attr ` ` tensor minus two Example x = torch tensor x to_sparse layout=torch sparse_coo tensor indices=tensor values=tensor size= nnz= layout=torch sparse_coo x to_sparse layout=torch sparse_bsr blocksize= tensor crow_indices=tensor col_indices=tensor values=tensor size= nnz= layout=torch sparse_bsr x to_sparse layout=torch sparse_bsr blocksize= RuntimeError Tensor size - needs divisible blocksize x to_sparse layout=torch sparse_csr blocksize= RuntimeError to_sparse Strided SparseCsr conversion does use specified blocksize x = torch tensor x to_sparse layout=torch sparse_csr dense_dim= tensor crow_indices=tensor col_indices=tensor values=tensor size= nnz= layout=torch sparse_csr add_docstr_all to_sparse_csr r to_sparse_csr dense_dim=None - Tensor Convert tensor compressed row storage format CSR Except strided tensors only works D tensors If attr ` ` strided then number dense dimensions could specified hybrid CSR tensor will created ` dense_dim ` dense dimensions ` dim - - dense_dim ` batch dimension Args dense_dim int optional Number dense dimensions resulting CSR tensor This argument should used only attr ` ` strided tensor must value between dimension attr ` ` tensor minus two Example dense = torch randn sparse = dense to_sparse_csr sparse _nnz dense = torch zeros dense = dense = dense = dense to_sparse_csr dense_dim= tensor crow_indices=tensor col_indices=tensor values=tensor size= nnz= layout=torch sparse_csr add_docstr_all to_sparse_csc r to_sparse_csc - Tensor Convert tensor compressed column storage CSC format Except strided tensors only works D tensors If attr ` ` strided then number dense dimensions could specified hybrid CSC tensor will created ` dense_dim ` dense dimensions ` dim - - dense_dim ` batch dimension Args dense_dim int optional Number dense dimensions resulting CSC tensor This argument should used only attr ` ` strided tensor must value between dimension attr ` ` tensor minus two Example dense = torch randn sparse = dense to_sparse_csc sparse _nnz dense = torch zeros dense = dense = dense = dense to_sparse_csc dense_dim= tensor ccol_indices=tensor row_indices=tensor values=tensor size= nnz= layout=torch sparse_csc add_docstr_all to_sparse_bsr r to_sparse_bsr blocksize dense_dim - Tensor Convert tensor block sparse row BSR storage format given blocksize If attr ` ` strided then number dense dimensions could specified hybrid BSR tensor will created ` dense_dim ` dense dimensions ` dim - - dense_dim ` batch dimension Args blocksize list tuple ` torch Size ` optional Block size resulting BSR tensor A block size must tuple length two such its items evenly divide two sparse dimensions dense_dim int optional Number dense dimensions resulting BSR tensor This argument should used only attr ` ` strided tensor must value between dimension attr ` ` tensor minus two Example dense = torch randn sparse = dense to_sparse_csr sparse_bsr = sparse to_sparse_bsr sparse_bsr col_indices tensor dense = torch zeros dense = dense = dense = dense to_sparse_bsr tensor crow_indices=tensor col_indices=tensor values=tensor size= nnz= layout=torch sparse_bsr add_docstr_all to_sparse_bsc r to_sparse_bsc blocksize dense_dim - Tensor Convert tensor block sparse column BSC storage format given blocksize If attr ` ` strided then number dense dimensions could specified hybrid BSC tensor will created ` dense_dim ` dense dimensions ` dim - - dense_dim ` batch dimension Args blocksize list tuple ` torch Size ` optional Block size resulting BSC tensor A block size must tuple length two such its items evenly divide two sparse dimensions dense_dim int optional Number dense dimensions resulting BSC tensor This argument should used only attr ` ` strided tensor must value between dimension attr ` ` tensor minus two Example dense = torch randn sparse = dense to_sparse_csr sparse_bsc = sparse to_sparse_bsc sparse_bsc row_indices tensor dense = torch zeros dense = dense = dense = dense to_sparse_bsc tensor ccol_indices=tensor row_indices=tensor values=tensor size= nnz= layout=torch sparse_bsc add_docstr_all to_mkldnn r to_mkldnn - Tensor Returns copy tensor ` ` torch mkldnn ` ` layout add_docstr_all trace r trace - Tensor See func ` torch trace ` add_docstr_all transpose r transpose dim dim - Tensor See func ` torch transpose ` add_docstr_all transpose_ r transpose_ dim dim - Tensor In-place version meth ` ~Tensor transpose ` add_docstr_all triangular_solve r triangular_solve A upper=True transpose=False unitriangular=False - Tensor Tensor See func ` torch triangular_solve ` add_docstr_all tril r tril diagonal= - Tensor See func ` torch tril ` add_docstr_all tril_ r tril_ diagonal= - Tensor In-place version meth ` ~Tensor tril ` add_docstr_all triu r triu diagonal= - Tensor See func ` torch triu ` add_docstr_all triu_ r triu_ diagonal= - Tensor In-place version meth ` ~Tensor triu ` add_docstr_all true_divide r true_divide value - Tensor See func ` torch true_divide ` add_docstr_all true_divide_ r true_divide_ value - Tensor In-place version meth ` ~Tensor true_divide_ ` add_docstr_all trunc r trunc - Tensor See func ` torch trunc ` add_docstr_all fix r fix - Tensor See func ` torch fix ` add_docstr_all trunc_ r trunc_ - Tensor In-place version meth ` ~Tensor trunc ` add_docstr_all fix_ r fix_ - Tensor In-place version meth ` ~Tensor fix ` add_docstr_all type r type dtype=None non_blocking=False kwargs - str Tensor Returns type ` dtype ` provided casts object specified type If already correct type no copy performed original object returned Args dtype dtype string The desired type non_blocking bool If ` ` True ` ` source pinned memory destination GPU vice versa copy performed asynchronously respect host Otherwise argument has no effect kwargs For compatibility may contain key ` ` async ` ` place ` ` non_blocking ` ` argument The ` ` async ` ` arg deprecated add_docstr_all type_as r type_as tensor - Tensor Returns tensor cast type given tensor This no-op tensor already correct type This equivalent ` ` type tensor type ` ` Args tensor Tensor tensor which has desired type add_docstr_all unfold r unfold dimension size step - Tensor Returns view original tensor which contains all slices size attr ` size ` attr ` ` tensor dimension attr ` dimension ` Step between two slices given attr ` step ` If ` sizedim ` size dimension attr ` dimension ` attr ` ` size dimension attr ` dimension ` returned tensor will ` sizedim - size step + ` An additional dimension size attr ` size ` appended returned tensor Args dimension int dimension which unfolding happens size int size each slice unfolded step int step between each slice Example x = torch arange x tensor x unfold tensor x unfold tensor add_docstr_all uniform_ r uniform_ from= to= generator=None - Tensor Fills attr ` ` tensor numbers sampled continuous uniform distribution math f x = \dfrac \text - \text add_docstr_all unsqueeze r unsqueeze dim - Tensor See func ` torch unsqueeze ` add_docstr_all unsqueeze_ r unsqueeze_ dim - Tensor In-place version meth ` ~Tensor unsqueeze ` add_docstr_all var r var dim=None correction= keepdim=False - Tensor See func ` torch var ` add_docstr_all vdot r vdot other - Tensor See func ` torch vdot ` add_docstr_all view r view shape - Tensor Returns new tensor same data attr ` ` tensor different attr ` shape ` The returned tensor shares same data must have same number elements may have different size For tensor viewed new view size must compatible its original size stride i e each new view dimension must either subspace original dimension only span across original dimensions math ` d d+ \dots d+k ` satisfy following contiguity-like condition math ` \forall i = d \dots d+k- ` math \text stride i = \text stride i+ \times \text size i+ Otherwise will possible view attr ` ` tensor attr ` shape ` without copying e g via meth ` contiguous ` When unclear whether meth ` view ` can performed advisable use meth ` reshape ` which returns view shapes compatible copies equivalent calling meth ` contiguous ` otherwise Args shape torch Size int desired size Example x = torch randn x size torch Size y = x view y size torch Size z = x view - size - inferred other dimensions z size torch Size = torch randn size torch Size b = transpose Swaps nd rd dimension b size torch Size c = view Does change tensor layout memory c size torch Size torch equal b c False method view dtype - Tensor noindex Returns new tensor same data attr ` ` tensor different attr ` dtype ` If element size attr ` dtype ` different than ` ` dtype ` ` then size last dimension output will scaled proportionally For instance attr ` dtype ` element size twice ` ` dtype ` ` then each pair elements last dimension attr ` ` will combined size last dimension output will half attr ` ` If attr ` dtype ` element size half ` ` dtype ` ` then each element last dimension attr ` ` will split two size last dimension output will double attr ` ` For possible following conditions must true ` ` dim ` ` must greater than ` ` stride - ` ` must Additionally element size attr ` dtype ` greater than ` ` dtype ` ` following conditions must true well ` ` size - ` ` must divisible ratio between element sizes dtypes ` ` storage_offset ` ` must divisible ratio between element sizes dtypes The strides all dimensions except last dimension must divisible ratio between element sizes dtypes If any above conditions met error thrown warning This overload supported TorchScript using Torchscript program will cause undefined behavior Args dtype ` torch dtype ` desired dtype Example x = torch randn x tensor - - - - - - - - - - - x dtype torch float y = x view torch int y tensor - - - - - - - - - - - dtype=torch int y = x tensor - - - - - - - - - - - x view torch cfloat tensor - j - j - + j - j - - j - - j - + j - - j x view torch cfloat size torch Size x view torch uint tensor dtype=torch uint x view torch uint size torch Size add_docstr_all view_as r view_as other - Tensor View tensor same size attr ` other ` ` ` view_as other ` ` equivalent ` ` view other size ` ` Please see meth ` ~Tensor view ` more information about ` ` view ` ` Args other ` torch Tensor ` The result tensor has same size attr ` other ` add_docstr_all expand r expand sizes - Tensor Returns new view attr ` ` tensor singleton dimensions expanded larger size Passing - size dimension means changing size dimension Tensor can also expanded larger number dimensions new ones will appended front For new dimensions size cannot set - Expanding tensor does allocate new memory only creates new view existing tensor where dimension size one expanded larger size setting ` ` stride ` ` Any dimension size can expanded arbitrary value without allocating new memory Args sizes torch Size int desired expanded size warning More than one element expanded tensor may refer single memory location As result in-place operations especially ones vectorized may result incorrect behavior If you need write tensors please clone them first Example x = torch tensor x size torch Size x expand tensor x expand - - means changing size dimension tensor add_docstr_all expand_as r expand_as other - Tensor Expand tensor same size attr ` other ` ` ` expand_as other ` ` equivalent ` ` expand other size ` ` Please see meth ` ~Tensor expand ` more information about ` ` expand ` ` Args other ` torch Tensor ` The result tensor has same size attr ` other ` add_docstr_all sum_to_size r sum_to_size size - Tensor Sum ` ` ` ` tensor attr ` size ` attr ` size ` must broadcastable ` ` ` ` tensor size Args size int sequence integers defining shape output tensor add_docstr_all zero_ r zero_ - Tensor Fills attr ` ` tensor zeros add_docstr_all matmul r matmul tensor - Tensor See func ` torch matmul ` add_docstr_all chunk r chunk chunks dim= - List Tensors See func ` torch chunk ` add_docstr_all unsafe_chunk r unsafe_chunk chunks dim= - List Tensors See func ` torch unsafe_chunk ` add_docstr_all unsafe_split r unsafe_split split_size dim= - List Tensors See func ` torch unsafe_split ` add_docstr_all tensor_split r tensor_split indices_or_sections dim= - List Tensors See func ` torch tensor_split ` add_docstr_all hsplit r hsplit split_size_or_sections - List Tensors See func ` torch hsplit ` add_docstr_all vsplit r vsplit split_size_or_sections - List Tensors See func ` torch vsplit ` add_docstr_all dsplit r dsplit split_size_or_sections - List Tensors See func ` torch dsplit ` add_docstr_all stft r stft frame_length hop fft_size=None return_onesided=True window=None pad_end= align_to_window=None - Tensor See func ` torch stft ` add_docstr_all istft r istft n_fft hop_length=None win_length=None window=None center=True normalized=False onesided=True length=None - Tensor See func ` torch istft ` add_docstr_all det r det - Tensor See func ` torch det ` add_docstr_all where r where condition y - Tensor ` ` where condition y ` ` equivalent ` ` torch where condition y ` ` See func ` torch where ` add_docstr_all logdet r logdet - Tensor See func ` torch logdet ` add_docstr_all slogdet r slogdet - Tensor Tensor See func ` torch slogdet ` add_docstr_all unbind r unbind dim= - seq See func ` torch unbind ` add_docstr_all pin_memory r pin_memory - Tensor Copies tensor pinned memory s already pinned By default device pinned memory will current ref ` accelerator accelerators ` add_docstr_all pinverse r pinverse - Tensor See func ` torch pinverse ` add_docstr_all index_add r index_add dim index source alpha= - Tensor Out-of-place version meth ` torch Tensor index_add_ ` add_docstr_all index_copy r index_copy dim index tensor - Tensor Out-of-place version meth ` torch Tensor index_copy_ ` add_docstr_all index_fill r index_fill dim index value - Tensor Out-of-place version meth ` torch Tensor index_fill_ ` add_docstr_all scatter r scatter dim index src - Tensor Out-of-place version meth ` torch Tensor scatter_ ` add_docstr_all scatter_add r scatter_add dim index src - Tensor Out-of-place version meth ` torch Tensor scatter_add_ ` add_docstr_all scatter_reduce r scatter_reduce dim index src reduce include_self=True - Tensor Out-of-place version meth ` torch Tensor scatter_reduce_ ` add_docstr_all masked_scatter r masked_scatter mask tensor - Tensor Out-of-place version meth ` torch Tensor masked_scatter_ ` note The inputs attr ` ` attr ` mask ` ref ` broadcast broadcasting-semantics ` Example = torch tensor mask = torch tensor dtype=torch bool source = torch tensor masked_scatter mask source tensor add_docstr_all xlogy r xlogy other - Tensor See func ` torch xlogy ` add_docstr_all xlogy_ r xlogy_ other - Tensor In-place version meth ` ~Tensor xlogy ` add_docstr_all masked_fill r masked_fill mask value - Tensor Out-of-place version meth ` torch Tensor masked_fill_ ` add_docstr_all grad r This attribute ` ` None ` ` default becomes Tensor first time call func ` backward ` computes gradients ` ` ` ` The attribute will then contain gradients computed future calls func ` backward ` will accumulate add gradients into add_docstr_all grad_dtype r The allowed dtype attr ` ` grad ` ` tensor attr ` ` grad_dtype ` ` can set specific dtype ` ` None ` ` By default ` ` t grad_dtype == t dtype ` ` When None autograd engine casts incoming gradients dtype This attribute only accessible settable leaf tensors warning Use caution Diverging dtypes tensor its gradient may break downstream systems assume they match Example x = torch tensor requires_grad=True x grad_dtype torch float x grad_dtype = torch float x grad_dtype torch float Allow any gradient dtype x grad_dtype = None x grad_dtype add_docstr_all retain_grad r retain_grad - None Enables Tensor have their attr ` grad ` populated during func ` backward ` This no-op leaf tensors add_docstr_all retains_grad r Is ` ` True ` ` Tensor non-leaf its attr ` grad ` enabled populated during func ` backward ` ` ` False ` ` otherwise add_docstr_all requires_grad r Is ` ` True ` ` gradients need computed Tensor ` ` False ` ` otherwise note The fact gradients need computed Tensor do mean attr ` grad ` attribute will populated see attr ` is_leaf ` more details add_docstr_all is_leaf r All Tensors have attr ` requires_grad ` which ` ` False ` ` will leaf Tensors convention For Tensors have attr ` requires_grad ` which ` ` True ` ` they will leaf Tensors they created user This means they result operation so attr ` grad_fn ` None Only leaf Tensors will have their attr ` grad ` populated during call func ` backward ` To get attr ` grad ` populated non-leaf Tensors you can use func ` retain_grad ` Example = torch rand requires_grad=True is_leaf True b = torch rand requires_grad=True cuda b is_leaf False b created operation cast cpu Tensor into cuda Tensor c = torch rand requires_grad=True + c is_leaf False c created addition operation d = torch rand cuda d is_leaf True d does require gradients so has no operation creating tracked autograd engine e = torch rand cuda requires_grad_ e is_leaf True e requires gradients has no operations creating f = torch rand requires_grad=True device= cuda f is_leaf True f requires grad has no operation creating add_docstr_all names r Stores names each tensor s dimensions ` ` names idx ` ` corresponds name tensor dimension ` ` idx ` ` Names either string dimension named ` ` None ` ` dimension unnamed Dimension names may contain characters underscore Furthermore dimension name must valid Python variable name i e does start underscore Tensors may have two named dimensions same name warning The named tensor API experimental subject change add_docstr_all is_cuda r Is ` ` True ` ` Tensor stored GPU ` ` False ` ` otherwise add_docstr_all is_cpu r Is ` ` True ` ` Tensor stored CPU ` ` False ` ` otherwise add_docstr_all is_xla r Is ` ` True ` ` Tensor stored XLA device ` ` False ` ` otherwise add_docstr_all is_ipu r Is ` ` True ` ` Tensor stored IPU ` ` False ` ` otherwise add_docstr_all is_xpu r Is ` ` True ` ` Tensor stored XPU ` ` False ` ` otherwise add_docstr_all is_quantized r Is ` ` True ` ` Tensor quantized ` ` False ` ` otherwise add_docstr_all is_meta r Is ` ` True ` ` Tensor meta tensor ` ` False ` ` otherwise Meta tensors like normal tensors they carry no data add_docstr_all is_mps r Is ` ` True ` ` Tensor stored MPS device ` ` False ` ` otherwise add_docstr_all is_sparse r Is ` ` True ` ` Tensor uses sparse COO storage layout ` ` False ` ` otherwise add_docstr_all is_sparse_csr r Is ` ` True ` ` Tensor uses sparse CSR storage layout ` ` False ` ` otherwise add_docstr_all device r Is ` torch device ` where Tensor add_docstr_all ndim r Alias meth ` ~Tensor dim ` add_docstr_all itemsize r Alias meth ` ~Tensor element_size ` add_docstr_all nbytes r Returns number bytes consumed view elements Tensor Tensor does use sparse storage layout Defined meth ` ~Tensor numel ` meth ` ~Tensor element_size ` add_docstr_all T r Returns view tensor its dimensions reversed If ` ` n ` ` number dimensions ` ` x ` ` ` ` x T ` ` equivalent ` ` x permute n- n- ` ` warning The use func ` Tensor T ` tensors dimension other than reverse their shape deprecated will throw error future release Consider attr ` ~ Tensor mT ` transpose batches matrices ` x permute torch arange x ndim - - - ` reverse dimensions tensor add_docstr_all H r Returns view matrix -D tensor conjugated transposed ` ` x H ` ` equivalent ` ` x transpose conj ` ` complex matrices ` ` x transpose ` ` real matrices seealso attr ` ~ Tensor mH ` An attribute also works batches matrices add_docstr_all mT r Returns view tensor last two dimensions transposed ` ` x mT ` ` equivalent ` ` x transpose - - ` ` add_docstr_all mH r Accessing property equivalent calling func ` adjoint ` add_docstr_all adjoint r adjoint - Tensor Alias func ` adjoint ` add_docstr_all real r Returns new tensor containing real values attr ` ` tensor complex-valued input tensor The returned tensor attr ` ` share same underlying storage Returns attr ` ` attr ` ` real-valued tensor tensor Example x=torch randn dtype=torch cfloat x tensor + j - - j - - j - - j x real tensor - - - add_docstr_all imag r Returns new tensor containing imaginary values attr ` ` tensor The returned tensor attr ` ` share same underlying storage warning func ` imag ` only supported tensors complex dtypes Example x=torch randn dtype=torch cfloat x tensor + j - - j - - j - - j x imag tensor - - - add_docstr_all as_subclass r as_subclass cls - Tensor Makes ` ` cls ` ` instance same data pointer ` ` ` ` Changes output mirror changes ` ` ` ` output stays attached autograd graph ` ` cls ` ` must subclass ` ` Tensor ` ` add_docstr_all crow_indices r crow_indices - IntTensor Returns tensor containing compressed row indices attr ` ` tensor when attr ` ` sparse CSR tensor layout ` ` sparse_csr ` ` The ` ` crow_indices ` ` tensor strictly shape attr ` ` size + type ` ` int ` ` ` ` int ` ` When using MKL routines such sparse matrix multiplication necessary use ` ` int ` ` indexing order avoid downcasting potentially losing information Example csr = torch eye to_sparse_csr csr crow_indices tensor dtype=torch int add_docstr_all col_indices r col_indices - IntTensor Returns tensor containing column indices attr ` ` tensor when attr ` ` sparse CSR tensor layout ` ` sparse_csr ` ` The ` ` col_indices ` ` tensor strictly shape attr ` ` nnz type ` ` int ` ` ` ` int ` ` When using MKL routines such sparse matrix multiplication necessary use ` ` int ` ` indexing order avoid downcasting potentially losing information Example csr = torch eye to_sparse_csr csr col_indices tensor dtype=torch int add_docstr_all to_padded_tensor r to_padded_tensor padding output_size=None - Tensor See func ` to_padded_tensor `