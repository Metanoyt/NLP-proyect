mypy ignore-errors torch copy deepcopy torch utils _pytree tree_map torch utils _pytree pytree TODO Move LoggingTensor here torch testing _internal logging_tensor LoggingTensor Base wrapper-style tensors WrapperTensor torch Tensor staticmethod __new__ cls args kwargs t kwargs = cls get_wrapper_properties args kwargs size kwargs size = t size size = kwargs size del kwargs size dtype kwargs kwargs dtype = t dtype layout kwargs kwargs layout = t layout device kwargs kwargs device = t device requires_grad kwargs kwargs requires_grad = False Ignore memory_format pin memory now I don t know how safely access them Tensor possible wrapper = torch Tensor _make_wrapper_subclass cls size kwargs wrapper _validate_methods wrapper classmethod get_wrapper_properties cls args kwargs Should both example Tensor dictionary kwargs override any example Tensor s properly This very similar ` t new_ args ` API raise NotImplementedError You need implement get_wrapper_properties _validate_methods Skip debug mode Changing these python side wrong would properly reflected c++ side This doesn t catch attributes set __init__ forbidden_overrides = size stride dtype layout device requires_grad el forbidden_overrides getattr __class__ el getattr torch Tensor el raise RuntimeError f Subclass __class__ __name__ overwriting f property el allowed such change would reflected c++ callers WrapperTensorWithCustomSizes WrapperTensor classmethod get_wrapper_properties cls t requires_grad=False t requires_grad requires_grad dispatch_sizes_strides_policy sizes __init__ t requires_grad=False t = t classmethod __torch_dispatch__ cls func types args= kwargs=None all issubclass cls t t types NotImplemented kwargs None kwargs = unwrap e e t isinstance e WrapperTensorWithCustomSizes e wrap e WrapperTensorWithCustomSizes e isinstance e torch Tensor e rs = tree_map wrap func tree_map unwrap args tree_map unwrap kwargs rs __repr__ super __repr__ tensor_contents=f t= t WrapperTensorWithCustomStrides WrapperTensor classmethod get_wrapper_properties cls t requires_grad=False t requires_grad requires_grad dispatch_sizes_strides_policy strides __init__ t requires_grad=False t = t classmethod __torch_dispatch__ cls func types args= kwargs=None all issubclass cls t t types NotImplemented kwargs None kwargs = unwrap e e t isinstance e WrapperTensorWithCustomStrides e wrap e WrapperTensorWithCustomStrides e isinstance e torch Tensor e rs = tree_map wrap func tree_map unwrap args tree_map unwrap kwargs rs __repr__ super __repr__ tensor_contents=f t= t DiagTensorBelow WrapperTensor classmethod get_wrapper_properties cls diag requires_grad=False assert diag ndim == diag size diag size + diag size requires_grad requires_grad __init__ diag requires_grad=False diag = diag handled_ops = classmethod __torch_dispatch__ cls func types args= kwargs=None all issubclass cls t t types NotImplemented For everything call handler fn = cls handled_ops get func __name__ None fn fn args kwargs Note here because we don t need provide autograd formulas we can have default fallback creates plain Tensor based diag elements calls func again unwrap e e diag diag isinstance e DiagTensorBelow e wrap e isinstance e torch Tensor e ndim == DiagTensorBelow e isinstance e torch Tensor e ndim == e count_nonzero == e diag count_nonzero DiagTensorBelow e diag e rs = tree_map wrap func tree_map unwrap args tree_map unwrap kwargs rs __repr__ super __repr__ tensor_contents=f diag= diag SparseTensor WrapperTensor classmethod get_wrapper_properties cls size values indices requires_grad=False assert values device == indices device values size size requires_grad requires_grad __init__ size values indices requires_grad=False values = values indices = indices __repr__ super __repr__ tensor_contents=f values= values indices= indices sparse_to_dense res = torch zeros size dtype=self values dtype res indices unbind = values res staticmethod from_dense t indices = t nonzero values = t indices unbind SparseTensor t size values indices classmethod __torch_dispatch__ cls func types args= kwargs=None func_name = f func __module__ func __name__ res = cls _try_call_special_impl func_name args kwargs res NotImplemented res Otherwise use default implementation construct dense tensors use compute values unwrap e e sparse_to_dense isinstance e SparseTensor e Wrap back all Tensors into our custom wrap e Check zeros use get indices SparseTensor from_dense e isinstance e torch Tensor e rs = tree_map wrap func tree_map unwrap args tree_map unwrap kwargs rs To show how things happen later __rmul__ other super __rmul__ other _SPECIAL_IMPLS = classmethod _try_call_special_impl cls func args kwargs func cls _SPECIAL_IMPLS NotImplemented cls _SPECIAL_IMPLS func args kwargs Example non-wrapper subclass stores extra state NonWrapperTensor torch Tensor __new__ cls data t = torch Tensor _make_subclass cls data t extra_state = last_func_called None t classmethod __torch_function__ cls func types args= kwargs=None result = super __torch_function__ func types args kwargs isinstance result cls Do something extra state For example here just store name last function called skip deepcopy so copy has same extra state func torch Tensor __deepcopy__ result extra_state = deepcopy args extra_state result extra_state = last_func_called func __name__ result new_empty must defined deepcopy work new_empty shape type torch empty shape Class used store info about subclass tensors used testing SubclassInfo __slots__ = name create_fn closed_under_ops __init__ name create_fn closed_under_ops=True name = name create_fn = create_fn create_fn shape - tensor instance closed_under_ops = closed_under_ops Helper function create subclass given possibly cache sizes strides _create_and_access_shape cls shape sub = cls torch randn shape NB Wrapper subclasses custom dispatched sizes strides cache info first call via non-serializable PyCapsules We purposefully trigger cache population here serialization deepcopy tests verify presence cache info doesn t cause problems sub size sub stride sub subclass_db = torch Tensor SubclassInfo base_tensor create_fn=torch randn NonWrapperTensor SubclassInfo non_wrapper_tensor create_fn=lambda shape NonWrapperTensor torch randn shape LoggingTensor SubclassInfo logging_tensor create_fn=lambda shape LoggingTensor torch randn shape SparseTensor SubclassInfo sparse_tensor create_fn=lambda shape SparseTensor from_dense torch randn shape relu DiagTensorBelow SubclassInfo diag_tensor_below create_fn=lambda shape DiagTensorBelow torch randn shape closed_under_ops=False sparse semantics WrapperTensorWithCustomSizes SubclassInfo wrapper_with_custom_sizes create_fn=lambda shape _create_and_access_shape WrapperTensorWithCustomSizes shape closed_under_ops=False WrapperTensorWithCustomStrides SubclassInfo wrapper_with_custom_strides create_fn=lambda shape _create_and_access_shape WrapperTensorWithCustomStrides shape closed_under_ops=False SubclassWithTensorFactory torch Tensor staticmethod __new__ cls src shape = src shape kwargs = kwargs strides = src stride kwargs storage_offset = src storage_offset kwargs device = src device kwargs layout = src layout kwargs requires_grad = src requires_grad kwargs dtype = src dtype out = torch Tensor _make_wrapper_subclass cls shape kwargs out __init__ src src = src __repr__ f __class__ __name__ __tensor_flatten__ src None classmethod __tensor_unflatten__ cls inner_tensors meta outer_size outer_stride src = inner_tensors src cls src classmethod __torch_dispatch__ cls func types args kwargs kwargs None kwargs = _fn x x src torch ones x src shape x src dtype == torch float x src _args = pytree tree_map_only cls _fn args _kwargs = pytree tree_map_only cls _fn kwargs _out = func _args _kwargs _out_flat _out_spec = pytree tree_flatten _out out_flat = cls o isinstance o torch Tensor o o _out_flat pytree tree_unflatten out_flat _out_spec