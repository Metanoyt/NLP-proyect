usr bin env python flake noqa F importlib logging os re subprocess sys types warnings try common BenchmarkRunner download_retry_decorator load_yaml_file main reset_rng_state except ImportError common BenchmarkRunner download_retry_decorator load_yaml_file main reset_rng_state torch torch _dynamo testing collect_results torch _dynamo utils clone_inputs log = logging getLogger __name__ Enable FX graph caching TORCHINDUCTOR_FX_GRAPH_CACHE os environ torch _inductor config fx_graph_cache = True Enable Autograd caching TORCHINDUCTOR_AUTOGRAD_CACHE os environ torch _functorch config enable_autograd_cache = True pip_install package subprocess check_call sys executable -m pip install package Disable flake warnings imports Flake does provide way disable just warning entire file Disabling flake entirely imports = AlbertForPreTraining AutoConfig AutoModelForCausalLM AutoModelForMaskedLM AutoModelForSeq SeqLM BigBirdConfig BlenderbotForConditionalGeneration BlenderbotModel BlenderbotSmallModel CLIPModel CLIPVisionModel ElectraForPreTraining GPT ForSequenceClassification GPTJForSequenceClassification GPTNeoForSequenceClassification HubertForSequenceClassification LxmertForPreTraining LxmertForQuestionAnswering MarianForCausalLM MarianModel MarianMTModel PegasusModel ReformerConfig ViTForImageClassification ViTForMaskedImageModeling ViTModel process_hf_reformer_output out assert isinstance out list second output unstable elem i elem enumerate out i = try mod = importlib import_module transformers cls imports hasattr mod cls raise ModuleNotFoundError except ModuleNotFoundError print Installing HuggingFace Transformers pip_install git+https github com huggingface transformers git#egg=transformers finally cls imports exec f transformers cls These models contain models present huggingface_models_list It combination models supported HF Fx parser some manually supplied models For these models we already know largest batch size can fit A GPUs - GB BATCH_SIZE_KNOWN_MODELS = Run only selected group models leave empty run everything TORCHBENCH_ONLY_MODELS = m strip m os getenv TORCHBENCH_ONLY_MODELS split m strip TODO sdym use batch-size-file parameter common main like torchbench py Get list models their batch sizes MODELS_FILENAME = os path join os path dirname __file__ huggingface_models_list txt assert os path exists MODELS_FILENAME open MODELS_FILENAME fh lines = fh readlines lines = line rstrip line lines line lines model_name batch_size = line split TORCHBENCH_ONLY_MODELS model_name TORCHBENCH_ONLY_MODELS continue batch_size = int batch_size BATCH_SIZE_KNOWN_MODELS model_name = batch_size assert BATCH_SIZE_KNOWN_MODELS try huggingface_llm_models HF_LLM_MODELS except ImportError huggingface_llm_models HF_LLM_MODELS get_module_cls_by_model_name model_cls_name _module_by_model_name = Speech Text Decoder transformers models speech_to_text_ modeling_speech_to_text_ TrOCRDecoder transformers models trocr modeling_trocr module_name = _module_by_model_name get model_cls_name transformers module = importlib import_module module_name getattr module model_cls_name get_sequence_length model_cls model_name model_name startswith Blenderbot seq_length = model_name startswith GPT Bart T PLBart MBart seq_length = model_name AllenaiLongformerBase BigBird seq_length = model_name startswith OPT seq_length = Reformer model_name seq_length = model_name startswith Albert Deberta Layout Electra XLNet MegatronBert Bert Roberta model_name DistillGPT GoogleFnet YituTechConvBert seq_length = model_name TrOCRForCausalLM seq_length = model_name startswith MobileBert seq_length = model_name startswith Wav Vec If too short will fail something like ValueError ` mask_length ` has smaller than ` sequence_length ` got ` mask_length ` ` sequence_length ` ` seq_length = NB more realistic size log info f Sequence Length defined model_name Choosing arbitrarily noqa G seq_length = seq_length generate_inputs_for_model model_cls model model_name bs device include_loss_args=False TODO - Check following values representative num_choices = num_visual_features = seq_length = get_sequence_length model_cls model_name vocab_size = model config vocab_size model_name startswith Wav Vec TODO If we add more input_values style models try work into overall control flow target_length = input_values torch randn bs seq_length device=device Added because s what example training script has attention_mask rand_int_tensor device bs seq_length labels rand_int_tensor device vocab_size bs target_length model_name endswith MultipleChoice input = rand_int_tensor device vocab_size bs num_choices seq_length model_name startswith Roberta input = rand_int_tensor device bs seq_length input = rand_int_tensor device vocab_size bs seq_length Bart model_name input - = model config eos_token_id input_dict = input_ids input model_name startswith T M M MT model_cls BlenderbotModel BlenderbotSmallModel BlenderbotForConditionalGeneration PegasusModel MarianModel MarianMTModel input_dict decoder_input_ids = input model_name startswith Lxmert visual_feat_dim visual_pos_dim = model config visual_feat_dim model config visual_pos_dim input_dict visual_feats = torch randn bs num_visual_features visual_feat_dim input_dict visual_pos = torch randn bs num_visual_features visual_pos_dim include_loss_args model_name endswith PreTraining model_cls ElectraForPreTraining LxmertForPreTraining input_dict labels = rand_int_tensor device bs seq_length label_name = sentence_order_label model_cls AlbertForPreTraining next_sentence_label input_dict labels = rand_int_tensor device vocab_size bs seq_length input_dict label_name = rand_int_tensor device bs model_name endswith QuestionAnswering input_dict start_positions = rand_int_tensor device seq_length bs input_dict end_positions = rand_int_tensor device seq_length bs model_name endswith MaskedLM HeadModel CausalLM DoubleHeadsModel input_dict labels = rand_int_tensor device vocab_size bs seq_length model_name endswith TokenClassification input_dict labels = rand_int_tensor device model config num_labels - bs seq_length model_name endswith MultipleChoice input_dict labels = rand_int_tensor device num_choices bs model_name endswith SequenceClassification input_dict labels = rand_int_tensor device model config num_labels - bs model_name endswith NextSentencePrediction input_dict labels = rand_int_tensor device bs model_name endswith ForConditionalGeneration input_dict labels = rand_int_tensor device vocab_size - bs seq_length model_name EXTRA_MODELS input_dict labels = rand_int_tensor device vocab_size bs seq_length raise NotImplementedError f Class model_name unsupported training test input_dict rand_int_tensor device low high shape torch randint low high shape device=device dtype=torch int requires_grad=False EXTRA_MODELS = AllenaiLongformerBase AutoConfig from_pretrained allenai longformer-base- AutoModelForMaskedLM Reformer ReformerConfig AutoModelForMaskedLM T Small AutoConfig from_pretrained t -small AutoModelForSeq SeqLM BigBird BigBirdConfig attention_type= block_sparse AutoModelForMaskedLM DistillGPT AutoConfig from_pretrained distilgpt AutoModelForCausalLM GoogleFnet AutoConfig from_pretrained google fnet-base AutoModelForMaskedLM YituTechConvBert AutoConfig from_pretrained YituTech conv-bert-base AutoModelForMaskedLM HuggingfaceRunner BenchmarkRunner __init__ super __init__ suite_name = huggingface property _config load_yaml_file huggingface yaml property _skip _config skip property _accuracy _config accuracy property skip_models _skip all property skip_models_for_cpu _skip device cpu property fp _only_models _config only_fp property skip_models_due_to_control_flow _skip control_flow use_larger_multiplier_for_smaller_tensor name name GPT ForSequenceClassification _get_model_cls_and_config model_name model_name EXTRA_MODELS model_cls = get_module_cls_by_model_name model_name config_cls = model_cls config_class config = config_cls NB some models need pad token defined handle BS model_cls GPT ForSequenceClassification GPTNeoForSequenceClassification GPTJForSequenceClassification model_cls __name__ startswith Roberta model_cls __name__ startswith Marian config pad_token_id = config model_cls = EXTRA_MODELS model_name model_cls config download_retry_decorator _download_model model_name model_cls config = _get_model_cls_and_config model_name auto model_cls __module__ Handle auto classes model = model_cls from_config config model = model_cls config model load_model device model_name batch_size=None extra_args=None is_training = args training use_eval_mode = args use_eval_mode dtype = torch float reset_rng_state Get batch size model_name BATCH_SIZE_KNOWN_MODELS batch_size_default = BATCH_SIZE_KNOWN_MODELS model_name batch_size None batch_size_default = log info f Batch size specified model_name Setting batch_size= noqa G batch_size None batch_size = batch_size_default batch_size_divisors = _config batch_size divisors model_name batch_size_divisors batch_size = max int batch_size batch_size_divisors model_name log info f Running smaller batch size= batch_size model_name orig batch_size= batch_size_default noqa G Get model example inputs model_name HF_LLM_MODELS benchmark_cls = HF_LLM_MODELS model_name model example_inputs = benchmark_cls get_model_and_inputs model_name device Set flag so when we test speedup we use model generate instead using model forward hf_llm = True generate _ example_inputs collect_outputs=True model generate example_inputs generate = types MethodType generate hf_llm = False model_cls config = _get_model_cls_and_config model_name model = _download_model model_name model = model device dtype=dtype example_inputs = generate_inputs_for_model model_cls model model_name batch_size device include_loss_args=True So we can check correct gradients without eliminating dropout computation attr dir config drop attr isinstance getattr config attr float setattr config attr e- Turning off kv cache torchbench models This right thing do pt dashboard outdated Real transformers benchmarks will added soon using different infra hasattr model config hasattr model config use_cache model config use_cache = False args enable_activation_checkpointing model gradient_checkpointing_enable is_training use_eval_mode args accuracy model_name _config only_inference model train model eval validate_model model example_inputs device model_name model example_inputs batch_size iter_model_names args model_names = list BATCH_SIZE_KNOWN_MODELS keys + list EXTRA_MODELS keys model_names = set model_names model_names = sorted model_names start end = get_benchmark_indices len model_names index model_name enumerate model_names index start index = end continue re search &#124; join args filter model_name re IGNORECASE re search &#124; join args exclude model_name re IGNORECASE model_name args exclude_exact model_name skip_models continue yield model_name property skip_accuracy_checks_large_models_dashboard args dashboard args accuracy _accuracy skip large_models set property get_output_amp_train_process_func pick_grad name is_training is_training torch enable_grad torch no_grad get_tolerance_and_cosine_flag is_training current_device name cosine = args cosine is_training torch _inductor config inductor_config name _config tolerance higher_training inductor_config max_autotune name _config tolerance higher_max_autotune_training e- cosine e- cosine current_device == cpu name _config tolerance higher_inference_cpu e- cosine name _config tolerance higher_inference e- cosine e- cosine compute_loss pred pred forward_pass mod inputs collect_outputs=True autocast autocast_arg res = mod inputs res logits hf_llm res forward_and_backward_pass mod inputs collect_outputs=True cloned_inputs = clone_inputs inputs optimizer_zero_grad mod autocast autocast_arg pred = mod cloned_inputs loss = compute_loss pred grad_scaler scale loss backward optimizer_step collect_outputs collect_results mod None loss cloned_inputs None refresh_model_names_and_batch_sizes This function reads HF Fx tracer supported models finds largest batch size could fit GPU PyTorch eager The resulting data written huggingface_models_list txt Note - We only need run function we believe HF Fx tracer now supports more models transformers utils fx hf_fx family = lm_seen = set family_seen = set cls_name hf_fx _SUPPORTED_MODELS For cls_name continue model_cls = get_module_cls_by_model_name cls_name TODO AttributeError Config object has no attribute vocab_size model_cls CLIPModel CLIPVisionModel SwinForImageClassification SwinForImageClassification SwinForMaskedImageModeling SwinModel ViTForImageClassification ViTForMaskedImageModeling ViTModel continue TODO AssertionError Padding_idx must within num_embeddings model_cls MarianForCausalLM MarianMTModel MarianModel continue TODO model supported yet HFTracer model_cls HubertForSequenceClassification continue TODO shape mismatch loss calculation model_cls LxmertForQuestionAnswering continue family_name = cls_name split For family_name family family family_name = cls_name endswith MaskedLM CausalLM family_name lm_seen family family_name append cls_name lm_seen add family_name cls_name endswith SequenceClassification ConditionalGeneration QuestionAnswering family_name family_seen family family_name append cls_name family_seen add family_name cls_name endswith ImageClassification family family_name append cls_name chosen_models = set members family values chosen_models update set members Add EXTRA_MODELS chosen_models update set EXTRA_MODELS keys model_name sorted chosen_models try subprocess check_call sys executable + sys argv + -- find-batch-sizes + f -- only= model_name + f -- output= MODELS_FILENAME except subprocess SubprocessError log warning f Failed find suitable batch size model_name noqa G huggingface_main Code refresh model names batch sizes -- find-batch-sizes sys argv refresh_model_names_and_batch_sizes logging basicConfig level=logging WARNING warnings filterwarnings ignore main HuggingfaceRunner __name__ == __main__ huggingface_main