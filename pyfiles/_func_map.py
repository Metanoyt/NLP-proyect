mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates functools collections abc Callable Sequence typing Optional Union torch torch distributed _functional_collectives AsyncCollectiveTensor torch distributed tensor DeviceMesh DTensor torch distributed tensor placement_types Placement try torch utils _cxx_pytree pytree except ImportError torch utils _pytree pytree type ignore no-redef __all__ = local_map PlacementType = Optional Sequence Placement InputPlacements = Optional tuple PlacementType OutputPlacements = Union PlacementType tuple PlacementType local_map func Optional Callable = None out_placements OutputPlacements = None in_placements InputPlacements = None in_grad_placements InputPlacements = None device_mesh Optional DeviceMesh = None redistribute_inputs bool = False meth ` local_map ` experimental API allows users pass ` DTensor ` s function written applied ` ` torch Tensor ` ` s It done extracting local components ` DTensor ` call function wrap outputs ` DTensor ` according ` ` out_placements ` ` Args func Callable function applied each local shard ` DTensor ` s out_placements Union ` PlacementType ` Tuple ` PlacementType ` desired placements ` DTensor ` s ` ` func ` ` s flattened output If flattened ` ` output ` ` single value ` ` out_placements ` ` should type ` PlacementType ` Otherwise flattened ` ` output ` ` has multiple values ` ` out_placements ` ` should tuple ` PlacementType ` values mapping flattened ` ` output ` ` Besides ` Tensor ` output we use ` PlacementType ` its placements ` Tuple Placement ` value For non-Tensor output ` PlacementType ` should ` None ` Note only exception when no ` DTensor ` argument passed In case even ` out_placements ` ` None ` result function should ignore desired placements because function running ` DTensor ` s in_placements Tuple ` PlacementType ` optional required placements ` DTensor ` s flattened inputs ` ` func ` ` If ` ` in_placements ` ` specified meth ` local_map ` would examine whether placements each ` DTensor ` argument same required placements If placements same ` ` redistribute_inputs ` ` ` ` False ` ` exception will raised Otherwise ` ` redistribute_inputs ` ` ` ` True ` ` argument will first redistributed required sharding placements before passing its local tensor ` ` func ` ` The only exception when required placements ` ` None ` ` argument ` torch Tensor ` In case placements examination will skipped argument will directly passed ` ` func ` ` If ` ` in_placements ` ` ` ` None ` ` no placements examination will performed Default None in_grad_placements Tuple ` PlacementType ` optional placements hint ` DTensor ` s gradient corresponds flattened input DTensor This argument hint user can give meth ` to_local ` case gradient layout local tensor input does match its ` DTensor ` input layout If specified we will assume gradient layout local tensor input remains same original ` DTensor ` input use gradient computation Default None device_mesh ` DeviceMesh ` optional device mesh output ` DTensor ` s placed If specified will inferred first input ` DTensor ` s device mesh Default None Keyword Args redistribute_inputs bool optional bool value indicating whether reshard input ` DTensor ` s when their placements different required input placements If value ` ` False ` ` some ` DTensor ` input has different placement exception will raised Default False Returns A ` ` Callable ` ` applies ` ` func ` ` each local shard input ` DTensor ` returns ` DTensor ` constructed value ` ` func ` ` Raises AssertionError For any non-DTensor output we require its corresponding output placement ` ` out_placements ` ` None An AssertionError will raised case ValueError If ` ` redistribute_inputs=False ` ` input ` DTensor ` needs redistribution according ` ` in_placements ` ` Example xdoctest +SKIP distributed mm_allreduce_forward device_mesh W X partial_sum_tensor = torch mm W X reduced_tensor = funcol all_reduce partial_sum_tensor sum device_mesh reduced_tensor W = torch randn requires_grad=False X = torch randn requires_grad=False Y = torch mm W X row_wise = Shard row-wise sharding placements -d mesh col_wise = Shard col-wise sharding placements -d mesh local_mm_allreduce_forward function wrapped DTensor Tensor conversion local_mm_allreduce_forward = local_map mm_allreduce_forward out_placements= Replicate in_placements= col_wise row_wise device_mesh=device_mesh W_dt = distribute_tensor W device_mesh col_wise col-wisely sharded W tensor X_dt = distribute_tensor X device_mesh row_wise row-wisely sharded X tensor Y_dt = local_mm_allreduce_forward device_mesh W_dt X_dt apply local_mm_allreduce_forward DTensors note This API currently experimental subject change func None decorator mode decorated func local_map func=func out_placements=out_placements in_placements=in_placements in_grad_placements=in_grad_placements device_mesh=device_mesh redistribute_inputs=redistribute_inputs decorated functools partial _local_map_wrapped func out_placements in_placements in_grad_placements device_mesh redistribute_inputs _local_map_wrapped func Callable out_placements OutputPlacements in_placements InputPlacements in_grad_placements InputPlacements device_mesh Optional DeviceMesh redistribute_inputs bool args kwargs process input args flat_args args_spec = pytree tree_flatten args in_placements None assert len in_placements == len flat_args f in_placements length len in_placements does match number f input args len flat_args we assume every DTensor object placed same device mesh flat_local_args = seen_dtensor_arg = False idx arg enumerate flat_args isinstance arg DTensor TODO current code doesn t consider uneven sharding case Need think about what consequence when input DTensor uneven sharded device_mesh None infer device mesh DTensor arg device_mesh = arg device_mesh function applied least one DTensor argument seen_dtensor_arg = True in_placements None spec = in_placements idx assert spec None f DTensor input arg expects placements received spec isinstance spec tuple spec = tuple spec arg placements = spec redistribute_inputs redistribute input placements arg = arg redistribute placements=spec raise ValueError f arg arg local_map has mismatched placements f arg placements arg placements input f placements spec If redistribute_inputs wanted set redistribute_inputs=True local_map in_grad_placements None spec = in_grad_placements idx assert spec None f DTensor input arg expects grad placements received spec isinstance spec tuple spec = tuple spec local_arg = arg to_local grad_placements=spec local_arg = arg to_local isinstance local_arg AsyncCollectiveTensor local_arg = local_arg wait flat_local_args append local_arg Non-Tensor input must have None ` in_placements ` in_placements None isinstance arg torch Tensor spec = in_placements idx assert spec None f Non-Tensor input arg expects None placements f received spec flat_local_args append arg pyrefly ignore bad-argument-type local_args = pytree tree_unflatten flat_local_args args_spec out = func local_args kwargs seen_dtensor_arg process output DTensor we ve seen DTensor inputs flat_out out_spec = pytree tree_flatten out flat_dist_out = out_placements_tuple = out_placements isinstance out_placements tuple out_placements assert len flat_out == len out_placements_tuple local_map requires one PlacementType provided each output value f received len out_placements_tuple out_placements f len flat_out expected out spec zip flat_out out_placements_tuple isinstance out torch Tensor assert isinstance out DTensor f torch Tensor output expected received type out out flat_dist_out append DTensor from_local out device_mesh spec run_check=False assert spec None f Non-tensor output out expects None placements received spec flat_dist_out append out pyrefly ignore bad-argument-type pytree tree_unflatten flat_dist_out out_spec out