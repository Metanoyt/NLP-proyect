argparse logging os torch torch distributed c d FORMAT = asctime s - name s - levelname s - message s log = logging getLogger log log setLevel logging INFO handler = logging StreamHandler formatter = logging Formatter FORMAT handler setFormatter formatter log addHandler handler log propagate = False Prevent log duplication __name__ == __main__ parser = argparse ArgumentParser description= Simple script simulate NCCL errors The script supposed run multiple different nodes simultaneously appropriate rank world_size The script run allreduce rank node aborts all other nodes simulate error NCCL parser add_argument addr help= address master node connect parser add_argument port help= port master node connect parser add_argument rank help= rank node parser add_argument world_size help= number nodes process group args = parser parse_args rank = int args rank world_size = int args world_size port = int args port store = c d TCPStore args addr port world_size rank == process_group = c d ProcessGroupNCCL store rank world_size log info Running first allreduce process_group allreduce torch rand cuda rank wait rank == log info Running second allreduce only rank work = process_group allreduce torch rand cuda rank log info Waiting allreduce complete work wait log info Second allreduce successful s work is_success log info Aborting all other ranks os abort