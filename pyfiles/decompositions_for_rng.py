mypy allow-untyped-decorators mypy allow-untyped-defs functools collections defaultdict collections abc Callable torch torch _decomp decomp torch _decomp get_decompositions torch _ops OpOverload aten = torch ops aten rng_decompositions dict str dict OpOverload Callable = defaultdict dict register_rng_decomposition aten_op decomp register_decomposition aten_op rng_decompositions throw_on_non_cuda device raise RuntimeError f You trying functionalize device type RNG operator device type does f use Philox counter-based RNG Therefore functionalizing device type RNG operator supported We discussing possibility Philox-based RNG implementation CPU TODO - We have register many more distributions here also higher level ops like dropout which have fused implementation can hide rand inside register_rng_decomposition aten rand rand shape dtype=None layout=torch strided device=None pin_memory=False device device type = cuda throw_on_non_cuda device seed offset = PhiloxStateTracker get_state_as_tuple dtype = dtype torch float out offset_jump = torch ops rngprims philox_rand shape seed offset None device dtype PhiloxStateTracker advance_offset offset_jump out register_rng_decomposition aten rand_like rand_like x torch Tensor dtype=None layout=None device=None pin_memory=False memory_format=torch preserve_format device = device x device device type = cuda throw_on_non_cuda device dtype = dtype x dtype seed offset = PhiloxStateTracker get_state_as_tuple out offset_jump = torch ops rngprims philox_rand x shape seed offset None device dtype PhiloxStateTracker advance_offset offset_jump out PhiloxState Represents PhiloxRngState - seed offset where offset = base_offset + relative_offset seed base_offset basically point rng state just before tracing starts relative offset tracks totally consumed offset trace time __init__ - None reset reset seed = torch tensor base_offset = torch tensor relative_offset = offset_advanced_alteast_once = False validate_state assert seed numel = base_offset numel = advance_offset consumed_offset offset_advanced_alteast_once = True relative_offset = relative_offset + consumed_offset set_state seed base_offset relative_offset= seed = seed base_offset = base_offset relative_offset = relative_offset get_state_as_tuple validate_state seed base_offset + relative_offset get_state_as_tensor Only needed because we override get_rng_state validate_state torch stack seed base_offset + relative_offset set_state_from_tensor state Only needed because we override set_rng_state seed base_offset = torch unbind state relative_offset = PhiloxStateTracker Singleton track philox rng state during AOT Autograd tracing For each aot tracing instance AOT Autograd resets tracker keeps track both forward backward offsets At runtime we only care about total consumed forward backward offsets For dynamic shapes these offsets function input shapes Therefore AOT generated graphs have additional outputs compute total consumed forward backward offsets running_state PhiloxState fwd_state PhiloxState bwd_state PhiloxState __enter__ PhiloxStateTracker reset __exit__ exc_type exc_cal exc_tb PhiloxStateTracker reset classmethod reset cls cls running_state = PhiloxState cls fwd_state = PhiloxState cls bwd_state = PhiloxState classmethod mark_beginning_of_forward cls Tells tracker use fwd_state running state cls running_state = cls fwd_state classmethod mark_beginning_of_backward cls Tells tracker use bwd_state running state cls running_state = cls bwd_state classmethod record_state cls seed offset mode Records seed offset tensors These tensors used invoke philox_rand functional primitives mode == forward cls fwd_state set_state seed offset cls mark_beginning_of_forward assert mode == backward cls bwd_state set_state seed offset classmethod get_state_as_tensor cls The only reason exists because we override get_rng_state set_rng_state during tracing get_rng_state expects tensor output so seed offset tuple upset other parts program like ctx saved_tensors A bad consequence user saves restores rng state we have little bit ugliness generated code where we first concat seed offset create tensor get_rng_state then split back get seed offset tuple set_rng_state TODO Investigate there better way wrap tuple false Tensor object then desugar later cls running_state get_state_as_tensor classmethod get_state_as_tuple cls cls running_state get_state_as_tuple classmethod set_state_from_tensor cls x This only needed because we override set_rng_state Look comment get_state_from_tensor method cls running_state set_state_from_tensor x classmethod advance_offset cls consumed_offset cls running_state advance_offset consumed_offset classmethod get_current_relative_offset cls cls running_state relative_offset staticmethod multiple_of_ offset torch cuda rng state offset must multiple For inductor we sum up all numel result might multiple This method achieves offset + classmethod get_updated_fwd_offset cls Short circuit no rand ops observed cls fwd_state offset_advanced_alteast_once cls fwd_state base_offset cls multiple_of_ cls fwd_state base_offset + cls fwd_state relative_offset classmethod get_updated_bwd_offset cls Short circuit no rand ops observed cls bwd_state offset_advanced_alteast_once cls bwd_state base_offset cls multiple_of_ cls bwd_state base_offset + cls bwd_state relative_offset Adding more decompositions which eventually use rand_like inside decomps Adding these rng_decompositions ensures functionalization rand_like ops used these decomps The list copied inductor codebase which uses similar purpose Caution - These decomps do have same accuracy eager However we can t just disable them config flag like fallback_random because functionalization rng ops we have decompose these ops extra_random_decomps = get_decompositions aten cauchy aten cauchy_ aten exponential aten exponential_ aten geometric aten geometric_ aten native_dropout aten normal aten normal_ aten normal_functional aten log_normal aten log_normal_ aten rrelu_with_noise aten rrelu_with_noise_ aten uniform_ register_extra_random_decomp = functools partial decomp register_decomposition registry=extra_random_decomps register_extra_random_decomp aten bernoulli_ bernoulli_ p= device == torch device cpu NotImplemented copy_ torch rand_like dtype=torch float p register_extra_random_decomp aten bernoulli p bernoulli_p p= generator=None device == torch device cpu NotImplemented assert generator None torch rand_like dtype=torch float p rng_decompositions update extra_random_decomps type ignore arg-type