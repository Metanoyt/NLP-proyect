Copyright c Meta Platforms Inc affiliates implement matrix related ops distributed tensor typing Optional torch torch distributed device_mesh DeviceMesh torch distributed tensor _dtensor_spec DTensorSpec TensorMeta torch distributed tensor _op_schema OpSchema OpSpec OpStrategy PlacementList RuntimeSchemaInfo torch distributed tensor _ops _einsum_strategy gen_einsum_strategies torch distributed tensor _ops utils expand_to_full_mesh_op_strategy generate_redistribute_costs infer_broadcast_dims_map is_tensor_shardable map_placements_after_broadcast prod register_op_strategy torch distributed tensor _utils compute_local_shape_and_global_offset compute_local_stride torch distributed tensor placement_types Partial Placement Replicate Shard aten = torch ops aten register_op_strategy aten t default transpose_strategy op_schema OpSchema - OpStrategy self_strategy = op_schema args_schema isinstance self_strategy OpStrategy raise AssertionError f Expected OpStrategy got type self_strategy transpose_strategies = input_strategy self_strategy strategies input_spec = input_strategy output_spec follow input spec transpose Shard placements output_placements = Shard - p dim isinstance p Shard p p input_spec placements transpose_strategy = OpSpec output_specs=DTensorSpec mesh=input_strategy mesh placements=tuple output_placements input_specs= input_strategy output_spec transpose_strategies append transpose_strategy OpStrategy strategies=transpose_strategies _mm_like_strategy mm_equation str mesh DeviceMesh op_schema OpSchema - OpStrategy self_strategy mat _strategy = op_schema args_schema isinstance self_strategy OpStrategy raise AssertionError f Expected OpStrategy got type self_strategy isinstance mat _strategy OpStrategy raise AssertionError f Expected OpStrategy got type mat _strategy generate all possible strategies mm mm_strategy = gen_einsum_strategies mm_equation mesh filter out invalid strategies associate costs strategies = mm_strategy strategies filtered_strategies = strtg strategies strtg input_specs None raise AssertionError f Expected input_specs None got strtg input_specs self_spec = strtg input_specs mat _spec = strtg input_specs is_tensor_shardable self_strategy shape self_spec is_tensor_shardable mat _strategy shape mat _spec redistribute_cost = generate_redistribute_costs self_strategy self_spec generate_redistribute_costs mat _strategy mat _spec strtg redistribute_cost = redistribute_cost filtered_strategies append strtg mm_strategy strategies = filtered_strategies mm_strategy _addmm_like_strategy mm_equation str mesh DeviceMesh op_schema OpSchema - OpStrategy self_strategy mat _strategy mat _strategy = op_schema args_schema isinstance self_strategy OpStrategy raise AssertionError f Expected OpStrategy got type self_strategy isinstance mat _strategy OpStrategy raise AssertionError f Expected OpStrategy got type mat _strategy isinstance mat _strategy OpStrategy raise AssertionError f Expected OpStrategy got type mat _strategy self_shape = self_strategy shape mm_out_shape = torch Size mat _strategy shape - i == len mat _strategy shape - dim_size i dim_size enumerate mat _strategy shape generate all possible strategies mm mm_strategy = gen_einsum_strategies mm_equation mesh filter out invalid strategies associate costs strategies = mm_strategy strategies filtered_strategies = strtg strategies construct new strategy consider arg strtg input_specs None raise AssertionError f Expected input_specs None got strtg input_specs mat _spec = strtg input_specs mat _spec = strtg input_specs out_spec = strtg output_spec arg s spec should follow output mm need consider broadcast arg broadcast_dims_map = infer_broadcast_dims_map mm_out_shape self_shape self_placements = map_placements_after_broadcast out_spec placements mm_out_shape broadcast_dims_map self_spec = DTensorSpec mesh=mesh placements=self_placements is_tensor_shardable mat _strategy shape mat _spec is_tensor_shardable mat _strategy shape mat _spec update input specs new spec strtg input_specs = self_spec mat _spec mat _spec associate costs redistribute_cost = generate_redistribute_costs self_strategy self_spec generate_redistribute_costs mat _strategy mat _spec generate_redistribute_costs mat _strategy mat _spec strtg redistribute_cost = redistribute_cost filtered_strategies append strtg mm_strategy strategies = filtered_strategies mm_strategy _scaled_mm_like_strategy mm_equation str mesh DeviceMesh op_schema OpSchema - OpStrategy self_strategy mat _strategy scale_self_strategy scale_mat _strategy bias_strategy scale_result_strategy _ = op_schema args_schema isinstance self_strategy OpStrategy raise AssertionError f Expected OpStrategy got type self_strategy isinstance mat _strategy OpStrategy raise AssertionError f Expected OpStrategy got type mat _strategy isinstance scale_self_strategy OpStrategy raise AssertionError f Expected OpStrategy got type scale_self_strategy isinstance scale_mat _strategy OpStrategy raise AssertionError f Expected OpStrategy got type scale_mat _strategy TODO add support these later bias_strategy None raise AssertionError _scaled_mm DTensors doesn t support bias scale_result_strategy None raise AssertionError _scaled_mm DTensors doesn t support scale_result generate all possible strategies mm mm_strategy = gen_einsum_strategies mm_equation mesh filter out invalid strategies associate costs strategies = mm_strategy strategies filtered_strategies = strtg strategies strtg input_specs None raise AssertionError f Expected input_specs None got strtg input_specs self_spec = strtg input_specs mat _spec = strtg input_specs propagate operands specs their scales except tensor-wise scaling which can have any numbers dims legacy hence sharding dims won t map tensor-wise anyways we can only do replication scale_self_spec = DTensorSpec self_spec mesh Replicate prod scale_self_strategy shape == self_spec scale_mat _spec = DTensorSpec mat _spec mesh Replicate prod scale_mat _strategy shape == mat _spec strtg input_specs = list strtg input_specs + scale_self_spec scale_mat _spec is_tensor_shardable self_strategy shape self_spec is_tensor_shardable mat _strategy shape mat _spec is_tensor_shardable scale_self_strategy shape scale_self_spec is_tensor_shardable scale_mat _strategy shape scale_mat _spec redistribute_cost = generate_redistribute_costs self_strategy self_spec generate_redistribute_costs mat _strategy mat _spec generate_redistribute_costs scale_self_strategy scale_self_spec generate_redistribute_costs scale_mat _strategy scale_mat _spec strtg redistribute_cost = redistribute_cost filtered_strategies append strtg mm_strategy strategies = filtered_strategies mm_strategy register_op_strategy aten dot default dot_strategy op_schema OpSchema - OpStrategy mesh = op_schema get_mesh_from_args _mm_like_strategy i i- mesh op_schema register_op_strategy aten mm default mm_strategy op_schema OpSchema - OpStrategy mesh = op_schema get_mesh_from_args _mm_like_strategy mk kn- mn mesh op_schema register_op_strategy aten addmm default addmm_strategy op_schema OpSchema - OpStrategy mesh = op_schema get_mesh_from_args _addmm_like_strategy mk kn- mn mesh op_schema register_op_strategy aten bmm default bmm_strategy op_schema OpSchema - OpStrategy mesh = op_schema get_mesh_from_args _mm_like_strategy bmk bkn- bmn mesh op_schema register_op_strategy aten baddbmm default baddmm_strategy op_schema OpSchema - OpStrategy mesh = op_schema get_mesh_from_args _addmm_like_strategy bmk bkn- bmn mesh op_schema register_op_strategy aten _scaled_mm default scaled_mm_strategy op_schema OpSchema - OpStrategy mesh = op_schema get_mesh_from_args _scaled_mm_like_strategy mk kn- mn mesh op_schema register_op_strategy aten _scaled_dot_product_flash_attention default schema_info=RuntimeSchemaInfo scaled_dot_product_flash_attention_strategy op_schema OpSchema - OpStrategy NOTE currently we only support some simple strategies support tensor parallelism TODO sdpa might good candidate us explore decomposed sharding propagation involves matmul pointwise reduction ops together mesh = op_schema get_mesh_from_args return_debug_mask = len op_schema args_schema = op_schema args_schema q_input_strategy = op_schema args_schema isinstance q_input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type q_input_strategy assuming q k v have same shape single_mesh_dim_strategies = placement list stores placements outputs inputs spda case we have valid tensor outputs tensor inputs first we can always accept full replication both inputs outputs all_replicate PlacementList = Replicate Replicate None cum_seq_q None cum_seq_k None max_q None max_k Replicate rng_state None unused Replicate Replicate Replicate Replicate single_mesh_dim_strategies append all_replicate second we can accept sharding pattern tensor parallelism which shard num head dim qkv_sharding = Shard num head dim output_sharding = Shard num head dim logsumexp_sharding = Shard num head dim return_debug_mask debug_attn_mask_sharding Placement = Shard num head dim empty debug mask replicated debug_attn_mask_sharding = Replicate num_heads_dim_sharding PlacementList = output_sharding logsumexp_sharding None cum_seq_q None cum_seq_k None max_q None max_k Replicate rng_state None unused debug_attn_mask_sharding qkv_sharding qkv_sharding qkv_sharding single_mesh_dim_strategies append num_heads_dim_sharding Shard batch dimension debug_attn_mask_sharding = Shard return_debug_mask Replicate single_mesh_dim_strategies append Shard output Shard logsumexp None cum_seq_q None cum_seq_k None max_q None max_k Replicate rng_state None unused debug_attn_mask_sharding debugattn Shard q Shard k Shard v Context Parallelism shards sequence dim debug_attn_mask_sharding = Shard return_debug_mask Replicate single_mesh_dim_strategies append Shard output Shard logsumexp None cum_seq_q None cum_seq_k None max_q None max_k Replicate rng_state None unused debug_attn_mask_sharding debugattn Shard q Shard k Shard v expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies input_index= register_op_strategy aten _scaled_dot_product_flash_attention_backward default scaled_dot_product_flash_attention_backward_strategy op_schema OpSchema - OpStrategy backward op does need validate mesh since forward op has already done mesh = op_schema get_mesh_from_args validate=False q_input_strategy = op_schema args_schema isinstance q_input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type q_input_strategy assuming q k v have same shape tensor_input_indices = i i arg_spec enumerate op_schema args_schema isinstance arg_spec OpStrategy num_tensor_inputs = len tensor_input_indices single_mesh_dim_strategies = placement list stores placements outputs inputs spda backward case we have tensor outputs tensor inputs first we can always accept full replication both inputs outputs all_replicate PlacementList = Replicate + num_tensor_inputs single_mesh_dim_strategies append all_replicate second we can accept sharding pattern tensor parallelism which shard num head dim grad_output_sharding = Shard num head dim qkv_sharding = Shard num head dim output_sharding = Shard num head dim logsumexp_sharding = Shard num head dim grad_qkv_sharding = Shard num head dim num_heads_dim_sharding PlacementList = grad_qkv_sharding grad_qkv_sharding grad_qkv_sharding grad_output_sharding qkv_sharding qkv_sharding qkv_sharding output_sharding logsumexp_sharding accept replicate rest tensor inputs potentially cum_seq_q cum_seq_k philox_seed philox_offset indices respectively num_heads_dim_sharding extend Replicate num_tensor_inputs - single_mesh_dim_strategies append num_heads_dim_sharding Batch sharding batch_dim_sharding PlacementList = Shard grad_q Shard grad_k Shard grad_v Shard grad_output Shard q Shard k Shard v Shard output Shard logsumexp accept replicate rest tensor inputs potentially cum_seq_q cum_seq_k philox_seed philox_offset indices respectively batch_dim_sharding extend Replicate num_tensor_inputs - single_mesh_dim_strategies append batch_dim_sharding Context Parallelism shards sequence dim seq_dim_sharding PlacementList = Shard grad_q Shard grad_k Shard grad_v Shard grad_output Shard q Shard k Shard v Shard output Shard logsumexp accept replicate rest tensor inputs potentially cum_seq_q cum_seq_k philox_seed philox_offset indices respectively seq_dim_sharding extend Replicate num_tensor_inputs - single_mesh_dim_strategies append seq_dim_sharding expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies input_index= register_op_strategy aten constant_pad_nd default constant_pad_nd_strategy op_schema OpSchema - OpStrategy mesh = op_schema get_mesh_from_args validate=False TODO d l k implement more correct strategy constant_pad_nd OpStrategy OpSpec output_specs=DTensorSpec mesh Replicate input_specs= DTensorSpec mesh Replicate DTensorSpec mesh Replicate redistribute_cost= register_op_strategy aten _scaled_dot_product_efficient_attention default schema_info=RuntimeSchemaInfo scaled_dot_product_efficient_attention_strategy op_schema OpSchema - OpStrategy NOTE currently we only support some simple strategies support tensor parallelism mesh = op_schema get_mesh_from_args q_input_strategy = op_schema args_schema isinstance q_input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type q_input_strategy assuming q k v have same shape has_attn_bias = op_schema args_schema None compute_log_sumexp = op_schema args_schema single_mesh_dim_strategies list PlacementList = placement list stores placements outputs inputs spda case we have valid tensor outputs tensor inputs first we can always accept full replication both inputs outputs all_replicate PlacementList = Replicate Replicate None None Replicate Replicate Replicate has_attn_bias all_replicate append Replicate attn bias Context Parallelism shards sequence dim single_mesh_dim_strategies append Shard output Shard logsumexp None philox_seed None philox_offset Shard q Shard k Shard v single_mesh_dim_strategies append all_replicate second we can accept sharding pattern tensor parallelism which shard heads dimension qkv_sharding = Shard output_sharding = Shard compute_log_sumexp logsumexp_sharding Placement = Shard empty logsumexp replicated logsumexp_sharding = Replicate num_heads_dim_sharding = output_sharding logsumexp_sharding None None qkv_sharding qkv_sharding qkv_sharding has_attn_bias num_heads_dim_sharding append Shard single_mesh_dim_strategies append num_heads_dim_sharding batch sharding compute_log_sumexp logsumexp_sharding_dp Placement = Shard empty logsumexp replicated logsumexp_sharding_dp = Replicate batch_sharding = Shard output logsumexp_sharding_dp logsumexp None philox_seed None philox_offset Shard q Shard k Shard v has_attn_bias batch_sharding append Shard single_mesh_dim_strategies append batch_sharding expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies input_index= register_op_strategy aten _scaled_dot_product_efficient_attention_backward default scaled_dot_product_efficient_attention_backward_strategy op_schema OpSchema - OpStrategy backward op does need validate mesh since forward op has already done mesh = op_schema get_mesh_from_args validate=False q_input_strategy = op_schema args_schema isinstance q_input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type q_input_strategy assuming q k v have same shape has_attn_bias = op_schema args_schema None single_mesh_dim_strategies = placement list stores placements outputs inputs spda backward case we have tensor outputs tensor inputs NOTE Output sharding grad_bias heads dim attn_bias present otherwise grad_bias will empty its DTensorSpec will removed first we can always accept full replication both inputs outputs all_replicate PlacementList = Replicate + has_attn_bias has_attn_bias all_replicate = None grad bias None attn_bias present single_mesh_dim_strategies append all_replicate second we can accept sharding pattern tensor parallelism which shard heads dimension grad_output_sharding = Shard qkv_sharding = Shard output_sharding = Shard logsumexp_sharding = Shard grad_qkv_sharding = Shard grad_bias_sharding = Shard has_attn_bias None num_heads_dim_sharding PlacementList = grad_qkv_sharding grad_qkv_sharding grad_qkv_sharding grad_bias_sharding grad_output_sharding qkv_sharding qkv_sharding qkv_sharding place optional input attn_bias output_sharding logsumexp_sharding input sharding attn_bias heads dim present has_attn_bias num_heads_dim_sharding insert Shard accept replicate rest scalar tensor inputs namely philox_seed philox_offset num_heads_dim_sharding extend Replicate Replicate single_mesh_dim_strategies append num_heads_dim_sharding Shards batch dim batch_dim_sharding PlacementList = Shard grad_q Shard grad_k Shard grad_v Shard has_attn_bias None grad_bias Shard grad_output Shard q Shard k Shard v Shard output Shard logsumexp accept replicate rest tensor inputs potentially cum_seq_q cum_seq_k philox_seed philox_offset indices respectively has_attn_bias batch_dim_sharding insert Shard batch_dim_sharding extend Replicate Replicate single_mesh_dim_strategies append batch_dim_sharding Context Parallelism shards sequence dim seq_dim_sharding PlacementList = Shard grad_q Shard grad_k Shard grad_v Shard has_attn_bias None grad_bias Shard grad_output Shard q Shard k Shard v Shard output Shard logsumexp accept replicate rest tensor inputs potentially cum_seq_q cum_seq_k philox_seed philox_offset indices respectively has_attn_bias num_heads_dim_sharding insert Shard seq_dim_sharding extend Replicate Replicate single_mesh_dim_strategies append seq_dim_sharding expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies input_index= register_op_strategy aten _scaled_dot_product_cudnn_attention default schema_info=RuntimeSchemaInfo scaled_dot_product_cudnn_attention_strategy op_schema OpSchema - OpStrategy mesh = op_schema get_mesh_from_args query_strategy query _ key _ value attn_bias_strategy compute_log_sumexp compute_log_sumexp rest_args optional args dropout_p is_causal return_debug_mask scale = op_schema args_schema return_debug_mask = len op_schema args_schema = rest_args has_attn_bias = attn_bias_strategy None debug_attn_mask_sharding Optional Placement = Replicate return_debug_mask None isinstance query_strategy OpStrategy raise AssertionError f Expected OpStrategy got type query_strategy assuming q k v have same shape single_mesh_dim_strategies = placement list stores placements outputs inputs spda case we have valid tensor outputs tensor inputs first we can always accept full replication both inputs outputs all_replicate PlacementList = Replicate output Replicate logsumexp None cum_seq_q None cum_seq_k None max_q None max_k None philox_seed None philox_offset NOTE debug_attn_mask supported pytorch always empty tensor https github com pytorch pytorch blob b eb d c ade b aten src ATen native transformers cuda attention cu#L -L debug_attn_mask_sharding debug_attn_mask Replicate q Replicate k Replicate v has_attn_bias all_replicate append Replicate attn bias single_mesh_dim_strategies append all_replicate second we can accept sharding pattern tensor parallelism which shard num head dim tp_sharding = Shard num head dim qkv_sharding = tp_sharding output_sharding = tp_sharding logsumexp_sharding = tp_sharding compute_log_sumexp Replicate debug_attn_mask_sharding = tp_sharding return_debug_mask None num_heads_dim_sharding PlacementList = output_sharding logsumexp_sharding None cum_seq_q None cum_seq_k None max_q None max_k None philox_seed None philox_offset debug_attn_mask_sharding qkv_sharding qkv_sharding qkv_sharding single_mesh_dim_strategies append num_heads_dim_sharding batch parallelism logsumexp_sharding = Shard compute_log_sumexp Replicate debug_attn_mask_sharding = Shard return_debug_mask None batch_dim_sharding PlacementList = Shard output logsumexp_sharding None cum_seq_q None cum_seq_k None max_q None max_k None philox_seed None philox_offset debug_attn_mask_sharding Shard q Shard k Shard v single_mesh_dim_strategies append batch_dim_sharding Context Parallelism shards sequence dim cp_sharding = Shard seq dim logsumexp_sharding = cp_sharding compute_log_sumexp Replicate debug_attn_mask_sharding = cp_sharding return_debug_mask None single_mesh_dim_strategies append cp_sharding output logsumexp_sharding logsumexp None cum_seq_q None cum_seq_k None max_q None max_k None philox_seed None philox_offset debug_attn_mask_sharding debug_attn_mask cp_sharding q cp_sharding k cp_sharding v expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies input_index= register_op_strategy aten _scaled_dot_product_cudnn_attention_backward default scaled_scaled_dot_product_cudnn_attention_backward_strategy op_schema OpSchema - OpStrategy backward op does need validate mesh since forward op has already done mesh = op_schema get_mesh_from_args validate=False len op_schema args_schema raise AssertionError f Expected least args_schema got len op_schema args_schema has_attn_bias = op_schema args_schema None has_scale = len op_schema args_schema = False query_strategy = op_schema args_schema isinstance query_strategy OpStrategy raise AssertionError f Expected OpStrategy got type query_strategy assuming q k v have same shape single_mesh_dim_strategies = placement list stores placements outputs inputs cudnn outputs Tensor dq Tensor dk Tensor dv cudnn inputs Tensor grad_out Tensor query Tensor key Tensor value Tensor out Tensor logsumexp Tensor philox_seed Tensor philox_offset Tensor attn_bias Tensor cum_seq_q Tensor cum_seq_k SymInt max_q SymInt max_k float dropout_p bool is_causal int scale case we can always accept full replication both inputs outputs all_replicate_out PlacementList = Replicate dq Replicate dk Replicate dv all_replicate_inp PlacementList = Replicate all_replicate_inp += Replicate philox_seed philox_offset casted Replicate DTensor all_replicate_inp += Replicate has_attn_bias None all_replicate_inp += None has_scale all_replicate_inp append None all_replicate PlacementList = all_replicate_out + all_replicate_inp single_mesh_dim_strategies append all_replicate case we can accept sharding pattern tensor parallelism which shards num head dim qkv_sharding = Shard num head dim output_sharding = Shard num head dim logsumexp_sharding = Shard num head dim num_heads_dim_sharding_out PlacementList = qkv_sharding num_heads_dim_sharding_inp PlacementList = qkv_sharding num_heads_dim_sharding_inp += output_sharding num_heads_dim_sharding_inp += logsumexp_sharding num_heads_dim_sharding_inp += Replicate philox_seed philox_offset casted Replicate DTensor num_heads_dim_sharding_inp += Shard has_attn_bias None num_heads_dim_sharding_inp += None has_scale num_heads_dim_sharding_inp append None num_heads_dim_sharding = num_heads_dim_sharding_out + num_heads_dim_sharding_inp single_mesh_dim_strategies append num_heads_dim_sharding case Context Parallelism which shards sequence dim context_parallel_sharding_out PlacementList = Shard context_parallel_sharding_inp PlacementList = Shard context_parallel_sharding_inp += Replicate philox_seed philox_offset casted Replicate DTensor context_parallel_sharding_inp += Shard has_attn_bias None context_parallel_sharding_inp += None has_scale context_parallel_sharding_inp append None context_parallel_sharding = context_parallel_sharding_out + context_parallel_sharding_inp single_mesh_dim_strategies append context_parallel_sharding case we can accept sharding pattern batch parallelism which shards batch dimension qkv_sharding = Shard output_sharding = Shard logsumexp_sharding = Shard batch_dim_sharding_out PlacementList = qkv_sharding batch_dim_sharding_inp PlacementList = qkv_sharding batch_dim_sharding_inp += output_sharding batch_dim_sharding_inp += logsumexp_sharding batch_dim_sharding_inp += Replicate philox_seed philox_offset casted Replicate DTensor batch_dim_sharding_inp += Shard has_attn_bias None batch_dim_sharding_inp += None has_scale batch_dim_sharding_inp append None batch_dim_sharding = batch_dim_sharding_out + batch_dim_sharding_inp single_mesh_dim_strategies append batch_dim_sharding expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies input_index= register_op_strategy aten _grouped_mm default grouped_mm_strategy op_schema OpSchema - OpStrategy mesh = op_schema get_mesh_from_args mat _strategy = op_schema args_schema isinstance mat _strategy OpStrategy raise AssertionError f Expected OpStrategy got type mat _strategy mat _strategy = op_schema args_schema isinstance mat _strategy OpStrategy raise AssertionError f Expected OpStrategy got type mat _strategy len op_schema args_schema bias_strategy = op_schema args_schema bias_strategy None raise AssertionError grouped_mm doesn t support bias yet single_mesh_dim_strategies = offs_placement = None len op_schema args_schema op_schema args_schema None offs_placement = Replicate offs should always replicated all_replicate PlacementList = Replicate Replicate mat Replicate mat offs_placement offs None bias partial_replicate PlacementList = Partial Partial mat Replicate mat offs_placement offs None bias replicate_partial PlacementList = Partial Replicate mat Partial mat offs_placement offs None bias single_mesh_dim_strategies = all_replicate partial_replicate replicate_partial mat _strategy ndim == mat _strategy ndim == rowwise_replicate dx d supported replicate_colwise_ x PlacementList = Shard Replicate mat Shard mat offs_placement offs None bias colwise_rowwise_ x PlacementList = Partial Shard mat Shard mat offs_placement offs None bias single_mesh_dim_strategies extend replicate_colwise_ x colwise_rowwise_ x mat _strategy ndim == mat _strategy ndim == replicate_colwise dx d supported colwise_rowwise_ x PlacementList = Partial Shard mat Shard mat offs_placement offs None bias rowwise_replicate_ x PlacementList = Shard Shard mat Replicate mat offs_placement offs None bias single_mesh_dim_strategies extend colwise_rowwise_ x rowwise_replicate_ x mat _strategy ndim == mat _strategy ndim == colwise_rowwise dx d supported replicate_colwise_ x PlacementList = Shard Replicate mat Shard mat offs_placement offs None bias rowwise_replicate_ x PlacementList = Shard Shard mat Replicate mat offs_placement offs None bias single_mesh_dim_strategies extend replicate_colwise_ x rowwise_replicate_ x mat _strategy ndim == mat _strategy ndim == replicate_colwise_ x PlacementList = Shard Replicate mat Shard mat offs_placement offs None bias rowwise_replicate_ x PlacementList = Shard Shard mat Replicate mat offs_placement offs None bias colwise_rowwise_ x PlacementList = Partial Shard mat Shard mat offs_placement offs None bias batch_dim_sharding PlacementList = Shard Shard mat Shard mat offs_placement offs None bias single_mesh_dim_strategies extend replicate_colwise_ x rowwise_replicate_ x colwise_rowwise_ x batch_dim_sharding valid_grouped_mm_strides input_specs list DTensorSpec output_specs tuple Optional DTensorSpec - bool compute local-tensor shape strides given sharding proposal apply logic groped_mm meta function UGH input DTensorSpecs missing their tensormetas so i can get them another way local_meta spec OpSpec placements tuple Placement - TensorMeta isinstance spec output_specs DTensorSpec raise AssertionError f Expected DTensorSpec got type spec output_specs isinstance spec output_specs tensor_meta TensorMeta raise AssertionError f Expected TensorMeta got type spec output_specs tensor_meta meta TensorMeta = spec output_specs tensor_meta local_stride = compute_local_stride meta stride mesh placements local_shape _ = compute_local_shape_and_global_offset meta shape mesh placements TensorMeta torch Size local_shape local_stride meta dtype pyrefly ignore missing-attribute mat _meta = local_meta mat _strategy strategies input_specs placements pyrefly ignore missing-attribute mat _meta = local_meta mat _strategy strategies input_specs placements check_valid_strides meta TensorMeta - bool copied ` _meta_grouped_mm_common ` meta_registrations py end_dim = len meta shape - alignment = meta dtype itemsize meta stride end_dim - == meta stride end_dim = max meta shape end_dim - meta stride end_dim alignment = False meta stride end_dim == meta stride end_dim - = max meta shape end_dim meta stride end_dim - alignment = False False True mat _valid = check_valid_strides mat _meta mat _valid = check_valid_strides mat _meta mat _valid mat _valid expand_to_full_mesh_op_strategy mesh op_schema single_mesh_dim_strategies input_index= is_valid_strategy_cb=valid_grouped_mm_strides