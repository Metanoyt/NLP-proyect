======= BEGIN Dynamo patch ======= Owner s module dynamo ruff noqa flake noqa Test copied https raw githubusercontent com python cpython refs tags v Lib test test_tuple py sys torch torch _dynamo test_case unittest torch _dynamo test_case CPythonTestCase torch testing _internal common_utils run_tests __TestCase = CPythonTestCase redirect statements sys importlib abc redirect_imports = test mapping_tests test typinganndata test test_grammar test test_math test test_iter test typinganndata ann_module RedirectImportFinder importlib abc MetaPathFinder find_spec fullname path target=None Check problematic one fullname redirect_imports try Attempt standalone module name = fullname removeprefix test r = importlib import_module name Redirect module sys modules sys modules fullname = r Return module spec found module importlib util find_spec name except ImportError None None Add custom finder sys meta_path sys meta_path insert RedirectImportFinder ======= END DYNAMO PATCH ======= test support seq_tests unittest gc pickle For tuple hashes we normally only run test ensure we get same results across platforms handful cases If s so there s no real point running more Set RUN_ALL_HASH_TESTS run more anyway That s usually real interest only when analyzing changing hash algorithm In which case s usually also most useful set JUST_SHOW_HASH_RESULTS see all results instead wrestling test failures See bottom file extensive notes what we re testing here why RUN_ALL_HASH_TESTS = False JUST_SHOW_HASH_RESULTS = False RUN_ALL_HASH_TESTS just display TupleTest seq_tests CommonTest type test = tuple test_getitem_error t = msg = tuple indices must integers slices assertRaisesRegex TypeError msg t test_constructors super test_constructors calling built-in types without argument must empty assertEqual tuple t _ = t _ _bis = tuple t _ assertTrue t _ t _ _bis assertEqual tuple assertEqual tuple assertEqual tuple assertEqual tuple spam s p m assertEqual tuple x x range x test_keyword_args assertRaisesRegex TypeError keyword argument tuple sequence= test_keywords_in_subclass torch _dynamo error_on_graph_break False subclass tuple pass u = subclass assertIs type u subclass assertEqual list u assertRaises TypeError subclass sequence= torch _dynamo error_on_graph_break False subclass_with_init tuple __init__ arg newarg=None newarg = newarg u = subclass_with_init newarg= assertIs type u subclass_with_init assertEqual list u assertEqual u newarg torch _dynamo error_on_graph_break False subclass_with_new tuple __new__ cls arg newarg=None = super __new__ cls arg newarg = newarg u = subclass_with_new newarg= assertIs type u subclass_with_new assertEqual list u assertEqual u newarg test_truth super test_truth assertTrue assertTrue test_len super test_len assertEqual len assertEqual len assertEqual len test_iadd super test_iadd u = u = u u += assertTrue u u test_imul super test_imul u = u = u u = assertTrue u u test_tupleresizebug Check specific bug _PyTuple_Resize squashed f i range yield i assertEqual list tuple f list range We expect tuples whose base components have deterministic hashes have deterministic hashes too - indeed same hashes across platforms hash codes same bit width test_hash_exact check_one_exact t e e got = hash t expected = e support NHASHBITS == e got = expected msg = f FAIL hash t r == got = expected fail msg check_one_exact check_one_exact - check_one_exact - - check_one_exact - check_one_exact - - Various tests hashing tuples check we get few collisions Does something only RUN_ALL_HASH_TESTS true Earlier versions tuple hash algorithm had massive collisions reported - https bugs python org issue - https bugs python org issue test_hash_optional itertools product RUN_ALL_HASH_TESTS If specified ` expected ` -tuple expected number_of_collisions pileup values test fails those aren t values we get Also specified test fails z ` zlimit ` tryone_inner tag nbins hashes expected=None zlimit=None collections Counter nballs = len hashes mean sdev = support collision_stats nbins nballs c = Counter hashes collisions = nballs - len c z = collisions - mean sdev pileup = max c values - del c got = collisions pileup failed = False prefix = zlimit None z zlimit failed = True prefix = f FAIL z zlimit expected None got = expected failed = True prefix += f FAIL got = expected failed JUST_SHOW_HASH_RESULTS msg = f prefix tag pileup pileup mean mean f msg += f coll collisions z z + f JUST_SHOW_HASH_RESULTS sys print msg file=sys __stdout__ fail msg tryone tag xs native =None native =None hi =None lo =None zlimit=None NHASHBITS = support NHASHBITS hashes = list map hash xs tryone_inner tag + f NHASHBITS -bit hash codes NHASHBITS hashes native NHASHBITS == native zlimit NHASHBITS shift = NHASHBITS - tryone_inner tag + -bit upper hash codes h shift h hashes hi zlimit mask = - tryone_inner tag + -bit lower hash codes h mask h hashes lo zlimit Tuples smallish positive integers common - nice we get better than random these tryone range list product range repeat= A previous hash had systematic problems when mixing integers similar magnitude opposite sign obscurely related j ^ - == -j when j odd cands = list range - - + list range Note - omitted because hash - == hash - == - there s nothing tuple hash can do avoid collisions inherited collisions tuple components hashes tryone - list product cands repeat= del cands The hashes here weird mix values where all variation lowest bits across single high-order bit - middle bits all zeroes A decent hash has both propagate low bits left high bits right This also complicated bit there collisions among hashes integers L alone L = n n range tryone list product L repeat= del L Used suffer massive number collisions tryone - list product - repeat= And even worse hash has only single bit set high end A decent hash needs propagate high bits right tryone list product repeat= Hashes ints floats same across platforms String hashes vary even single platform across runs due hash randomization strings So we can t say exactly what should do Instead we insist collisions no more than sdevs above theoretically random mean Even tuple hash can t achieve its own string hash trying decently pseudo-random all bit positions _its_ own We can least test tuple hash doesn t systematically ruin tryone -char tuples list product abcdefghijklmnopqrstuvwxyz repeat= zlimit= The old tuple test See https bugs python org issue Ensures example hash non-commutative spreads closely spaced values doesn t exhibit cancellation tuples like x x y N = base = list range N xp = list product base repeat= inps = base + list product base xp + \ list product xp base + xp + list zip base tryone old tuple test inps del base xp inps The new tuple test See https bugs python org issue Even more tortured nesting mix signed ints very small magnitude n = A = x x range -n n+ x = - B = A + A L = list product A repeat= L = L + list product A repeat= L = L + list product A repeat= T = list testcases These consist all possibly nested most levels deep tuples containing most items set A T = A T += B + L T += product L B T += product L repeat= T += product B L T += product B B L T += product B L B T += product L B B T += product B repeat= assert len T == tryone new tuple test T test_repr l = tuple l = = type test l = type test l assertEqual str repr l assertEqual str repr l assertEqual repr assertEqual repr _not_tracked t Nested tuples can take several collections untrack gc collect gc collect assertFalse gc is_tracked t t _tracked t assertTrue gc is_tracked t t gc collect gc collect assertTrue gc is_tracked t t support cpython_only test_track_literals Test GC-optimization tuple literals x y z = _not_tracked _not_tracked _not_tracked _not_tracked _not_tracked None True False int _not_tracked object _not_tracked x y Tuples mutable elements always tracked even those elements tracked right now _tracked _tracked _tracked _tracked set _tracked x y z check_track_dynamic tp always_track x y z = check = _tracked always_track _not_tracked check tp check tp check tp set check tp x y check tp obj obj x y check tp set x y check tp tuple obj obj x y check tuple tp obj obj x y _tracked tp z _tracked tp x y _tracked tp x y _tracked tp obj obj x y z _tracked tp tuple obj obj x y z _tracked tuple tp obj obj x y z support cpython_only test_track_dynamic Test GC-optimization dynamically constructed tuples check_track_dynamic tuple False support cpython_only test_track_subtypes Tuple subtypes must always tracked torch _dynamo error_on_graph_break False MyTuple tuple pass check_track_dynamic MyTuple True support cpython_only test_bug Trying untrack unfinished tuple could crash Python _not_tracked tuple gc collect i range test_repr_large Check repr large list objects check n l = n s = repr l assertEqual s + join n + check check our checking code check test_iterator_pickle Userlist iterators don t support pickling yet since they based generators data = type test proto range pickle HIGHEST_PROTOCOL + itorg = iter data d = pickle dumps itorg proto = pickle loads d assertEqual type itorg type assertEqual type test type test data = pickle loads d next d = pickle dumps proto assertEqual type test type test data test_reversed_pickle data = type test proto range pickle HIGHEST_PROTOCOL + itorg = reversed data d = pickle dumps itorg proto = pickle loads d assertEqual type itorg type assertEqual type test type test reversed data = pickle loads d next d = pickle dumps proto assertEqual type test type test reversed data test_no_comdat_folding Issue In PGO build MSVC linker s COMDAT folding optimization causes failures code relies distinct function addresses torch _dynamo error_on_graph_break False T tuple pass assertRaises TypeError + T test_lexicographic_ordering Issue = type test b = type test c = type test assertLess b assertLess b c Notes testing hash codes The primary thing Python doesn t care about random hash codes To contrary we like them very regular when possible so low-order bits evenly distributed possible For integers easy hash i == i all not-huge i except i==- For tuples mixed type there s really no hope so we want randomish here instead But getting close pseudo-random all bit positions more expensive than we ve been willing pay We can tolerate large deviations random - what we don t want catastrophic pileups relative handful hash codes The dict set lookup routines remain effective provided full-width hash codes not-equal objects distinct So we compute various statistics here based what truly random hash would do don t automate pass fail based those results Instead those viewed inputs human judgment automated tests merely ensure we get _same_ results across platforms In fact we normally don t bother run them all - set RUN_ALL_HASH_TESTS force When global JUST_SHOW_HASH_RESULTS True tuple hash statistics just displayed stdout A typical output line looks like old tuple test -bit upper hash codes \ pileup mean coll z + old tuple test just string name test being run -bit upper hash codes means run under -bit build we ve shifted away lower bits hash codes pileup there no collisions across those hash codes It s less than maximum number times any single hash code seen So case there least one hash code seen times hash code piled up more times than ideal mean number collisions perfectly random hash function would have yielded average coll number collisions actually seen z coll - mean divided standard deviation number collisions perfectly random hash function would suffer A positive value worse than random negative value better than random Anything magnitude greater than would highly suspect hash function claimed random It s essentially impossible truly random function would deliver result sdevs worse than random But we don t care here That s why test isn t coded fail Knowing something about how high-order hash code bits behave provides insight irrelevant how dict set lookup code performs The low-order bits much more important same test those did just like random old tuple test -bit lower hash codes \ pileup mean coll z - So there always tradeoffs consider For another -bit hash codes \ pileup mean coll z - That run under -bit build spectacularly better than random On -bit build wider hash codes fine too -bit hash codes \ pileup mean coll z - their lower bits poor -bit lower hash codes \ pileup mean coll z + In statistical sense s waaaaay too many collisions collisions out million hash codes isn t anywhere near being real problem b worst pileup single hash code measly extra It s relatively poor case tuple hash still fine practical use This isn t which what Python produced hashes itertools product repeat= Even fat -bit hashcode highest pileup over - making dict set lookup one colliding values thousands times slower average than we expect -bit hash codes \ pileup mean coll z + -bit lower hash codes \ pileup mean coll z + __name__ == __main__ run_tests