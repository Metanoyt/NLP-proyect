Owner s module inductor sys unittest torch testing _internal common_utils IS_CI IS_WINDOWS skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_GPU requires_gpu IS_WINDOWS IS_CI sys stderr write Windows CI does have necessary dependencies test_memory_planning yet\n __name__ == __main__ sys exit raise unittest SkipTest requires sympy functorch filelock noqa F torch torch _C FileCheck torch _dynamo utils same torch _inductor config torch _inductor test_case run_tests TestCase torch _inductor utils run_and_get_cpp_code torch export Dim try test_aot_inductor AOTIRunnerUtil except ImportError test_aot_inductor manual=fbcode caffe test inductor test_aot_inductor-library AOTIRunnerUtil requires_gpu config patch memory_planning=True TestMemoryPlanning TestCase device = GPU_TYPE _generate device Generate simple test case has multiple simultaneously-live intermediate tensors Foo torch nn Module forward x y z t = x matmul y t = x matmul z t = x transpose matmul t t = x matmul t t sum + t sum x = torch randn device=device y = torch randn device=device z = torch randn device=device Foo x y z test_python_wrapper f args = _generate device=GPU_TYPE compiled = torch compile f dynamic=True result code = run_and_get_cpp_code compiled args FileCheck check pool = empty_strided_ + GPU_TYPE + s s + align s s check_next buf = alloc_from_pool pool torch float s s s check buf = alloc_from_pool pool align s s run code assertTrue same f args result test_cpp_wrapper f args = _generate device=GPU_TYPE compiled = torch compile f dynamic=True config patch cpp_wrapper True result code = run_and_get_cpp_code compiled args FileCheck check aoti_torch__alloc_from_pool pool cached_torch_dtype_float int_array_ int_array_ tmp_tensor_handle_ check_next auto buf = RAIIAtenTensorHandle tmp_tensor_handle_ check auto buf = RAIIAtenTensorHandle tmp_tensor_handle_ run code assertTrue same f args result skipIfXpu msg= aoti doesn t work XPU test_aoti f args = _generate device=GPU_TYPE dim _x = Dim dim _x min= max= dynamic_shapes = dim _x None None result code = run_and_get_cpp_code lambda AOTIRunnerUtil run f args dynamic_shapes=dynamic_shapes FileCheck check int _t int_array_ = L + align L s check_next int _t int_array_ = L check_next AtenTensorHandle pool _handle check_next aoti_torch_empty_strided int_array_ int_array_ check_next RAIIAtenTensorHandle pool pool _handle check_next int _t int_array_ = s L check_next int _t int_array_ = L L check_next AtenTensorHandle tmp_tensor_handle_ check_next aoti_torch__alloc_from_pool pool run code assertTrue same f args result config patch triton autotune_at_compile_time False test_unbacked_symint when allocation s size has unbacked symints unbacked symints only available after computed device = GPU_TYPE raise unittest SkipTest requires GPU Repro torch nn Module forward x y x = x + u = x item torch _check u = s = y size expr = u s sevens = torch empty_strided size= expr stride= expr device=x device fill_ sevens example_inputs = torch scalar_tensor dtype=torch int device=self device torch ones device=self device model = Repro device result code = run_and_get_cpp_code lambda AOTIRunnerUtil run model example_inputs assertTrue same model example_inputs result check allocation done after unbacked symint computed FileCheck check auto u = u _raw check const int _t int_array_ = L L u L check AtenTensorHandle pool _handle check aoti_torch_empty_strided int_array_ int_array_ run code all AtenTensorHandle allocated using aoti_torch__alloc_from_pool wrapped RAIIAtenTensorHandle otherwise we ll have memory leak FileCheck check_count aoti_torch__alloc_from_pool pool exactly=True check_count aoti_torch__alloc_from_pool pool exactly=True run code FileCheck check AOTI_TORCH_ERROR_CODE_CHECK aoti_torch__alloc_from_pool pool cached_torch_dtype_int int_array_ int_array_ tmp_tensor_handle_ noqa B check RAIIAtenTensorHandle tmp_tensor_handle_ check AOTI_TORCH_ERROR_CODE_CHECK aoti_torch__alloc_from_pool pool cached_torch_dtype_float int_array_ int_array_ tmp_tensor_handle_ noqa B check RAIIAtenTensorHandle tmp_tensor_handle_ run code __name__ == __main__ HAS_GPU run_tests