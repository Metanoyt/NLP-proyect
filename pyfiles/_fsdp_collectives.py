math collections abc Callable Sequence itertools chain typing Any cast NamedTuple Optional Union torch torch distributed dist torch distributed device_mesh _get_device_handle torch distributed distributed_c d ReduceOp torch distributed fsdp _fully_shard _fsdp_api AllGather ReduceScatter torch distributed tensor DTensor _fsdp_api _ReduceOp _fsdp_common _get_dim _padded_size _raise_assert_with_print _to_dtype_if_needed compiled_autograd_enabled _fsdp_param FSDPParam ShardedState AllGatherResult NamedTuple all_gather_output torch Tensor all_gather_event Optional torch Event all_gather_work Optional dist distributed_c d Work For each parameter all-gather input dtype each input param_all_gather_input_dtypes list list torch dtype For each parameter all-gather input numel each input param_all_gather_input_numels list list int D flattened version ` param_all_gather_input_numels ` saved avoid CPU overhead recomputing all_gather_input_split_sizes list int lib = torch library Library fsdp FRAGMENT noqa TOR lib define all_gather_copy_in Tensor all_gather_inputs Tensor all_gather_output SymInt inp_split_sizes SymInt all_gather_input_numel SymInt rank - Tensor Tensor DefaultAllocMixin allocate size Sequence Union int torch SymInt dtype torch dtype device torch device - torch Tensor torch empty size dtype=dtype device=device ProcessGroupAllocMixin __init__ group dist ProcessGroup args Any kwargs Any _group = group super __init__ args kwargs allocate size Sequence Union int torch SymInt dtype torch dtype device torch device - torch Tensor backend = _group _get_backend device backend supports_tensor_alloc device size_ d = math prod int s s size backend allocate_tensor size_ d dtype=dtype device=device torch empty size dtype=dtype device=device DefaultAllGather DefaultAllocMixin AllGather __call__ output_tensor torch Tensor input_tensor torch Tensor group dist ProcessGroup async_op bool = False - Optional dist Work dist all_gather_into_tensor output_tensor input_tensor group=group async_op=async_op ProcessGroupAllocAllGather ProcessGroupAllocMixin AllGather __init__ group dist ProcessGroup - None super __init__ group __call__ output_tensor torch Tensor input_tensor torch Tensor group dist ProcessGroup async_op bool = False - Optional dist Work dist all_gather_into_tensor output_tensor input_tensor group=group async_op=async_op DefaultReduceScatter DefaultAllocMixin ReduceScatter __call__ output_tensor torch Tensor input_tensor torch Tensor group dist ProcessGroup op _ReduceOp async_op bool = False - dist Work dist reduce_scatter_tensor output=output_tensor input=input_tensor group=group op=op async_op=async_op ProcessGroupAllocReduceScatter ProcessGroupAllocMixin ReduceScatter __init__ group dist ProcessGroup - None super __init__ group __call__ output_tensor torch Tensor input_tensor torch Tensor group dist ProcessGroup op _ReduceOp async_op bool = False - dist Work dist reduce_scatter_tensor output=output_tensor input=input_tensor group=group op=op async_op=async_op torch library impl lib all_gather_copy_in Meta all_gather_copy_in_meta all_gather_inputs list torch Tensor all_gather_output torch Tensor inp_split_sizes list int all_gather_input_numel int rank int - tuple torch Tensor torch Tensor all_gather_input = all_gather_output narrow all_gather_input_numel rank all_gather_input_numel all_gather_input all_gather_output torch library impl lib all_gather_copy_in CUDA torch library impl lib all_gather_copy_in XPU torch library impl lib all_gather_copy_in HPU torch library impl lib all_gather_copy_in CPU torch library impl lib all_gather_copy_in MTIA torch library impl lib all_gather_copy_in PrivateUse all_gather_copy_in_cuda all_gather_inputs list torch Tensor all_gather_output torch Tensor inp_split_sizes list int all_gather_input_numel int rank int - tuple torch Tensor torch Tensor all_gather_input = all_gather_output narrow all_gather_input_numel rank all_gather_input_numel foreach_copy_dsts = torch split all_gather_input inp_split_sizes torch no_grad torch _foreach_copy_ foreach_copy_dsts all_gather_inputs all_gather_input all_gather_output lib define split_with_sizes_copy Tensor all_gather_output SymInt all_gather_input_split_sizes int dim= Tensor out - torch library impl lib split_with_sizes_copy Meta torch library impl lib split_with_sizes_copy CUDA torch library impl lib split_with_sizes_copy XPU torch library impl lib split_with_sizes_copy HPU torch library impl lib split_with_sizes_copy CPU torch library impl lib split_with_sizes_copy MTIA torch library impl lib split_with_sizes_copy PrivateUse split_with_sizes_copy all_gather_output torch Tensor all_gather_input_split_sizes list int dim int out list torch Tensor - None torch split_with_sizes_copy all_gather_output all_gather_input_split_sizes dim=dim out=out lib define chunk_cat Tensor tensors int dim int num_chunks Tensor out - torch library impl lib chunk_cat Meta torch library impl lib chunk_cat CUDA torch library impl lib chunk_cat XPU torch library impl lib chunk_cat HPU torch library impl lib chunk_cat CPU torch library impl lib chunk_cat MTIA torch library impl lib chunk_cat PrivateUse chunk_cat tensors list torch Tensor dim int num_chunks int out torch Tensor - None torch _chunk_cat tensors dim num_chunks out=out torch no_grad foreach_all_gather fsdp_params list FSDPParam group dist ProcessGroup async_op bool all_gather_copy_in_stream torch Stream all_gather_stream torch Stream device torch device all_gather_comm AllGather - Optional AllGatherResult world_size rank = group size group rank device_handle = _get_device_handle device type device_handle stream all_gather_copy_in_stream param_all_gather_inputs = _get_param_all_gather_inputs fsdp_params param_all_gather_input_dtypes param_all_gather_input_numels dtype = _get_all_gather_input_metadatas param_all_gather_inputs dtype == torch uint all_gather_inputs = t view torch uint ts param_all_gather_inputs t ts all_gather_inputs = chain from_iterable param_all_gather_inputs inp_split_sizes = t numel t all_gather_inputs all_gather_input_numel = sum inp_split_sizes all_gather_output = all_gather_comm allocate all_gather_input_numel world_size dtype=dtype device=device all_gather_input all_gather_output = torch ops fsdp all_gather_copy_in all_gather_inputs all_gather_output inp_split_sizes all_gather_input_numel rank del param_all_gather_inputs all_gather_stream wait_stream all_gather_copy_in_stream device_handle stream all_gather_stream all_gather_work = all_gather_comm output_tensor=all_gather_output input_tensor=all_gather_input group=group async_op=async_op all_gather_event = all_gather_stream record_event AllGatherResult all_gather_output all_gather_event all_gather_work param_all_gather_input_dtypes param_all_gather_input_numels inp_split_sizes torch no_grad _get_param_all_gather_inputs fsdp_params list FSDPParam - list list torch Tensor compiled_autograd_enabled fsdp_param all_gather_inputs fsdp_param fsdp_params Intentionally try run fast-path bypasses abstractions common FSDP case bf fp mixed precision order use foreach copy lower CPU overhead more efficient copying eager use_foreach_copy fsdp_param FSDPParam - bool fsdp_param param_dtype None fsdp_param offload_to_cpu hasattr fsdp_param _sharded_local_tensor fsdp_pre_all_gather param_all_gather_inputs list list torch Tensor = _ fsdp_params foreach_copy_indices list int = foreach_copy_inputs list torch Tensor = foreach_copy_input_numels list int = st pass foreach-copy parameters get inputs metadata foreach copy others actually get their all-gather inputs i fsdp_param enumerate fsdp_params use_foreach_copy fsdp_param foreach_copy_indices append i all_gather_input = fsdp_param _sharded_param_data fsdp_param sharded_state == ShardedState SHARDED cast torch Tensor fsdp_param _sharded_post_forward_param_data foreach_copy_inputs append all_gather_input foreach_copy_input_numels append all_gather_input numel param_all_gather_inputs i = fsdp_param all_gather_inputs nd pass use foreach copy compute remaining all-gather inputs foreach_copy_inputs fsdp_param_ = fsdp_params foreach_copy_indices param_dtype device = fsdp_param_ param_dtype fsdp_param_ device flat_foreach_copy_input = torch empty sum foreach_copy_input_numels device=device dtype=param_dtype splits = torch split flat_foreach_copy_input foreach_copy_input_numels torch _foreach_copy_ splits foreach_copy_inputs i split zip foreach_copy_indices splits param_all_gather_inputs i = split param_all_gather_inputs torch no_grad foreach_all_gather_copy_out all_gather_result AllGatherResult fsdp_params list FSDPParam group dist ProcessGroup - None all_gather_output all_gather_event all_gather_work param_all_gather_input_dtypes param_all_gather_input_numels all_gather_input_split_sizes = all_gather_result _dtype device = all_gather_output dtype all_gather_output device device_handle = _get_device_handle device type all_gather_event None sync op device_handle current_stream wait_event all_gather_event isinstance all_gather_work dist distributed_c d Work async op all_gather_work wait world_size device = group size all_gather_output device split_with_sizes_out list torch Tensor = shard_i_copy_infos list tuple FSDPParam list torch Tensor = all_gather_input_numels all_gather_input_dtypes fsdp_param zip param_all_gather_input_numels param_all_gather_input_dtypes fsdp_params NOTE Under compile make sure we always recreate all_gather_outputs per AllGather See Note Invariants torch compile Traceable FSDP force_recreate = compiled_autograd_enabled fsdp_param init_all_gather_outputs all_gather_input_numels all_gather_input_dtypes world_size device force_recreate=force_recreate force_recreate fsdp_param alloc_all_gather_outputs param_all_gather_outputs = fsdp_param all_gather_outputs fsdp_param fsdp_placement dim = Copy temporary then chunk-cat into final all-gather output tensors param_all_gather_outputs = torch empty_like t t param_all_gather_outputs shard_i_copy_infos append fsdp_param param_all_gather_outputs split_with_sizes_out extend param_all_gather_outputs all_gather_output = all_gather_output view world_size - all_gather_output dtype == torch uint out = t view world_size - view torch uint t split_with_sizes_out out = t view world_size - t split_with_sizes_out only avoid VC bump we inference mode torch _dynamo is_compiling For torch compile we turn off inference_mode fake tensor propagation therefore graph break is_inference For ` compile ` we don t care about VCs so just skip optimization non_inference_outs = non_inference_outs = o o out o is_inference len non_inference_outs torch autograd _unsafe_preserve_version_counter tuple non_inference_outs torch ops fsdp split_with_sizes_copy all_gather_output all_gather_input_split_sizes dim= out=out torch ops fsdp split_with_sizes_copy all_gather_output all_gather_input_split_sizes dim= out=out fsdp_param param_all_gather_outputs shard_i_copy_infos Chunk-cat temporary final all-gather output tensors shard_dim = fsdp_param fsdp_placement dim torch autograd _unsafe_preserve_version_counter tuple fsdp_param all_gather_outputs param_all_gather_output target_all_gather_output zip param_all_gather_outputs fsdp_param all_gather_outputs padded_sharded_size = fsdp_param padded_sharded_param_size fsdp_param sharded_state == ShardedState SHARDED cast torch Tensor fsdp_param _sharded_post_forward_param_data size pre_param_size = list padded_sharded_size pre_param_size = world_size chunks = torch chunk param_all_gather_output view pre_param_size world_size dim= post_param_size = list padded_sharded_size post_param_size shard_dim = world_size cat_out = target_all_gather_output view post_param_size torch cat chunks dim=shard_dim out=cat_out torch no_grad foreach_reduce fsdp_params list FSDPParam unsharded_grads list torch Tensor reduce_scatter_group dist ProcessGroup reduce_scatter_stream torch Stream reduce_scatter_comm ReduceScatter orig_dtype Optional torch dtype reduce_dtype Optional torch dtype device torch device gradient_divide_factor Optional float all_reduce_group Optional dist ProcessGroup ` None ` iff HSDP all_reduce_stream torch Stream all_reduce_grads bool partial_reduce_output Optional torch Tensor only used HSDP all_reduce_hook Optional Callable torch Tensor None force_sum_reduction_for_comms bool = False - tuple torch Tensor torch Event torch Event Optional torch Tensor Optional torch Event Optional torch Tensor ` ` unsharded_grads ` ` owns references gradients computed autograd so clearing list frees gradients grad_dtypes = grad dtype grad unsharded_grads len grad_dtypes = Check runtime since could real runtime error e g fp weights do produce correct higher precision gradients _raise_assert_with_print f FSDP reduce-scatter expects uniform gradient dtype got grad_dtypes grad_dtype = unsharded_grads dtype reduce_dtype = reduce_dtype grad_dtype predivide_factor postdivide_factor reduce_scatter_op all_reduce_op = _get_gradient_divide_factors reduce_scatter_group all_reduce_group reduce_dtype device type gradient_divide_factor force_sum_reduction_for_comms world_size = reduce_scatter_group size device_handle = _get_device_handle device type current_stream = device_handle current_stream world_size i fsdp_param unsharded_grad enumerate zip fsdp_params unsharded_grads shard_dim = fsdp_param fsdp_placement dim == continue unsharded_grad size shard_dim world_size = raise AssertionError f Shard shard_dim requires even sharding unsharded_grad size = world_size= chunks = torch chunk unsharded_grad world_size dim=shard_dim unsharded_grads i = torch cat chunks dim= padded_unsharded_sizes = tuple _get_dim _padded_size grad size world_size grad unsharded_grads reduce_scatter_input_numel = sum s numel s padded_unsharded_sizes reduce_scatter_output_numel = reduce_scatter_input_numel world_size reduce_scatter_input = reduce_scatter_comm allocate reduce_scatter_input_numel dtype=reduce_dtype device=device foreach_reduce_scatter_copy_in unsharded_grads reduce_scatter_input world_size Only after copy-in finishes can we free gradients unsharded_grads clear reduce_scatter_stream wait_stream current_stream all_reduce_input = None all_reduce_event = None device_handle stream reduce_scatter_stream reduce_output = reduce_scatter_comm allocate reduce_scatter_output_numel dtype=reduce_dtype device=device _div_if_needed reduce_scatter_input predivide_factor world_size reduce_scatter_comm output_tensor=reduce_output input_tensor=reduce_scatter_input group=reduce_scatter_group op=reduce_scatter_op For single GPU just copy input output no actual reduce-scatter needed reduce_output copy_ reduce_scatter_input reduce_scatter_event = reduce_scatter_stream record_event post_reduce_stream = reduce_scatter_stream all_reduce_group None HSDP Accumulations must run reduce-scatter stream all_reduce_grads partial_reduce_output None partial_reduce_output += reduce_output partial_reduce_output = reduce_output reduce_scatter_input reduce_scatter_event post_reduce_stream record_event all_reduce_input all_reduce_event partial_reduce_output partial_reduce_output None reduce_output += partial_reduce_output post_reduce_stream = all_reduce_stream world_size = all_reduce_stream wait_stream reduce_scatter_stream all_reduce_stream wait_stream current_stream device_handle stream all_reduce_stream dist all_reduce reduce_output group=all_reduce_group op=all_reduce_op all_reduce_input = reduce_output all_reduce_event = all_reduce_stream record_event -- END ops reduce_scatter stream all_reduce_hook None Execute user-specified all reduce hook If native HSDP used executed after HSDP all reduce If -d FSDP used executed post reduce-scatter post_reduce_stream = all_reduce_stream all_reduce_stream wait_stream reduce_scatter_stream device_handle stream all_reduce_stream all_reduce_hook reduce_output -- END ops post reduce_scatter device_handle stream post_reduce_stream _div_if_needed reduce_output postdivide_factor reduce_output = _to_dtype_if_needed reduce_output orig_dtype View out accumulate sharded gradients flat_grad_offset = reduce_scatter_output_numel - padded_unsharded_size fsdp_param zip padded_unsharded_sizes fsdp_params Assume even sharding Shard i i otherwise would require copy-out contiguous strides new_sharded_grad = torch as_strided reduce_output size=fsdp_param sharded_size stride=fsdp_param contiguous_sharded_stride storage_offset=flat_grad_offset to_accumulate_grad = fsdp_param sharded_param grad None fsdp_param offload_to_cpu Only overlap D H copy copying pinned memory accumulating gradients since CPU add kernel depends copy result we cannot run add callback non_blocking = fsdp_param pin_memory to_accumulate_grad Since GPU sharded gradient allocated RS stream we can free here keeping ref without waiting D H copy since future RS-stream ops run after copy new_sharded_grad = new_sharded_grad torch device cpu non_blocking=non_blocking non_blocking Record event which block CPU thread ensure D H copy finishes before optimizer fsdp_param grad_offload_event = post_reduce_stream record_event to_accumulate_grad isinstance fsdp_param sharded_param grad DTensor raise AssertionError f Expected fsdp_param sharded_param grad DTensor got type fsdp_param sharded_param grad fsdp_param sharded_param grad _local_tensor += new_sharded_grad new_sharded_dtensor_grad = fsdp_param to_sharded_dtensor new_sharded_grad fsdp_param sharded_param grad = new_sharded_dtensor_grad compiled_autograd_enabled hook getattr fsdp_param sharded_param _post_accumulate_grad_hooks values hook fsdp_param sharded_param padded_sharded_numel = padded_unsharded_size numel world_size flat_grad_offset += padded_sharded_numel post_reduce_event = post_reduce_stream record_event The RS output allocated RS stream used default stream optimizer To ensure its memory reused later RSs we do need extra synchronization since sharded parameters hold refs through end backward reduce_scatter_input reduce_scatter_event post_reduce_event all_reduce_input all_reduce_event None foreach_reduce_scatter_copy_in unsharded_grads list torch Tensor reduce_scatter_input torch Tensor world_size int - None reduce_scatter_input = reduce_scatter_input view world_size - torch ops fsdp chunk_cat unsharded_grads dim= num_chunks=world_size out=reduce_scatter_input _get_all_gather_input_metadatas param_all_gather_inputs list list torch Tensor - tuple list list torch dtype list list int torch dtype param_all_gather_input_dtypes list list torch dtype = param_all_gather_input_numels list list int = all_gather_dtype = param_all_gather_inputs dtype all_gather_inputs param_all_gather_inputs input_dtypes list torch dtype = input_numels list int = all_gather_input all_gather_inputs all_gather_input dtype = all_gather_dtype all_gather_dtype = torch uint input_dtypes append all_gather_input dtype input_numels append all_gather_input numel param_all_gather_input_dtypes append input_dtypes param_all_gather_input_numels append input_numels param_all_gather_input_dtypes param_all_gather_input_numels all_gather_dtype _get_gradient_divide_factors reduce_scatter_group dist ProcessGroup all_reduce_group Optional dist ProcessGroup reduce_dtype torch dtype device_type str = factor Optional float = None force_sum_reduction_for_comms bool = False - tuple Optional float Optional float Union dist ReduceOp dist ReduceOp RedOpType Union dist ReduceOp dist ReduceOp RedOpType MTIA appears only support SUM reduction hence we force implicitly device_type == mtia force_sum_reduction_for_comms = True For fp bf we do need worry about overflow underflow so we use NCCL s built-in division avoid separate div kernels overflow_risk = reduce_dtype torch float torch bfloat data_parallel_size = reduce_scatter_group size all_reduce_group None data_parallel_size = all_reduce_group size factor None factor = float data_parallel_size overflow_risk force_sum_reduction_for_comms factor == data_parallel_size Warning NCCL ReduceOp AVG may produce incorrect results world size data_parallel_size == None None ReduceOp SUM ReduceOp SUM None None ReduceOp AVG ReduceOp AVG reduce_scatter_op = torch distributed _make_nccl_premul_sum factor None None reduce_scatter_op ReduceOp SUM pre_factor Optional float overflow_risk Since fp has smaller dynamic range than fp bf we want avoid overflow underflow For N data parallel workers each worker computes g_i they collectively reduce g_ + + g_N N To avoid overflow underflow we divide ~sqrt N before after reduction pre_factor = while factor pre_factor == factor pre_factor pre_factor pre_factor = post_factor = factor pre_factor Prefer post-multiplying operates less data thus faster pre_factor post_factor = None factor pre_factor post_factor ReduceOp SUM ReduceOp SUM _div_if_needed tensor torch Tensor div_factor Optional float - None div_factor None div_factor = tensor div_ div_factor