__future__ annotations re dataclasses dataclass typing cast TYPE_CHECKING torchgen local torchgen api cpp torchgen api types BaseCType Binding NamedCType tensorListT torchgen model BaseTy BaseType FunctionSchema ListType NativeFunction NativeFunctionsViewGroup SchemaKind Type torchgen utils IDENT_REGEX TYPE_CHECKING collections abc Sequence Represents saved attribute involved backward calculation Note can derived property input argument e g we could save ` other scalar_type ` instead entire ` other ` tensor dataclass frozen=True SavedAttribute The NamedCType holds updated name cpp type attribute name Suffix appended s derived property e g ` other_scalar_type ` nctype NamedCType The expression read derived property save time e g ` other scalar_type ` expr str Represents backward formula calculates derivatives one more tensors dataclass frozen=True Derivative The formula string legit C++ expression Note expressions against input arguments have been replaced corresponding saved attributes E g raw formula ` mul_tensor_backward grad other scalar_type ` here ` mul_tensor_backward grad other_scalar_type ` formula str The formula string before input argument replacement original_formula str Names arguments which formula calculates derivatives var_names tuple str Saved inputs referenced formula saved_inputs tuple SavedAttribute Saved outputs referenced formula saved_outputs tuple SavedAttribute Gradients referenced name formula named_gradients set str Represents forward formula calculates forward derivatives one tensor dataclass frozen=True ForwardDerivative The formula string legit C++ expression Note special keywords such linear element_wise have been replaced automatically generated formula formula str Name output arguments which formula calculates forward derivatives var_names tuple str Type output arguments which formula calculates forward derivatives var_types tuple Type Inputs which forward derivatives required formula required_inputs_fw_grad tuple str &#124; None Inputs which primal required formula required_inputs_primal tuple str &#124; None Flag specify formula requires original value This only used inplace operations required_original_self_value bool If formula specified derivatives yaml we reusing out place formula inplace is_reusing_outplace_formula bool Represents differentiability info NativeFunction dataclass frozen=True DifferentiabilityInfo The base name read derivatives yaml name str The matching native function There can multiple NativeFunction having same base name - different overloads different types input arguments - in-place out functional variants same function We first use schema string under name key derivatives yaml find NativeFunction having same schema string Then we find in-place out functional variants matching function Among these variants we choose one having same name derivatives yaml entry If there no exact match then we choose in-place variant TODO maybe logic search all variants no longer necessary func NativeFunction The name generated autograd function It s set only we will calculate derivative i e args_with_derivatives empty op str &#124; None The derivatives formulae function Note length sequence number differentiable inputs derivatives Sequence Derivative The forward derivatives formulae function Note length sequence number differentiable outputs forward_derivatives Sequence ForwardDerivative The union saved_inputs all derivatives all_saved_inputs Sequence SavedAttribute The union saved_outputs all derivatives all_saved_outputs Sequence SavedAttribute All named gradients available use same order grads vector available_named_gradients Sequence str The named gradients used any derivatives Invariant all name available_named_gradients name used_named_gradients used_named_gradients set str The function s input arguments which calculates derivatives It s union var_names all derivatives sorted argument order function schema args_with_derivatives Sequence Binding Names arguments whose derivative formula non_differentiable non_differentiable_arg_names Sequence str Raw data read derivatives yaml output_differentiability list bool &#124; None output_differentiability derivatives yaml can list conditions express output differentiable In case number conditions must match number outputs NB we only support one condition right now output_differentiability gets populated True each condition while output_differentiability_conditions gets populated conditions output_differentiability_conditions list str &#124; None property has_derivatives - bool len args_with_derivatives Generates new DifferentiabilityInfo using exact same set derivative information new operator name This used when generating copy variants view ops which able use exact same derivative formula original view op See Note Codegen d view _copy Operators create_view_copy_from_view_derivative g NativeFunctionsViewGroup - DifferentiabilityInfo &#124; None g view_copy None None f = g view_copy name_split_by_period = name split maxsplit= Append _copy base name operator keep overload name same view_copy_name = f name_split_by_period _copy + join name_split_by_period view_copy_op_name = None op None f op _copy DifferentiabilityInfo Use _copy version name func op name=view_copy_name func=f op=view_copy_op_name But keep all derivative info same derivatives=self derivatives forward_derivatives=self forward_derivatives all_saved_inputs=self all_saved_inputs all_saved_outputs=self all_saved_outputs available_named_gradients=self available_named_gradients used_named_gradients=self used_named_gradients args_with_derivatives=self args_with_derivatives non_differentiable_arg_names=self non_differentiable_arg_names output_differentiability=self output_differentiability output_differentiability_conditions=self output_differentiability_conditions uses_ident info DifferentiabilityInfo &#124; None ident str - bool info None False derivative info derivatives formula = derivative formula re search IDENT_REGEX format ident formula True False uses_retain_variables info DifferentiabilityInfo &#124; None - bool uses_ident info retain_variables uses_single_grad info DifferentiabilityInfo &#124; None - bool uses_ident info grad Represents differentiable ` Argument ` How different ` Argument ` type - It s processed Arguments which differentiable only used context autograd codegen - It can represent SelfArgument regular Argument TensorOptionsArgument dataclass frozen=True DifferentiableInput name str type Type TODO only keep byte-for-byte compatible old codegen should remove cpp_type str Represents differentiable ` Return ` How different ` Return ` type - The name ` Return ` optional Here always populated using same ` cpp return_names ` method TODO some cpp naming logic e g resolving name conflict might irrelevant - It s processed Returns which differentiable compliance ` output_differentiability ` field defined derivatives yaml specified only used context autograd codegen dataclass frozen=True DifferentiableOutput name str type Type TODO only keep byte-for-byte compatible old codegen should remove cpp_type str dataclass frozen=True NativeFunctionWithDifferentiabilityInfo func NativeFunction info dict str DifferentiabilityInfo &#124; None fw_derivatives dict str Sequence ForwardDerivative &#124; None TODO Update comment below since out date dispatch_strategy fn NativeFunctionWithDifferentiabilityInfo - str How we going call underlying implementation declaration There two strategies - use_derived we want call implementation CPUDoubleType similar derived Type instance Because these derived instances deal Tensors Variables s completely different object so doesn t dispatch back VariableType code dispatch path needs wrap unwrap tensors If derived implementation takes returns tensors implementation usually differentiable although we also use derived dispatch path non-differentiable functions we still want dispatch derived Type instance e g size - use_type we want call implementation Type because implemented concretely functions invokes will get dispatched back VariableType which will ensure they differentiable fn derived long any its per-key differentiability infos has_derivatives dispatch_strategy used guard generation fns VariableType ADInplaceOrViewType We want generate these functions long derivative defined ANY dispatch key fn func is_abstract fn info None any info has_derivatives info fn info values If function abstract implemented Type we must call implementation derived type unpacked tensors If function has derivative specified concrete we could call either implementation We prefer calling derived type s implementation unpacked tensors because more performant some cases any internal calls other ATen functions won t have history tracked If function has type dispatched argument i e factory we prefer calling derived type s implementation both because more performant ensure factory functions tensors _version probably strictly necessary nice have keeps versions simple understand use_derived If function concrete we don t have override we didn t declare derivatives yaml we ll assume actually implemented out differentiable functions This assumption might hold then you ll see gradcheck fail use_type is_foreach_func f NativeFunction - bool f func name name base startswith _foreach_ note crcrpar Most foreach functions can reference out-place ` torch ` function whose schema kind functional their backward derivatives forward derivatives future i e they would find such one ` functional_info_by_signature ` There however some exceptions _foreach_with_inplace_ref = _foreach_zero_ _foreach_with_tensor_overload = _foreach_add Tensor _foreach_mul Tensor _foreach_div Tensor The following do support alpha kwarg which nonforeach versions support _skip_argument_len_check = _foreach_add Scalar _foreach_add_ Scalar _foreach_add ScalarList _foreach_add_ ScalarList _foreach_sub Scalar _foreach_sub_ Scalar _foreach_sub ScalarList _foreach_sub_ ScalarList Checks ` function_schema ` native non-foreach function which ` f ` foreach function reference generate derivatives is_reference_for_foreach f NativeFunction function_schema FunctionSchema - bool f func name name base split _foreach_ - == function_schema name name base function_schema name name inplace str f func name _foreach_with_inplace_ref str f func name _skip_argument_len_check len f func arguments flat_non_out == len function_schema arguments flat_non_out all ref_arg type arg type getattr arg type elem None arg ref_arg zip f func arguments flat_non_out function_schema arguments flat_non_out TODO crcrpar Avoid hard coding Default ideally gen_foreach_derivativeinfo foreach_function NativeFunction functional_info_by_signature dict FunctionSchema dict str DifferentiabilityInfo non_functional_info_by_signature dict FunctionSchema dict str DifferentiabilityInfo dispatch_key str = Default - tuple DifferentiabilityInfo &#124; None bool Generate DifferentiabilityInfo out-place foreach function existing one in-place The second value indicates whether info generated function ref_diff_info DifferentiabilityInfo &#124; None = None function_schema diff_info functional_info_by_signature items is_reference_for_foreach foreach_function function_schema continue ref_diff_info = diff_info dispatch_key ref_diff_info None break note crcrpar It seems like ` zero ` s info isn t available functional_info_by_signature while info ` zero_ ` non_functional_info_by_signature ref_diff_info None foreach_function func kind == SchemaKind inplace str foreach_function func name _foreach_with_inplace_ref function_schema diff_info non_functional_info_by_signature items is_reference_for_foreach foreach_function function_schema continue ref_diff_info = diff_info dispatch_key ref_diff_info None break ref_diff_info None None False non out-place uses existing Derivative foreach_function func kind == SchemaKind inplace ref_diff_info False map_refarg foreacharg map_name arg = i arg ref_arg enumerate zip foreach_function func arguments flat_non_out function_schema arguments flat_non_out map_refarg foreacharg ref_arg name = arg name map_name arg arg name = arg all_saved_inputs all_saved_outputs all_var_names = modified_derivative_formulas = i derivative enumerate ref_diff_info derivatives modified_formula = derivative formula replace grad grads i replace result result i saved_inputs saved_outputs = note crcrpar This context seems necessary call ` cpp argument_type ` local parametrize use_const_ref_for_mutable_tensors=foreach_function use_const_ref_for_mutable_tensors use_ilistref_for_tensor_lists=foreach_function part_of_structured_group ref_input derivative saved_inputs ref_input_jit_name = ref_input expr split mapped_name = map_refarg foreacharg ref_input_jit_name isinstance map_name arg mapped_name type ListType mapped_expr = mapped_name + i mapped_expr = mapped_name new_expr = ref_input expr replace ref_input_jit_name mapped_expr modified_formula = modified_formula replace cast str ref_input nctype name new_expr nctype = cpp argument_type map_name arg mapped_name binds=mapped_name canonical_nctype = NamedCType nctype name nctype type remove_const_ref saved_inputs append SavedAttribute nctype=canonical_nctype expr=mapped_name ref_output derivative saved_outputs ref_output nctype name == result saved_outputs append SavedAttribute nctype=NamedCType name= result type=BaseCType tensorListT expr= result raise RuntimeError var_names = map_refarg foreacharg var var derivative var_names all_var_names extend var_names all_saved_inputs extend saved_inputs all_saved_outputs extend saved_outputs modified_derivative = Derivative formula=modified_formula original_formula=derivative formula var_names=tuple var_names saved_inputs=tuple saved_inputs saved_outputs=tuple saved_outputs named_gradients=set modified_derivative_formulas append modified_derivative local parametrize use_const_ref_for_mutable_tensors=foreach_function use_const_ref_for_mutable_tensors use_ilistref_for_tensor_lists=foreach_function part_of_structured_group args_with_derivatives = Binding name=arg name nctype=cpp argument_type arg binds=arg name argument=arg default=None arg foreach_function func arguments flat_non_out arg name all_var_names forward_derivatives list ForwardDerivative = fw_derivative ForwardDerivative fw_derivative ref_diff_info forward_derivatives var_names list str = list fw_derivative var_names type ignore no-redef var_types list Type = list fw_derivative var_types required_inputs_fw_grad list str = required_inputs_primal list str = fw_derivative required_inputs_fw_grad None required_inputs_fw_grad = list fw_derivative required_inputs_fw_grad fw_derivative required_inputs_primal required_inputs_primal = list fw_derivative required_inputs_primal modified_formula = fw_derivative formula Foreach s result TensorList result modified_formula modified_formula = fw_derivative formula replace result result i foreach_arg ref_arg zip foreach_function func arguments flat_non_out ref_diff_info func func arguments flat_non_out Modify reference forward formula isinstance foreach_arg type ListType foreach_arg type is_tensor_like Assuming ScalarList modified_formula = modified_formula replace ref_arg name foreach_arg name + i foreach_arg type is_tensor_like Assuming TensorList Tensor assert isinstance foreach_arg type ListType f foreach_function func name foreach_arg type assert isinstance foreach_arg type ListType foreach_arg type == BaseType BaseTy Tensor str foreach_function func name _foreach_with_tensor_overload f foreach_function func name foreach_arg type suffix _p _t curr_expr = ref_arg name + suffix curr_expr modified_formula new_expr = foreach_arg name + suffix modified_formula = modified_formula replace curr_expr new_expr Assuming Scalar foreach_arg name = ref_arg name modified_formula = modified_formula replace ref_arg name foreach_arg name note crcrpar there should exist cooler way i name enumerate var_names name == ref_arg name var_names i = foreach_arg name var_types i = foreach_arg type i name enumerate required_inputs_fw_grad name == ref_arg name required_inputs_fw_grad i = foreach_arg name i name enumerate required_inputs_primal name == ref_arg name required_inputs_primal i = foreach_arg name forward_derivatives append ForwardDerivative formula=modified_formula var_names=tuple var_names var_types=tuple var_types required_inputs_fw_grad=tuple required_inputs_fw_grad required_inputs_primal=tuple required_inputs_primal required_original_self_value=fw_derivative required_original_self_value is_reusing_outplace_formula=fw_derivative is_reusing_outplace_formula DifferentiabilityInfo name=foreach_function func name name base func=foreach_function op=f Foreach ref_diff_info op foreach_function func name overload_name derivatives=modified_derivative_formulas forward_derivatives=forward_derivatives all_saved_inputs=tuple set all_saved_inputs all_saved_outputs=tuple set all_saved_outputs available_named_gradients= used_named_gradients=set args_with_derivatives=args_with_derivatives non_differentiable_arg_names= output_differentiability=None output_differentiability_conditions=None True match_differentiability_info native_functions list NativeFunction differentiability_infos dict FunctionSchema dict str DifferentiabilityInfo - list NativeFunctionWithDifferentiabilityInfo Sets derivative key declarations matching autograd function In-place functions will use out-of-place derivative definition there no in-place specific derivative functional_info_by_signature = schema signature strip_default=True info_dict schema info_dict differentiability_infos items schema kind == SchemaKind functional non_functional_info_by_signature = schema signature strip_default=True info_dict schema info_dict differentiability_infos items schema kind = SchemaKind functional find_info f NativeFunction - tuple dict str DifferentiabilityInfo &#124; None bool Don t bother matching info generated out= variants generated f tags f func kind == SchemaKind out None False Check exact match f func differentiability_infos differentiability_infos f func True If no exact match check out-of-place variant operator has match i e mul mul_ mul_out note crcrpar Check foreach because in-place foreach functions use backward defined existing native functions instead out-place counterparts f_sig = f func signature strip_default=True f_sig functional_info_by_signature is_foreach_func f functional_info_by_signature f_sig False Some operators have derivative explicitly defined mutable variant get code-generated out-of-place variant which does come derivative formula For generated out-of-place variant use mutable variant s formula exists generated f tags f_sig non_functional_info_by_signature info_dict = non_functional_info_by_signature f_sig See https github com pytorch pytorch pull files#r assert any any str input nctype name input info all_saved_inputs info info_dict values f \ Attempted convert derivative formula mutable operator used automatically its functional variant str f func currently supported we d need fix up formula codegen info_dict False Generate derivative information foreach functions none defined ` derivatives yaml ` is_foreach_func f assert f func differentiability_infos diff_info is_generated = gen_foreach_derivativeinfo f functional_info_by_signature non_functional_info_by_signature diff_info None None False TODO crcrpar Avoid hard coding Default ideally diff_info_dict = Default diff_info is_generated differentiability_infos f func = diff_info_dict functional_info_by_signature f func = diff_info_dict diff_info_dict is_generated None False result list NativeFunctionWithDifferentiabilityInfo = f native_functions info_dict is_exact_match = find_info f Currently strides strides_or_error replacement does support derivatives inplace function so we must check case f func kind == SchemaKind inplace info_dict None info info_dict values derivative info derivatives derivative var_names saved_input derivative saved_inputs assert strides_or_error saved_input expr Calling strides derivative formula f in-place function supported f func info_dict result append NativeFunctionWithDifferentiabilityInfo func=f info=None fw_derivatives=None continue fw_derivative_dict dict str Sequence ForwardDerivative = key info info_dict items info forward_derivatives fw_derivative_dict key = continue forward_derivatives = info forward_derivatives For functions have single out-of-place inplace like abs f func kind == SchemaKind inplace For inplace functions there little bit work do Validate formula make sure input modified used - If there formula inplace variant function is_exact_match == True then we make sure original value input being modified inplace self_p used formula Note formula can use original_self_p here would trigger clone original input - If we reusing out place formula is_exact_match == False then we replace every occurrence self_p self_t original_self_p original_self_t These will populated cloned version original input either clone done backward AD logic also used backward formula special clone we add At point there cannot self_p formula Change result into self_p design inplace function codegen result simply called modified inplace Update required primals data case used contain result should now contain If exact match user formula modifying existing forward grad inplace should So add some code makes sure we do so forward grad already exists assert len info forward_derivatives == Only single output inplace should exist fw_info = info forward_derivatives formula = fw_info formula replace_self_with_original_self formula str postfix str - str repl m re Match str - str f m group original_self postfix m group re sub IDENT_REGEX format f postfix repl formula re search IDENT_REGEX format self_p formula is_exact_match For manually defined formulas don t allow original value used raise RuntimeError f The formula f func name using original value being modified inplace This would lead wrong forward gradients Please use result formula only When original formula out place we save clone primal value able access value needed replace self_p self_t formula original_self_p original_self_t formula = replace_self_with_original_self formula _p formula = replace_self_with_original_self formula _t replace result formula self_p repl m re Match str - str f m group self_p m group formula = re sub IDENT_REGEX format result repl formula required_primals = fw_info required_inputs_primal re search IDENT_REGEX format self_p formula required_primals = required_primals + required_primals is_exact_match NOTE In-place forward AD formula Optimization This optimization transforms formula directly do inplace i e instead self_t copy_ self_t op we do self_t op_ when following met formula satisfies pattern self_t op args op needs same op derivative may seem too strict currently only ops satisfy also satisfy If there need we can relax allow any op has in-place variant is_single_method_on_self_t = False directly_do_inplace = False op_name str &#124; None = None between_parens str &#124; None = None match = re fullmatch r self_t \w \ \ formula match op_name between_parens = match group match group We want Match self_t op other_p op arg Avoid self_t op args + self_t op args Avoid self_t op other_p op arg + self_t op args check_parens_nest_level_gt_zero s str - bool level = ch s ch == level -= level == False ch == level += True is_single_method_on_self_t = check_parens_nest_level_gt_zero between_parens directly_do_inplace = is_single_method_on_self_t op_name == info name directly_do_inplace assert op_name None assert between_parens None formula = f self_t_raw defined self_t_raw op_name _ between_parens formula Make sure forward grad modified inplace when original formula out place formula = f self_t_raw defined self_t_raw copy_ formula formula required_original_self_value = bool re search IDENT_REGEX format original_self_p formula bool re search IDENT_REGEX format original_self_t formula forward_derivatives = ForwardDerivative formula=formula var_names= var_types=fw_info var_types required_inputs_fw_grad=fw_info required_inputs_fw_grad required_inputs_primal=required_primals required_original_self_value=required_original_self_value is_reusing_outplace_formula=not is_exact_match fw_derivative_dict key = forward_derivatives result append NativeFunctionWithDifferentiabilityInfo func=f info=info_dict fw_derivatives=fw_derivative_dict result is_differentiable name str type Type info DifferentiabilityInfo &#124; None - bool type is_tensor_like info None name info non_differentiable_arg_names gen_differentiable_outputs fn NativeFunctionWithDifferentiabilityInfo key str = Default - list DifferentiableOutput f = fn func info = fn info key fn info None outputs list DifferentiableOutput = DifferentiableOutput name=name type=ret type cpp_type=cpp return_type ret symint=True cpp_type name ret zip cpp return_names f f func returns output_differentiability = info output_differentiability info None output_differentiability None len output_differentiability = len outputs raise RuntimeError f The length output_differentiability len output_differentiability f does match number outputs len outputs differentiable_outputs list DifferentiableOutput = False output_differentiability f func kind == SchemaKind inplace raise RuntimeError output_differentiability=False inplace operation version_counter won t get updated differentiable output zip output_differentiability outputs differentiable differentiable_outputs append output differentiable_outputs candidate_differentiable_outputs = list filter lambda r is_differentiable r name r type info outputs uses_single_grad info candidate_differentiable_outputs candidate_differentiable_outputs