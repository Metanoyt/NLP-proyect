Owner s oncall distributed functools itertools os tempfile unittest collections abc Callable enum auto Enum typing Union torch torch nn nn torch nn functional F torch distributed fsdp _wrap_utils _validate_frozen_params torch distributed fsdp fully_sharded_data_parallel BackwardPrefetch CPUOffload FullyShardedDataParallel FSDP MixedPrecision ShardingStrategy torch distributed fsdp wrap _or_policy _Policy _wrap_module_cls_individually always_wrap_policy CustomPolicy enable_wrap ModuleWrapPolicy size_based_auto_wrap_policy transformer_auto_wrap_policy wrap torch nn TransformerDecoderLayer TransformerEncoderLayer torch nn modules batchnorm _BatchNorm torch testing _internal common_cuda TEST_MULTIGPU torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp _move_to_device DEVICEInitMode DummyProcessGroup FSDPInitMode FSDPTest TransformerWithSharedParams torch testing _internal common_utils FILE_SCHEMA find_free_port instantiate_parametrized_tests parametrize run_tests TEST_CUDA TEST_XPU TestCase device_type = acc type acc = torch accelerator current_accelerator cpu backend = torch distributed get_default_backend_for_device device_type BatchNormNet nn Module __init__ - None super __init__ lin = nn Linear bias=False bn = nn BatchNorm d bn = nn BatchNorm d bn = nn BatchNorm d sync_bn = nn SyncBatchNorm LoraModel nn Module This toy LoRA decoder model __init__ - None super __init__ embed_tokens = nn Embedding layers = nn ModuleList LoraDecoder _ range norm = nn LayerNorm embed_tokens weight requires_grad_ False norm weight requires_grad_ False norm bias requires_grad_ False LoraDecoder nn Module __init__ - None super __init__ attn = LoraAttention mlp = LoraMLP inp_layernorm = nn LayerNorm post_attn_layernorm = nn LayerNorm inp_layernorm weight requires_grad_ False inp_layernorm bias requires_grad_ False post_attn_layernorm weight requires_grad_ False post_attn_layernorm bias requires_grad_ False LoraAttention nn Module __init__ - None super __init__ q_proj = nn Linear bias=False lora_A = nn Linear bias=False lora_B = nn Linear bias=False k_proj = nn Linear bias=False v_proj = nn Linear bias=False o_proj = nn Linear bias=False q_proj weight requires_grad_ False k_proj weight requires_grad_ False v_proj weight requires_grad_ False o_proj weight requires_grad_ False LoraMLP nn Module __init__ - None super __init__ proj = nn Linear bias=False proj = nn Linear bias=False proj weight requires_grad_ False proj weight requires_grad_ False WrapMethod Enum FSDP_CTOR = auto FSDP_CTOR supported way forward keep WRAP_API case we miss any use cases fix them work FSDP_CTOR over time WRAP_API = auto TestFSDPWrap FSDPTest Tests main API wrapping FSDP which pass auto_wrap_policy into FSDP constructor setUp - None super setUp NestedSequentialModel staticmethod get_model device=True sequential = nn Sequential nn Linear nn Linear nn Sequential nn Linear nn Linear device sequential = sequential device=device_type sequential staticmethod verify_model_all_wrapped cls model cls assertTrue isinstance model FSDP cls assertTrue isinstance model module FSDP cls assertTrue isinstance model module FSDP cls assertTrue isinstance model module FSDP cls assertTrue isinstance model module module FSDP cls assertTrue isinstance model module module FSDP staticmethod verify_model cls model cls assertTrue isinstance model FSDP cls assertTrue isinstance model module nn Linear cls assertTrue isinstance model module nn Linear cls assertTrue isinstance model module FSDP following modules wrapped policy cls assertTrue isinstance model module module nn Linear cls assertTrue isinstance model module module nn Linear _get_linear fin fout nn Linear fin fout bias=False _get_already_wrapped_fsdp device_init_mode=DEVICEInitMode DEVICE_BEFORE nested=False - FSDP fn_self = MyModel nn Module __init__ nested super __init__ TODO test various init modes move_to_device = device_init_mode == DEVICEInitMode DEVICE_BEFORE nested=True FSDP module will nested one layer deep we should pick up nested lin = nn Sequential _move_to_device fn_self _get_linear move_to_device FSDP _move_to_device fn_self _get_linear move_to_device lin = FSDP _move_to_device fn_self _get_linear move_to_device lin = FSDP _move_to_device fn_self _get_linear move_to_device lin = FSDP _move_to_device fn_self _get_linear move_to_device forward input torch Tensor - torch Tensor lin lin lin input model = MyModel nested=nested model skip_if_lt_x_gpu parametrize nested True False parametrize device_init_mode DEVICEInitMode DEVICE_AFTER DEVICEInitMode DEVICE_BEFORE test_error_already_wrapped nested device_init_mode Test error raised we attempt wrap when submodules already FSDP wrapped_fsdp = _get_already_wrapped_fsdp nested=nested device_init_mode=device_init_mode device_init_mode == DEVICEInitMode DEVICE_AFTER wrapped_fsdp = wrapped_fsdp device=device_type wrapped_module_name = lin nested lin assertRaisesRegex ValueError FSDP auto wrapping requires modules already have FSDP f applied found wrapped_module_name FSDP wrapped_fsdp auto_wrap_policy=size_based_auto_wrap_policy skip_if_lt_x_gpu parametrize use_or_policy True False test_wrap_batchnorm_individually use_or_policy never_wrap_policy args kwargs False wrap_batchnorm_individually = functools partial _wrap_module_cls_individually module_classes= _BatchNorm policy = functools partial _or_policy policies= never_wrap_policy wrap_batchnorm_individually use_or_policy wrap_batchnorm_individually model = BatchNormNet fsdp = FSDP model auto_wrap_policy=policy Batchnorms should wrapped layer fsdp bn fsdp bn fsdp bn fsdp sync_bn assertTrue isinstance layer FSDP assertFalse isinstance fsdp lin FSDP skip_if_lt_x_gpu test_bn_always_wrapped_individually Ensures using _or_policy _wrap_module_cls_individually even other policy results module containing BN unit being wrapped contained BN unit will still individually wrapped MyModule nn Module __init__ - None super __init__ bn_container = BatchNormNet wrap_bn_container module recurse args kwargs recurse True isinstance module BatchNormNet wrap_batchnorm_individually = functools partial _wrap_module_cls_individually module_classes= _BatchNorm my_policy = functools partial _or_policy policies= wrap_bn_container wrap_batchnorm_individually mod = MyModule fsdp = FSDP mod auto_wrap_policy=my_policy Wrapping should FSDP FSDP BatchNormNet FSDP BN FSDP FSDP BatchNormNet BN latter inner BN individually wrapped bn fsdp bn_container bn fsdp bn_container bn fsdp bn_container bn fsdp bn_container sync_bn assertTrue isinstance bn FSDP we just wrapped BN container individual batchnorms wrapped mod = MyModule fsdp = FSDP mod auto_wrap_policy=wrap_bn_container assertTrue isinstance mod bn_container FSDP bn fsdp bn_container bn fsdp bn_container bn fsdp bn_container bn fsdp bn_container sync_bn assertFalse isinstance bn FSDP skip_if_lt_x_gpu parametrize cpu_offload CPUOffload offload_params=False CPUOffload offload_params=True parametrize backward_prefetch BackwardPrefetch BACKWARD_POST BackwardPrefetch BACKWARD_PRE parametrize forward_prefetch False True parametrize device_init_mode DEVICEInitMode DEVICE_AFTER DEVICEInitMode DEVICE_BEFORE test_main_wrap_api cpu_offload CPUOffload backward_prefetch BackwardPrefetch forward_prefetch bool device_init_mode DEVICEInitMode device_init_mode == DEVICEInitMode DEVICE_AFTER cpu_offload offload_params they don t work together expected move_to_device = device_init_mode == DEVICEInitMode DEVICE_BEFORE Nested nn Module __init__ - None super __init__ nested_lin = _move_to_device nn Linear bias=False move_to_device forward input nested_lin input MyModel nn Module __init__ - None super __init__ lin = _move_to_device nn Linear bias=False move_to_device lin = _move_to_device nn Linear bias=False move_to_device lin = _move_to_device nn Linear bias=False move_to_device lin = Nested forward input lin lin lin lin input model = MyModel wrapped_model = FSDP model auto_wrap_policy=functools partial size_based_auto_wrap_policy min_num_params= wrap all modules cpu_offload=cpu_offload backward_prefetch=backward_prefetch forward_prefetch=forward_prefetch device_init_mode == DEVICEInitMode DEVICE_AFTER wrapped_model = wrapped_model device=device_type modules_in_fsdp_graph_order = wrapped_model module lin wrapped_model module lin wrapped_model module lin wrapped_model module lin module nested_lin wrapped_model module lin wrapped_model module modules_in_fsdp_graph_order assertTrue isinstance module FSDP _check_cpu_offload module cpu_offload _check_backward_prefetch module backward_prefetch _check_forward_prefetch module forward_prefetch Run model few times sanity check optim = torch optim SGD wrapped_model parameters lr= e- momentum= inp = torch ones device=device_type _ range optim zero_grad loss = wrapped_model inp sum loss backward optim step skip_if_lt_x_gpu test_zero_argument ZeroArguModel nn Module __init__ - None super __init__ = torch tensor forward model = FSDP ZeroArguModel assertEqual model torch tensor TestAutoWrap TestCase setUp - None super setUp For all tests here we use fake group process_group = DummyProcessGroup rank= size= unittest skipIf TEST_MULTIGPU Requires least GPUs parametrize wrap_method WrapMethod FSDP_CTOR WrapMethod WRAP_API test_wrap wrap_method wrap_method == WrapMethod WRAP_API enable_wrap wrapper_cls=FSDP process_group=self process_group layer = wrap nn Linear assert wrap_method == WrapMethod FSDP_CTOR layer = FSDP nn Linear process_group=self process_group auto_wrap_policy=functools partial size_based_auto_wrap_policy min_num_params= assertTrue isinstance layer FSDP assertEqual layer rank process_group rank assertEqual layer world_size process_group size unittest skipIf TEST_MULTIGPU Requires least GPUs test_wrap_disabled_outside_context pg = process_group MyModel nn Module __init__ - None super __init__ lin = wrap nn Linear process_group=pg model = MyModel enable_wrap wrapper_cls=FSDP process_group=pg model = wrap model assertTrue isinstance model FSDP assertFalse isinstance model lin FSDP assertTrue isinstance model lin nn Linear unittest skipIf TEST_MULTIGPU Requires least GPUs test_wrap_override_defaults new_process_group = DummyProcessGroup rank= size= enable_wrap wrapper_cls=FSDP process_group=self process_group layer = wrap nn Linear process_group=new_process_group assertTrue isinstance layer FSDP assertTrue layer process_group new_process_group assertEqual layer rank assertEqual layer world_size unittest skipIf TEST_CUDA TEST_XPU Test Requires CUDA XPU test_always_wrap Test ensure ` always_wrap_policy ` passed into FSDP all submodules wrapped seq = TestFSDPWrap NestedSequentialModel get_model device=True model = FSDP seq process_group=self process_group auto_wrap_policy=always_wrap_policy TestFSDPWrap NestedSequentialModel verify_model_all_wrapped model unittest skipIf TEST_MULTIGPU Requires least GPUs test_transformer_auto_wrap_policy Tests ` ` transformer_auto_wrap_policy ` ` auto_wrap_policy = functools partial transformer_auto_wrap_policy transformer_layer_cls= TransformerEncoderLayer TransformerDecoderLayer _test_transformer_wrapping auto_wrap_policy unittest skipIf TEST_MULTIGPU Requires least GPUs test_module_wrap_policy Tests ` ` ModuleWrapPolicy ` ` auto_wrap_policy = ModuleWrapPolicy TransformerEncoderLayer TransformerDecoderLayer _test_transformer_wrapping auto_wrap_policy unittest skipIf TEST_MULTIGPU Requires least GPUs test_module_wrap_policy_callable Tests ` ` ModuleWrapPolicy ` ` ` ` Callable ` ` auto_wrap_policy = ModuleWrapPolicy TransformerEncoderLayer TransformerDecoderLayer callable_policy = functools partial _or_policy policies= auto_wrap_policy _test_transformer_wrapping callable_policy _test_transformer_wrapping auto_wrap_policy Union Callable _Policy fsdp_kwargs = auto_wrap_policy auto_wrap_policy fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE fsdp_kwargs modules = list fsdp_model modules encoder_layers = set fsdp_model module transformer encoder layers decoder_layers = set fsdp_model module transformer decoder layers module modules module fsdp_model module encoder_layers module decoder_layers assertTrue isinstance module FSDP assertFalse isinstance module FSDP unittest skipIf TEST_MULTIGPU Requires least GPUs test_custom_policy Tests ` ` CustomPolicy ` ` both lambda function uses uniform kwargs so only returns ` ` False ` ` ` ` True ` ` lambda function uses non-uniform kwargs so returns dict override root kwargs use_uniform_kwargs False True _test_custom_policy use_uniform_kwargs _test_custom_policy use_uniform_kwargs bool print f use_uniform_kwargs= use_uniform_kwargs model = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE use_uniform_kwargs lambda_fn module nn Module module model bn True isinstance module TransformerEncoderLayer TransformerDecoderLayer True False lambda_fn module nn Module module model bn sharding_strategy ShardingStrategy NO_SHARD isinstance module TransformerEncoderLayer True isinstance module TransformerDecoderLayer sharding_strategy ShardingStrategy SHARD_GRAD_OP backward_prefetch BackwardPrefetch BACKWARD_POST False policy = CustomPolicy lambda_fn Use size- dummy PG avoid clamping sharding strategy ` NO_SHARD ` size- PG process_group = DummyProcessGroup rank= size= fp _mp = MixedPrecision param_dtype=torch float fp _mp = MixedPrecision model = FSDP model process_group=process_group auto_wrap_policy=policy mixed_precision=fp _mp encoder_layers = set model module transformer encoder layers decoder_layers = set model module transformer decoder layers bn = model module bn bn_strategy = ShardingStrategy FULL_SHARD use_uniform_kwargs ShardingStrategy NO_SHARD bn_prefetch = BackwardPrefetch BACKWARD_PRE encoder_strategy = root_strategy = ShardingStrategy FULL_SHARD encoder_prefetch = root_prefetch = BackwardPrefetch BACKWARD_PRE decoder_strategy = ShardingStrategy FULL_SHARD use_uniform_kwargs ShardingStrategy SHARD_GRAD_OP decoder_prefetch = BackwardPrefetch BACKWARD_PRE use_uniform_kwargs BackwardPrefetch BACKWARD_POST module model modules module bn assertTrue isinstance module FSDP assertEqual module sharding_strategy bn_strategy assertEqual module backward_prefetch bn_prefetch We currently override batch norm modules use fp assertEqual module mixed_precision fp _mp module encoder_layers assertTrue isinstance module FSDP assertEqual module sharding_strategy encoder_strategy assertEqual module backward_prefetch encoder_prefetch assertEqual module mixed_precision fp _mp module decoder_layers assertTrue isinstance module FSDP assertEqual module sharding_strategy decoder_strategy assertEqual module backward_prefetch decoder_prefetch assertEqual module mixed_precision fp _mp module model assertTrue isinstance module FSDP assertEqual module sharding_strategy root_strategy assertEqual module backward_prefetch root_prefetch assertEqual module mixed_precision fp _mp assertFalse isinstance module FSDP unittest skipIf TEST_MULTIGPU Requires least GPUs test_auto_wrap_api Test ensure auto wrap we wrap child modules correctly based min_num_params ` ` nn Linear ` ` does exceed bucket size combined they do sequential = TestFSDPWrap NestedSequentialModel get_model device=False my_auto_wrap_policy = functools partial size_based_auto_wrap_policy min_num_params= model = FSDP sequential process_group=self process_group auto_wrap_policy=my_auto_wrap_policy TestFSDPWrap NestedSequentialModel verify_model model unittest skipIf TEST_MULTIGPU Requires least GPUs test_auto_wrap_preset_exclude_wrap Test ensure excluded modules wrapped regardless total param size greater than min_num_params size_based_auto_wrap_policy excludes wrapping nn ModuleList nn ModuleDict sequential = nn ModuleList nn Linear nn Linear my_auto_wrap_policy = functools partial size_based_auto_wrap_policy min_num_params= model = FSDP sequential process_group=self process_group auto_wrap_policy=my_auto_wrap_policy assertTrue isinstance model FSDP assertTrue isinstance model nn Linear assertTrue isinstance model nn Linear unittest skipIf TEST_MULTIGPU Requires least GPUs test_auto_wrap_preset_exclude_wrap_include_children Test ensure excluded modules wrapped children param size greater than min_num_params sequential = nn ModuleList nn Linear my_auto_wrap_policy = functools partial size_based_auto_wrap_policy min_num_params= model = FSDP sequential process_group=self process_group auto_wrap_policy=my_auto_wrap_policy assertTrue isinstance model FSDP assertTrue isinstance model FSDP unittest skipIf TEST_MULTIGPU Requires least GPUs test_auto_wrap_preset_force_leaf Test ensure force-leaf modules wrapped children wrapped The size_based_auto_wrap_policy forces leaf modules type nn MultiheadAttention wrapped sequential = nn Sequential nn Linear nn MultiheadAttention my_auto_wrap_policy = functools partial size_based_auto_wrap_policy min_num_params= model = FSDP sequential process_group=self process_group auto_wrap_policy=my_auto_wrap_policy assertTrue isinstance model module FSDP Assert children multihead attention wrapped assertTrue isinstance model module nn MultiheadAttention assertTrue isinstance model module out_proj nn Linear unittest skipIf TEST_MULTIGPU Requires least GPUs test_auto_wrap_preset_force_leaf_custom Test ensure force-leaf modules wrapped my_auto_wrap_policy = functools partial size_based_auto_wrap_policy min_num_params= force_leaf_modules=size_based_auto_wrap_policy FORCE_LEAF_MODULES union nn Linear sequential = nn Sequential nn Linear nn ModuleList nn Linear model = FSDP sequential process_group=self process_group auto_wrap_policy=my_auto_wrap_policy Model wrapped FSDP no inner modules wrapped assertTrue isinstance model FSDP assertTrue isinstance model module nn Linear assertTrue isinstance model module nn ModuleList unittest skipIf TEST_CUDA TEST_XPU Test Requires CUDA XPU parametrize device_init_mode DEVICEInitMode DEVICE_BEFORE DEVICEInitMode DEVICE_AFTER parametrize cpu_offload CPUOffload offload_params=False CPUOffload offload_params=True parametrize use_device_id True False test_auto_wrap_smoke_test device_init_mode cpu_offload use_device_id CPU offload CUDA after don t work together expected cpu_offload offload_params device_init_mode == DEVICEInitMode DEVICE_AFTER device = torch device device_type torch accelerator set_device_index device_id = torch device device_type torch accelerator current_device_index use_device_id None Random port case next test run quickly same port would cause conflict os environ MASTER_ADDR = localhost os environ MASTER_PORT = str find_free_port file_name = tempfile NamedTemporaryFile delete=False name torch distributed init_process_group backend=backend init_method=f FILE_SCHEMA _ file_name rank= world_size= NOTE We move model GPU after init FSDP simulate real use cases where full model cannot loaded onto GPU their shards can device_after_init = device_init_mode == DEVICEInitMode DEVICE_AFTER try sequential = TestFSDPWrap NestedSequentialModel get_model device= device_after_init my_auto_wrap_policy = functools partial size_based_auto_wrap_policy min_num_params= model = FSDP sequential cpu_offload=cpu_offload auto_wrap_policy=my_auto_wrap_policy device_id=device_id TestFSDPWrap NestedSequentialModel verify_model model device_after_init model = model device=device_type input = torch rand dtype=torch float device output = model input loss = F mse_loss input output loss backward finally torch distributed destroy_process_group try os remove file_name except FileNotFoundError pass unittest skipIf TEST_MULTIGPU Requires least GPUs parametrize wrap_method WrapMethod FSDP_CTOR WrapMethod WRAP_API test_always_wrap_with_ignored_modules wrap_method WrapMethod sequential = TestFSDPWrap NestedSequentialModel get_model device=False ignored_modules = sequential sequential fsdp_kwargs = process_group process_group auto_wrap_policy always_wrap_policy ignored_modules ignored_modules wrap_method == WrapMethod FSDP_CTOR model = FSDP sequential fsdp_kwargs wrap_method == WrapMethod WRAP_API enable_wrap wrapper_cls=FSDP fsdp_kwargs model = wrap sequential assert f Unsupported wrap method wrap_method All non-ignored modules should wrapped FSDP assertTrue isinstance model FSDP assertTrue isinstance model module FSDP assertTrue isinstance model module nn Linear assertTrue isinstance model module FSDP assertTrue isinstance model module module nn Linear assertTrue isinstance model module module FSDP unittest skipIf TEST_MULTIGPU Requires least GPUs parametrize wrap_method WrapMethod FSDP_CTOR WrapMethod WRAP_API test_auto_wrap_with_ignored_modules wrap_method WrapMethod sequential = TestFSDPWrap NestedSequentialModel get_model device=False ignored_modules = sequential sequential my_auto_wrap_policy = functools partial size_based_auto_wrap_policy min_num_params= fsdp_kwargs = process_group process_group auto_wrap_policy my_auto_wrap_policy ignored_modules ignored_modules wrap_method == WrapMethod FSDP_CTOR model = FSDP sequential fsdp_kwargs wrap_method == WrapMethod WRAP_API enable_wrap wrapper_cls=FSDP fsdp_kwargs model = wrap sequential assert f Unsupported wrap method wrap_method Since nd linear ` sequential ` ignored wrapping policy does exceed parameter threshold before inner sequential ` sequential ` anymore hence flattens ` sequential ` ` sequential ` into ` model ` leaves ` sequential ` ` sequential ` as-is since they ignored assertTrue isinstance model FSDP assertTrue isinstance model module nn Linear assertTrue isinstance model module nn Linear assertTrue isinstance model module nn Sequential assertTrue isinstance model module nn Linear assertTrue isinstance model module nn Linear unittest skipIf TEST_MULTIGPU Requires least GPUs test_frozen_params Tests mixing frozen non-frozen parameters FSDP instance raises ` ` use_orig_params=False ` ` warns ` ` True ` ` module_classes = LoraAttention LoraMLP LoraDecoder module_wrap_policy = ModuleWrapPolicy module_classes lambda_fn_uniform module nn Module isinstance module module_classes lambda_fn_nonuniform module nn Module isinstance module LoraAttention sharding_strategy ShardingStrategy SHARD_GRAD_OP isinstance module module_classes True False lambda_wrap_policy_uniform = CustomPolicy lambda_fn_uniform lambda_wrap_policy_nonuniform = CustomPolicy lambda_fn_nonuniform use_orig_params policy itertools product True False module_wrap_policy lambda_wrap_policy_uniform lambda_wrap_policy_nonuniform _test_frozen_params use_orig_params policy _test_frozen_params use_orig_params bool policy _Policy model = LoraModel device=device_type msg = layers attn has both parameters requires_grad=True False use_orig_params msg += We do recommend wrapping such modules ctx = assertWarnsRegex UserWarning msg msg += FSDP does support wrapping such modules when use_orig_params=False ctx = assertRaisesRegex ValueError msg ctx FSDP model process_group=self process_group auto_wrap_policy=policy use_orig_params=use_orig_params TestWrapUtils TestCase test_validate_frozen_params Tests method ` ` _validate_frozen_params ` ` use_orig_params True False _test_validate_frozen_params use_orig_params _test_validate_frozen_params use_orig_params bool model = LoraModel Wrap only LoRA modules modules_to_wrap = module module_name module model named_modules lora_A module_name lora_B module_name _validate_frozen_params model modules_to_wrap set use_orig_params Additionally wrap attention module model modules isinstance module LoraAttention modules_to_wrap add module _validate_frozen_params model modules_to_wrap set use_orig_params Additionally wrap decoders module model modules isinstance module LoraDecoder modules_to_wrap add module _validate_frozen_params model modules_to_wrap set use_orig_params Do wrap LoRA-A modules meaning mixed frozen non-frozen module_name module model named_modules lora_A module_name modules_to_wrap remove module regex = layers attn has both parameters requires_grad=True False use_orig_params Wrapping attention manages all parameters except those LoRA-B module which separately wrapped all nonfrozen lorab_numel = sum p numel p model layers attn lora_B parameters attn_frozen_param_numel = sum p numel p model layers attn parameters p requires_grad attn_nonfrozen_param_numel = sum p numel p model layers attn parameters p requires_grad - lorab_numel attn_total_param_numel = attn_frozen_param_numel + attn_nonfrozen_param_numel regex += We do recommend wrapping such modules since r gradient memory usage will higher than expected \ f attn_total_param_numel numel instead attn_nonfrozen_param_numel numel r before sharding via reduce-scatter\ regex += FSDP does support wrapping such modules when use_orig_params=False regex += If possible wrap frozen parameters FSDP separately \n regex += The following parameters have requires_grad=True \n r \ layers attn lora_A weight \ \n The following parameters have requires_grad=False \n r \ layers attn q_proj weight layers attn k_proj weight r layers attn v_proj weight layers attn o_proj weight \ use_orig_params ctx = assertWarnsRegex UserWarning regex ctx = assertRaisesRegex ValueError regex ctx _validate_frozen_params model modules_to_wrap set use_orig_params Now ignore those LoRA-A modules parameters ignored_params = set module_name module model named_modules lora_A module_name ignored_params update module parameters _validate_frozen_params model modules_to_wrap ignored_params use_orig_params instantiate_parametrized_tests TestFSDPWrap instantiate_parametrized_tests TestAutoWrap __name__ == __main__ run_tests