Owner s module dynamo collections re sys time io StringIO torch _dynamo test_case torch _dynamo testing torch _dynamo comptime comptime Because we don t support free variables comptime moment we have communicate via globals This also means these tests cannot run parallel single process you d ever want do FILE = None SELF = None ComptimeTests torch _dynamo test_case TestCase test_print_single global FILE FILE = StringIO cnt = torch _dynamo testing CompileCounter comptime_print e comptime _ ctx ctx print ctx get_local e file=FILE Employee = collections namedtuple Employee name id mylist list pass torch compile backend=cnt dynamic=True f x y = x comptime_print y comptime_print comptime_print y comptime_print y comptime_print foo y comptime_print range comptime_print Employee foo comptime_print mylist comptime_print collections defaultdict lambda None comptime_print set comptime_print b comptime_print x size y + f torch randn assertEqual cnt frame_count assertExpectedInline FILE getvalue strip \ FakeTensor size= s FakeTensor size= s FakeTensor size= s foo FakeTensor size= s range Employee name= foo id= UserDefinedListVariable mylist defaultdict NestedUserFunctionVariable set b s test_print_graph global FILE FILE = StringIO cnt = torch _dynamo testing CompileCounter torch compile backend=cnt f x y = x comptime _ ctx ctx print_graph verbose=False file=FILE Test compact notation doesn t error graph break you ll have visually inspect see printed comptime print_graph y + f torch randn assertEqual cnt frame_count assertExpectedInline FILE getvalue strip \ forward L_x_ torch Tensor l_x_ = L_x_ y = l_x_ l_x_ = y = None test_print_disas global FILE FILE = StringIO cnt = torch _dynamo testing CompileCounter torch compile backend=cnt f x y = x comptime _ ctx ctx print_disas file=FILE comptime print_disas y + munge_disas s noqa F re sub r ^ +\d+ + -- \+\d+ A-Za-z - _ + \ \ s flags=re MULTILINE f torch randn assertEqual cnt frame_count out = FILE getvalue Check instruction offset working assertIn -- out Check bytecode resembles what we expect assertIn STORE_FAST out sys version_info assertIn BINARY_MULTIPLY out assertIn BINARY_OP out test_print_value_stack global FILE FILE = StringIO cnt = torch _dynamo testing CompileCounter g x comptime _ ctx ctx print_value_stack file=FILE stacklevel= x torch compile backend=cnt f x y = x + g x y + comptime print_value_stack_and_return y f torch randn assertEqual cnt frame_count assertExpectedInline FILE getvalue \ - FakeTensor size= test_print_locals global FILE FILE = StringIO cnt = torch _dynamo testing CompileCounter torch compile backend=cnt f x y = x comptime _ ctx ctx print_locals file=FILE comptime print_locals y + f torch randn assertEqual cnt frame_count assertExpectedInline FILE getvalue \ x = FakeTensor size= y = FakeTensor size= Just make sure doesn t crash test_print_direct cnt = torch _dynamo testing CompileCounter torch compile backend=cnt f x z y = x lambda z comptime print z y + f torch randn torch randn test_sleep sleep_time = cnt = torch _dynamo testing CompileCounter torch compile backend=cnt f x z should_sleep should_sleep comptime sleep sleep_time y = x y + start = time time f torch randn torch randn False total_no_sleep = time time - start start = time time f torch randn torch randn True total_with_sleep = time time - start assertTrue total_with_sleep sleep_time Hopefully won t flaky assertTrue abs total_with_sleep - sleep_time - total_no_sleep Just make sure doesn t crash test_get_local_closure_variable global SELF SELF = cnt = torch _dynamo testing CompileCounter torch compile backend=cnt f x z = g comptime _ ctx r = ctx get_local z SELF assertEqual repr r comptime print z y = x g y + f torch randn test_print_bt global FILE FILE = StringIO cnt = torch _dynamo testing CompileCounter g x comptime _ ctx ctx print_bt file=FILE comptime print_bt x + torch compile backend=cnt f x y = x y = g y y + munge_filenames s noqa F re sub r File ^ + line \d+ File X line X s f torch randn assertEqual cnt frame_count bt = FILE getvalue assertIn y = g y bt test_print_guards global FILE FILE = StringIO cnt = torch _dynamo testing CompileCounter torch compile backend=cnt f x y = x comptime _ ctx ctx print_guards file=FILE comptime print_guards y + f torch randn assertEqual cnt frame_count assertExpectedInline re sub r \s+$ FILE getvalue rstrip flags=re MULTILINE \ local L x TENSOR_MATCH guard_types None code None obj_weakref None guarded_class None global AUTOGRAD_SAVED_TENSORS_HOOKS guard_types None code None obj_weakref None guarded_class None global GRAD_MODE guard_types None code None obj_weakref None guarded_class None global DETERMINISTIC_ALGORITHMS guard_types None code None obj_weakref None guarded_class None global TORCH_FUNCTION_STATE guard_types None code None obj_weakref None guarded_class None global DEFAULT_DEVICE guard_types None code None obj_weakref None guarded_class None shape_env SHAPE_ENV guard_types None code None obj_weakref None guarded_class None test_graph_break cnt = torch _dynamo testing CompileCounter torch compile backend=cnt f x y = x comptime _ ctx pass y + f torch randn assertEqual cnt frame_count cnt frame_count = torch compile backend=cnt g x y = x comptime _ ctx ctx graph_break y = y + comptime graph_break y g torch randn assertEqual cnt frame_count test_get_local global SELF FILE SELF = FILE = StringIO cnt = torch _dynamo testing CompileCounter torch compile backend=cnt f x y = x lit = noqa F comptime _ ctx y = ctx get_local y SELF assertEqual y as_fake size SELF assertEqual y size Trigger graph write TODO so useful right now there s no way make use output proxy maybe s useful inserting side-effectful operations into graph y as_proxy + ctx print_graph verbose=False file=FILE SELF assertIs y python_type torch Tensor lit = ctx get_local lit SELF assertEqual lit as_python_constant y + f torch randn assertEqual cnt frame_count assertExpectedInline FILE getvalue strip \ forward L_x_ torch Tensor l_x_ = L_x_ y = l_x_ l_x_ = None add = y + y = add = None __name__ == __main__ torch _dynamo test_case run_tests run_tests