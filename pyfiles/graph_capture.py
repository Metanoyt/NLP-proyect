mypy allow-untyped-defs This module dispatches graphs either forward-only joint compilation pathways taking into account AOTConfig collected ViewAndMutationMetadata contextlib dataclasses typing Any Optional torch torch utils _pytree pytree torch utils dlpack torch _dispatch python enable_python_dispatcher torch _dynamo utils detect_fake_mode lazy_format_graph_code torch _logging getArtifactLogger trace_structured torch _subclasses functional_tensor FunctionalTensorMode torch fx experimental proxy_tensor make_fx torchgen utils dataclass_repr config descriptors AOTInput BackwardTokenAOTInput functional_utils assert_functional_graph propagate_input_mutation_stacktraces graph_capture_wrappers aot_dispatch_subclass create_functionalized_fn create_joint fn_input_mutations_to_outputs fn_prepped_for_autograd handle_effect_tokens_fn schemas AOTConfig FxValue SubclassMeta TraceFn ViewAndMutationMeta utils call_and_expect_output_descs copy_fwd_metadata_to_bw_nodes fn_wrappers register_buffer_assignment_hook root_module_when_exporting_non_strict simple_wraps unlift_tokens aot_graphs_log = getArtifactLogger __name__ aot_graphs _create_graph f args list torch Tensor args_descs Optional list AOTInput = None keep compat old clients maybe we should split into two impls aot_config AOTConfig - torch fx GraphModule FunctionalTensorMode must enabled here See Note Accessing grad_fn FunctionalTensor out_descs = None args_descs None inner_f = f simple_wraps f inner_f args nonlocal out_descs assert out_descs None out out_descs = call_and_expect_output_descs f args out aot_config disable_functionalization ctx = contextlib nullcontext ctx = FunctionalTensorMode type ignore assignment pre_dispatch=aot_config pre_dispatch export=aot_config is_export Allow token discovery joint fn tracing tokens can used backward _allow_token_discovery=True enable_python_dispatcher ctx fx_g = make_fx inner_f decomposition_table=aot_config decompositions record_module_stack=True pre_dispatch=aot_config pre_dispatch args args_descs None flat_args_descs _ = pytree tree_flatten args_descs flat_out_descs _ = pytree tree_flatten out_descs Unfortunately flat_args_descs guaranteed match number actual arguments show up FX graph Specifically allow_token_discovery=True means we will silently add extra token arguments backwards graph Although there few ways detect what these tokens we going settle something dodgy simple implement match tangents_token placeholders specifically these only placeholders created token discovery NB there NO other code treats name load bearing so bit naughty I originally wanted detect tokens exactly same way they detected normal runtime honest normal runtime detection pretty strange seems backward tokens reliably end argument list precede RNG arguments I don t understand why case And unlift_tokens token arguments detected seeing they feed into effects call Dastardly Why didn t we just introduce new type i = j = n fx_g graph nodes n op == placeholder n name startswith tangents_token n meta desc = BackwardTokenAOTInput j j += assert i len flat_args_descs fn_wrappers inner_f n n fx_g graph nodes n op == placeholder flat_args_descs n meta desc = flat_args_descs i i += n op == output n meta desc = flat_out_descs fx_g TODO Refactor following code so detach persists item_memo _detach_and_copy_item_memo t detached_t = t detach hasattr t item_memo detached_t item_memo = t item_memo detached_t aot_dispatch_base_graph flat_fn TraceFn flat_args list FxValue flat_args_descs list AOTInput aot_config AOTConfig fw_metadata ViewAndMutationMeta - tuple torch fx GraphModule list FxValue list AOTInput Optional SubclassMeta aot_dispatch_base requires functionalization doesn t need handle many cases autograd case The cases aot_dispatch_base doesn t need handle include - outputs aliases graph intermediates - outputs aliases graph inputs While cases does need handle include - input mutations including when inputs aliases each other - input metadata mutations fn_to_trace = fn_input_mutations_to_outputs flat_fn flat_args_descs fw_metadata keep_data_input_mutations=aot_config keep_inference_input_mutations aot_config disable_functionalization updated_flat_args updated_flat_args_descs = flat_args flat_args_descs fn_to_trace updated_flat_args updated_flat_args_descs = create_functionalized_fn fn_to_trace flat_args flat_args_descs meta=fw_metadata aot_config=aot_config trace_joint=False TODO replace AOTDispatchSubclassWrapper once we refactor fn_input_mutations_to_outputs create_functionalized_fn into CompilerWrappers fn_to_trace updated_flat_args_subclasses_desugared updated_flat_args_subclasses_desugared_descs maybe_subclass_meta = aot_dispatch_subclass fn_to_trace updated_flat_args updated_flat_args_descs is_joint_structure=False meta=fw_metadata fw_only=flat_fn aot_config disable_functionalization fn_to_trace updated_flat_args_subclasses_desugared updated_flat_args_subclasses_desugared_descs = handle_effect_tokens_fn fn_to_trace updated_flat_args_subclasses_desugared updated_flat_args_subclasses_desugared_descs meta=fw_metadata trace_joint=False aot_graphs_log debug aot_config id s fw_metadata= s subclass_metadata= s str aot_config aot_id str fw_metadata str maybe_subclass_meta We track buffer assignments when exporting non-strict mode In contrast strict mode errors any attribute assignment mod_when_exporting_non_strict = root_module_when_exporting_non_strict flat_fn aot_config is_export mod_when_exporting_non_strict None For any buffer assigned we want associate final proxy node assigned This node can then added buffer mutation output assigned_buffers dict str str = hook = register_buffer_assignment_hook mod_when_exporting_non_strict assigned_buffers fake_mode = detect_fake_mode fake_mode saved_updated_flat_args_subclasses_desugared = pytree tree_map_only torch Tensor _detach_and_copy_item_memo updated_flat_args_subclasses_desugared saved_updated_flat_args_subclasses_desugared = pytree tree_map_only torch Tensor lambda t t detach updated_flat_args_subclasses_desugared saved_updated_flat_args_subclasses_desugared_descs = updated_flat_args_subclasses_desugared_descs fw_module = _create_graph fn_to_trace updated_flat_args_subclasses_desugared updated_flat_args_subclasses_desugared_descs aot_config=aot_config aot_config is_export mod_when_exporting_non_strict None We update metadata consider any assigned buffers buffer mutations i = len dict mod_when_exporting_non_strict named_parameters name _ mod_when_exporting_non_strict named_buffers name assigned_buffers fw_metadata input_info i mutates_data type ignore possibly-undefined fw_metadata input_info i = dataclasses replace fw_metadata input_info i mutates_data=True fw_metadata num_mutated_inp_runtime_indices += i += We add nodes corresponding buffer assignments output nodes graph add_nodes = output_node = list fw_module graph nodes - name assigned_buffers values type ignore possibly-undefined node fw_module graph nodes node name == name add_nodes append node node users output_node = None output_node args = add_nodes output_node args hook remove type ignore possibly-undefined As long we opted remove input mutations then there should NO mutating ops graph point aot_config disable_functionalization copy_count = assert_functional_graph fw_module graph fw_module graph eliminate_dead_code fw_module recompile copy_count = assert_functional_graph fw_module graph propagate_input_mutation_stacktraces fw_module graph assert copy_count == copy_count fw_module graph eliminate_dead_code See Note Side-Effectful Tokens AOTAutograd num_tokens = len fw_metadata tokens num_tokens = config unlift_effect_tokens unlift_tokens fw_module fw_metadata aot_config saved_updated_flat_args_subclasses_desugared = saved_updated_flat_args_subclasses_desugared num_tokens saved_updated_flat_args_subclasses_desugared_descs = saved_updated_flat_args_subclasses_desugared_descs num_tokens aot_config enable_log aot_graphs_log info s lazy_format_graph_code Forward graph fw_module aot_config aot_id include_stride=True include_device=True colored=True trace_structured artifact metadata_fn=lambda name aot_forward_graph_fw_metadata encoding string payload_fn=lambda dataclass_repr fw_metadata maybe_subclass_meta None trace_structured artifact metadata_fn=lambda name aot_forward_graph_fw_subclass_metadata encoding string payload_fn=lambda dataclass_repr maybe_subclass_meta trace_structured aot_inference_graph payload_fn=lambda fw_module print_readable print_output=False include_stride=True include_device=True expanded_def=True TODO should factor into separate function export always only returns just graph aot_config is_export assert maybe_subclass_meta None aot_export_module does support tensor subclass inputs now fw_module saved_updated_flat_args_subclasses_desugared saved_updated_flat_args_subclasses_desugared_descs maybe_subclass_meta Has precondition there no duplicate arguments flat_args e g same Tensor object never shows up twice However two tensor inputs MAY alias same storage so long they have separate TensorImpls aot_dispatch_autograd_graph flat_fn TraceFn flat_args list Any flat_args_descs list AOTInput aot_config AOTConfig fw_metadata ViewAndMutationMeta - tuple torch fx GraphModule tuple list Any list Any tuple list AOTInput list AOTInput Optional SubclassMeta NB flat_fn here original user function far aot_module_simplified concerned traced_tangents corresponds set outputs traced forward should get grad_outputs traced backward It includes outputs original forward any updated inputs due input mutations However does include any outputs aliases inputs intermediates any metadata-only input mutations joint_inputs = flat_args fw_metadata traced_tangents joint_inputs_descs = flat_args_descs fw_metadata traced_tangents_descs fn_prepared_for_autograd = fn_prepped_for_autograd flat_fn flat_args_descs fw_metadata aot_config joint_fn_to_trace = create_joint fn_prepared_for_autograd flat_args_descs aot_config=aot_config joint_fn_handle = joint_fn_to_trace handle aot_config disable_functionalization updated_joint_inputs updated_joint_inputs_descs = joint_inputs joint_inputs_descs joint_fn_to_trace updated_joint_inputs updated_joint_inputs_descs = create_functionalized_fn joint_fn_to_trace joint_inputs joint_inputs_descs meta=fw_metadata aot_config=aot_config trace_joint=True joint_fn_handle=joint_fn_handle TODO replace AOTDispatchSubclassWrapper once we refactor fn_input_mutations_to_outputs create_functionalized_fn into CompilerWrappers subclass_tracing_info = aot_dispatch_subclass joint_fn_to_trace updated_joint_inputs updated_joint_inputs_descs is_joint_structure=True meta=fw_metadata fw_only=flat_fn joint_fn_to_trace = subclass_tracing_info plain_tensor_trace_fn updated_joint_inputs = subclass_tracing_info plain_tensor_args updated_joint_inputs_descs = subclass_tracing_info plain_tensor_args_descs aot_config disable_functionalization joint_fn_to_trace updated_joint_inputs updated_joint_inputs_descs = handle_effect_tokens_fn joint_fn_to_trace updated_joint_inputs updated_joint_inputs_descs meta=fw_metadata trace_joint=True When we call _create_graph may mutate metadata joint inputs But callers expecting get original joint inputs So we make aliases all inputs make sure we have copy doesn t get modified This destroys requires_grad grad_fn information However backends beneath AOTAutograd indifferent information so doesn t matter fake_mode = detect_fake_mode fake_mode saved_updated_joint_inputs = pytree tree_map_only torch Tensor _detach_and_copy_item_memo updated_joint_inputs saved_updated_joint_inputs = pytree tree_map_only torch Tensor lambda t t detach updated_joint_inputs maybe_subclass_meta = subclass_tracing_info maybe_subclass_meta fx_g = _create_graph joint_fn_to_trace updated_joint_inputs updated_joint_inputs_descs aot_config=aot_config Redundant check above worth having case tracing introduced fake tensor Unlikely See Note Fake Modules AOTAutograd torch _dynamo utils assert_no_fake_params_or_buffers fx_g Have copy before eliminate_dead_code otherwise fw node match might erased copy_fwd_metadata_to_bw_nodes fx_g fx_g graph eliminate_dead_code aot_config disable_functionalization There should NO mutating ops graph point assert_functional_graph fx_g graph fx_g recompile TODO AOTAutograd we create metadata like _indices_of_inps_to_detach detect when we need manually detach some inputs forward Higher order ops might eventually need do same aot_config is_export assert maybe_subclass_meta None aot_export_module does support tensor subclass inputs now fx_g saved_updated_joint_inputs updated_joint_inputs_descs maybe_subclass_meta