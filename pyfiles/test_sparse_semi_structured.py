Owner s module sparse ruff noqa F itertools random unittest torch torch nn torch nn functional F torch sparse SparseSemiStructuredTensor SparseSemiStructuredTensorCUSPARSELT SparseSemiStructuredTensorCUTLASS to_sparse_semi_structured torch sparse _semi_structured_conversions sparse_semi_structured_from_dense_cutlass _sparse_semi_structured_tile _compute_compressed_swizzled_bitmask torch testing make_tensor torch testing _internal common_cuda _get_torch_cuda_version PLATFORM_SUPPORTS_FP xfailIfSM torch testing _internal common_device_type dtypes instantiate_device_type_tests torch testing _internal common_dtype all_types_and_complex torch _dynamo test_case torch testing _internal common_utils parametrize run_tests subtest TestCase TEST_WITH_ROCM IS_WINDOWS torch testing _internal inductor_utils HAS_GPU pytest SEMI_STRUCTURED_SUPPORTED_BACKENDS = dict _IS_SM X = False _IS_SM X = False _IS_HIPSPARSELT_AVAILABLE = False torch cuda is_available _IS_SM X = torch version cuda None torch cuda get_device_capability == _IS_SM X = torch version cuda None torch cuda get_device_capability == _IS_HIPSPARSELT_AVAILABLE = torch version hip None tuple int v v torch version hip split CUTLASS kernels only work Ampere _IS_SM X SEMI_STRUCTURED_SUPPORTED_BACKENDS cutlass = SparseSemiStructuredTensorCUTLASS add cuSPASRELt tests available torch backends cusparselt is_available _IS_SM X _IS_SM X _IS_HIPSPARSELT_AVAILABLE SEMI_STRUCTURED_SUPPORTED_BACKENDS cusparselt = SparseSemiStructuredTensorCUSPARSELT inference_dtypes = dtypes torch float torch bfloat torch int training_dtypes = dtypes torch float torch bfloat parametrize_backends = parametrize backend SEMI_STRUCTURED_SUPPORTED_BACKENDS atol_rtol_kw = torch float rtol e- atol e- torch bfloat rtol e- atol e- sparse _largest_mask_ d original sparse = SparseSemiStructuredTensorCUTLASS prune_dense_static_sort original sparse to_dense bool sparsify _dense original sparse _largest_mask_ d original original rand_sparse_semi_structured_mask r c dtype=torch float device= cuda choice=None This function returns sparse matrix size r c Note means matrix will also sparse well choices = mask_entries = choice random choice choices i range r c torch tensor mask_entries dtype=dtype device=device reshape r c contiguous rand_sparse_semi_structured r c dtype device choice=None pattern = dtype = torch float pattern == ksparse = choices = pattern == ksparse = choices = mask_entries = choice random choice choices i range r c ksparse mask = torch tensor mask_entries dtype=torch bool view r c device dense = make_tensor r c dtype=dtype device=device dense dense == = To prevent zeros except where mask applied dense = dense masked_fill ~mask dense rand_sparse_semi_structured_all_patterns r c dtype device pattern = dtype = torch float pattern == ksparse = choices = pattern == ksparse = choices = mask_rows = random randint len choices - i range r c ksparse COL_INV COL_VAL = mask_entries_inv = choices i COL_INV i mask_rows mask_entries_val = choices i COL_VAL i mask_rows mask_inv = torch tensor mask_entries_inv dtype=torch bool view r c device mask_val = torch tensor mask_entries_val dtype=torch bool view r c device dense = make_tensor r c dtype=dtype device=device dense dense == = To prevent zeros except where mask below applied dense_inv = dense masked_fill ~mask_inv dense_val = dense_inv masked_fill ~mask_val dense_inv dense_val SparseSemiStructuredTensorCompileTest torch _dynamo test_case TestCase setUp len SEMI_STRUCTURED_SUPPORTED_BACKENDS == skipTest semi-structured sparsity has no available backend super setUp tearDown super tearDown staticmethod _test_mlp_contiguous_relu_compile backend dense_input_shape Test nn Linear + contiguous + nn ReLU SparseSemiStructuredTensor + torch compile We expect The sparse tensor subclass should turn nn Linear into ` aten _structured_sparse_addmm ` + ` aten contiguous ` Inductor should fuse contiguous call into relu Model nn Module __init__ - None super __init__ linear = nn Linear forward x x = linear x x = x contiguous torch nn functional relu x input = torch rand dense_input_shape device= cuda half model = Model eval cuda half mod_linear = model linear m n = mod_linear weight shape mask = torch Tensor tile m n bool cuda set masked weight mod_linear weight = nn Parameter mod_linear weight mask dense_result = model input mod_linear weight = nn Parameter SEMI_STRUCTURED_SUPPORTED_BACKENDS backend from_dense mod_linear weight sparse_result = model input model = torch compile model backend= inductor fullgraph=True sparse_compile_result = model input test sparse_compile_result dense_result numerically close torch testing assert_close dense_result sparse_compile_result rtol= e- atol= e- assert sparse sparse_compile have same strides meta registrations may contiguous tensors when output transposed https github com pytorch pytorch pull assert sparse_result stride == sparse_compile_result stride unittest skipIf IS_WINDOWS torch compile supported windows unittest skipIf cusparselt SEMI_STRUCTURED_SUPPORTED_BACKENDS cusparselt supported machine unittest skipIf TEST_WITH_ROCM Not supported ROCm test_mlp_contiguous_relu_compile_cusparselt test cuSPASRELt meta registrations _cslt_sparse_mm + torch compile dense_input_shape SparseSemiStructuredTensorCompileTest _test_mlp_contiguous_relu_compile cusparselt dense_input_shape unittest skipIf cutlass SEMI_STRUCTURED_SUPPORTED_BACKENDS cutlass supported machine unittest skipIf IS_WINDOWS torch compile supported windows unittest skipIf TEST_WITH_ROCM Not supported ROCm test_mlp_contiguous_relu_compile_cutlass test CUTLASS meta registrations _sparse_semi_structured_addmm + torch compile dense_input_shape SparseSemiStructuredTensorCompileTest _test_mlp_contiguous_relu_compile cutlass dense_input_shape unittest skipIf IS_WINDOWS torch compile supported windows unittest skipIf cusparselt SEMI_STRUCTURED_SUPPORTED_BACKENDS cusparselt supported machine unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_sp _compile - None x = torch randn device= cuda dtype=torch float requires_grad=True fn x y = SparseSemiStructuredTensorCUSPARSELT prune_dense_static_sort x y = y t x y Eager output = fn x output backward output Torch compile output = torch compile fn x output backward output TestSparseSemiStructured TestCase setUp len SEMI_STRUCTURED_SUPPORTED_BACKENDS == skipTest semi-structured sparsity has no available backend IS_WINDOWS skipTest torch compile supported windows inference_dtypes parametrize_backends test_to_sparse_semi_structured dtype backend SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass A = rand_sparse_semi_structured_mask dtype=dtype A_sparse = to_sparse_semi_structured A assert A shape == A_sparse shape assert A device == A_sparse device assert A dtype == A_sparse dtype assert isinstance A torch Tensor assert isinstance A_sparse SparseSemiStructuredTensor inference_dtypes parametrize_backends parametrize dense_input_shape test_mm_sparse_first_NN dense_input_shape dtype device backend Ensure torch mm A_sparse B correct float will throw error int SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass A = rand_sparse_semi_structured_mask dtype=dtype A_sparse = to_sparse_semi_structured A B = torch rand dense_input_shape device=A_sparse device dtype Currently we don t support int matmul GPU so evaluate CPU copy over dtype torch int backend == cutlass assertRaisesRegex RuntimeError spgemm_cutlass_dispatch_layouts sparse_result = torch mm A_sparse B assertRaisesRegex RuntimeError CUDA error operation supported when calling ` cusparseLtMatmulDescriptorInit sparse_result = torch mm A_sparse B dense_result = torch mm A B sparse_result = torch mm A_sparse B torch testing assert_close dense_result sparse_result rtol= e- atol= e- inference_dtypes parametrize_backends parametrize dense_input_shape test_mm_sparse_first_NT dense_input_shape dtype device backend Ensure torch mm A_sparse B t correct float bfloat will throw error int + padding SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass A = rand_sparse_semi_structured_mask dtype=dtype A_sparse = to_sparse_semi_structured A B = torch rand dense_input_shape device=A_sparse device dtype Currently we don t support int matmul GPU so evaluate CPU copy over dtype torch int dense_input_shape padding int throws error because transposing B yields contiguous output row-row sparse dense NN supported cuSPARSELt CUTLASS backend == cutlass assertRaisesRegex RuntimeError spgemm_cutlass_dispatch_layouts sparse_result = torch mm A_sparse B t assertRaisesRegex RuntimeError CUDA error operation supported when calling ` cusparseLtMatmulDescriptorInit sparse_result = torch mm A_sparse B t dtype torch int test transpose dense_result = torch mm A cpu B t cpu device dtype=torch int sparse_result = torch mm A_sparse B t torch testing assert_close dense_result sparse_result rtol= e- atol= e- test transpose dense_result = torch mm A B t sparse_result = torch mm A_sparse B t torch testing assert_close dense_result sparse_result rtol= e- atol= e- inference_dtypes parametrize dense_input_shape parametrize_backends test_mm_sparse_first_TN dtype dense_input_shape device backend Ensure torch mm A_sparse t B throws error SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass backend == cutlass IS_WINDOWS skipTest CUTLASS supported Windows A = rand_sparse_semi_structured_mask dtype=dtype A_sparse = to_sparse_semi_structured A B = torch rand dense_input_shape device=A_sparse device dtype assertRaisesRegex NotImplementedError r ` SparseSemiStructuredTensor ` matmul operation supported torch mm A_sparse t B inference_dtypes parametrize dense_input_shape parametrize_backends test_mm_sparse_second_NT dense_input_shape dtype device backend Ensure torch mm A B_sparse t correct SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass backend == cutlass IS_WINDOWS skipTest CUTLASS supported Windows B = rand_sparse_semi_structured_mask dtype=dtype B_sparse = to_sparse_semi_structured B A = torch rand dense_input_shape device=B_sparse device dtype Currently we don t support int matmul GPU so evaluate CPU copy over dtype torch int dense_result = torch mm A cpu B t cpu device dtype=torch int sparse_result = torch mm A B_sparse t dense_result = torch mm A B t sparse_result = torch mm A B_sparse t torch testing assert_close dense_result sparse_result rtol= e- atol= e- inference_dtypes parametrize dense_input_shape parametrize_backends test_mm_sparse_second_NN dense_input_shape dtype device backend Ensure torch mm A B_sparse throws error SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass backend == cutlass IS_WINDOWS skipTest CUTLASS supported Windows B = rand_sparse_semi_structured_mask dtype=dtype B_sparse = to_sparse_semi_structured B A = torch rand dense_input_shape device=B_sparse device dtype assertRaisesRegex NotImplementedError r ` SparseSemiStructuredTensor ` matmul operation supported sparse_result = torch mm A B_sparse parametrize dense_input_shape parametrize inference_mode subtest True subtest False parametrize_backends test_linear dense_input_shape inference_mode device backend Test nn Linear has same numerics SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass backend == cutlass IS_WINDOWS skipTest CUTLASS supported Windows input = torch rand dense_input_shape device=device half model = nn Linear device half m n = model weight shape mask = rand_sparse_semi_structured_mask m n device=device dtype=torch bool set masked weight model weight = nn Parameter model weight mask dense_result = model input model weight = nn Parameter to_sparse_semi_structured model weight inference_mode torch inference_mode sparse_result = model input sparse_result = model input torch testing assert_close dense_result sparse_result rtol= e- atol= e- parametrize dense_input_shape parametrize_backends test_mlp device dense_input_shape backend SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass input = torch rand dense_input_shape device=device half model = nn Sequential nn Linear nn Linear half device i range m n = model i weight shape mask = rand_sparse_semi_structured_mask m n device=device dtype=torch bool set masked weight model i weight = nn Parameter model i weight mask dense_result = model input i range model i weight = nn Parameter to_sparse_semi_structured model i weight sparse_result = model input torch testing assert_close dense_result sparse_result rtol= e- atol= e- parametrize_backends test_values backend SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass backend == cutlass IS_WINDOWS skipTest CUTLASS supported Windows A = rand_sparse_semi_structured_mask A_sparse = to_sparse_semi_structured A assert A_sparse values shape == assert A_sparse values == all parametrize_backends test_indices backend SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass backend == cutlass IS_WINDOWS skipTest CUTLASS supported Windows A = rand_sparse_semi_structured_mask A_sparse = to_sparse_semi_structured A assert A_sparse indices shape == inference_dtypes parametrize_backends test_min_sparse_shape dtype device backend SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass config = SEMI_STRUCTURED_SUPPORTED_BACKENDS backend _DTYPE_SHAPE_CONSTRAINTS dtype A = rand_sparse_semi_structured_mask config sparse_min_rows config sparse_min_cols dtype=dtype device=device A_sparse = to_sparse_semi_structured A B = torch rand config sparse_min_cols config dense_min_cols device=device dtype dtype == torch int dense_res = torch mm A cpu B cpu device dtype=torch int int sparse matmul supported R R - R layout so we transpose one arguments get R C - R B_t = B t contiguous sparse_res = torch mm A_sparse B_t t dense_res = torch mm A B sparse_res = torch mm A_sparse B torch testing assert_close sparse_res dense_res rtol= e- atol= e- inference_dtypes parametrize_backends test_unsupported_shape dtype device backend SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass backend == cutlass IS_WINDOWS skipTest CUTLASS supported Windows A = rand_sparse_semi_structured_mask dtype=dtype device=device assertRaisesRegex RuntimeError Error original_tensor shape A_sparse = to_sparse_semi_structured A dtypes all_types_and_complex parametrize_backends test_unsupported_dtype dtype device backend SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass backend == cutlass IS_WINDOWS skipTest CUTLASS supported Windows A = rand_sparse_semi_structured_mask dtype=dtype device=device dtype SEMI_STRUCTURED_SUPPORTED_BACKENDS backend _DTYPE_SHAPE_CONSTRAINTS assertRaisesRegex RuntimeError Error original_tensor dtype A_sparse = to_sparse_semi_structured A A_sparse = to_sparse_semi_structured A parametrize_backends test_unsupported_dim device backend SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass backend == cutlass IS_WINDOWS skipTest CUTLASS supported Windows A = torch rand device=device dtype=torch float assertRaisesRegex RuntimeError Error original_tensor dim A_sparse = to_sparse_semi_structured A create_random_mask shape - torch Tensor r = random Random mask = torch zeros shape dtype=torch bool line range mask shape col range mask shape sparsity = r choice False False True True False True False True True False False True False True True False True False True False True True False False mask line col col + = torch tensor sparsity dtype=torch bool mask TestSparseSemiStructuredTraining TestCase setUp _IS_SM X skipTest SparseSemiStructuredTensor training only supported SM x Ampere IS_WINDOWS skipTest CUTLASS supported windows training_dtypes unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_prune_dense_static_sort dtype - None Ideally we would like clone compare won t work because sorting order will different instead we pass pruned matrix CUDA implementation preserve sparsity pattern dense = torch randn device= cuda dtype=dtype pruned = _sparse_semi_structured_tile dense CUTLASS reference_cutlass = SparseSemiStructuredTensorCUTLASS prune_dense_static_sort pruned algorithm= largest_abs_values_greedy torch testing assert_close pruned reference_cutlass to_dense packed_cutlass meta_cutlass = sparse_semi_structured_from_dense_cutlass pruned packed_t_cutlass meta_t_cutlass = sparse_semi_structured_from_dense_cutlass pruned t contiguous meta_cutlass = meta_cutlass as_strided reference_cutlass meta shape reference_cutlass meta stride meta_t_cutlass = meta_t_cutlass as_strided reference_cutlass meta_t shape reference_cutlass meta_t stride compressed_swizzled_bitmask = _compute_compressed_swizzled_bitmask pruned compressed_swizzled_bitmask = compressed_swizzled_bitmask as_strided reference_cutlass compressed_swizzled_bitmask shape reference_cutlass compressed_swizzled_bitmask stride cutlass = SparseSemiStructuredTensorCUTLASS dense shape packed_cutlass meta_cutlass packed_t_cutlass meta_t_cutlass compressed_swizzled_bitmask torch testing assert_close reference_cutlass to_dense cutlass to_dense CUSPARSELT reference_cusparselt = SparseSemiStructuredTensorCUSPARSELT prune_dense_static_sort pruned algorithm= largest_abs_values_greedy torch testing assert_close pruned reference_cusparselt to_dense packed_cusparselt = torch _cslt_compress pruned packed_t_cusparselt = torch _cslt_compress pruned t contiguous cusparselt = SparseSemiStructuredTensorCUSPARSELT dense shape packed_cusparselt None packed_t_cusparselt None compressed_swizzled_bitmask torch testing assert_close reference_cusparselt to_dense cusparselt to_dense training_dtypes parametrize_backends unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_pruning_algo_largest_abs_values_greedy dtype backend - None inp = torch tensor - - - device= cuda dtype=dtype inp = F pad inp - - constant sInp = SEMI_STRUCTURED_SUPPORTED_BACKENDS backend prune_dense_static_sort inp algorithm= largest_abs_values_greedy mask = sInp to_dense inp assert mask int tolist == training_dtypes test_gemm dtype - None M N K = = torch randn M K device= cuda dtype=dtype b = torch randn K N device= cuda dtype=dtype mask = rand_sparse_semi_structured_mask M K dtype=torch bool masked_fill_ ~mask a_sparse = to_sparse_semi_structured masked_a = mask ref_out = masked_a b sp _out = a_sparse b torch testing assert_close ref_out sp _out atol_rtol_kw dtype training_dtypes parametrize_backends unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_pack_both_ways_meta_correctness dtype backend - None M N = Construct x make sure we always have exactly elements per x tile = torch arange None + torch arange None = repeat M N assert shape == M N = cuda dtype b = torch randn shape device= cuda dtype=dtype a_sparse = SEMI_STRUCTURED_SUPPORTED_BACKENDS backend prune_dense_static_sort mask_dense = sparse _largest_mask_ d dtype backend == cutlass assert isinstance a_sparse SparseSemiStructuredTensorCUTLASS packed meta packed_t meta_t bitmask = torch _sparse_semi_structured_tile mask_dense use_cutlass=True sparse_mask = SparseSemiStructuredTensorCUTLASS mask_dense shape packed=packed meta=meta packed_t=packed_t meta_t=meta_t compressed_swizzled_bitmask=bitmask torch testing assert_close a_sparse meta view torch short sparse_mask meta ref_gemm = mask_dense b pack_gemm = a_sparse b torch testing assert_close ref_gemm pack_gemm atol_rtol_kw dtype training_dtypes unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_pack_both_ways_id dtype - None N = torch manual_seed = torch randn N N dtype=dtype device= cuda b = torch eye N dtype=dtype device= cuda packed meta packed_t meta_t = torch _sparse_semi_structured_tile Heuristic ensure we pack same values torch testing assert_close packed torch float sum packed_t torch float sum mask_dense = sparse _largest_mask_ d dtype ref_gemm = mask_dense Test A B pack_gemm = torch _sparse_semi_structured_linear b t packed meta t max_diff = ref_gemm - pack_gemm abs argmax torch testing assert_close ref_gemm pack_gemm atol_rtol_kw dtype msg=f packed wrong pos max_diff N max_diff N Test A t B pack_gemm = torch _sparse_semi_structured_linear b t packed_t meta_t max_diff = ref_gemm - pack_gemm abs argmax torch testing assert_close ref_gemm pack_gemm atol_rtol_kw dtype msg=f packed_t wrong pos max_diff N max_diff N training_dtypes unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_pack_both_ways_edge_case dtype - None In case heuristic will keep values out instead let s see how kernel handles quad = torch tensor - - - Should packed ` null ` - - - - - - dtype=dtype device= cuda = torch randn dtype=dtype device= cuda = quad packed meta packed_t meta_t = torch _sparse_semi_structured_tile Check first line A assert packed item == assert packed item == And first column A t assert packed_t item == assert packed_t item == training_dtypes unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_sp _apply dtype - None M N = x = torch randn M N dtype=dtype device= cuda packed meta packed_t meta_t bitmask = torch _sparse_semi_structured_tile x packed packed_t = torch _sparse_semi_structured_apply x bitmask torch testing assert_close packed packed torch testing assert_close packed_t packed_t training_dtypes unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_sp _apply_dense dtype - None M N = x = torch randn M N dtype=dtype device= cuda packed meta packed_t meta_t bitmask = torch _sparse_semi_structured_tile x expected = SparseSemiStructuredTensorCUTLASS x shape packed=packed meta=meta packed_t=packed_t meta_t=meta_t compressed_swizzled_bitmask=bitmask to_dense packed packed_t = torch _sparse_semi_structured_apply x bitmask sparse = SparseSemiStructuredTensorCUTLASS x shape packed=packed meta=meta packed_t=packed_t meta_t=meta_t compressed_swizzled_bitmask=bitmask dense = torch _sparse_semi_structured_apply_dense x bitmask torch testing assert_close dense expected torch testing assert_close sparse to_dense expected training_dtypes unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_sp _matmuls dtype - None M N K = = torch randn M K device= cuda dtype=dtype b = torch randn K N device= cuda dtype=dtype a_m = sparse _largest_mask_ d b_m = sparse _largest_mask_ d b packed meta packed_t meta_t bitmask = torch _sparse_semi_structured_tile a_s = SparseSemiStructuredTensorCUTLASS shape packed=packed meta=meta packed_t=packed_t meta_t=meta_t compressed_swizzled_bitmask=bitmask packed meta packed_t meta_t bitmask = torch _sparse_semi_structured_tile b b_s = SparseSemiStructuredTensorCUTLASS b shape packed=packed meta=meta packed_t=packed_t meta_t=meta_t compressed_swizzled_bitmask=bitmask torch testing assert_close a_s b a_m b rtol= e- atol= e- torch testing assert_close b_s b b_m rtol= e- atol= e- torch testing assert_close a_s t a_m t rtol= e- atol= e- torch testing assert_close a_s t a_m t rtol= e- atol= e- unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_sp _matmuls_mat_vec - None = torch randn device= cuda dtype=torch float b = torch randn device= cuda dtype=torch float a_m = sparse _largest_mask_ d a_s = to_sparse_semi_structured pytest raises NotImplementedError torch testing assert_close a_s b a_m b atol_rtol_kw dtype unittest skipIf TEST_WITH_ROCM Not supported ROCm unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_sp _matmuls_bmm - None = torch randn device= cuda dtype=torch float b = torch randn device= cuda dtype=torch float a_m = sparse _largest_mask_ d a_s = to_sparse_semi_structured pytest raises NotImplementedError torch testing assert_close a_s b a_m b atol_rtol_kw dtype TestSparseSemiStructuredCUTLASS TestCase This contains CUTLASS specific tests - torch _sparse_semi_structured_linear setUp SparseSemiStructuredTensor _FORCE_CUTLASS = True cutlass SEMI_STRUCTURED_SUPPORTED_BACKENDS skipTest CUTLASS enabled tearDown SparseSemiStructuredTensor _FORCE_CUTLASS = False super tearDown unittest skipIf TEST_WITH_ROCM IS_WINDOWS ROCm Windows doesn t support CUTLASS inference_dtypes test_linear_cutlass device dtype run_test batch_shape m n k device dtype dtype_out add_bias activation rtol atol weight = rand_sparse_semi_structured m k dtype device input = make_tensor batch_shape n k dtype=dtype device=device bias = make_tensor m dtype=dtype_out device=device add_bias None dtype_dense = torch float input_dense = input dtype_dense weight_dense = weight dtype_dense bias_dense = bias dtype_dense add_bias None output = torch nn functional linear input_dense weight_dense bias=bias_dense activation == relu relu = torch nn ReLU output = relu output activation == silu silu = torch nn SiLU output = silu output compressed = to_sparse_semi_structured weight weight_sparse = compressed values meta = compressed indices output = torch _sparse_semi_structured_linear input weight_sparse meta bias=bias activation=activation out_dtype=dtype_out dtype == torch int None torch testing assert_close output dtype_dense output rtol=rtol atol=atol dtype == torch float Inputs converted TF internally sparse GEMM so make dense GEMM do same matching results orig = torch backends cuda matmul allow_tf torch backends cuda matmul allow_tf = True batch_shapes = dtype_out = torch int torch int torch half torch half torch bfloat torch bfloat torch float torch float activations = None relu silu rtol atol = e- e- dtype == torch bfloat rtol atol = e- e- dtype == torch float rtol atol = e- e- batch_shape m n k add_bias activation \ itertools product batch_shapes range range range False True activations activation == silu dtype == torch int continue SiLU supported integer inputs m = m n = n k = k run_test batch_shape m n k device dtype dtype_out dtype add_bias activation rtol atol dtype == torch float torch backends cuda matmul allow_tf = orig unittest skipIf TEST_WITH_ROCM IS_WINDOWS ROCm Windows doesn t support CUTLASS parametrize backend cutlass inference_dtypes test_sparse_semi_structured_ops_cutlass device dtype backend SparseSemiStructuredTensor _FORCE_CUTLASS = backend == cutlass backend == cutlass IS_WINDOWS skipTest CUTLASS supported Windows run_test m n k device dtype dtype_out use_input rtol atol mat = rand_sparse_semi_structured m k dtype device mat transposed int case supports only row-major column-major combination mat = make_tensor n k dtype=dtype device=device t input = make_tensor m dtype=dtype_out device=device use_input None use_input dtype is_floating_point alpha = beta = - alpha = beta = - dtype_dense = torch float mat _dense = mat dtype_dense mat _dense = mat dtype_dense use_input output = torch mm mat _dense mat _dense input_dense = input dtype_dense None output = torch addmm input_dense mat _dense mat _dense alpha=alpha beta=beta compressed = to_sparse_semi_structured mat mat _sparse = compressed values mat _meta = compressed indices use_input output = torch _sparse_semi_structured_mm mat _sparse mat _meta mat out_dtype=dtype_out output = torch _sparse_semi_structured_addmm input mat _sparse mat _meta mat alpha=alpha beta=beta out_dtype=dtype_out torch testing assert_close output dtype_dense output rtol=rtol atol=atol dtype == torch float Inputs converted TF internally sparse GEMM so make dense GEMM do same matching results orig = torch backends cuda matmul allow_tf torch backends cuda matmul allow_tf = True dtype_out = torch int torch int torch half torch half torch bfloat torch bfloat torch float torch float rtol atol = e- e- dtype == torch bfloat rtol atol = e- e- dtype == torch float rtol atol = e- e- m n k use_input \ itertools product range range range False True m = m n = n k = k run_test m n k device dtype dtype_out dtype use_input rtol atol dtype == torch float torch backends cuda matmul allow_tf = orig unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch inference_dtypes unittest skipIf TEST_WITH_ROCM Not supported ROCm test_conversions device dtype run_test r c device dtype dense_ref = rand_sparse_semi_structured r c dtype device compressed = to_sparse_semi_structured dense_ref The torch ops aten _to_sparse_semi_structured operator uses CUTLASS perform conversion given dense matrix pair corresponding sparse metadata matrices later used here reference compare metadata matrix produced conversion performed SparseSemiStructuredTensor constructor against _ meta_ref = torch ops aten _to_sparse_semi_structured dense_ref meta = compressed indices torch testing assert_close meta meta_ref rtol= atol= dense = compressed to_dense torch testing assert_close dense dense_ref rtol= atol= shapes = r c shapes run_test r c device dtype unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch inference_dtypes unittest skipIf TEST_WITH_ROCM Not supported ROCm test_conversions_all_patterns device dtype r c = dense_inv dense_val = rand_sparse_semi_structured_all_patterns r c dtype device compressed = to_sparse_semi_structured dense_inv dense = compressed to_dense torch testing assert_close dense dense_val rtol= atol= CUSPARSELT_MIXED_DTYPE_SUPPORT = torch float torch bfloat torch int to_float x dtype=torch float _e m fn finfo = torch finfo dtype Calculate scale dtype max divided absmax scale = finfo max x abs max clamp min= e- scale clamp tensor bring representative range float data type default cast unsaturated x_scl_sat = x scale clamp min=finfo min max=finfo max Return both float data inverse scale float both required inputs torch _scaled_mm x_scl_sat dtype scale float reciprocal TestSparseSemiStructuredCUSPARSELT TestCase This contains cuSPARSELt specific tests torch _cslt_compress torch _cslt_sparse_mm setUp SparseSemiStructuredTensor _FORCE_CUTLASS = False cusparselt SEMI_STRUCTURED_SUPPORTED_BACKENDS skipTest cuSPARSELt enabled unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + SM MI + devices xfailIfSM parametrize dense_input_shape test_sparse_fp fp _mm dense_input_shape device torch backends cusparselt version skipTest fp matmul requires cuSPARSELt v + A = rand_sparse_semi_structured_mask dtype=torch float B = torch rand dense_input_shape device=device torch float t A_fp A_scale = to_float A B_fp B_scale = to_float B A_fp _sparse = to_sparse_semi_structured A_fp assertRaisesRegex NotImplementedError r ` SparseSemiStructuredTensor _scaled_mm dense_result = torch mm A_fp _sparse B_fp unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + SM MI + devices xfailIfSM test_sparse_semi_structured_scaled_mm_fp device - None k l m = x = rand_sparse_semi_structured_mask k l dtype=torch float _e m fn device=device y = torch full m l device=device dtype=torch float _e m fn t scale_a = torch tensor device=device scale_b = torch tensor device=device out_fp = torch _scaled_mm x y scale_a=scale_a scale_b=scale_b out_dtype=torch float _e m fn x_sparse = to_sparse_semi_structured x out_fp _sparse = torch _scaled_mm x_sparse y scale_a=scale_a scale_b=scale_b out_dtype=torch float _e m fn fails ROCm currently because hipblaslt doesn t have amax op out_fp = out_fp torch float out_fp _sparse = out_fp _sparse torch float torch testing assert_close out_fp out_fp _sparse rtol= e- atol= e- unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + SM MI + devices xfailIfSM parametrize out_dtype torch float torch bfloat torch float parametrize dense_input_shape test_sparse_semi_structured_scaled_mm dense_input_shape device out_dtype A = rand_sparse_semi_structured_mask dtype=torch float B = torch rand dense_input_shape device=device torch float t A_fp A_scale = to_float A B_fp B_scale = to_float B A_fp _sparse = to_sparse_semi_structured A_fp dense_result = torch _scaled_mm A_fp B_fp scale_a=A_scale scale_b=B_scale out_dtype=out_dtype sparse_result = torch _scaled_mm A_fp _sparse B_fp scale_a=A_scale scale_b=B_scale out_dtype=out_dtype torch testing assert_close dense_result sparse_result rtol= e- atol= e- parametrize out_dtype torch float torch bfloat torch int parametrize dense_input_shape test_cslt_sparse_mm_mixed_dtype dense_input_shape out_dtype device A = rand_sparse_semi_structured_mask dtype=torch int A_compressed = torch _cslt_compress A B = torch rand dense_input_shape device=device torch int dense_result = torch mm A cpu torch int B t cpu torch int device dtype=out_dtype sparse_result = torch _cslt_sparse_mm A_compressed B t out_dtype=out_dtype torch testing assert_close dense_result sparse_result rtol= e- atol= e- unittest skip cuSPARSELt v x does support bfloat float alpha scaling training_dtypes unittest skipIf TEST_WITH_ROCM Not supported ROCm test_cslt_sparse_mm_alpha dtype device A = torch Tensor tile dtype cuda B = torch ones device=device dtype alpha = torch Tensor -i i range cuda bias = torch ones device=device dtype A_compressed = torch _cslt_compress A sparse_result = torch _cslt_sparse_mm A_compressed B alpha=alpha bias=bias alpha_scaled = torch stack alpha t dense_result = alpha_scaled torch mm A torch float B torch float dense_result = dense_result dtype torch testing assert_close sparse_result dense_result rtol= e- atol= e- parametrize out_dtype torch float torch bfloat torch int unittest skipIf TEST_WITH_ROCM Not supported ROCm test_cslt_sparse_mm_alpha_compile_autotune device out_dtype A = torch Tensor tile torch int device B = torch ones device=device dtype=torch int t alpha = torch Tensor -i i range cuda A_compressed = torch _cslt_compress A cslt_sparse_mm_c = torch compile torch _cslt_sparse_mm mode= max-autotune sparse_result = cslt_sparse_mm_c A_compressed B alpha=alpha out_dtype=out_dtype disable otherwise inductor will attempt reorder strides pass contiguous B torch compiler disable get_dense_result alpha_scaled = torch stack alpha t cpu float dense_result = alpha_scaled torch mm A torch int cpu B torch int cpu dense_result = dense_result out_dtype dense_result torch testing assert_close sparse_result cpu get_dense_result rtol= e- atol= e- parametrize out_dtype torch float torch bfloat torch int unittest skipIf TEST_WITH_ROCM Not supported ROCm test_cslt_sparse_mm_alpha_mixed_dtype out_dtype device A = torch Tensor tile torch int cuda B = torch ones device=device torch int t alpha = torch Tensor -i out_dtype torch int i range cuda A_compressed = torch _cslt_compress A sparse_result = torch _cslt_sparse_mm A_compressed B alpha=alpha out_dtype=out_dtype cpu alpha_scaled = torch stack alpha t dense_result = alpha_scaled cpu torch mm A torch int cpu B torch int cpu dense_result = dense_result out_dtype torch testing assert_close sparse_result dense_result rtol= e- atol= e- inference_dtypes test_cslt_sparse_mm_search device dtype A = rand_sparse_semi_structured_mask dtype=dtype A_compressed = torch _cslt_compress A B = torch ones device=device dtype A_compressed = torch _cslt_compress A alg_id = torch _cslt_sparse_mm_search A_compressed B t sparse_result = torch _cslt_sparse_mm A_compressed B t alg_id=alg_id dense_result = torch mm A torch float B torch float dense_result = dense_result dtype torch testing assert_close sparse_result dense_result rtol= e- atol= e- inference_dtypes test_csrc_cslt_sparse_mm_search device dtype A = rand_sparse_semi_structured_mask dtype=dtype A_compressed = torch _cslt_compress A B = torch ones device=device dtype A_compressed = torch _cslt_compress A alg_id split_k split_k_mode _ = torch _C _cusparselt mm_search A_compressed B t None None None False sparse_result = torch _cslt_sparse_mm A_compressed B t alg_id=alg_id split_k=split_k split_k_mode=split_k_mode dense_result = torch mm A torch float B torch float dense_result = dense_result dtype torch testing assert_close sparse_result dense_result rtol= e- atol= e- test_cusparselt_backend version = _get_torch_cuda_version assert torch backends cusparselt is_available PyTorch CUDA + using cuSPARSELt v + version = assert torch backends cusparselt version = assert torch backends cusparselt version None len SEMI_STRUCTURED_SUPPORTED_BACKENDS instantiate_device_type_tests TestSparseSemiStructured globals only_for= cuda cutlass SEMI_STRUCTURED_SUPPORTED_BACKENDS instantiate_device_type_tests TestSparseSemiStructuredCUTLASS globals only_for= cuda instantiate_device_type_tests TestSparseSemiStructuredTraining globals only_for= cuda cusparselt SEMI_STRUCTURED_SUPPORTED_BACKENDS instantiate_device_type_tests TestSparseSemiStructuredCUSPARSELT globals only_for= cuda __name__ == __main__ run_tests