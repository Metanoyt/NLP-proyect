mypy allow-untyped-defs sys threading weakref dataclasses dataclass functools partial reduce typing Optional Union torch torch distributed dist torch _C _distributed_c d _create_work_from_future AllgatherOptions AllreduceOptions AllToAllOptions BarrierOptions BroadcastOptions ReduceOp ReduceScatterOptions ScatterOptions Store torch distributed distributed_c d _CollOp _store_based_barrier P POp torch futures Future torch utils _pytree pytree TODO Lots missing collectives Collectives validation Make timeout robust making collectives respect test deadline Make tests robust making collectives interruptible We need some synchronization around cleanup ensure timedout ranks don t cause spurious failures flatten_list lst pytree tree_leaves lst ret_work ret fut = Future fut set_result ret _create_work_from_future fut binop_reduce tensors op res = op torch stack tensors dim= isinstance res torch Tensor res min max namedtuple res values bitwise_reduce tensors op reduce op tensors _reduce_ops = ReduceOp SUM partial binop_reduce op=torch sum ReduceOp AVG partial binop_reduce op=torch mean ReduceOp PRODUCT partial binop_reduce op=torch prod ReduceOp MIN partial binop_reduce op=torch min ReduceOp MAX partial binop_reduce op=torch max ReduceOp BAND partial bitwise_reduce op=torch bitwise_and ReduceOp BOR partial bitwise_reduce op=torch bitwise_or ReduceOp BXOR partial bitwise_reduce op=torch bitwise_xor Note Hide collectives mutation autograd ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Threaded PG intended closely simulate behavior regular process groups However our regular PG implementations perform dispatch through c d whereas Threaded PG does some reason some superficial very convincing reasons include Threaded PG implemented Python you can t override Backend Python you can only override ProcessGroup Python thereby bypassing dispatch step Now we have problem c d s signatures LIES they mutate their output tensor arguments their annotations don t have mutations them so we don t actually update any view metadata you do differentiation This ordinarily doesn t matter because distributed collectives aren t differentiable anyway s possible tickle testing someone tries touch grad_fn Tensor There few ways fix easiest way use detach trick hide mutations autograd AllToAll torch no_grad work data world_size = len data dest_rank range world_size output_tensor_list _ = data dest_rank src_rank range world_size _ input_tensor_list = data src_rank See Note Hide collectives mutation autograd output_tensor_list src_rank detach copy_ input_tensor_list dest_rank AllToAllBase torch no_grad work data world_size = len data dest_rank range world_size output_buffer _ output_split_sizes _ = data dest_rank output_indexes = _size_cumsum output_buffer size output_split_sizes world_size src_rank range world_size _ input_buffer _ input_split_sizes = data src_rank input_indexes = _size_cumsum input_buffer size input_split_sizes world_size See Note Hide collectives mutation autograd output_buffer output_indexes src_rank output_indexes src_rank + detach copy_ input_buffer input_indexes dest_rank input_indexes dest_rank + _size_cumsum buf_size int sizes Union torch Tensor list int None world_size int - torch Tensor sizes None len sizes == sizes = torch full world_size buf_size world_size dtype=torch int isinstance sizes torch Tensor sizes = torch tensor sizes dtype=torch int assert sizes dtype == torch int sizes = torch cumsum torch cat torch tensor dtype=torch int device=sizes device sizes dim= dim= sizes AllReduce __init__ op op op _reduce_ops raise NotImplementedError f AllReduce op op op supported multithreaded pg now op = op op torch no_grad work data i range len data use rank device sum rank_ _device = data i device collect all data list make them all rank device tensors = data src_rank i rank_ _device src_rank range len data now mimic reduce across all ranks res = _reduce_ops op tensors copy all reduced value each rank src_rank range len data See Note Hide collectives mutation autograd data src_rank i detach copy_ res data src_rank i device AllGather torch no_grad work data src_rank range len data in_tensor_list = data src_rank Can t handle all_gather multiple tensors assert len in_tensor_list == src_tensor = in_tensor_list dest data dest_tensor = dest src_rank See Note Hide collectives mutation autograd dest_tensor detach copy_ src_tensor Scatter __init__ src src = src torch no_grad work data src_in_tensor_list = data src Can t handle scatter multiple input tensor list assert len src_in_tensor_list == src_in_tensors = src_in_tensor_list rank each_rank_data enumerate data out_tensor_list = each_rank_data Can t handle scatter multiple output tensor assert len out_tensor_list == dest_tensor = out_tensor_list See Note Hide collectives mutation autograd dest_tensor detach copy_ src_in_tensors rank Gather __init__ dst dst = dst torch no_grad work data Can t handle gather multiple tensor lists assert len data dst == out_tensor_list = data dst rank each_rank_data enumerate data src_in_tensor_list = each_rank_data Can t handle gather multiple tensor lists assert len src_in_tensor_list == dest_tensor = out_tensor_list rank See Note Hide collectives mutation autograd dest_tensor detach copy_ src_in_tensor_list ReduceScatter __init__ op op = dist ReduceOp SUM op = dist ReduceOp AVG raise NotImplementedError f ReduceScatter does support op op = op torch no_grad work data start_reduction = False _ range len data each_rank_data data Can t handle reduce_scatter multiple scatter list assert len each_rank_data == to_scatter = each_rank_data i range len to_scatter dest_tensor_on_rank_i = data i Can t handle reduce_scatter multiple output tensor assert len dest_tensor_on_rank_i == dst_tensor_device = dest_tensor_on_rank_i device start_reduction i See Note Hide collectives mutation autograd dest_tensor_on_rank_i detach copy_ to_scatter i dst_tensor_device start_reduction i = True See Note Hide collectives mutation autograd dest_tensor_on_rank_i detach add_ to_scatter i dst_tensor_device op == dist ReduceOp AVG num_ranks = len data each_rank_data data See Note Hide collectives mutation autograd each_rank_data detach div_ num_ranks Broadcast __init__ src src = src torch no_grad work data in_tensor_list = flatten_list data src i range len data i == src continue out_tensor_list = flatten_list data i j range len in_tensor_list See Note Hide collectives mutation autograd out_tensor_list j detach copy_ in_tensor_list j Collective __init__ world_size collective pg _world_size = world_size _collective = collective _start_cond = threading Condition _done_cond = threading Condition _data = None world_size _count = _done = False _pg = pg join rank data _start_cond _data rank = data _count += notify rank _count == _world_size rank _start_cond notify rank == _start_cond wait_for lambda _count == _world_size _pg _terminate is_set SystemExit subclass Exception BaseException can distinguished normal exception raised program errors so we can hide exception queue _pg _terminate is_set sys exit Test termination event occurs _done_cond wait rank finish rank _done_cond wait_for lambda _done _pg _terminate is_set _pg _terminate is_set sys exit Test termination event occurs copy data around _collective work _data _done = True _done_cond notify_all ret_work data ProcessLocalGroup dist ProcessGroup _coll_lock = threading Lock _cur_coll_on_pgs = _terminate = threading Event classmethod _start_coll cls collective pg cls _coll_lock pg_name unique we use record mapping between pg collective pg pg_name cls _cur_coll_on_pgs cls _cur_coll_on_pgs pg pg_name = Collective pg size collective cls cls _cur_coll_on_pgs pg pg_name classmethod _end_coll cls collective pg This racily called all ranks so only one will work cls _coll_lock pg pg_name cls _cur_coll_on_pgs cls _cur_coll_on_pgs pg pg_name == collective cls _cur_coll_on_pgs pop pg pg_name classmethod exception_handle cls exc cls _terminate set coll cls _cur_coll_on_pgs values coll _start_cond coll _start_cond notify coll _done_cond coll _done_cond notify_all classmethod reset cls cls _coll_lock cls _cur_coll_on_pgs = cls _terminate clear alltoall_base output_buffer torch Tensor input_buffer torch Tensor output_split_sizes Optional list int input_split_sizes Optional list int opts=AllToAllOptions - torch Tensor coll = ProcessLocalGroup _start_coll AllToAllBase res = coll join _rank output_buffer input_buffer output_split_sizes input_split_sizes ProcessLocalGroup _end_coll coll res alltoall output_tensor_list input_tensor_list opts=AllToAllOptions coll = ProcessLocalGroup _start_coll AllToAll res = coll join _rank output_tensor_list input_tensor_list ProcessLocalGroup _end_coll coll res allreduce tensor_list opts=AllreduceOptions coll = ProcessLocalGroup _start_coll AllReduce opts reduceOp res = coll join _rank tensor_list ProcessLocalGroup _end_coll coll res allreduce_coalesced tensor_list opts=AllreduceOptions coll = ProcessLocalGroup _start_coll AllReduce opts reduceOp res = coll join _rank tensor_list ProcessLocalGroup _end_coll coll res barrier opts=BarrierOptions allreduce tensor_list= torch ones allgather output_tensors input_tensor opts=AllgatherOptions coll = ProcessLocalGroup _start_coll AllGather res = coll join _rank output_tensors input_tensor ProcessLocalGroup _end_coll coll res _allgather_base output_tensor input_tensor opts=AllgatherOptions tensor_list = list torch chunk output_tensor _world_size allgather tensor_list input_tensor opts broadcast tensor_list opts=BroadcastOptions coll = ProcessLocalGroup _start_coll Broadcast opts rootRank res = coll join _rank tensor_list ProcessLocalGroup _end_coll coll res scatter output_tensors input_tensors opts=ScatterOptions coll = ProcessLocalGroup _start_coll Scatter opts rootRank res = coll join _rank output_tensors input_tensors ProcessLocalGroup _end_coll coll res gather output_tensors input_tensors opts=ScatterOptions coll = ProcessLocalGroup _start_coll Gather opts rootRank res = coll join _rank output_tensors input_tensors ProcessLocalGroup _end_coll coll res reduce_scatter output_tensor scatter_list opts=ReduceScatterOptions coll = ProcessLocalGroup _start_coll ReduceScatter opts reduceOp res = coll join _rank output_tensor scatter_list ProcessLocalGroup _end_coll coll res _reduce_scatter_base output_tensor input_tensor opts=ReduceScatterOptions tensor_list = list torch chunk input_tensor _world_size reduce_scatter output_tensor tensor_list opts reduce_scatter_tensor_coalesced output_tensors input_tensors opts=ReduceScatterOptions works = _reduce_scatter_base output_tensor input_tensor opts output_tensor input_tensor zip output_tensors input_tensors strict=True work works - work wait works - allgather_into_tensor_coalesced output_tensor_list input_tensor_list opts=AllgatherOptions res = None o_t i_t zip output_tensor_list input_tensor_list strict=True res = _allgather_base o_t i_t res __init__ rank world_size super __init__ rank world_size _rank = rank _world_size = world_size world = dist distributed_c d _world isinstance world ThreadLocalWorld world = world _get_world _world = weakref ref world _ctx = torch autograd set_multithreading_enabled False size _world_size property pg_name global registered name current pg world _world pg_names property group_name pg_name getBackendName threaded __repr__ f ThreadedPG world_size _world_size rank _rank _create_threaded_pg prefix_store rank world_size timeout pg = ProcessLocalGroup rank world_size https github com pytorch pytorch pull changed store based barrier optional When device mesh involves sub groups while store based barrier enabled c d even though threaded pg actual collectives assumed single threaded different threads may initializing different groups leading race conditions For example we have mesh sub groups dim would initialized different threads independently In case we can no longer rely global variables have rely store based barrier make sure each group ready separately before we can invoke collectives any groups prefix store already per group so we pass empty name here _store_based_barrier rank prefix_store world_size timeout pg dist Backend register_backend threaded _create_threaded_pg devices= cpu cuda dataclass WorldData default_pg dist ProcessGroup pg_map dict dist ProcessGroup tuple str Optional Store pg_names dict dist ProcessGroup str pg_group_ranks dict dist ProcessGroup dict int int pg_backend_config dict dist ProcessGroup str group_count int tags_to_pg dict str list dist ProcessGroup pg_to_tag dict dist ProcessGroup str pg_coalesce_state dict dist ProcessGroup list Union _CollOp P POp ThreadLocalWorld _world = threading local _get_world - WorldData hasattr ThreadLocalWorld _world world ThreadLocalWorld _world world = WorldData None ThreadLocalWorld _world world property default_pg _get_world default_pg default_pg setter default_pg value _get_world default_pg = value property pg_map _get_world pg_map property pg_names _get_world pg_names property pg_group_ranks _get_world pg_group_ranks property pg_backend_config _get_world pg_backend_config property group_count - int _get_world group_count group_count setter group_count value _get_world group_count = value property tags_to_pg _get_world tags_to_pg property pg_to_tag _get_world pg_to_tag property pg_coalesce_state - dict dist ProcessGroup list Union _CollOp P POp _get_world pg_coalesce_state _old_pg_world = None _ctx_manager = None _install_threaded_pg global _old_pg_world global _ctx_manager _old_pg_world = dist distributed_c d _world dist distributed_c d _world = ThreadLocalWorld _ctx_manager = torch autograd set_multithreading_enabled False dist distributed_c d _world _uninstall_threaded_pg dist distributed_c d _world = _old_pg_world