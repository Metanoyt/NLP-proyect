Logic converting human-readable benchmarks into executable form This mostly string manipulation just bit importlib magic mypy ignore-errors importlib abc importlib util itertools os re textwrap uuid typing Optional TYPE_CHECKING torch TYPE_CHECKING See note api py why necessary torch utils benchmark utils timer Language torch utils benchmark Language core api AutogradMode AutoLabels GroupedBenchmark RuntimeMode TimerArgs core types FlatDefinition FlatIntermediateDefinition Label core utils get_temp_dir _ALL_MODES = tuple product RuntimeMode AutogradMode Language _generate_torchscript_file model_src str name str - Optional str Returns path saved model one can constructed ` spec ` Because TorchScript requires actual source code order script model we can t simply ` eval ` appropriate model string Instead we must write correct source temporary Python file then TorchScript model temporary file ` model_src ` must contain ` jit_model = ` which ` materialize ` will supply Double check assert jit_model = model_src f Missing jit_model definition \n model_src ` torch utils benchmark Timer ` will automatically torch so we need match convention model_src = f torch\n model_src model_root = os path join get_temp_dir TorchScript_models os makedirs model_root exist_ok=True module_path = os path join model_root f torchscript_ name py artifact_path = os path join model_root f torchscript_ name pt os path exists module_path The uuid ` name ` should protect against doesn t hurt confirm raise ValueError f File module_path already exists open module_path w f f write model_src Import magic actually load our function module_spec = importlib util spec_from_file_location f torchscript__ name module_path assert module_spec None module = importlib util module_from_spec module_spec loader = module_spec loader assert loader None loader exec_module module And again type checker has no way knowing line valid jit_model = module jit_model type ignore attr-defined assert isinstance jit_model torch jit ScriptFunction torch jit ScriptModule f Expected ScriptFunction ScriptModule got type jit_model jit_model save artifact_path type ignore call-arg Cleanup now we have actual serialized model os remove module_path artifact_path _get_stmt benchmark GroupedBenchmark runtime RuntimeMode autograd AutogradMode language Language - Optional str Specialize GroupedBenchmark particular configuration is_python = language == Language PYTHON During GroupedBenchmark construction py_fwd_stmt cpp_fwd_stmt set eager invocation So RuntimeMode EAGER case we can simply reuse them For RuntimeMode JIT case we need generate appropriate ` jit_model ` invocation runtime == RuntimeMode EAGER stmts = benchmark py_fwd_stmt benchmark cpp_fwd_stmt assert runtime == RuntimeMode JIT assert benchmark signature_args None stmts = GroupedBenchmark _make_model_invocation benchmark signature_args benchmark signature_output RuntimeMode JIT stmt = stmts is_python autograd == AutogradMode FORWARD_BACKWARD stmt None assert benchmark signature_output None backward = f benchmark signature_output In C++ we have get Tensor out IValue call ` backward ` f toTensor runtime == RuntimeMode JIT language == Language CPP f backward language == Language CPP stmt = f stmt \n backward stmt _get_setup benchmark GroupedBenchmark runtime RuntimeMode language Language stmt str model_path Optional str - str Specialize GroupedBenchmark particular configuration Setup requires two extra pieces information The benchmark stmt This needed warm up model avoid measuring lazy initialization The model path so we can load during benchmark These only used when ` runtime == RuntimeMode JIT ` By time we get here details about how set up model have already been determined GroupedBenchmark Or set None appropriate We simply need collect package code blocks language == Language PYTHON setup = benchmark setup py_setup model_setup = benchmark py_model_setup assert language == Language CPP setup = benchmark setup cpp_setup model_setup = benchmark cpp_model_setup runtime == RuntimeMode EAGER \n join setup model_setup assert runtime == RuntimeMode JIT assert model_path None We template ` model_path ` so quotes would break model loading The model path generated within benchmark so just abundance caution rather than something expected practice assert model_path ` stmt ` may contain newlines so we can t use f-strings Instead we need generate templates so dedent works properly language == Language PYTHON setup_template str = textwrap dedent f jit_model = torch jit load model_path Warmup ` jit_model ` _ range stmt assert language == Language CPP setup_template = textwrap dedent f const std string fpath = model_path auto jit_model = torch jit load fpath Warmup ` jit_model ` int i = i i++ stmt model_load = setup_template format stmt=textwrap indent stmt \n join setup model_load materialize benchmarks FlatIntermediateDefinition - FlatDefinition Convert heterogeneous benchmark into executable state This entails generation TorchScript model artifacts splitting GroupedBenchmarks into multiple TimerArgs tagging results AutoLabels results list tuple Label AutoLabels TimerArgs = label args benchmarks items isinstance args TimerArgs User provided explicit TimerArgs so no processing necessary auto_labels = AutoLabels RuntimeMode EXPLICIT AutogradMode EXPLICIT args language results append label auto_labels args assert isinstance args GroupedBenchmark model_path Optional str = None args py_model_setup args torchscript model_setup = f args py_model_setup \njit_model = torch jit script model This just debugging We just need unique name model embedding label makes debugging easier name str = re sub r ^a-z - _ _ _ join label lower name = f name _ uuid uuid model_path = _generate_torchscript_file model_setup name=name runtime autograd language num_threads product _ALL_MODES args num_threads runtime == RuntimeMode EXPLICIT autograd == AutogradMode EXPLICIT continue runtime == RuntimeMode JIT args torchscript continue autograd == AutogradMode FORWARD_BACKWARD args autograd continue stmt = _get_stmt args runtime autograd language stmt None continue setup = _get_setup args runtime language stmt model_path global_setup str = language == Language CPP runtime == RuntimeMode JIT global_setup = textwrap dedent #include string #include vector #include torch script h autolabels = AutoLabels runtime autograd language timer_args = TimerArgs stmt=stmt setup=setup global_setup=global_setup num_threads=num_threads language=language results append label autolabels timer_args tuple results