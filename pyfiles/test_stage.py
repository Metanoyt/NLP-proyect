Copyright c Meta Platforms Inc affiliates Owner s oncall distributed os model_registry ExampleCode ModelWithKwargs MultiMLP torch torch distributed dist torch distributed pipelining build_stage pipeline PipelineStage ScheduleGPipe torch distributed pipelining _utils PipeliningShapeError torch testing _internal common_distributed MultiProcContinuousTest MultiProcessTestCase requires_accelerator_dist_backend torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests skip_but_pass_in_sandcastle_if torch utils _pytree tree_map_only d_hid = batch_size = chunks = device_type = acc type acc = torch accelerator current_accelerator cpu backend = dist get_default_backend_for_device device_type TEST_MULTIACCELERATOR = torch accelerator device_count = torch manual_seed get_dtype_change_hook new_dtype A simple hook simulating mixed precision dtype_change_hook module input output f x x new_dtype tree_map_only torch Tensor f output dtype_change_hook get_flatten_hook A simple hook simulating wrong model output shape flatten_hook module input output f x x flatten tree_map_only torch Tensor f output flatten_hook StageTest MultiProcContinuousTest classmethod backend_str cls - str Testing NCCL backend backend classmethod device_type cls - str device_type property device - torch device torch device device_type rank requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ModelClass ExampleCode MultiMLP test_tracer ModelClass mod = ModelClass d_hid world_size mod device x = torch randn batch_size d_hid device=self device x_mb = x chunk chunks split_spec = mod split_spec hasattr mod split_spec None pipe = pipeline mod mb_args= x_mb split_spec=split_spec stage = pipe build_stage rank device Attach schedule schedule = ScheduleGPipe stage chunks Run _run_step x rank == schedule step x schedule step out = _run_step x Last rank checks result rank == world_size - ref_out = mod x torch testing assert_close out ref_out atol= e- rtol= e- Test qualname mapping submod_keys = stage submod state_dict keys Confirm keys consistent original model old_keys = mod state_dict keys assert all k old_keys k submod_keys requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs parametrize ModelClass ModelWithKwargs test_tracer_kwargs ModelClass mod = ModelClass d_hid world_size mod device x = torch randn batch_size d_hid device=self device y = torch randn batch_size d_hid device=self device x_mb = x chunk chunks y_mb = y chunk chunks pipe = pipeline mod mb_args= x_mb mb_kwargs= y y_mb stage_mod = pipe get_stage_module rank Test build_stage stage = build_stage stage_mod rank pipe info device Attach schedule schedule = ScheduleGPipe stage chunks Run rank == out = schedule step x y=y out = schedule step Last rank checks result rank == world_size - ref_out = mod x y=y torch testing assert_close out ref_out atol= e- rtol= e- Test qualname mapping submod_keys = stage submod state_dict keys Confirm keys consistent original model old_keys = mod state_dict keys assert all k old_keys k submod_keys requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs test_manual full_mod = MultiMLP d_hid n_layers=self world_size full_mod device stage_mod = full_mod get_submodule f layers rank x = torch randn batch_size d_hid device=self device stage = PipelineStage stage_mod rank world_size device Attach schedule schedule = ScheduleGPipe stage chunks Run _run_step x rank == schedule step x schedule step out = _run_step x Last rank checks result rank == world_size - ref_out = full_mod x torch testing assert_close out ref_out requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs test_custom_dw_with_fb_schedule Tests separate weight grad function dw_runner gets run under schedule s only aware F B full_mod = MultiMLP d_hid n_layers=self world_size full_mod device stage_mod = full_mod get_submodule f layers rank x = torch randn batch_size d_hid device=self device target = torch randn batch_size d_hid device=self device CustomState __init__ - None i = dw_builder This simulates function attached model custom backward Each call builder gives new dw_runner has some updated state compute latest dw dw_runner This inner function would called PipelineStage during ` backward_weight_one_chunk ` print f dw called i th time i += dw_runner cs = CustomState stage = PipelineStage stage_mod rank world_size device dw_builder=cs dw_builder Attach schedule schedule = ScheduleGPipe stage chunks loss_fn=torch nn MSELoss reduction= sum Run _run_step x rank == schedule step x rank == world_size - schedule step target=target schedule step out = _run_step x assertEqual cs i chunks Last rank checks result rank == world_size - ref_out = full_mod x torch testing assert_close out ref_out requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs test_output_chunks_memory_usage Test output_chunks doesn t store memory non-first stages full_mod = MultiMLP d_hid n_layers=self world_size full_mod device stage_mod = full_mod get_submodule f layers rank x = torch randn batch_size d_hid device=self device target = torch randn batch_size d_hid device=self device stage = PipelineStage stage_mod rank world_size device assertEqual len stage output_chunks output_chunks should empty initially schedule = ScheduleGPipe stage chunks loss_fn=torch nn MSELoss reduction= sum _run_step x rank == schedule step x rank == world_size - schedule step target=target schedule step _run_step x Verify fwd_cache empty assertEqual len stage fwd_cache fwd_cache should cleared Check output_chunks state after step rank == world_size - assertEqual len stage output_chunks chunks Last stage should store output chunks assertEqual len stage output_chunks f Non-last stage rank rank should store output chunks Clear schedule stage caches stage clear_runtime_states rank == world_size - Last stage should have output_chunks populated assertEqual len stage output_chunks Last stage should store output chunks instantiate_parametrized_tests StageTest StageNegativeTest MultiProcessTestCase property world_size - int torch get_device_module device_type device_count property device - torch device torch device device_type rank setUp super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass init_pg store = dist FileStore file_name world_size dist init_process_group backend=backend store=store rank=self rank world_size=self world_size device_id=self device requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs test_shape_prop_mismatch Tests shape prop errors raised init_pg full_mod = MultiMLP d_hid n_layers=self world_size full_mod device stage_mod = full_mod get_submodule f layers rank x = torch randn batch_size d_hid device=self device stage = PipelineStage stage_mod rank world_size device Attach schedule schedule = ScheduleGPipe stage chunks Run _run_step x rank == schedule step x schedule step _run_step x rank == assertRaisesRegex PipeliningShapeError shape mismatch _run_step torch randn batch_size + d_hid device=self device assertRaisesRegex PipeliningShapeError dtype mismatch _run_step x torch int output stage s mlp layer will flattened hook stage should err handle = stage_mod register_forward_hook get_flatten_hook assertRaisesRegex PipeliningShapeError shape mismatch _run_step x handle remove stage_mod register_forward_hook get_dtype_change_hook torch bfloat assertRaisesRegex PipeliningShapeError dtype mismatch _run_step x requires_accelerator_dist_backend nccl xccl skip_but_pass_in_sandcastle_if TEST_MULTIACCELERATOR f backend test requires + GPUs test_custom_dw_errors Tests expected errors raised init_pg full_mod = MultiMLP d_hid n_layers=self world_size full_mod device stage_mod = full_mod get_submodule f layers rank stage_with_dw_builder = PipelineStage stage_mod rank world_size device dw_builder=lambda None stage_with_dw_builder _has_backward = True assertRaisesRegex AssertionError backward_one_chunk stage_with_dw_builder backward_weight_one_chunk bwd_chunk_id= __name__ == __main__ run_tests