Owner s oncall distributed copy dataclasses functools typing Optional Union torch torch distributed dist torch distributed _functional_collectives funcol torch nn nn torch distributed fsdp fully_shard MixedPrecisionPolicy torch distributed fsdp _fully_shard _fsdp_collectives _get_gradient_divide_factors torch distributed tensor Shard torch testing _internal common_distributed requires_nccl_version SaveForwardInputsModel skip_if_lt_x_gpu torch testing _internal common_fsdp check_sharded_parity FSDPTest FSDPTestMultiThread get_devtype MLP patch_reduce_scatter reduce_scatter_with_assert torch testing _internal common_utils run_tests skipIfRocmVersionLessThan TEST_HPU device_type = torch device get_devtype TestFullyShardMixedPrecisionTraining FSDPTest property world_size - int min torch get_device_module device_type device_count _init_models_and_optims reshard_after_forward Union bool int param_dtype Optional torch dtype reduce_dtype Optional torch dtype use_shard_placement_fn torch manual_seed model = nn Sequential MLP torch device cpu _ range ref_model = copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters lr= e- _shard_placement_fn param nn Parameter - Optional Shard largest_dim = - largest_dim_size = - dim dim_size enumerate param shape dim_size largest_dim_size largest_dim = dim largest_dim_size = dim_size assert largest_dim = f param shape Shard largest_dim mp_policy = MixedPrecisionPolicy param_dtype=param_dtype reduce_dtype=reduce_dtype shard_placement_fn = _shard_placement_fn use_shard_placement_fn None fully_shard_fn = functools partial fully_shard reshard_after_forward=reshard_after_forward mp_policy=mp_policy shard_placement_fn=shard_placement_fn mlp model fully_shard_fn mlp fully_shard_fn model optim = torch optim Adam model parameters lr= e- foreach=True ref_model ref_optim model optim _get_use_shard_placement_fn_vals_for_bf _reduce use_shard_placement_fn_vals = False world_size == For world size gradient elements get reduced different orders baseline vs dim- sharding leading numeric differences bf reduction so only test world size use_shard_placement_fn_vals append True use_shard_placement_fn_vals skipIfRocmVersionLessThan skip_if_lt_x_gpu requires_nccl_version Need NCCL + bf collectives test_compute_dtype use_shard_placement_fn_vals = _get_use_shard_placement_fn_vals_for_bf _reduce run_subtests param_dtype torch bfloat torch float reshard_after_forward False True use_shard_placement_fn use_shard_placement_fn_vals _test_compute_dtype _test_compute_dtype param_dtype torch dtype reshard_after_forward Union bool int use_shard_placement_fn bool ref_model ref_optim model optim = _init_models_and_optims reshard_after_forward param_dtype=param_dtype reduce_dtype=None use_shard_placement_fn=use_shard_placement_fn ref_model_bf = copy deepcopy ref_model param_dtype orig_reduce_scatter = dist reduce_scatter_tensor assert_fn output torch Tensor assertEqual output dtype param_dtype reduce_scatter = functools partial reduce_scatter_with_assert orig_reduce_scatter assert_fn predivide_factor postdivide_factor _ _ = _get_gradient_divide_factors process_group all_reduce_group=None reduce_dtype=param_dtype torch manual_seed + rank + inp = torch randn device=device_type type dtype=param_dtype iter_idx range optim zero_grad set_to_none= iter_idx == fsdp_loss = model inp sum patch_reduce_scatter reduce_scatter fsdp_loss backward optim step ref_optim zero_grad set_to_none= iter_idx == ref_loss = ref_model_bf inp param_dtype sum ref_loss backward param ref_model_bf parameters Use reduce-scatter - all-gather all-reduce because world size = NCCL all-reduce shows numeric differences compared NCCL reduce-scatter predivide_factor None predivide_factor param grad div_ predivide_factor predivide_factor None param grad div_ world_size output = torch zeros_like torch chunk param grad world_size dist reduce_scatter_tensor output param grad dist all_gather_into_tensor param grad output postdivide_factor None postdivide_factor param grad div_ postdivide_factor param_fp param_bf zip ref_model parameters ref_model_bf parameters param_fp grad = param_bf grad param_fp dtype param_bf grad = None ref_optim step fp optimizer step param_fp param_bf zip ref_model parameters ref_model_bf parameters param_bf detach copy_ param_fp assertEqual fsdp_loss ref_loss check_sharded_parity ref_model model skipIfRocmVersionLessThan skip_if_lt_x_gpu requires_nccl_version Need NCCL + bf collectives test_reduce_dtype run_subtests reshard_after_forward False True use_shard_placement_fn False True _test_reduce_dtype_fp _reduce use_shard_placement_fn_vals = _get_use_shard_placement_fn_vals_for_bf _reduce run_subtests reshard_after_forward False True use_shard_placement_fn use_shard_placement_fn_vals _test_reduce_dtype_bf _reduce _test_reduce_dtype_fp _reduce reshard_after_forward Union bool int use_shard_placement_fn bool world_size isinstance reshard_after_forward int use_shard_placement_fn param_dtype reduce_dtype = torch bfloat torch float ref_model ref_optim model optim = _init_models_and_optims reshard_after_forward param_dtype=param_dtype reduce_dtype=reduce_dtype use_shard_placement_fn=use_shard_placement_fn ref_model_bf = copy deepcopy ref_model param_dtype orig_reduce_scatter = dist reduce_scatter_tensor assert_fn output torch Tensor assertEqual output dtype reduce_dtype reduce_scatter = functools partial reduce_scatter_with_assert orig_reduce_scatter assert_fn torch manual_seed + rank + inp = torch randn device=device_type type dtype=param_dtype iter_idx range optim zero_grad set_to_none= iter_idx == fsdp_loss = model inp sum patch_reduce_scatter reduce_scatter fsdp_loss backward optim step ref_optim zero_grad set_to_none= iter_idx == ref_loss = ref_model_bf inp param_dtype sum ref_loss backward param ref_model_bf parameters param grad data = param grad torch float dist all_reduce param grad fp reduction param grad div_ world_size param_fp param_bf zip ref_model parameters ref_model_bf parameters param_fp grad = param_bf grad param_bf grad = None ref_optim step fp optimizer step param_fp param_bf zip ref_model parameters ref_model_bf parameters param_bf detach copy_ param_fp assertEqual fsdp_loss ref_loss check_sharded_parity ref_model model _test_reduce_dtype_bf _reduce reshard_after_forward Union bool int use_shard_placement_fn bool param_dtype reduce_dtype = torch float torch bfloat ref_model ref_optim model optim = _init_models_and_optims reshard_after_forward param_dtype=param_dtype reduce_dtype=reduce_dtype use_shard_placement_fn=use_shard_placement_fn group = dist distributed_c d _get_default_group orig_reduce_scatter = dist reduce_scatter_tensor assert_fn output torch Tensor assertEqual output dtype reduce_dtype reduce_scatter = functools partial reduce_scatter_with_assert orig_reduce_scatter assert_fn torch manual_seed + rank + inp = torch randn device=device_type type dtype=param_dtype iter_idx range optim zero_grad set_to_none= iter_idx == fsdp_loss = model inp sum patch_reduce_scatter reduce_scatter fsdp_loss backward optim step ref_optim zero_grad set_to_none= iter_idx == ref_loss = ref_model inp sum ref_loss backward param ref_model parameters param_grad = param grad reduce_dtype Use reduce-scatter - all-gather implement all-reduce since world size bf all-reduce reduce-scatter have numeric differences sharded_grad = funcol reduce_scatter_tensor param_grad scatter_dim= reduceOp= avg group=group bf reduction param grad = funcol all_gather_tensor sharded_grad gather_dim= group=group param dtype upcast fp ref_optim step fp optimizer step assertEqual fsdp_loss ref_loss check_sharded_parity ref_model model skip_if_lt_x_gpu test_grad_acc_with_reduce_dtype Tests gradient accumulation without reduce-scatter when using bf compute fp reduction accumulates unsharded gradients fp run_subtests reshard_after_forward True False _test_grad_acc_with_reduce_dtype _test_grad_acc_with_reduce_dtype reshard_after_forward bool torch manual_seed param_dtype reduce_dtype = torch bfloat torch float mp_policy = MixedPrecisionPolicy param_dtype=param_dtype reduce_dtype=reduce_dtype model = nn Sequential MLP torch device cpu _ range To emulate mixed precision implementation where forward backward compute use bf optimizer uses fp we maintain both fp bf copy reference model ref_model = copy deepcopy model device_type ref_model_compute = copy deepcopy ref_model param_dtype ref_optim = torch optim Adam ref_model parameters lr= e- mlp model fully_shard mlp reshard_after_forward=reshard_after_forward mp_policy=mp_policy fully_shard model reshard_after_forward=reshard_after_forward mp_policy=mp_policy optim = torch optim Adam model parameters lr= e- orig_reduce_scatter = dist reduce_scatter_tensor assert_fn output torch Tensor assertEqual output dtype reduce_dtype reduce_scatter = functools partial reduce_scatter_with_assert orig_reduce_scatter assert_fn torch manual_seed + rank + device = device_type Train same input avoid loss explosion num_microbatches = inp = torch randn num_microbatches device=device dtype=param_dtype iter_idx range microbatch_inps = torch chunk inp microbatch_idx range num_microbatches is_last_microbatch = microbatch_idx == num_microbatches - model set_requires_gradient_sync is_last_microbatch model set_reshard_after_backward is_last_microbatch reshard_after_forward losses list torch Tensor = _model ref_model_compute model losses append _model microbatch_inps microbatch_idx detach sum assertEqual losses - dtype param_dtype patch_reduce_scatter reduce_scatter losses - backward assertEqual losses losses Manually accumulate gradients into base reference model compute reference model fp ref_param ref_param_compute zip ref_model parameters ref_model_compute parameters assertTrue ref_param_compute grad None assertEqual ref_param dtype torch float ref_param grad None ref_param grad += ref_param_compute grad ref_param grad = ref_param_compute grad ref_param dtype ref_param_compute grad = None Manually reduce gradients reference model last microbatch implement data parallelism is_last_microbatch ref_param ref_model parameters assertTrue ref_param grad None dist all_reduce ref_param grad ref_param grad = world_size check_sharded_parity ref_model model ref_optim step optim step ref_optim zero_grad set_to_none= iter_idx == optim zero_grad set_to_none= iter_idx == Manually copy parameters base reference model compute reference model run optimizer step latter ref_param ref_param_compute zip ref_model parameters ref_model_compute parameters ref_param_compute detach copy_ ref_param TestFullyShardMixedPrecisionCasts FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu test_float _on_one_submodule x = torch zeros device=device_type Subtest use fp second child submodule -- does require any additional casting logic forward_inputs dict str nn Module = model = SaveForwardInputsModel forward_inputs cast_forward_inputs=False device_type fully_shard model c mp_policy=MixedPrecisionPolicy param_dtype=torch float fully_shard model model x sum backward assertEqual forward_inputs model dtype torch float assertEqual forward_inputs model c dtype torch float assertEqual forward_inputs model c dtype torch float Subtest use fp second child module where user module owns cast forward_inputs dict nn Module torch Tensor = model = SaveForwardInputsModel forward_inputs=forward_inputs cast_forward_inputs=True device_type fully_shard model c mp_policy=MixedPrecisionPolicy param_dtype=torch float cast_forward_inputs=False fully_shard model model x sum backward assertEqual forward_inputs model dtype torch float assertEqual forward_inputs model c dtype torch float assertEqual forward_inputs model c dtype torch float Subtest use fp first child module specify its output dtype so second child module does need cast forward_inputs dict nn Module torch Tensor = model = SaveForwardInputsModel forward_inputs=forward_inputs cast_forward_inputs=False device_type fully_shard model c mp_policy=MixedPrecisionPolicy param_dtype=torch float output_dtype=torch float fully_shard model model x sum backward assertEqual forward_inputs model dtype torch float assertEqual forward_inputs model c dtype torch float assertEqual forward_inputs model c dtype torch float skip_if_lt_x_gpu test_submodules_with_external_inputs run_subtests enable_submodule_cast False True _test_submodules_with_external_inputs _test_submodules_with_external_inputs enable_submodule_cast bool ToyModule nn Module __init__ forward_inputs dict str torch Tensor - None super __init__ l = nn Linear forward_inputs = forward_inputs forward x torch Tensor y torch Tensor - torch Tensor forward_inputs l _input_x = x forward_inputs l _input_y = y l x ToyModel nn Module __init__ forward_inputs dict str torch Tensor - None super __init__ l = nn Linear l = ToyModule forward_inputs forward_inputs = forward_inputs forward x torch Tensor - torch Tensor forward_inputs model_input_x = x y = torch ones device=device_type type dtype=torch float external input l l x y forward_inputs dict str torch Tensor = model = ToyModel forward_inputs device_type x = torch zeros device=device_type type dtype=torch float fully_shard model l mp_policy=MixedPrecisionPolicy param_dtype=torch float cast_forward_inputs=enable_submodule_cast fully_shard model mp_policy=MixedPrecisionPolicy param_dtype=torch float model x sum backward If we enable ` model l ` cast default then ` l _input_y ` gets cast fp we disable then says fp assertEqual forward_inputs model_input_x dtype torch float assertEqual forward_inputs l _input_x dtype torch float assertEqual forward_inputs l _input_y dtype torch float enable_submodule_cast torch float skip_if_lt_x_gpu requires_nccl_version Need NCCL + bf collectives test_norm_modules_bf mp_policy = MixedPrecisionPolicy param_dtype=torch bfloat _test_norm_modules mp_policy skip_if_lt_x_gpu test_norm_modules_fp mp_policy = MixedPrecisionPolicy param_dtype=torch float _test_norm_modules mp_policy _test_norm_modules mp_policy MixedPrecisionPolicy inner model nn Module x torch Tensor Run forward backward check no type mismatch errors z = model x assertEqual z dtype mp_policy param_dtype z sum backward Layer norm model = nn Sequential nn Linear nn LayerNorm nn Linear module model model model model fully_shard module mp_policy=mp_policy inner model torch randn Batch norm D model = nn Sequential nn Linear nn BatchNorm d nn Linear module model model model model fully_shard module mp_policy=mp_policy inner model torch randn Batch norm D error backward buffer dtype mismatch model = nn Sequential nn Conv d nn BatchNorm d nn Conv d module model model model model fully_shard module mp_policy=mp_policy TEST_HPU inner model torch randn assertRaisesRegex RuntimeError Expected running_mean have type Error seen HPUs hence can skipped Errors batch norm D backward inner model torch randn Batch norm D cast buffers down lower precision model = nn Sequential nn Conv d nn BatchNorm d nn Conv d module model model model model fully_shard module mp_policy=mp_policy Casting batch norm buffers lower precision allows backward model running_mean = model running_mean mp_policy param_dtype model running_var = model running_var mp_policy param_dtype inner model torch randn Batch norm D use special mixed precision policy model = nn Sequential nn Conv d nn BatchNorm d nn Conv d bn_mp_policy = MixedPrecisionPolicy output_dtype=mp_policy param_dtype fully_shard model mp_policy=bn_mp_policy module model model model fully_shard module mp_policy=mp_policy inner model torch randn skip_if_lt_x_gpu test_clamp_reduce_dtype Initialize model directly bf init_dtype = torch bfloat model = nn Sequential nn Linear dtype=init_dtype nn Linear dtype=init_dtype device_type type mp_policy = MixedPrecisionPolicy param_dtype=torch bfloat reduce_dtype=torch bfloat Check we did clamp reduce dtype assertEqual mp_policy reduce_dtype torch bfloat module model fully_shard module mp_policy=mp_policy fully_shard model mp_policy=mp_policy Check reduce-scatter runs bf even after we change model bf fp model torch float orig_reduce_scatter = dist reduce_scatter_tensor assert_fn output torch Tensor assertEqual output dtype torch bfloat reduce_scatter = functools partial reduce_scatter_with_assert orig_reduce_scatter assert_fn patch_reduce_scatter reduce_scatter inp = torch randn device=device_type type loss = model inp sum loss backward skip_if_lt_x_gpu test_dataclass_input dataclasses dataclass Input x torch Tensor Model nn Module __init__ args kwargs - None super __init__ args kwargs _layer = nn Linear forward input Input _layer input x mp_policy = MixedPrecisionPolicy torch bfloat torch bfloat torch bfloat True model = Model inp = Input torch randn device_type fully_shard model mp_policy=mp_policy loss = model inp sum loss backward __name__ == __main__ run_tests