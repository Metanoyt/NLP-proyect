Owner s module inductor torch torch _inductor config inductor_config torch nn functional F torch _dynamo utils same torch _inductor metrics utils torch _inductor test_case run_tests TestCase torch testing _internal common_utils instantiate_parametrized_tests parametrize torch testing _internal inductor_utils GPU_TYPE HAS_GPU TestBase TestCase setUp super setUp metrics reset check_numeric f args tol= e- ref = f args act = torch compile f args assertTrue same ref act tol=tol SkipPatternTest TestBase Illustate cases we skip mix-order reduction We skip cases like when outer reduction followed pointwise load un-reduced tensor inductor_config patch split_reductions=False test_dimension_too_close Skip two reduction size too close We require one reduction dimension much larger so we can split dimension make efficient f x out = x sum dim= out = x sum dim= out out x = torch randn device=GPU_TYPE torch compile f x assertEqual metrics generated_kernel_count inductor_config patch split_reductions=False test_skip_if_outer_reduction_followed_by_full_pointwise Skip now outer reduction followed pointwise node accessing original tensor Accessing reduced tensor fine e g support torch mean f x out = x sum dim= out = x sum dim= keepdim=True + x out out x = torch randn device=GPU_TYPE check_numeric f x assertEqual metrics codegen_mix_order_reduction instantiate_parametrized_tests MixOrderReductionTest TestBase parametrize name sum prod mean parametrize swap False True parametrize split_reductions False True parametrize shape test_mix_order_reduction name swap split_reductions shape torch prod does accept tuple dim argument name == prod len shape == skipTest Invalid combination f x outer_red len shape == reduction_fn x dim= assert len shape == reduction_fn x dim= swap outer_red reduction_fn x dim=- reduction_fn x dim=- outer_red reduction_fn = getattr torch name dtype = torch float x = torch randn shape dtype=dtype device=GPU_TYPE opt_f = torch compile f options= split_reductions split_reductions ref = f x act = opt_f x assertTrue same ref act tol= e- f ref \n ref \nact \n act assertEqual inductor_config triton mix_order_reduction metrics codegen_mix_order_reduction inductor_config patch unroll_reductions_threshold= test_ layer_split_reduction Use larger M smaller N trigger layer split reduction inductor_config triton mix_order_reduction skipTest Mix order reduction enabled f x x sum dim=- x sum dim= x = torch randn dtype=torch float device=GPU_TYPE check_numeric f x We don t do mix order reduction split redutions more than layers assertEqual metrics codegen_mix_order_reduction test_independent_split_size Make sure mix order reduction can pick split size wants inductor_config triton mix_order_reduction skipTest Mix order reduction enabled f x x sum dim=- x sum dim= check_one_split_size split_size torch _dynamo reset inductor_config patch triton mix_order_reduction_split_size split_size check_numeric f x assertEqual inductor_config triton mix_order_reduction metrics codegen_mix_order_reduction _ code = utils run_and_get_code torch compile f x assertTrue f RSPLIT_SIZE split_size code x = torch randn dtype=torch float device=GPU_TYPE check_one_split_size check_one_split_size inductor_config patch split_reductions=False test_non_contiguous_input f x x sum dim=- x sum dim= x = torch randn dtype=torch float device=GPU_TYPE permute check_numeric f x assertEqual inductor_config triton mix_order_reduction metrics codegen_mix_order_reduction inductor_config patch split_reductions=False test_multi_workspace_allocation f x y x sum dim= x sum dim= y sum dim= y sum dim= x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE check_numeric f x y expected_mix_order_reduction = inductor_config triton mix_order_reduction assertEqual expected_mix_order_reduction metrics codegen_mix_order_reduction parametrize wdtype torch bfloat extra down cast dw needed torch float parametrize split_reductions False True parametrize shape test_rms_norm_bwd wdtype split_reductions shape f x w eps orig_dtype = x dtype x = x float rsqrt = torch rsqrt x x sum dim=- x shape - + eps y = x rsqrt None w dtype=orig_dtype y fwd_bwd f x grad = None w grad = None out = f x w eps out backward dy x grad w grad torch manual_seed M N = M N = shape x = torch randn M N dtype=torch bfloat device=GPU_TYPE requires_grad=True w = torch randn N dtype=wdtype device=GPU_TYPE requires_grad=True dy = torch randn_like x eps = e- opt_f = torch compile f options= split_reductions split_reductions ref = fwd_bwd f act _ bwd_wrapper = utils run_and_get_code fwd_bwd opt_f assertTrue same ref act tol= e- f ref \n ref \nact \n act assertEqual inductor_config triton mix_order_reduction metrics codegen_mix_order_reduction parametrize wbdtype torch bfloat extra down cast dw db needed torch float parametrize split_reductions False True parametrize shape test_layer_norm_bwd_with_bias wbdtype split_reductions shape f x w b eps F layer_norm x x shape - w float b float eps fwd_bwd f x grad = None w grad = None b grad = None out = f x w b eps out backward dy x grad w grad b grad M N = M N = shape xdtype = torch float x = torch randn M N dtype=xdtype device=GPU_TYPE requires_grad=True w = torch randn N dtype=wbdtype device=GPU_TYPE requires_grad=True b = torch randn N dtype=wbdtype device=GPU_TYPE requires_grad=True dy = torch randn_like x eps = e- opt_f = torch compile f options= split_reductions split_reductions ref = fwd_bwd f act _ bwd_wrapper = utils run_and_get_code fwd_bwd opt_f assertTrue same ref act tol= e- f ref \n ref \nact \n act assertEqual inductor_config triton mix_order_reduction metrics codegen_mix_order_reduction parametrize split_reductions False True parametrize shape test_layer_norm_bwd_no_bias split_reductions shape f x w eps F layer_norm x x shape - w bias=None eps=eps fwd_bwd f x grad = None w grad = None out = f x w eps out backward dy x grad w grad M N = M N = shape xdtype = torch float wbdtype = torch float x = torch randn M N dtype=xdtype device=GPU_TYPE requires_grad=True w = torch randn N dtype=wbdtype device=GPU_TYPE requires_grad=True dy = torch randn_like x eps = e- opt_f = torch compile f options= split_reductions split_reductions ref = fwd_bwd f act _ bwd_wrapper = utils run_and_get_code fwd_bwd opt_f assertTrue same ref act tol= e- f ref \n ref \nact \n act assertEqual inductor_config triton mix_order_reduction metrics codegen_mix_order_reduction inductor_config patch triton mix_order_reduction inductor_config triton mix_order_reduction NoMixOrderReductionTest MixOrderReductionTest pass __name__ == __main__ HAS_GPU run_tests