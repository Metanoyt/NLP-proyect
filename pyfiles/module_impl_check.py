The purpose test check we have implementation parity between Python ` torch nn ` module its corresponding C++ ` torch nn ` module Concretely test does following Get test params dict common_nn py run forward backward Python module created using test params Serialize Python module s parameters buffers its forward input arguments deserialize them C++ load them into C++ module Run same forward backward passes C++ module serialize C++ module s forward output backward gradients Compare Python C++ module s forward output backward gradients If they same then we have implementation parity between Python C++ module os pprint tempfile types string Template torch cpp_api_parity sample_module SAMPLE_MODULE_CPP_SOURCE cpp_api_parity utils add_test compile_cpp_code_inline compute_arg_dict compute_cpp_args_construction_stmts_and_forward_arg_symbols compute_temp_file_path decorate_test_fn generate_error_msg is_torch_nn_functional_test move_python_tensors_to_device serialize_arg_dict_as_script_module set_python_tensors_requires_grad TORCH_NN_COMMON_TEST_HARNESS TorchNNModuleTestParams try_remove_folder torch jit _pickle restore_type_tag Expected substitutions $ module_variant_name e g ` Linear_no_bias_cpu ` $ module_qualified_name e g ` torch nn Linear ` $ cpp_args_construction_stmts $ cpp_constructor_args $ device $ cpp_forward_args_symbols TORCH_NN_MODULE_TEST_FORWARD_BACKWARD = Template void $ module_variant_name _test_forward_backward const std string arg_dict_file_path const std string module_file_path const std string forward_output_file_path const std string backward_grad_dict_file_path pybind gil_scoped_release no_gil Declare arguments auto arg_dict = load_dict_from_file arg_dict_file_path $ cpp_args_construction_stmts Construct module load params buffers Python module $ module_qualified_name module$ cpp_constructor_args module- std string $ device torch load module module_file_path Some modules such ` RReLU ` create random tensors their forward pass To make sure random tensors created same Python C++ we need set RNG seed manually torch manual_seed Forward pass auto cpp_output = module $ cpp_forward_args_symbols Save output into file compared Python later write_ivalue_to_file torch IValue cpp_output forward_output_file_path Backward pass cpp_output is_complex cpp_output sum abs backward cpp_output sum backward Put all gradients into c Dict save into file compared Python later c Dict std string torch Tensor grad_dict const auto param module- named_parameters torch Tensor grad = param value grad grad is_sparse grad_dict insert param key + _grad_indices grad coalesce indices grad_dict insert param key + _grad_values grad coalesce values grad_dict insert param key + _grad grad write_ivalue_to_file torch IValue grad_dict backward_grad_dict_file_path run_python_forward_backward unit_test_class test_params device = test_params device module = test_params test_instance constructor test_params test_instance constructor_args device inputs = set_python_tensors_requires_grad move_python_tensors_to_device arg_value _ arg_value test_params arg_dict input device inputs += move_python_tensors_to_device arg_value _ arg_value test_params arg_dict target device inputs += move_python_tensors_to_device arg_value _ arg_value test_params arg_dict extra_args device Some modules such ` RReLU ` create random tensors their forward pass To make sure random tensors created same Python C++ we need set RNG seed manually torch manual_seed Forward pass python_output = module inputs NOTE This workaround allow any module traced We can do because we only interested transferring Python module s parameters buffers C++ module module forward = types MethodType lambda input input module script_module = torch jit trace module torch tensor Backward pass python_output dtype is_complex python_output sum abs backward python_output sum backward Put all gradients into dict compared later python_grad_dict = name param module named_parameters grad = param grad grad is_sparse python_grad_dict name + _grad_indices = grad coalesce indices python_grad_dict name + _grad_values = grad coalesce values python_grad_dict name + _grad = grad script_module python_output python_grad_dict test_forward_backward unit_test_class test_params module_variant_name = test_params module_variant_name cpp_tmp_folder = test_params cpp_tmp_folder Remove temporary folder exists already try_remove_folder cpp_tmp_folder os mkdir cpp_tmp_folder Run forward backward Python module script_module python_output python_grad_dict = run_python_forward_backward unit_test_class test_params Save Python module arguments used C++ function module_file_path = compute_temp_file_path cpp_tmp_folder module_variant_name module arg_dict_file_path = compute_temp_file_path cpp_tmp_folder module_variant_name arg_dict script_module save module_file_path serialize_arg_dict_as_script_module test_params arg_dict save arg_dict_file_path cpp_test_name = f test_params module_variant_name _test_forward_backward cpp_test_fn = getattr unit_test_class module_impl_check_cpp_module cpp_test_name run_cpp_test_fn_and_check_output forward_output_file_path = compute_temp_file_path cpp_tmp_folder module_variant_name forward_output backward_grad_dict_file_path = compute_temp_file_path cpp_tmp_folder module_variant_name backward_grad_dict cpp_test_fn arg_dict_file_path module_file_path forward_output_file_path backward_grad_dict_file_path cpp_output = torch load forward_output_file_path weights_only need GLOBAL torch jit _pickle restore_type_tag torch serialization safe_globals restore_type_tag cpp_grad_dict = torch load backward_grad_dict_file_path Check forward outputs equal unit_test_class assertEqual python_output cpp_output msg=generate_error_msg forward output cpp_output python_output Check module parameter gradients equal after backward pass unit_test_class assertEqual len python_grad_dict len cpp_grad_dict msg=generate_error_msg parameters len cpp_grad_dict len python_grad_dict key python_grad_dict param_name = None suffix _grad _grad_indices _grad_values key endswith suffix param_name = key -len suffix break assert param_name None sparsity_str = sparse key endswith _grad_indices _grad_values dense unit_test_class assertTrue key cpp_grad_dict msg=generate_error_msg f Does module have parameter named ` param_name ` sparsity_str gradient False True unit_test_class assertEqual python_grad_dict key cpp_grad_dict key msg=generate_error_msg f ` param_name ` s sparsity_str gradient ` key ` cpp_grad_dict key python_grad_dict key run_cpp_test_fn_and_check_output Remove temporary folder stores C++ outputs try_remove_folder cpp_tmp_folder compute_module_name test_params_dict fullname = test_params_dict get fullname None fullname module_name = fullname split _ module_name = test_params_dict get module_name module_name process_test_params_for_module test_params_dict device test_instance_class module_name = compute_module_name test_params_dict test_params_dict constructor = test_params_dict get constructor getattr torch nn module_name test_instance = test_instance_class test_params_dict assert test_instance get_name startswith test_ Example output ` BCELoss_weights_cuda ` module_variant_name = test_instance get_name + _ + device device = cpu constructor_args test_params_dict assert cpp_constructor_args test_params_dict If ` constructor_args ` present test params dict enable C++ API parity test f ` cpp_constructor_args ` must present \n pprint pformat test_params_dict If you interested adding C++ API parity test please see \n NOTE How check NN module functional API parity between Python C++ frontends \n If please add ` test_cpp_api_parity=False ` test params dict file issue about TorchNNModuleTestParams module_name=module_name module_variant_name=module_variant_name test_instance=test_instance cpp_constructor_args=test_params_dict get cpp_constructor_args arg_dict=compute_arg_dict test_params_dict test_instance has_parity=test_params_dict get has_parity True device=device cpp_tmp_folder=tempfile mkdtemp write_test_to_test_class unit_test_class test_params_dict test_instance_class parity_table devices assert is_torch_nn_functional_test test_params_dict module_name = compute_module_name test_params_dict assert hasattr torch nn module_name f ` torch nn ` doesn t have module ` module_name ` If you adding new test please set ` fullname ` using format ` ModuleName_desc ` f set ` module_name ` using format ` ModuleName ` module test dict \n pprint pformat test_params_dict module_full_name = torch nn + module_name assert module_full_name parity_table torch nn f Please add ` module_full_name ` entry ` torch nn ` section ` test cpp_api_parity parity-tracker md ` f Discovered while processing\n pprint pformat test_params_dict device devices test_params = process_test_params_for_module test_params_dict=test_params_dict device=device test_instance_class=test_instance_class try_remove_folder test_params cpp_tmp_folder unit_test_name = f test_torch_nn_ test_params module_variant_name unit_test_class module_test_params_map unit_test_name = test_params test_fn test_forward_backward unit_test_class=self test_params=unit_test_class module_test_params_map _testMethodName test_fn = decorate_test_fn test_fn=test_fn test_cuda=test_params_dict get test_cuda True has_impl_parity=parity_table torch nn module_full_name test_params_dict get has_parity True device=device add_test unit_test_class unit_test_name test_fn generate_test_cpp_sources test_params template device = test_params device cpp_constructor_args = test_params cpp_constructor_args cpp_constructor_args = cpp_constructor_args = f cpp_constructor_args cpp_args_construction_stmts cpp_forward_args_symbols = compute_cpp_args_construction_stmts_and_forward_arg_symbols test_params test_cpp_sources = template substitute module_variant_name=test_params module_variant_name module_qualified_name=f torch nn test_params module_name cpp_args_construction_stmts= \n join cpp_args_construction_stmts cpp_constructor_args=cpp_constructor_args cpp_forward_args_symbols= join cpp_forward_args_symbols device=device test_cpp_sources Build all C++ tests together instead once per test build_cpp_tests unit_test_class print_cpp_source=False assert len unit_test_class module_test_params_map cpp_sources = TORCH_NN_COMMON_TEST_HARNESS + SAMPLE_MODULE_CPP_SOURCE functions = test_params unit_test_class module_test_params_map values cpp_sources += generate_test_cpp_sources test_params=test_params template=TORCH_NN_MODULE_TEST_FORWARD_BACKWARD functions append f test_params module_variant_name _test_forward_backward print_cpp_source print cpp_sources cpp_module = compile_cpp_code_inline name= module_impl_check cpp_sources=cpp_sources functions=functions unit_test_class module_impl_check_cpp_module = cpp_module