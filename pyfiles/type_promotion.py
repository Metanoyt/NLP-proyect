mypy allow-untyped-defs Owner s module onnx __future__ annotations abc dataclasses inspect logging typing Any TYPE_CHECKING torch torch _dispatch python torch _ops torch fx torch fx traceback fx_traceback torch _prims_common _refs torch _prims_common ELEMENTWISE_TYPE_PROMOTION_KIND wrappers _prims_common_wrappers torch _refs linalg _linalg_refs nn _nn_refs special _special_refs torch _refs nn functional _functional_refs torch fx experimental proxy_tensor torch onnx _internal fx _pass type_utils fx_type_utils torch utils _python_dispatch _pytree TYPE_CHECKING collections abc Callable Mapping Sequence types ModuleType torch _subclasses fake_tensor logger = logging getLogger __name__ _try_getclosurevars func try inspect getclosurevars func except TypeError None dataclasses dataclass TypePromotionSnapshot Type promotion snapshot fx node its inputs Contains promoted dtype args kwargs needs promoting Contains expected node output dtype args_dtypes Mapping int torch dtype Mapping arg position dtype promote kwargs_dtypes Mapping str torch dtype Mapping kwarg name dtype promote out_dtype torch dtype Expected output dtype node TypePromotionRule abc ABC Base type promotion rule per torch ops namespace op_name __init__ namespace str op_name str namespace = namespace op_name = op_name Make abstract well because subclass needs override __eq__ A overrides __eq__ does define __hash__ will have its __hash__ implicitly set None Ref https docs python org reference datamodel html#object __hash__ abc abstractmethod __hash__ - int abc abstractmethod __repr__ abc abstractmethod __eq__ other object - bool is_valid - bool Check rule valid This always returns module If module does exist will created module = getattr torch ops namespace py_op = getattr module op_name None py_op None logger warning Cannot find op s module s op_name namespace False isinstance py_op torch _ops OpOverloadPacket logger warning Op torch ops s s OpOverloadPacket got s namespace op_name type py_op False True abc abstractmethod preview_type_promotion args tuple kwargs dict - TypePromotionSnapshot Preview type promotion results provided set args kwargs Returns TypePromotionSnapshot object contains promoted dtypes arguments expected output dtype ElementwiseTypePromotionRule TypePromotionRule Defines how perform elementwise type promotion torch ops namespace op_name _USE_OPMATH bool = False Whether use opmath compute promoted input dtype If used upcasts will inserted everywhere lower precision models Set False have torchlib handle upcasts op implementation internally __init__ namespace str op_name str promote_args_positions Sequence int promote_kwargs_names Sequence str promotion_kind _prims_common ELEMENTWISE_TYPE_PROMOTION_KIND Constructs TypePromotionRule elementwise operators Args namespace Namespace op E g aten torch ops aten add op_name Name op E g add torch ops aten add promote_args_positions Positions args promote promote_kwargs_names Names kwargs promote promotion_kind Type promotion kind Refer _prims_common elementwise_dtypes https github com pytorch pytorch blob main torch _prims_common __init__ py detail noqa B super __init__ namespace op_name promote_args_positions = promote_args_positions promote_kwargs_names = promote_kwargs_names promotion_kind = promotion_kind __repr__ f ElementwiseTypePromotionRule namespace op_name f promote_args_positions promote_kwargs_names promotion_kind pyrefly ignore bad-override __eq__ other object - bool isinstance other ElementwiseTypePromotionRule False namespace == other namespace op_name == other op_name promote_args_positions == other promote_args_positions promote_kwargs_names == other promote_kwargs_names promotion_kind == other promotion_kind __hash__ - int f type namespace op_name __hash__ _consolidate_input_dtype computed_dtype torch dtype result_dtype torch dtype - torch dtype Although opmath right thing do retain on-par precision inserts upcasts everywhere graph This particularly hard backend optimize since there no way differentiate between inserted upcasts model code casts Hence we consolidate input dtype result dtype avoid _USE_OPMATH promotion_kind _prims_common ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT _prims_common ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT result_dtype computed_dtype preview_type_promotion args tuple kwargs dict - TypePromotionSnapshot candidate_args = i args i i promote_args_positions i len args args i None candidate_kwargs = name kwargs name name promote_kwargs_names name kwargs kwargs name None computed_dtype result_dtype = _prims_common elementwise_dtypes _pytree arg_tree_leaves candidate_args values candidate_kwargs type_promotion_kind=self promotion_kind consolidated_input_dtype = _consolidate_input_dtype computed_dtype result_dtype TypePromotionSnapshot dict fromkeys candidate_args keys consolidated_input_dtype dict fromkeys candidate_kwargs keys consolidated_input_dtype result_dtype DivElementwiseTypePromotionRule ElementwiseTypePromotionRule Reference type promotion rule torch _refs div Rule depends value ` rounding_mode ` argument __init__ super __init__ aten div promote_args_positions= promote_kwargs_names= promotion_kind=_prims_common ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT preview_type_promotion args tuple kwargs dict - TypePromotionSnapshot rounding_mode = kwargs get rounding_mode rounding_mode None true_divide promotion_kind = _prims_common ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT super preview_type_promotion args kwargs rounding_mode == trunc trunc_divide promotion_kind = _prims_common ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT super preview_type_promotion args kwargs rounding_mode == floor floor_divide promotion_kind = _prims_common ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT super preview_type_promotion args kwargs raise ValueError f Unknown rounding_mode rounding_mode ReductionTypePromotionRule TypePromotionRule __init__ namespace str op_name str promotion_kind _prims_common REDUCTION_OUTPUT_TYPE_KIND Constructs TypePromotionRule reduction operators Args namespace Namespace op E g aten torch ops aten sum op_name Name op E g sum torch ops aten sum promotion_kind Type promotion kind Refer _prims_common reduction_dtypes https github com pytorch pytorch blob main torch _prims_common __init__ py detail noqa B super __init__ namespace op_name promotion_kind = promotion_kind __repr__ f ReductionTypePromotionRule namespace op_name promotion_kind pyrefly ignore bad-override __eq__ other object - bool isinstance other ElementwiseTypePromotionRule False namespace == other namespace op_name == other op_name promotion_kind == other promotion_kind __hash__ - int f type namespace op_name __hash__ preview_type_promotion args tuple kwargs dict - TypePromotionSnapshot assert len args = f Reduction op torch ops namespace op_name expects least one argument arg = args assert isinstance arg torch Tensor f type arg = torch Tensor dtype torch dtype &#124; None = kwargs get dtype computation_dtype result_dtype = _prims_common reduction_dtypes arg promotion_kind dtype result_dtype None Inspecting code can only happen when ` promotion_kind ` ` KEEP_PROMOTED_TYPE ` Hence set same computation_dtype result_dtype = computation_dtype TypePromotionSnapshot computation_dtype result_dtype AllOrAnyReductionTypePromotionRule ReductionTypePromotionRule Reference type promotion rule torch ops aten all torch ops aten any This special case where computation dtype always torch bool The result dtype always uint ` dtype ` kwarg uint otherwise torch bool __init__ op_name str super __init__ aten op_name _prims_common REDUCTION_OUTPUT_TYPE_KIND ALWAYS_BOOL preview_type_promotion args tuple kwargs dict - TypePromotionSnapshot assert len args = f Reduction op torch ops namespace op_name expects least one argument arg = args assert isinstance arg torch Tensor f type arg = torch Tensor computation_dtype = torch bool Preserves uint -- probably legacy mask thing result_dtype = torch uint arg dtype == torch uint torch bool TypePromotionSnapshot computation_dtype result_dtype SumLikeReductionTypePromotionRule ReductionTypePromotionRule Reference type promotion rule torch ops aten sum This special case where computation dtype always torch int integral arg unless overridden ` dtype ` kwarg preview_type_promotion args tuple kwargs dict - TypePromotionSnapshot assert len args = f Reduction op torch ops namespace op_name expects least one argument arg = args assert isinstance arg torch Tensor f type arg = torch Tensor dtype torch dtype &#124; None = kwargs get dtype The below logic copied ` torch _refs __init__ py ` reduction ops impl dtype None _prims_common is_boolean_dtype arg dtype _prims_common is_integer_dtype arg dtype dtype = torch int dtype = arg dtype super preview_type_promotion args dtype dtype NOTE Update type promotion rule BELOW TABLE IS GENERATED FROM ` TypePromotionRuleSetGenerator generate_from_torch_refs ` DO NOT EDIT MANUALLY For missing rules discrepancies please Run ` pytest test onnx test_fx_type_promotion py ` validate generated rule set current If update new generated set If discrepancies still exist consider debugging torch _refs report bug If rules still missing add them ` _EXTRA_TYPE_PROMOTION_RULE_SET ` report bug Check ` TypePromotionRule ` how each rule defined used _GENERATED_ATEN_TYPE_PROMOTION_RULE_SET = ElementwiseTypePromotionRule aten abs ELEMENTWISE_TYPE_PROMOTION_KIND COMPLEX_TO_FLOAT ElementwiseTypePromotionRule aten abs_ ELEMENTWISE_TYPE_PROMOTION_KIND COMPLEX_TO_FLOAT ElementwiseTypePromotionRule aten acos ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten acos_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten acosh ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten acosh_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten add ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten add_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten addcdiv ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten addcdiv_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten addcmul ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten addcmul_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten addr ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten asin ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten asin_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten asinh ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten asinh_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten atan ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten atan ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten atan _ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten atan_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten atanh ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten atanh_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten bitwise_and ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_and_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_left_shift ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_left_shift_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_not ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_not_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_or ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_or_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_right_shift ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_right_shift_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_xor ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten bitwise_xor_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten cat ELEMENTWISE_TYPE_PROMOTION_KIND NO_OPMATH ElementwiseTypePromotionRule aten cauchy ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten cauchy_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten ceil ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten ceil_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten celu ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten celu_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten clamp ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten clamp_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten copysign ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten copysign_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten cos ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten cos_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten cosh ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten cosh_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten deg rad ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten deg rad_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten digamma ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten digamma_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten dot ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten elu ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten elu_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten eq ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten eq_ ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten erf ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten erf_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten erfc ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten erfc_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten erfinv ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten erfinv_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten exp ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten exp ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten exp _ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten exp_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten expm ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten expm _ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten exponential ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten exponential_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten fill ELEMENTWISE_TYPE_PROMOTION_KIND NO_OPMATH ElementwiseTypePromotionRule aten floor ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten floor_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten floor_divide ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten floor_divide_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten fmax ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten fmin ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten fmod ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten fmod_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten frac ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten frac_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten gcd ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten gcd_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten ge ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten ge_ ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten gelu ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten geometric ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten geometric_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten glu ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten gt ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten gt_ ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten hardtanh ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten heaviside ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten heaviside_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten huber_loss ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten hypot ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten hypot_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten i ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten i _ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten igamma ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten igamma_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten igammac ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten igammac_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten isfinite ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten isinf ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten isnan ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten isneginf ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten isposinf ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten isreal ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten l _loss ELEMENTWISE_TYPE_PROMOTION_KIND COMPLEX_TO_FLOAT ElementwiseTypePromotionRule aten lcm ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten lcm_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten le ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten le_ ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten leaky_relu ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten lerp ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten lerp_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten lgamma ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten lgamma_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten log ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten log ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten log _ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten log p ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten log p_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten log ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten log _ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten log_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten log_normal ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten log_normal_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten logaddexp ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten logaddexp ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten logical_and ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten logical_and_ ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten logical_not ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten logical_not_ ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten logical_or ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten logical_or_ ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten logical_xor ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten logical_xor_ ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten logit ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten logsumexp ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten lt ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten lt_ ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten maximum ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten minimum ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten mish ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten mish_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten mse_loss ELEMENTWISE_TYPE_PROMOTION_KIND COMPLEX_TO_FLOAT ElementwiseTypePromotionRule aten mul ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten mul_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten ne ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten ne_ ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten neg ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten neg_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten nextafter ELEMENTWISE_TYPE_PROMOTION_KIND NO_OPMATH ElementwiseTypePromotionRule aten nextafter_ ELEMENTWISE_TYPE_PROMOTION_KIND NO_OPMATH ElementwiseTypePromotionRule aten nll_loss ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten normal ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten pdist ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten poisson_nll_loss ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten prelu ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten rad deg ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten rad deg_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten reciprocal ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten reciprocal_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten relu ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten remainder ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten remainder_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten round ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten rsqrt ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten rsqrt_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten selu ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten selu_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten sgn ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten sgn_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten sigmoid ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten sigmoid_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten sign ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten sign_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten signbit ELEMENTWISE_TYPE_PROMOTION_KIND ALWAYS_BOOL ElementwiseTypePromotionRule aten sin ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten sin_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten sinc ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten sinc_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten sinh ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten sinh_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten smooth_l _loss ELEMENTWISE_TYPE_PROMOTION_KIND COMPLEX_TO_FLOAT ElementwiseTypePromotionRule aten softplus ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten sqrt ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten sqrt_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten square ELEMENTWISE_TYPE_PROMOTION_KIND BOOL_TO_LONG ElementwiseTypePromotionRule aten square_ ELEMENTWISE_TYPE_PROMOTION_KIND BOOL_TO_LONG ElementwiseTypePromotionRule aten sub ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten sub_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten tan ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten tan_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten tanh ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten tanh_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten threshold ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten threshold_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten true_divide ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten true_divide_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten trunc ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten trunc_ ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten vdot ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten where ELEMENTWISE_TYPE_PROMOTION_KIND NO_OPMATH ElementwiseTypePromotionRule aten xlogy ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT ElementwiseTypePromotionRule aten xlogy_ ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT Manually curated extra type promotion rules Please see NOTE Update type promotion rule before adding new rules _EXTRA_TYPE_PROMOTION_RULE_SET = torch _refs skips type promotion decoration ` clamp_min ` ` clamp_max ` since call routed decorated ` aten clamp ` op ElementwiseTypePromotionRule aten clamp_max promote_args_positions= promote_kwargs_names= promotion_kind=_prims_common ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT ElementwiseTypePromotionRule aten clamp_min promote_args_positions= promote_kwargs_names= promotion_kind=_prims_common ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT torch ops aten div Tensor_mode applies different type promotion rules depending value ` mode ` argument DivElementwiseTypePromotionRule Manually curating reduction ops since logic written inside op reference implementation AllOrAnyReductionTypePromotionRule all AllOrAnyReductionTypePromotionRule any ReductionTypePromotionRule aten amax promotion_kind=_prims_common REDUCTION_OUTPUT_TYPE_KIND SAME ReductionTypePromotionRule aten amin promotion_kind=_prims_common REDUCTION_OUTPUT_TYPE_KIND SAME torch ops aten mean special case does need type promotion ReductionTypePromotionRule aten std promotion_kind=_prims_common REDUCTION_OUTPUT_TYPE_KIND COMPLEX_TO_FLOAT ReductionTypePromotionRule aten std_mean promotion_kind=_prims_common REDUCTION_OUTPUT_TYPE_KIND COMPLEX_TO_FLOAT ReductionTypePromotionRule aten var promotion_kind=_prims_common REDUCTION_OUTPUT_TYPE_KIND COMPLEX_TO_FLOAT SumLikeReductionTypePromotionRule aten cumprod promotion_kind=_prims_common REDUCTION_OUTPUT_TYPE_KIND SAME SumLikeReductionTypePromotionRule aten cumsum promotion_kind=_prims_common REDUCTION_OUTPUT_TYPE_KIND SAME SumLikeReductionTypePromotionRule aten prod promotion_kind=_prims_common REDUCTION_OUTPUT_TYPE_KIND SAME SumLikeReductionTypePromotionRule aten sum promotion_kind=_prims_common REDUCTION_OUTPUT_TYPE_KIND SAME ElementwiseTypePromotionRuleSetGenerator Hackly distilling info reference ops decorated elementwise type promotion rule The goal retrieve decorator ` ` ` python elementwise_type_promotion_wrapper type_promoting_args= b type_promotion_kind=type_promotion_kind ` ` ` reference ops It provides info which arguments promoted what kind promotion applied classmethod generate_from_torch_refs cls - set ElementwiseTypePromotionRule Parse type promotion rules reference ops under torch _C _refs rule_set = set rule_set update cls _parse_torch_refs _refs rule_set update cls _parse_torch_refs _nn_refs rule_set update cls _parse_torch_refs _linalg_refs rule_set update cls _parse_torch_refs _special_refs rule_set update cls _parse_torch_refs _functional_refs rule_set classmethod _parse_torch_refs cls ref_module ModuleType - set ElementwiseTypePromotionRule logger info Processing module s ref_module __name__ rule_set = set name ref_module __all__ decorated_op = getattr ref_module name rule = cls _parse_type_promotion_rule_from_refs_op decorated_op rule None rule is_valid rule_set add rule rule_set classmethod _parse_type_promotion_rule_from_refs_op cls decorated_op Callable - ElementwiseTypePromotionRule &#124; None Retrieve parse type promotion decorator op under torch _refs fn = decorated_op type_promo_wrapper = None while fn_closure_vars = _try_getclosurevars fn fn fn_closure_vars nonlocals break fn_closure_vars nonlocals isinstance fn_closure_vars nonlocals _prims_common_wrappers elementwise_type_promotion_wrapper type_promo_wrapper = fn_closure_vars nonlocals break fn = fn_closure_vars nonlocals fn type_promo_wrapper None signature = inspect signature decorated_op pos = promote_args_positions = promote_kwargs_names = type_promo_wrapper type_promoting_arg_names None name param signature parameters items name type_promo_wrapper type_promoting_arg_names param kind param POSITIONAL_OR_KEYWORD param POSITIONAL_ONLY promote_args_positions append pos param kind == param KEYWORD_ONLY promote_kwargs_names append name pos += ElementwiseTypePromotionRule aten decorated_op __name__ promote_args_positions=promote_args_positions promote_kwargs_names=promote_kwargs_names promotion_kind=type_promo_wrapper type_promotion_kind logger warning Cannot find type promotion rule s s decorated_op __module__ decorated_op __name__ None TypePromotionTable Type promotion table torch ops __init__ _rule_table = rule _GENERATED_ATEN_TYPE_PROMOTION_RULE_SET add_rule rule rule _EXTRA_TYPE_PROMOTION_RULE_SET add_rule rule add_rule rule TypePromotionRule - None Add type promotion rule python op torch ops module Args rule Type promotion rule module Module containing op E g torch ops aten Raises ValueError If rule invalid rule is_valid raise ValueError f Invalid type promotion rule rule _rule_table f rule namespace rule op_name = rule get_rule py_op torch _ops OpOverloadPacket - TypePromotionRule &#124; None Get type promotion rule python op under torch ops namespace _rule_table get str py_op None get_type_promotion_rule node torch fx Node type_promotion_table TypePromotionTable - TypePromotionRule &#124; None Get type promotion rule node Args node Node get type promotion rule type_promotion_table Type promotion table Returns Type promotion rule node None no rule found node representing torch operator op = node target isinstance op torch _ops OpOverload None rule = type_promotion_table get_rule op overloadpacket None None rule _OpTraceDispatchMode _python_dispatch TorchDispatchMode Trace ops dispatched Utilize dispatch mechanism ` __torch_dispatch__ ` https dev-discuss pytorch org t what-and-why-is-torch-dispatch trace op overloads dispatched This used find compatible op overload given op overload packet different set args kwargs __init__ args kwargs super __init__ args kwargs traced_ops = __torch_dispatch__ func types args= kwargs=None traced_ops append func func args kwargs find_compatible_op_overload op torch _ops OpOverloadPacket args tuple kwargs dict - torch _ops OpOverload Find compatible OpOverload OpOverloadPacket using provided args kwargs Each call_function fx Node fx GraphModule has target represents torch _ops OpOverload The OpOverload contains OpOverloadPacket holds all available overloads operation During type promotion pass there cases where types args kwargs may change such promoting Python numbers tensors Consequently original OpOverload might compatible updated args kwargs This function used identify compatible OpOverload given args kwargs Args op OpOverloadPacket find compatible OpOverload args The positional arguments consider compatibility kwargs The keyword arguments consider compatibility Returns torch _ops OpOverload The compatible OpOverload found given args kwargs Raises RuntimeError If no compatible op overload found Examples torch packet = torch ops aten pow args = torch tensor find_compatible_op_overload packet args _overloadname Tensor_Scalar args = torch tensor torch tensor find_compatible_op_overload packet args _overloadname Tensor_Tensor Utilize dispatch mechanism find compatible op overload op_trace_dispatch_mode = _OpTraceDispatchMode op_trace_dispatch_mode op args kwargs assert len op_trace_dispatch_mode traced_ops = Expected least traced op got new_op_overload = op_trace_dispatch_mode traced_ops assert isinstance new_op_overload torch _ops OpOverload f Expected OpOverload got type new_op_overload assert new_op_overload overloadpacket == op f Expected same OpOverload packet got new_op_overload overloadpacket = op new_op_overload _TypePromotionInterpreter torch fx Interpreter Interpreter inserts type promotion each node __init__ module torch fx GraphModule type_promotion_table TypePromotionTable super __init__ module type_promotion_table = type_promotion_table _run_node_and_set_meta node - Any Run node set meta according ` fx_traceback get_current_meta ` This should used new nodes nodes have been modified By default ` Interpreter run_node ` does update ` node meta ` Set ` node meta ` current meta except ` node meta val ` which recomputed out = super run_node node Update interpreter env state new output value env node = out node meta update k v k v fx_traceback get_current_meta items k node meta node meta val = proxy_tensor extract_val out out _create_node graph torch fx Graph op_type str target torch fx node Target args tuple kwargs dict - torch fx Node Create node set its metadata assert op_type call_function call_method get_attr call_module placeholder output f Unexpected op_type op_type node = getattr graph op_type target args kwargs _run_node_and_set_meta node node _rerun_node_after_type_promotion node torch fx Node expected_out_dtype torch dtype - None Rerun node after type promotion update node meta val output value node_val = node meta get val None assert node_val None f Node node node meta val set args kwargs = fetch_args_kwargs_from_env node target = node target assert isinstance target torch _ops OpOverload f Expected OpOverload got type target node target = find_compatible_op_overload target overloadpacket args kwargs new_node_val = _run_node_and_set_meta node assert isinstance new_node_val type node_val f run_node output type should change between runs f Got type new_node_val expect type node_val isinstance node_val torch Tensor prev_node_dtype = node_val dtype assert prev_node_dtype == expected_out_dtype f node meta val dtype prev_node_dtype does agree f type promotion rule expected_out_dtype new_node_val dtype = expected_out_dtype With explicit type promotion expected result dtype may same computation dtype This referred op math We need explicitly cast output back expected dtype See more about op math topic ` _prims_common elementwise_dtypes ` graph = node graph graph inserting_after node output_cast_node = _create_node graph call_function torch ops prims convert_element_type default node dtype expected_out_dtype node replace_all_uses_with output_cast_node output_cast_node args = node logger info Node s output dtype becomes s due op math Cast back s node new_node_val dtype expected_out_dtype fx_type_utils is_torch_symbolic_type node_val raise NotImplementedError Type promotion does support node output sym types isinstance node_val list tuple raise NotImplementedError Type promotion does support node output list tuple raise RuntimeError f Unexpected node output type type node_val _maybe_promote_arg node torch fx Node fx_arg torch fx node Argument dtype torch dtype &#124; None - torch fx node Argument Promote fx_arg dtype necessary dtype None logger info Argument s promoted Not mentioned type promotion rule fx_arg fx_arg isinstance fx_arg torch fx Node arg_val = env fx_arg isinstance arg_val torch Tensor old_dtype = arg_val dtype = dtype Promote tensor dtype graph = node graph graph inserting_before node logger info Argument s s promoted s fx_arg old_dtype dtype _create_node graph call_function torch ops prims convert_element_type default fx_arg dtype dtype logger info Argument s promoted Already s fx_arg dtype fx_arg fx_type_utils is_torch_symbolic_type arg_val arg_type = type arg_val equivalent_dtype = fx_type_utils from_scalar_type_to_torch_dtype arg_type assert equivalent_dtype None f Unexpected arg_type arg_type equivalent_dtype = dtype Promote Sym number tensor dtype graph = node graph graph inserting_before node logger info Argument s Scalar equivalent dtype s promoted s fx_arg equivalent_dtype dtype _create_node graph call_function torch ops aten scalar_tensor default fx_arg dtype dtype logger info Argument s promoted Already s fx_arg dtype fx_arg equivalent_dtype = fx_type_utils from_scalar_type_to_torch_dtype type fx_arg None equivalent_dtype = dtype Promote number tensor dtype The op should have overload supports tensor arg otherwise type promotion rule should suggest promoting arg graph = node graph graph inserting_before node logger info Argument s Scalar equivalent dtype s promoted s fx_arg equivalent_dtype dtype _create_node graph call_function torch ops aten scalar_tensor default fx_arg dtype dtype logger info Argument s promoted Already s fx_arg dtype fx_arg isinstance fx_arg tuple list logger info Argument s tuple list Promoting each element fx_arg type fx_arg _maybe_promote_arg node fx_arg_elem dtype fx_arg_elem fx_arg raise NotImplementedError f Unknown fx arg type type fx_arg _maybe_promote_node node torch fx Node rule TypePromotionRule - torch fx Node Promote node inputs outputs according type promotion rule args kwargs = fetch_args_kwargs_from_env node type_promotion_info = rule preview_type_promotion args kwargs new_args = new_kwargs = i arg enumerate node args new_args append _maybe_promote_arg node arg type_promotion_info args_dtypes get i None name arg node kwargs items new_kwargs name = _maybe_promote_arg node arg type_promotion_info kwargs_dtypes get name None new_args = tuple new_args node args = new_args node kwargs = new_kwargs node args = new_args node kwargs = new_kwargs _rerun_node_after_type_promotion node type_promotion_info out_dtype node run_node n torch fx Node - Any This method override which inserts type promotion nodes needed For each ` call_function ` node initial check conducted determine type promotion rule applicable If relevant rule exists type casting nodes introduced corresponding arguments The OpOverload node updated one accommodates promoted types Should output type different type casting node inserted output The call ` super run_node node ` guaranteed invoked each node In case new modified nodes result ` super run_node node ` used update its ` node meta val ` value _set_current_node n rule = get_type_promotion_rule n type_promotion_table _maybe_promote_node n rule super run_node n InsertTypePromotion _pass Transform Explicitly insert type promotion ops graph Underneath main pass driven ` _TypePromotionInterpreter ` which subclass ` torch fx Interpreter ` interpret fx Graph perform insertion type promotion operations By re-running new modified nodes using interpreter we can update metadata specifically fake tensor stored under node meta val ensure reflects latest changes __init__ module torch fx GraphModule type_promotion_table TypePromotionTable &#124; None = None super __init__ module interpreter = _TypePromotionInterpreter module type_promotion_table TypePromotionTable _fetch_fake_args - Sequence fake_tensor FakeTensor &#124; float &#124; int &#124; bool &#124; torch SymInt &#124; torch SymFloat &#124; torch SymBool &#124; None Fetch fake args fx graph For each argument try fetch fake tensor matching placeholder node fake_args = node module graph nodes node op == placeholder try Meta value can torch Tensor int float bool torch SymInt torch SymFloat torch SymBool meta_value = _val = node meta get val None except RuntimeError e node users If placeholder used we can safely ignore put None placeholder meta_value = None raise RuntimeError Cannot fetch symbolic fake args fx graph InsertTypePromotion pass needs run pre-existing fake args Otherwise pass will produce inaccurate dynamic shape e fake_args append meta_value fake_args _run args kwargs - torch fx GraphModule assert args ` InsertTypePromotion ` deduces symbolic fake arguments graph It does accept concrete arguments input because pass requires re-running graph When executed newly faked concrete arguments pass loses symbolic dynamic shape information assert kwargs ` kwargs ` supported fake_args = _fetch_fake_args fake_mode = fake_mode assert fake_mode None Cannot detect fake_mode Use python dispatcher run through some python kernels which can better handle symints Without some SymInts can become static when there dynamic shapes dispatcher_mode = torch _dispatch python enable_python_dispatcher fake_mode dispatcher_mode fx_traceback preserve_node_meta interpreter run fake_args module