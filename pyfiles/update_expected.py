Update committed CSV files used reference points dynamo inductor CI Currently only cares about graph breaks so only saves those columns Hardcodes list job names artifacts per job builds lookup querying github sha finding associated github actions workflow ID CI jobs downloading artifact zips extracting CSVs filtering them Usage python benchmarks dynamo ci_expected_accuracy py sha pytorch commit has completed inductor benchmark jobs Known limitations - doesn t handle retry jobs CI same hash has more than one set artifacts gets first one argparse json os subprocess sys urllib io BytesIO itertools product pathlib Path urllib request urlopen zipfile ZipFile pandas pd requests WITH job SELECT job created_at time job name job_name workflow name workflow_name job id id job run_attempt run_attempt workflow id workflow_id FROM default workflow_job job final INNER JOIN default workflow_run workflow final workflow id = job run_id WHERE job name = ciflow_should_run AND job name = generate-test-matrix -- Filter out workflow_run-triggered jobs which have nothing do SHA AND workflow event = workflow_run -- Filter out repository_dispatch-triggered jobs which have nothing do SHA AND workflow event = repository_dispatch AND workflow head_sha = sha String AND job head_sha = sha String AND workflow repository full_name = repo String SELECT workflow_name workflowName job_name jobName CAST id String id run_attempt runAttempt CAST workflow_id String workflowId time job ORDER BY workflowName jobName ARTIFACTS_QUERY_URL = https console-api clickhouse cloud api query-endpoints lint-ignore c cdfadc- bb - -bbf - d e cd run format=JSON CSV_LINTER = str Path __file__ absolute parents tools linter adapters no_merge_conflict_csv_linter py query_job_sha repo sha params = queryVariables sha sha repo repo If you Meta employee go P get id secret Otherwise ask Meta employee give you id secret KEY_ID = os environ CH_KEY_ID KEY_SECRET = os environ CH_KEY_SECRET r = requests post url=ARTIFACTS_QUERY_URL data=json dumps params headers= Content-Type application json auth= KEY_ID KEY_SECRET r json data parse_job_name job_str part strip part job_str split parse_test_str test_str part strip part test_str strip split S _BASE_URL = https gha-artifacts s amazonaws com get_artifacts_urls results suites urls = r results r workflowName inductor inductor-periodic test r jobName build r jobName runner-determinator r jobName unit-test r jobName _ test_str = parse_job_name r jobName suite shard_id num_shards machine _ = parse_test_str test_str workflowId = r workflowId id = r id runAttempt = r runAttempt suite suites artifact_filename = f test-reports-test- suite - shard_id - num_shards - machine _ id zip s _url = f S _BASE_URL repo workflowId runAttempt artifact artifact_filename urls suite int shard_id = s _url print f suite shard_id num_shards s _url urls normalize_suite_filename suite_name strs = suite_name split _ subsuite = strs - timm subsuite subsuite = subsuite replace timm timm_models subsuite download_artifacts_and_extract_csvs urls dataframes = suite shard url urls items try resp = urlopen url subsuite = normalize_suite_filename suite artifact = ZipFile BytesIO resp read phase training inference name = f test test-reports phase _ subsuite csv try df = pd read_csv artifact open name df graph_breaks = df graph_breaks fillna astype int prev_df = dataframes get suite phase None dataframes suite phase = pd concat prev_df df prev_df None df except KeyError print f Warning Unable find name artifacts file url continuing except urllib error HTTPError print f Unable download url perhaps CI job isn t finished dataframes write_filtered_csvs root_path dataframes suite phase df dataframes items out_fn = os path join root_path f suite _ phase csv df to_csv out_fn index=False columns= name accuracy graph_breaks apply_lints out_fn apply_lints filename patch = json loads subprocess check_output sys executable CSV_LINTER filename patch get replacement open filename fd data = fd read replace patch original patch replacement open filename w fd fd write data __name__ == __main__ parser = argparse ArgumentParser description=__doc__ formatter_class=argparse RawDescriptionHelpFormatter parser add_argument sha args = parser parse_args repo = pytorch pytorch suites = f _ b b product aot_eager aot_inductor cpu_aot_inductor cpu_aot_inductor_amp_freezing cpu_aot_inductor_freezing cpu_inductor cpu_inductor_amp_freezing cpu_inductor_freezing dynamic_aot_eager dynamic_cpu_aot_inductor dynamic_cpu_aot_inductor_amp_freezing dynamic_cpu_aot_inductor_freezing dynamic_cpu_inductor dynamic_inductor dynamo_eager inductor huggingface timm torchbench root_path = benchmarks dynamo ci_expected_accuracy assert os path exists root_path f cd pytorch root ensure root_path exists results = query_job_sha repo args sha urls = get_artifacts_urls results suites dataframes = download_artifacts_and_extract_csvs urls write_filtered_csvs root_path dataframes print Success Now confirm changes csvs ` git add ` them satisfied