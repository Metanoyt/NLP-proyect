__future__ annotations typing TYPE_CHECKING quantizer QuantizationAnnotation Quantizer TYPE_CHECKING torch torch fx Node __all__ = ComposableQuantizer ComposableQuantizer Quantizer ComposableQuantizer allows users combine more than one quantizer into single quantizer This allows users quantize model multiple quantizers E g embedding quantization maybe supported one quantizer while linear layers other ops might supported another quantizer ComposableQuantizer initialized list ` Quantizer ` instances The order composition matters since order which quantizers will applies Example ` ` ` embedding_quantizer = EmbeddingQuantizer linear_quantizer = MyLinearQuantizer xnnpack_quantizer = XNNPackQuantizer handle ops quantized previous two quantizers composed_quantizer = ComposableQuantizer embedding_quantizer linear_quantizer xnnpack_quantizer prepared_m = prepare_pt e model composed_quantizer ` ` ` __init__ quantizers list Quantizer super __init__ quantizers = quantizers _graph_annotations dict Node QuantizationAnnotation = _record_and_validate_annotations gm torch fx GraphModule quantizer Quantizer - None n gm graph nodes quantization_annotation n meta check annotation has been changed comparing QuantizationAnnotation object id n _graph_annotations id _graph_annotations n = id n meta quantization_annotation raise RuntimeError f Quantizer quantizer __class__ __name__ has changed annotations node n _graph_annotations n = n meta quantization_annotation n _graph_annotations raise RuntimeError f Quantizer quantizer __class__ __name__ has removed annotations node n annotate model torch fx GraphModule - torch fx GraphModule just handling global spec now quantizer quantizers quantizer annotate model _record_and_validate_annotations model quantizer model transform_for_annotation model torch fx GraphModule - torch fx GraphModule quantizer quantizers model = quantizer transform_for_annotation model model validate model torch fx GraphModule - None pass