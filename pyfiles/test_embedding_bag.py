Owner s oncall distributed sys torch torch distributed dist torch distributed _shard shard_parameter torch testing _internal common_distributed requires_nccl skip_if_lt_x_gpu torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN torch testing _internal distributed _shard sharded_tensor ShardedTensorTestBase TEST_GPU_NUM with_comms torch testing _internal distributed _shard sharded_tensor _test_ops_common clone_module_parameter generate_chunk_sharding_specs_for_test generate_local_weight_sharding_params_for_test TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TestShardedEmbeddingBag ShardedTensorTestBase _run_sharded_embedding_bag spec input_size num_embeddings embedding_dim mode include_last_offset=False offset_size=None max_norm=None norm_type= padding_idx=None Use same seed torch manual_seed local_embedding_bag = torch nn EmbeddingBag num_embeddings embedding_dim mode=mode max_norm=max_norm norm_type=norm_type include_last_offset=include_last_offset padding_idx=padding_idx cuda rank sharded_embedding_bag = torch nn EmbeddingBag num_embeddings embedding_dim mode=mode max_norm=max_norm norm_type=norm_type include_last_offset=include_last_offset padding_idx=padding_idx Copy weights local embedding bag sharded_embedding_bag weight = clone_module_parameter local_embedding_bag weight Shard parameter shard_parameter sharded_embedding_bag weight spec Run sharded computation torch manual_seed rank inputs different each rank inp = torch randint num_embeddings tuple input_size cuda rank per_sample_weights = None mode == sum per_sample_weights = torch rand input_size cuda rank offsets = None len input_size == We need generate certain length offset each rank The current implementation dist API does support case when offset has different lengths input_size offset_size so while loop will too long while offsets None offsets size = offset_size offsets = torch randint input_size offset_size offsets = include_last_offset offsets - = input_size offsets = torch unique offsets sorted=True contiguous cuda rank If max_norm set we need ensure renorm has been applied across inputs all ranks max_norm None gathered_inputs = torch zeros_like inp _ range TEST_GPU_NUM dist all_gather gathered_inputs inp unique_inp = torch unique torch cat gathered_inputs offsets_dummy = torch tensor len unique_inp cuda rank local_embedding_bag unique_inp offsets=offsets_dummy sharded_output = sharded_embedding_bag inp offsets=offsets per_sample_weights=per_sample_weights Run local computation local_output = local_embedding_bag inp offsets=offsets per_sample_weights=per_sample_weights Compare local weight shared one ensure renorm expected max_norm None sharded_dim = spec dim sharded_weight = sharded_embedding_bag weight local_shards tensor start_pos chunk_size = generate_local_weight_sharding_params_for_test local_embedding_bag weight sharded_dim TEST_GPU_NUM spec rank local_weight_narrowed = local_embedding_bag weight narrow sharded_dim start_pos chunk_size assertEqual local_weight_narrowed sharded_weight Verify assertEqual local_output sharded_output Validate torch nn functional embedding_bag version local_output = torch nn functional embedding_bag inp local_embedding_bag weight offsets=offsets mode=mode per_sample_weights=per_sample_weights include_last_offset=include_last_offset max_norm=max_norm norm_type=norm_type padding_idx=padding_idx sharded_output = torch nn functional embedding_bag inp sharded_embedding_bag weight offsets=offsets mode=mode per_sample_weights=per_sample_weights include_last_offset=include_last_offset max_norm=max_norm norm_type=norm_type padding_idx=padding_idx assertEqual local_output sharded_output with_comms init_rpc=False skip_if_lt_x_gpu TEST_GPU_NUM requires_nccl test_sharded_embedding_bag_colwise spec generate_chunk_sharding_specs_for_test _test_sharded_embedding_bag_with_test_cases spec with_comms init_rpc=False skip_if_lt_x_gpu TEST_GPU_NUM requires_nccl test_sharded_embedding_bag_rowwise spec generate_chunk_sharding_specs_for_test _test_sharded_embedding_bag_with_test_cases spec _test_sharded_embedding_bag_with_test_cases spec sharded_dim _run_sharded_embedding_bag spec sum _run_sharded_embedding_bag spec mean _run_sharded_embedding_bag spec max _run_sharded_embedding_bag spec sum max_norm= _run_sharded_embedding_bag spec mean max_norm= norm_type= _run_sharded_embedding_bag spec max max_norm= norm_type= _run_sharded_embedding_bag spec sum padding_idx= _run_sharded_embedding_bag spec sum _run_sharded_embedding_bag spec max _run_sharded_embedding_bag spec sum offset_size= _run_sharded_embedding_bag spec mean offset_size= _run_sharded_embedding_bag spec max offset_size= _run_sharded_embedding_bag spec sum offset_size= include_last_offset=True _run_sharded_embedding_bag spec max offset_size= include_last_offset=True _run_sharded_embedding_bag spec sum offset_size= max_norm= _run_sharded_embedding_bag spec mean offset_size= max_norm= _run_sharded_embedding_bag spec max offset_size= max_norm= _run_sharded_embedding_bag spec sum padding_idx= _run_sharded_embedding_bag spec mean padding_idx= _run_sharded_embedding_bag spec max padding_idx= _run_sharded_embedding_bag spec sum offset_size= max_norm= padding_idx= _run_sharded_embedding_bag spec mean offset_size= max_norm= padding_idx= _run_sharded_embedding_bag spec max offset_size= max_norm= padding_idx= __name__ == __main__ run_tests