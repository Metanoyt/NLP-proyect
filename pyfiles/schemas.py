mypy allow-untyped-defs The various dataclasses Enums namedtuples etc used AOTAutograd This includes input output types metadata config function signatures etc __future__ annotations collections functools itertools dataclasses dataclass field enum Enum typing Any NewType Optional Protocol TYPE_CHECKING TypeVar Union torch torch utils _pytree pytree torch SymInt Tensor torch _subclasses FakeTensor torch _subclasses fake_tensor is_fake torch fx experimental _backward_state BackwardState torch utils _python_dispatch is_traceable_wrapper_subclass config functional_utils _check_if_mutation_can_be_in_graph ViewMetaSequence utils strict_zip TYPE_CHECKING contextlib collections abc Callable Iterable Sequence torch _guards Source torch _inductor output_code OutputCode torch _inductor utils InputType torch _ops OpOverload descriptors AOTInput AOTOutput graph_capture_wrappers JointFnHandle zip = strict_zip OutputType = Enum OutputType output alias non_alias output aliases input alias_of_input output input tensor is_input output has _base tensor which graph intermediate We need its _base graph output so its requires_grad info populated correctly Instructs runtime code regenerate current output base tensor graph_intermediates base_idx alias_of_intermediate_save_as_output Same above we don t need explicitly add its _base graph output because already graph output alias_of_intermediate Same above output s _base already user output Instructs runtime code regenerate current output base tensor user_outputs base_idx alias_of_intermediate_base_is_user_output See Note Intermediate Bases Optimization unsafe_view_alias output alias has custom autograd Function backward In case we don t want do view-replay since we won t able replay custom function Instead we ll treat output normally trace its backward into graph custom_function_view This stores info about every user output dataclass frozen=True OutputAliasInfo Tells us output regular non-aliased output alias forward input forward input special case alias_of_input alias intermediate aka alias output inner traced forward alias intermediate explicitly requires returning intermediate graph output alias intermediate where intermediate also user output output_type OutputType The raw type output torch Tensor SymInt etc raw_type type If above then - base_idx None If above then - Tells us base alias user_fwd_input base_idx This index into inputs before we make synthetic bases If above then - Tells us base alias output_graph_intermediates base_idx here refers index direct traced If above then - Tells us base alias output_user_fwds base_idx here refers index direct traced base_idx Optional int If Tensor what dynamic dims otherwise None dynamic_dims Optional set int requires_grad requires_grad bool Sequence ViewMeta objects Provides us means re-run view functions other tensors We need wrap actual list ViewMeta so we compare ViewMeta elements appropriately i e their type elements returned ` as_tuple ` call view_meta_sequence Optional ViewMetaSequence = None MutationType Enum NOT_MUTATED = MUTATED_IN_GRAPH = MUTATED_OUT_GRAPH = This tells us info about user inputs dataclass frozen=True InputAliasInfo is_leaf bool mutates_data bool mutates_metadata bool mutations_hidden_from_autograd bool mutations_under_no_grad_or_inference_mode bool mutation_inductor_storage_resize bool mutates_storage_metadata bool requires_grad bool keep_input_mutations bool __post_init__ mutates_storage_metadata For convenience we guarantee always true In practice If we call set_ then runtime there no need additionally fix up tensor metadata since our runtime call inp set_ updated_inp will already have right metadata assert mutates_metadata functools cached_property mutation_type - MutationType mutates_data mutates_metadata mutation_inductor_storage_resize MutationType NOT_MUTATED _check_if_mutation_can_be_in_graph keep_input_mutations mutates_data mutates_metadata mutations_hidden_from_autograd mutations_under_no_grad_or_inference_mode mutates_storage_metadata mutation_inductor_storage_resize requires_grad MutationType MUTATED_IN_GRAPH MutationType MUTATED_OUT_GRAPH dataclass MemoryFormatMeta For static shapes we assume tangents have same strideness outputs size Optional Sequence int = None stride Optional Sequence int = None For dynamic shapes we assume same memory format contiguous channels_last etc memory_format Optional torch memory_format = None staticmethod from_tensor t torch Tensor - Optional MemoryFormatMeta We only memorize expected memory format Traceable wrapper subclasses We can create restrided subclass tensor torch empty_strided works only dense tensors Dynamic shape tensors Support symbolic shapes implemented yet use_memory_format bool = torch _functorch config guess_tangent_strides_as_outputs is_traceable_wrapper_subclass t use_memory_format is_static_shape = True s itertools chain t shape t stride isinstance s int is_static_shape = False break use_memory_format = is_static_shape use_memory_format MemoryFormatMeta pyrefly ignore unbound-name memory_format=torch _prims_common suggest_memory_format t MemoryFormatMeta size=t size stride=t stride dataclass PlainTensorMeta unwrapped_idx int memory_format Optional MemoryFormatMeta = None dataclass SubclassCreationMeta Used AOTDispatch This dataclass gives us information we need reconstruct tensor subclass our flat inputs Why important The graph we d like trace out contains flat tensor inputs But user s original model may have subclass inputs outputs So we need wrap unwrap subclasses necessary translate between user s view subclass inps outs backend compiler s view graph no subclass args Complications arise mostly fact subclass can hold more than one inner tensor So given subclass input output we need carefully track which indices map subclass tensor corresponding dense-tensor-only graph In inner graph only takes dense tensor inputs maps first index tensors should go subclass wrapper flat_tensor_start_idx int arg_count inclusive arg_counts any inner tensor subclasses If I have TwoTensor both its inner elements TwoTensors then arg_count outer-most subclass will arg_count int Mark where symints included This flag only used one assertion wrap_tensor_subclasses included_subclass_symints bool meta attrs produced subclass s __tensor_flatten__ We need keep them around along outer_size outer_stride plumb them into __tensor_unflatten__ attrs dict str Union SubclassCreationMeta PlainTensorMeta outer_size Iterable Union None int torch SymInt outer_stride Iterable Union None int torch SymInt meta Any Stores original subclass itself This needed because we need autograd metadata original subclass guaranteed wrapper subclass holds fake tensor so holding onto runtime shouldn t leak memory This field nulled out after calling make_runtime_safe original_subclass Optional torch Tensor Used runtime determine subclass type so we don t need save original subclass original_subclass_type Optional type = None memory_format Optional MemoryFormatMeta = None compute_outer_size_and_stride all_args curr_start_idx int subclass_utils compute_symint_placeholders compute outer start_idx placeholders = compute_symint_placeholders outer has_symbolic = any placeholders has_symbolic start = curr_start_idx end = start_idx + sum placeholders it_args = iter all_args start end it_placeholders = iter placeholders pytree tree_map_only lambda _ next it_placeholders lambda _ next it_args outer start + len placeholders outer start_idx outer_size next_idx = compute outer_size curr_start_idx outer_stride _ = compute outer_stride next_idx outer_size outer_stride creation_fn all_args is_runtime bool inner_tensors = curr_start_idx = flat_tensor_start_idx attr creation_meta attrs items isinstance creation_meta PlainTensorMeta subclass = all_args curr_start_idx curr_start_idx += subclass = creation_meta creation_fn all_args is_runtime=is_runtime curr_start_idx += creation_meta arg_count inner_tensors attr = subclass is_runtime assert original_subclass_type None original_subclass_type = original_subclass_type original_subclass_type = type original_subclass is_runtime outer_size outer_stride = compute_outer_size_and_stride all_args curr_start_idx=curr_start_idx outer_size outer_stride = outer_size outer_stride rebuilt = original_subclass_type __tensor_unflatten__ type ignore attr-defined inner_tensors meta outer_size outer_stride is_runtime After wrapping up inner dense tensors into subclass we need make sure our new wrapper has correct autograd metadata since we ll tracing through autograd engine subclass We don t trace through autograd engine runtime though so no need compute extra metadata then torch _mirror_autograd_meta_to original_subclass rebuilt type ignore attr-defined rebuilt make_runtime_safe _make_size_runtime_safe x Union None int torch SymInt - Optional int dummy = - isinstance x torch SymInt Replace nested ints dummy value - NJT ignores outer_size outer_stride runtime dummy x node is_nested_int None x assert original_subclass None original_subclass_type = type original_subclass original_subclass = None Note NJT outer_size AOTDispatcher ` _make_size_runtime_safe ` replaces any nested int dummy value - prevent serializing SymInt runtime Internally nested tensor __tensor_unflatten__ designed safely ignore dummy value For more details see https github com pytorch pytorch blob ade e c e e dcc de da d torch nested _internal nested_tensor py#L -L noqa B outer_size = tuple map _make_size_runtime_safe outer_size outer_stride = tuple map _make_size_runtime_safe outer_stride Recurse nested subclass info creation_meta attrs values isinstance creation_meta SubclassCreationMeta creation_meta make_runtime_safe __post_init__ sanity assert make sure we don t leak memory assert is_fake original_subclass This encapsulates all aliasing + mutation info we need about forward graph See more detailed overview edge case handling https docs google com document d UoIh_SVrMy_b Sx ZaeOJttm P Qmyss rdBuyfoic edit NOTE This saved AOTAutogradCache If you adding elements make sure they covered warm cache tests dataclass eq=False ViewAndMutationMeta length = user inputs This gives us info about every input what sort mutation happened any input_info list InputAliasInfo length = user outputs This gives us info about every output mostly around whether aliases other tensors output_info list OutputAliasInfo length = number intermediate bases appended outputs end forward graph Note necessarily same thing len x x output_info x output_type == OutputType alias_of_intermediate Because outputs might share _base output s _base might itself another user output both cases we won t redundantly append bases end graph num_intermediate_bases int For inference only instructs us keep data-only input mutations directly graph keep_input_mutations bool length = inputs w data mutations + user outputs non_aliasing tensors + intermediate bases These FakeTensor potential SymInt outputs we traced our metadata pass user s forward function Their only use today pass them best-guess tangents when tracing joint Stashing them part our metadata makes simpler we want run our analysis pass once reuse output throughout AOTAutograd traced_tangents list Any TODO doc traced_tangents_descs list AOTInput Each these list telling us about subclasses inputs outputs grad_outs They used throughout AOTDispatch tell us how generate list subclass tensors Given potentially larger list plain torch tensors Taking subclass_inp_meta example subclass_inp_meta i = j int tells us The i th user input subclass corresponds inputs j plain-tensor graph subclass_inp_meta i = SubclassCreationMeta flat_tensor_start_idx= arg_count= The i th user input subclass holding two inner tensors which inputs inputs plain-tensor graph length = user inputs subclass_inp_meta list Union PlainTensorMeta SubclassCreationMeta So full set outputs forward graph looks something like mutated_inps user_outs intermediate_bases saved_for_bw_tensors where first those can subclasses saved_for_bw tensors since these internal compiler user visible so there s no point wrapping unwrapping them runtime This list contains subclass information all fw graph outputs except saved_for_bw_tensors subclass_fw_graph_out_meta list Union PlainTensorMeta SubclassCreationMeta length = backward graph inputs subclass_tangent_meta list Union PlainTensorMeta SubclassCreationMeta TODO we should kill need default break internal is_train bool = False length = inputs w data mutations + user outputs non_aliasing tensors + intermediate bases At runtime we don t keep traced_tangents around since they re serializable Instead we keep any necessary subclass metadata necessary about each traced_tangent This list generated after calling make_runtime_safe traced_tangent_metas Optional list Any = None num_symints_saved_for_bw Optional int = None The grad_enabled mutation will emitted runtime_wrapper epilogue NOTE AOTAutograd will assume ambient ` is_grad_enabled ` grad mode intended effect prior running graph keeping equivalence eager mode It responsibility upstream graph acquisition reset grad mode its pre-graph value prior calling aot_autograd grad_enabled_mutation Optional bool = None Keeps track whether ` torch use_deterministic_algorithms ` turned when forward run If deterministic mode turned off during forward turned during backward call then error raised deterministic Optional bool = None Keeps track which input indices store parameters which we will treat static static_input_indices list int = field default_factory=list Map effect type ex _EffectType ORDERED token If there side-effectful operators FunctionalTensorMode will populate dictionary telling us how many tokens we will need during tracing tokens dict Any torch Tensor = field default_factory=dict Only filled when we trace joint function If input requires grad mutated backward only safe keep mutation graph gradients disabled while backward runs grad mode disabled default when users run backward can turned create_graph=True At runtime during backward we use list indices error properly we find out safe include backward mutation graph indices_of_inputs_that_requires_grad_with_mutations_in_bw list int = field default_factory=list Indexes saved tensors which donated buffer Donated buffer means tensor alias any forward user input forward user output backward output bw_donated_idxs Optional list int = None Number tokens used backward appended end backward outputs Filled after tracing joint function num_backward_tokens int = Number rng states will get thread into forward backward cudagraph compatible run_and_save_rng num_graphsafe_rng_states int = graphsafe_rng_state_index Optional int = None __post_init__ pre-compute indices inputs mutated When keep_input_mutations set we don t need worry about our epilogue handling data-only mutations because we keep them directly graph mutated_inp_runtime_indices = i i m enumerate input_info m mutation_type == MutationType MUTATED_OUT_GRAPH mutated_graph_handled_indices = i i m enumerate input_info m mutation_type == MutationType MUTATED_IN_GRAPH mutated_graph_handled_indices = mutated_graph_handled_indices num_mutated_graph_handled_indices = len mutated_graph_handled_indices mutated_graph_handled_indices_seen_by_autograd = i i mutated_graph_handled_indices input_info i mutations_hidden_from_autograd mutated_graph_handled_indices_seen_by_autograd = mutated_graph_handled_indices_seen_by_autograd num_mutated_graph_handled_indices_seen_by_autograd = len mutated_graph_handled_indices_seen_by_autograd aliased_out_indices = i i m enumerate output_info m output_type OutputType non_alias OutputType unsafe_view_alias OutputType custom_function_view unsafe_view_out_indices = i i m enumerate output_info m output_type OutputType unsafe_view_alias This pre-computed post_init perf It contains index every element input_info corresponds mutation data metadata both mutated_inp_runtime_indices = mutated_inp_runtime_indices num_mutated_inp_runtime_indices = len mutated_inp_runtime_indices This pre-computed perf It contains index every element output_info corresponds alias either input intermediate aliased_out_indices = aliased_out_indices unsafe_view_out_indices = unsafe_view_out_indices num_outputs = len output_info num_outputs_non_aliased = len x x output_info x output_type OutputType non_alias OutputType unsafe_view_alias OutputType custom_function_view num_outputs_aliased_to_inputs = len x x output_info x output_type OutputType alias_of_input OutputType is_input num_unsafe_view_outputs = len unsafe_view_out_indices num_outputs_aliased_to_intermediates = len x x output_info x output_type OutputType alias_of_intermediate OutputType alias_of_intermediate_save_as_output OutputType alias_of_intermediate_base_is_user_output num_outputs_aliased = num_outputs_aliased_to_inputs + num_outputs_aliased_to_intermediates Record dynamic outputs Dynamo traced forward graph Mark them dynamic end runtime wrapper dynamic_outputs = any o dynamic_dims o output_info Record indices dynamic outputs partitioned forward graph Mark them dynamic runtime wrapper activation index - dynamic dims indices dynamic_saved_tensors_idxs dict int set int = See Note AOTAutograd Backward Guards This pre-computed fast asserts types our grad_outputs backward Eventually we should kill replace real backward guards we want precompute runtime types so replace FakeTensor torch Tensor output_types = torch Tensor isinstance x FakeTensor type x x traced_tangents is_rng_op_functionalized = config functionalize_rng_ops All above metadata collected tracing fw function However extra outputs rng offsets behave differently Both fwd bwd graphs have their own outputs total consumed offsets Unlike mutated inputs we don t have worry about sending right set tensors between fwd bwd Fwd bwd offsets independent simpler handle Therefore we track them separately num_outputs_rng_offset = is_rng_op_functionalized Our forward returns both tokens mutated_inputs outputs output_intermediate_bases saved_tensors saved_symints Tokens will split out before mutations view handling we do count them here num_forward_returns = num_mutated_inp_runtime_indices + num_outputs + num_intermediate_bases In case functionalization rng ops fw_module returns one additional output rng offset This rng offset used right away advance rng state passed raw outputs However we need know exact boundary identify which tensors saved bwd graph num_forward captures information num_forward = num_forward_returns + num_outputs_rng_offset make_runtime_safe There various fields ViewAndMutationMeta aren t serializable This function called after all tracing completed simplify certain fields metadata so they can safely cached Doing so may lose information case traced_tangents none information needed runtime TODO This function only best effort there other fields may cache safe i e there s no guarantee tensor_flatten returns serializable result SubclassCreationMeta cache safe assert traced_tangent_metas None extract_metadata t isinstance t torch Tensor is_traceable_wrapper_subclass t inner_tensors flatten_spec = t __tensor_flatten__ type ignore attr-defined Technically we only need flatten_spec inner tensors However some Tensor subclasses like TwoTensor may have flatten_spec = None And we want able assert metadata non-None distinguish between tensor subclass no metadata vs wasn t tensor subclass all inner_tensors flatten_spec None traced_tangent_metas = extract_metadata t t traced_tangents Clear traced tangents runtime traced_tangents = inp_meta subclass_inp_meta isinstance inp_meta SubclassCreationMeta inp_meta make_runtime_safe inp_meta subclass_fw_graph_out_meta isinstance inp_meta SubclassCreationMeta inp_meta make_runtime_safe inp_meta subclass_tangent_meta isinstance inp_meta SubclassCreationMeta inp_meta make_runtime_safe property tensors_saved_for_backwards_slice assert num_symints_saved_for_bw None num_symints_saved_for_bw slice num_forward -self num_symints_saved_for_bw slice num_forward None property symints_saved_for_backwards_slice assert num_symints_saved_for_bw None num_symints_saved_for_bw slice -self num_symints_saved_for_bw None slice empty slice __eq__ other isinstance other ViewAndMutationMeta NotImplemented input_info == other input_info output_info == other output_info num_intermediate_bases == other num_intermediate_bases keep_input_mutations == other keep_input_mutations is_rng_op_functionalized == other is_rng_op_functionalized num_outputs_rng_offset == other num_outputs_rng_offset len traced_tangents == len other traced_tangents all x shape == y shape x dtype == y dtype x y zip traced_tangents other traced_tangents num_backward_tokens == other num_backward_tokens dataclass eq=False SubclassMeta A copy all forward metadata computed dense tensor forward after desugaring subclasses So example user had model containing two ` TwoTensor ` inputs Then ` SubclassMeta fw_metadata input_infos ` would have length here fw_metadata ViewAndMutationMeta Note Computing Subclass Metadata about grad_inputs Given list flattened plain tensor grad_inputs tells us how reconstruct grad_input subclasses You might think why just assume all grad_inputs will have same subclass-ness original inputs AOTAutograd generally assumes other properties e g grad_outputs contiguous This doesn t really work though take example f DoubleTensor DenseTensor DoubleTensor DenseTensor In above example grad field both DoubleTensor DenseTensor will DoubleTensor When we trace out joint fw-bw graph we ll end up returning two subclasses two grad_inputs This means our backward graph will outputs two dense tensors each DoubleTensor grad_input we need properly store metadata tells us how turn these outputs back into DoubleTensors Note info cannot easily figured out ViewAndMutationMeta We can only compute info tracing entire joint examining grad_inputs we computed See Note AOTAutograd Backward Guards This will also eventually require us install backward guards case we made incorrect assumptions about subclass-ness our grad_outputs Optional field because we don t compute inference graphs grad_input_metas Optional list Union PlainTensorMeta SubclassCreationMeta = None __init__ - None The fields get set after its construction pass This exists because - autograd Function forward aot autograd returns outputs might alias inputs - we only care about metadata those aliases so we can regenerate them We do want them participate autograd Function We do wrapping them opaque so autograd Function does know treat them tensors dataclass frozen=True TensorAlias alias torch Tensor dataclass BackwardSignature Provides information about backward section exported joint forward-backward graph For particular fx GraphModule contains information A mapping each gradient backwards output parameter corresponds forward input A mapping each gradient backwards output user input corresponds forward input Which forward outputs corresponds loss we backprop Each string name ` node name ` corresponding node fx graph gradients_to_parameters dict str str gradients_to_user_inputs dict str str loss_output str GraphOutputName = NewType GraphOutputName str GraphInputName = NewType GraphInputName str FQN = NewType FQN str dataclass GraphSignature Provides information about exported module For particular fx GraphModule contains information Which graph inputs parameters buffers user inputs params buffers mapping name each graph argument its parameter buffer FQN original nn Module If there input mutations these represented extra outputs fx GraphModule We provide mapping these extra output names names actual inputs The pytree metadata how flatten unflatten inputs outputs The corresponding FX GraphModule only accepts returns pytree-flattened inputs outputs Optionally FX joint forward-backward graph we provide signature backward section joint graph parameters list FQN buffers list FQN user_inputs list GraphInputName user_outputs list GraphOutputName inputs_to_parameters dict GraphInputName FQN inputs_to_buffers dict GraphInputName FQN If user s module mutates buffer s represented graph extra graph output This dict mapping graph outputs correspond updated buffers FQN names those mutated buffers buffers_to_mutate dict GraphOutputName FQN parameters_to_mutate dict GraphOutputName FQN user_inputs_to_mutate dict GraphOutputName GraphInputName in_spec pytree TreeSpec out_spec pytree TreeSpec backward_signature Optional BackwardSignature input_tokens list GraphInputName output_tokens list GraphOutputName classmethod from_tracing_metadata cls in_spec pytree TreeSpec out_spec pytree TreeSpec graph_input_names list str graph_output_names list str view_mutation_metadata ViewAndMutationMeta named_parameters list str named_buffers list str num_user_inputs int num_user_outputs int trace_joint bool loss_index Optional int backward_signature Optional BackwardSignature - GraphSignature graph_inputs = graph_input_names graph_outputs = graph_output_names parameters = list named_parameters buffers = list named_buffers num_tokens = len view_mutation_metadata tokens Calling convention assumptions graph inputs = input_tokens params buffers user_inputs graph outputs = output_tokens mutated_inputs user_outs param_gradients If we capturing inference graph convention identical except param_gradients empty See Note Side-Effectful Tokens AOTAutograd information tokens Address input calling conventions start stop = num_tokens input_tokens = graph_inputs start stop start stop = stop stop + len parameters inputs_to_parameters = dict zip graph_inputs start stop parameters start stop = stop stop + len buffers inputs_to_buffers = dict zip graph_inputs start stop buffers start stop = stop stop + num_user_inputs user_inputs = graph_inputs start stop We should ve gone through all inputs now assert len graph_inputs - stop == Address output calling conventions start stop = num_tokens output_tokens = graph_outputs start stop names = input_tokens parameters buffers user_inputs mutations = idx input_info enumerate view_mutation_metadata input_info input_info mutates_data trace_joint Only buffers can mutated parameters assert idx = len parameters mutations append names idx + num_tokens assert len mutations == view_mutation_metadata num_mutated_inp_runtime_indices start stop = stop stop + view_mutation_metadata num_mutated_inp_runtime_indices outputs_to_mutations = dict zip graph_outputs start stop mutations user_inputs_to_mutate = buffers_to_mutate = parameters_to_mutate = output_name mutation_name outputs_to_mutations items mutation_name user_inputs pyrefly ignore unsupported-operation user_inputs_to_mutate output_name = mutation_name assert mutation_name buffers mutation_name parameters mutation_name buffers pyrefly ignore unsupported-operation buffers_to_mutate output_name = mutation_name pyrefly ignore unsupported-operation parameters_to_mutate output_name = mutation_name start stop = stop stop + num_user_outputs user_outputs = graph_outputs start stop unused_outputs = len graph_outputs - stop backward_signature None unused_outputs -= len backward_signature gradients_to_parameters + len backward_signature gradients_to_user_inputs assert unused_outputs == GraphSignature parameters=parameters type ignore arg-type buffers=buffers type ignore arg-type user_inputs=user_inputs type ignore arg-type user_outputs=user_outputs type ignore arg-type inputs_to_buffers=inputs_to_buffers type ignore arg-type inputs_to_parameters=inputs_to_parameters type ignore arg-type user_inputs_to_mutate=user_inputs_to_mutate buffers_to_mutate=buffers_to_mutate type ignore arg-type parameters_to_mutate=parameters_to_mutate type ignore arg-type in_spec=in_spec out_spec=out_spec backward_signature=backward_signature input_tokens=input_tokens type ignore arg-type output_tokens=output_tokens type ignore arg-type dataclass AOTAutogradCacheInfo cache_key str start_time_ns int forward_symints list torch SymInt dataclass AOTConfig Configuration AOTDispatcher fw_compiler Callable bw_compiler Callable partition_fn Callable decompositions dict OpOverload Callable num_params_buffers int aot_id int keep_inference_input_mutations bool is_export bool = False no_tangents bool = False dynamic_shapes bool = False aot_autograd_arg_pos_to_source Optional list Source = None static_input_indices Optional list int = None inference_compiler Optional Callable = None enable_log bool = True always false outside export pre_dispatch bool = False Key use AOTAutogradCache cache_info Optional AOTAutogradCacheInfo = None If we should ignore shape_env ambient tracing_context The net effect dynamic shapes we end up specializing example_inputs Used only standalone_compile ignore_shape_env bool = False precompile_backend_id Optional str = None force_non_lazy_backward_lowering bool = False This config makes sure check certain things like mutating input req_grad export joint tracing export_trace_joint bool = False disable_functionalization bool = False __post_init__ pre_dispatch assert is_export Can only have pre_dispatch IR export TODO types here plain_tensor_trace_fn when joint has tuple structure trace info too TODO needs generic parameterized AOTDescriptor SubclassTracingInfo = collections namedtuple SubclassTracingInfo plain_tensor_trace_fn plain_tensor_args plain_tensor_args_descs maybe_subclass_meta dataclass AOTState When we run AOTAutograd encapsulates state compiler which must preserved across stages This state traditional sense environment because some values structure change we progress through pipelines AOTAutograd Whether we need handle autograd when doing graph capture compilation Although calling convention non-autograd graph capture AOTAutograd simple can relied upon autograph capture calling convention quite complicated general you only expected pass aot_stage _compile process needs_autograd bool The FAKE flat arguments which we will do tracing Although you might naively expect immutable s when we perform tracing we may execute code modifies metadata inputs causing args become invalid It s also nontrivial have golden set fake values deepcopy them just time when you might destructively mutate them Voz I tried very hard do So we just periodically renew field Don t worry too much about unless you re specifically trying track down input metadata mutation bug By way NEVER joint inputs Those only ever go AOTGraphCapture flat_args list FxValue The descriptor each argument flat_args flat_args_descs list AOTInput This contains view mutation information about function which we detected doing initial trace when we created state fw_metadata ViewAndMutationMeta Top-level configuration This morally immutable sometimes we naughty mutate aot_config AOTConfig When performing AOTAutograd traces other passes we typically require lot active context managers most typically these either ensure we faithfully replicating original PyTorch context managers toggle some behaviors PyTorch make more suitable tracing When you use AOTState you re expected have created ExitStack entered then while we running AOTAutograd we will add things onto stack necessary When you re all done processing AOTAutograd you can exit stack All functions take AOTState expect ExitStack have been exited yet TODO We potentially could offer resumable context manager where you can cancel reenable later when you need stack contextlib ExitStack FxValue = Union Tensor int SymInt BackwardState CompilerWrapper AOTAutograd needs do many transformations calling convention user function tracing e g deduplicating inputs unpacking subclasses etc CompilerWrapper lets us factor these into compositional stages so we can handle each transformation incrementally instead having do all once Since there calling convention change there two parts wrpaper The prologue which about compile-time behavior given original function what new function modified calling convention we should trace AOTAutograd get FX graph we will do joint passes partitioning ultimate Inductor compilation We get flat_fn flat_args original function under trace inputs we going feed produce new function new inputs feed The epilogue which about run-time behavior we have now compiled modified calling convention function we need wrap so we have new function has original calling convention original function so our users can call old signature they expected We get compiled_fn real arguments newly compiled function we need wrap Note about caching we do NOT directly serialize runtime wrappers instead they reapplied compiled_fn after we have finished deserializing compiled_fn Extra metadata needed compute pre post compile can passed via attributes pre_compile flat_fn flat_args list FxValue flat_args_descs list AOTInput aot_config AOTConfig fw_metadata ViewAndMutationMeta - tuple Callable list FxValue list AOTInput ViewAndMutationMeta Process inputs compiler_fn You can pass extra metadata via kwargs Args flat_fn The function compile flat_args Metadata example inputs function compile aot_config AOTConfig passed compile time fw_metadata ViewAndMutationMeta generated flat_fn flat_args flat_fn flat_args flat_args_descs fw_metadata post_compile compiled_fn aot_config runtime_metadata - Callable Given output compiler wrap information received prologue Args compiled_fn Callable after calling compiler_fn aot_config AOTConfig after calling prologue runtime_metadata ViewAndMutationMeta after calling all wrappers s pre_compile steps Example wrapped_compiled_fn args do something args aot_config fw_metadata compiled_fn args wrapped_compiled_fn compiled_fn InductorWrapper This sort like CompilerWrapper happens different part lifecycle talks about transformations we do traced partitioned FX graph before we send Inductor compiler Once again there two parts The prologue which modifies FX graph before we send Inductor I say modifies because we don t really actually do anything nontrivial either our two implementations The epilogue which modifies compiled function produced Inductor Although hypothetically these wrappers could used compositionally centralized wrappers list practice they seem just invoked manually when needed NB The flat_args input sometimes mutated This probably naughty whatever pre_compile fw_module torch fx GraphModule flat_args list Tensor aot_config AOTConfig fw_metadata ViewAndMutationMeta - None Process inputs compiler_fn You can pass extra metadata via kwargs Args flat_fn The function compile flat_args Metadata example inputs function compile aot_config AOTConfig passed compile time fw_metadata ViewAndMutationMeta generated flat_fn flat_args post_compile compiled_fn aot_config runtime_metadata - Callable Given output compiler wrap information received prologue Args compiled_fn Callable after calling compiler_fn aot_config AOTConfig after calling prologue runtime_metadata ViewAndMutationMeta after calling all wrappers s pre_compile steps Example wrapped_compiled_fn args do something args aot_config fw_metadata compiled_fn args wrapped_compiled_fn compiled_fn dataclass AOTGraphCapture Produced aot_stage _graph_capture AOTAutograd typically operates taking complicated graphs desugaring them into simpler graphs use PyTorch features These wrappers establish invariants so when we actually do tracing we can assume these invariants hold leading simpler tracing implementation However means we have keep track how enter exit these wrappers when passing inputs into compiled graph among other things wrappers list CompilerWrapper The actual captured graph module In some circumstances export graph has specific calling convention can relied upon external callers In other situations calling convention unspecified only aot_stage _compile knows how deal them graph_module torch fx GraphModule When compiling autograd support joint_inputs which larger than original flat_args all tangents get inputs The tuple organizes into primals tangents When autograd s just plain list updated_flat_args Union list Any tuple list Any list Any updated_flat_args_descs Union list AOTInput tuple list AOTInput list AOTInput Metadata about subclass inputs outputs graph trace maybe_subclass_meta Any FakifiedFlatArgs = NewType FakifiedFlatArgs list Any TOutputCode = TypeVar TOutputCode bound= OutputCode AOTDispatchCompiler Protocol Represents fw bw_compiler passed AOTAutograd __call__ gm torch fx GraphModule example_inputs Sequence InputType - Any TODO bikeshed name SerializableAOTDispatchCompiler AOTDispatchCompiler Represents AOTDispatchCompiler returns OutputCode therefore cacheable SerializableAOTDispatchCompiler always OutputCode A _CompileFxCallable usually gets converted into AOTDispatchCompiler after binding all kwargs _CompileFxKwargs __init__ output_code_ty type TOutputCode compiler_fn Callable torch fx GraphModule Sequence InputType TOutputCode pyrefly ignore invalid-type-var output_code_ty = output_code_ty pyrefly ignore invalid-type-var compiler_fn = compiler_fn __call__ gm torch fx GraphModule example_inputs Sequence InputType - OutputCode compiler_fn gm example_inputs FlatFn Protocol __call__ args FxValue - list FxValue TraceFn Protocol __call__ args FxValue - tuple list FxValue list AOTOutput PreppedForAutogradTraceFn Protocol __call__ args FxValue - tuple tuple list FxValue list bool list AOTOutput JointTraceFn Protocol handle JointFnHandle __call__ primals list FxValue tangents list FxValue - tuple tuple list FxValue list Optional Tensor tuple list AOTOutput list Optional AOTOutput dataclass JointWithDescriptors _aot_state AOTState _aot_graph_capture AOTGraphCapture The exact order parameters buffers expected passed into final compiled function Parameters before buffers params_spec list str buffers_spec list str in_spec pytree TreeSpec out_spec pytree TreeSpec property graph_module _aot_graph_capture graph_module graph_module setter graph_module value _aot_graph_capture graph_module = value