mypy allow-untyped-defs contextlib dataclasses functools hashlib inspect itertools json logging math operator os re sys textwrap time collections abc Sequence concurrent futures as_completed ThreadPoolExecutor io StringIO pathlib Path types ModuleType typing Any Callable NamedTuple Optional TYPE_CHECKING Union typing_extensions Self unittest mock patch sympy torch torch _inductor async_compile noqa F required warm up AsyncCompile pools torch _dynamo device_interface get_interface_for_device torch _dynamo testing rand_strided torch _dynamo utils counters dynamo_timed get_chromium_event_logger identity preserve_rng_state torch _inductor await_utils await_sync torch _inductor utils clear_on_fresh_cache torch utils _filelock FileLock torch utils _ordered_set OrderedSet utils _sympy functions CeilDiv config ir autotune_process TensorMeta TritonBenchmarkRequest TritonCPUBenchmarkRequest TritonGPUBenchmarkRequest codecache code_hash PersistentCache PyCodeCache codegen common CSEVariable IndentedBuffer KernelTemplate OpOverrides WorkspaceArg WorkspaceZeroMode codegen simd_kernel_features SIMDKernelFeatures codegen subgraph SubgraphChoiceCaller codegen triton gen_common_triton_imports texpr TMACompatibilityChecker TritonKernel TritonScheduling codegen triton_utils config_of equal_ _arg_indices signature_to_meta codegen wrapper pexpr exc CUDACompileError fx_utils count_flops_fx ir ChoiceCaller PrimitiveInfoType ops_handler StoreMode runtime benchmarking benchmarker runtime hints DeviceProperties runtime triton_compat HAS_WARP_SPEC runtime triton_heuristics FixedGrid utils ceildiv do_bench_using_profiling FakeIndentedBuffer get_dtype_size is_gpu Placeholder restore_stdout_stderr sympy_dot sympy_index_symbol sympy_product triton_type triton_type_to_torch unique virtualized V log = logging getLogger __name__ correctness checks struggle fp tf VERIFY dict str Any = PRINT_AUTOTUNE = True DEBUG = False TYPE_CHECKING concurrent torch _inductor codegen simd IterationRangesEntry IterationRangesRoot codegen common CSE KernelNamespace pass these objects imported generated wrapper code extern_kernels = KernelNamespace dataclasses dataclass BenchmarkTensors Represents set inputs outputs autotuning template input_tensors list torch Tensor output_tensor Optional torch Tensor unpack input_tensors output_tensor dataclasses dataclass AutotuneArgs During autotuning we need pass same inputs all choices Note Since we typically have mix external choices triton choices we create two lists inputs same underlying buffers - External inputs aten kernels Include offset sliced tensors - Triton inputs Use base pointer sliced tensors without offset triton BenchmarkTensors extern BenchmarkTensors expected Optional torch Tensor = None get_benchmark_tensors extern=False - BenchmarkTensors Returns inputs output tensors given choice bench_tensors = extern extern triton bench_tensors classmethod from_choice_args cls example_inputs list torch Tensor example_inputs_extern list torch Tensor out torch Tensor out_extern torch Tensor expected Optional torch Tensor = None - Self Factory method create AutotuneInputs separate inputs outputs cls triton=BenchmarkTensors example_inputs out extern=BenchmarkTensors example_inputs_extern out_extern expected=expected verify kwargs Verify correctness benchmarking results torch testing assert_close extern output_tensor expected kwargs PartialRender Some parts template need generated end inserted into template start This allows doing bunch replacements after initial render HookFn = Callable str __init__ code str replacement_hooks dict str Optional HookFn - None super __init__ _code str = code replacement_hooks dict str Optional PartialRender HookFn = replacement_hooks property code - str The fully rendered code Will error any hooks have yet finalized remaining_active_hooks = key key fn replacement_hooks items fn None assert len remaining_active_hooks == f The following hooks have yet been finalized \n remaining_active_hooks= _code finalize_hook hook_key str strict bool = True - None Finalize hook name param strict If ` ` True ` ` raise error hook wasn t found NOTE Will error hook has already been finalized hook_key replacement_hooks strict raise RuntimeError f hook_key registered replacement_hooks hook = replacement_hooks hook_key assert hook None f Hook key hook_key can only called once _code = _code replace hook_key hook replacement_hooks hook_key = None finalize_remaining - str Finalize remaining active hooks This function can used cases where caller uses ` finalize_hook ` rather than ` finalize_all ` Note ` finalize_all ` errors hook has already been finalized attempted called again This function only attempts finalize active hooks key fn replacement_hooks items fn None finalize_hook key code finalize_all - str Finalize all active hooks NOTE unlike ` ` finalize_remaining ` ` method will error any hook has already been finalized key replacement_hooks finalize_hook key code This used store info needed lowering each subgraph triton templates dataclasses dataclass SubgraphInfo body IndentedBuffer template_mask Optional str = None template_out_shape Optional Union str tuple str = None compute IndentedBuffer = dataclasses field default_factory=IndentedBuffer indexing_code IndentedBuffer = dataclasses field default_factory=IndentedBuffer loads IndentedBuffer = dataclasses field default_factory=IndentedBuffer stores IndentedBuffer = dataclasses field default_factory=IndentedBuffer ops_handler Optional V WrapperHandler = None type ignore name-defined cse Optional CSE Any = None only copied over None range_trees Optional list IterationRangesRoot = None range_tree_nodes Optional dict sympy Symbol IterationRangesEntry = None numels Optional dict str sympy Expr = None __post_init__ only_copy_if_non_none_fields = range_trees range_tree_nodes numels cse to_dict field name getattr field name field dataclasses fields ModificationWrapper V WrapperHandler type ignore name-defined Handles placeholder substitutions during subgraph processing __init__ kernel subgraph_number int fixed_inputs dict str Any mask Optional str super __init__ V ops name = f PlaceholderSubstitution_ subgraph_number kernel = kernel fixed_inputs = fixed_inputs mask = mask load name str index sympy Expr Handle loading tensor fixed input name fixed_inputs index_str = _process_indexing index var = _add_kernel_input name buffer = V graph get_buffer name var_dtype = buffer dtype line = f tl load var + index_str var_dtype torch float torch bfloat config triton codegen_upcast_to_fp line += tl float var_dtype = torch float out = kernel cse generate kernel compute line dtype=var_dtype shape= out kernel cse generate kernel compute f fixed_inputs name dtype=torch float shape= indirect_indexing index_var str size check wrap_neg=True Convert index variable symbolic form sympy_index_symbol str index_var pyrefly ignore bad-override store name str index sympy Expr value CSEVariable mode StoreMode = None - str Currently only supports stores atomic adds coming scatter nodes This used flex_attention s backwards grad captured buffers see zeros_and_scatter lowering assert mask None Mask required inner stores modifications assert mode == atomic_add Only atomic_add supported inner stores buf_name = _add_kernel_input name index_str = _process_indexing index index_str = f tl broadcast_to index_str value shape store = f tl atomic_add buf_name + index_str value mask sem= relaxed store _add_kernel_input name str Add name input kernel input ref kernel args input name _process_indexing index Process rename indexing adding symbols kernel inputs kernel kexpr kernel rename_indexing index Function name followed args kwargs RecordedEventsType = list tuple str list Any dict str Any TritonTemplateKernel TritonKernel A specialized kernel Triton templates handles code generation templated Triton kernels This extends TritonKernel provide additional functionality template-based kernel generation including support subgraphs workspace arguments prologue epilogue fusion __init__ kernel_name input_nodes tuple ir IRNode output_node defines num_stages num_warps grid_fn meta call_sizes num_consumer_groups= num_buffers_warp_spec= use_jit=False tma_store=False prefix_args= suffix_args= epilogue_fn=identity subgraphs Optional list ir ComputedBuffer = None workspace_arg Optional WorkspaceArg = None prologue_loads_all_inputs=False hint_override Optional int = None - None tma_store pass numel = sympy_product output_node get_size tma_store assert len output_node get_size == TMA store only supported D templates tiling = x output_node get_size y output_node get_size r _ sympy S One tiling = x numel r _ sympy S One super __init__ tiling features=SIMDKernelFeatures numel hint_override=hint_override input_nodes = input_nodes output_node = output_node named_input_nodes = type ignore var-annotated defines = defines kernel_name = kernel_name use_jit = use_jit tma_store = tma_store num_stages = num_stages num_warps = num_warps num_consumer_groups = num_consumer_groups num_buffers_warp_spec = num_buffers_warp_spec grid_fn = grid_fn meta = meta call_sizes = call_sizes templates fixed epilogues prefix_args = prefix_args suffix_args = suffix_args pyrefly ignore invalid-type-var epilogue_fn = epilogue_fn render_hooks = type ignore var-annotated triton_meta Optional dict str object = None For Templated Attention can list ir Subgraph subgraphs Optional list ir ComputedBuffer = subgraphs Some templates use extra global memory workspace workspace_arg = workspace_arg workspace_arg None args workspace_args append workspace_arg The following attributes body template_mask output_val all used triton kernel codegen They swapped onto TritonTemplateKernel object ` set_subgraph_body ` subgraph_bodies dict str SubgraphInfo = input buffers which we allowed prologue fuse into prologue_supported_inputs OrderedSet str = OrderedSet input buffers which we fusing into prologue_fused_inputs OrderedSet str = OrderedSet input buffers which we fusing into which preserve zero mask prologue_fused_inputs_preserve_zero OrderedSet str = OrderedSet The following attributes all used triton kernel codegen They swapped onto TritonTemplateKernel object ` set_subgraph_body ` NB names here must match fields SubgraphInfo body IndentedBuffer = FakeIndentedBuffer compute IndentedBuffer = FakeIndentedBuffer indexing_code IndentedBuffer = FakeIndentedBuffer loads IndentedBuffer = FakeIndentedBuffer stores IndentedBuffer = FakeIndentedBuffer template_mask Optional str = None template_out_shape Optional Union str tuple str = None ops_handler Optional V WrapperHandler = None type ignore name-defined When caching enabled generated code dependent input nodes names symbolic sizes names However some variables returned generate_and_load computed during triton template expansions code generation dependent those In order cache code generation avoid redoing similar inputs varies only input names symbol names we do record replay method During template expansions we record all function calls change input_dependent_preserved_state replay them cache hit regenerate them cached_replay_events Optional RecordedEventsType = None Update each time input marked frozen used replay freezing inputs cache hit frozen_layouts_cnt = When prologue_loads_all_inputs true prologue_supported_inputs populated during def_kernel adding all inputs prologue_loads_all_inputs = prologue_loads_all_inputs Extra functions exposed during partial template rendering extra_template_env_fns list Callable Any = Tracking intermediate variables tmp_var_ctr = itertools count _gen_tmp_var - str f _tmp_var next tmp_var_ctr input_dependent_preserved_state - str Not adding args output_buffers purpose But we do need reproduce cache hit never accessed repr args input_buffers args sizevars args workspace_args prologue_supported_inputs frozen_layouts_cnt record_input_dependent_tracked_event - Callable Any decorator fn - Callable Any wrapper args kwargs - Any pre_state = input_dependent_preserved_state result = fn args kwargs post_state = input_dependent_preserved_state pre_state = post_state assert cached_replay_events None cached_replay_events append fn __name__ args kwargs result wrapper decorator replay_cached_events events RecordedEventsType - None f args kwargs events getattr f args kwargs contextlib contextmanager set_subgraph_body body_name str assert all hasattr field name field dataclasses fields SubgraphInfo old_state = key name getattr key name key dataclasses fields SubgraphInfo assert body_name subgraph_bodies body_name subgraph = subgraph_bodies body_name key value subgraph to_dict items value None key subgraph only_copy_if_non_none_fields continue setattr key value context = contextlib nullcontext ops_handler pyrefly ignore not-callable lambda V set_ops_handler ops_handler V get_ops_handler context type ignore operator yield subgraph_bodies body_name = SubgraphInfo key name getattr key name key dataclasses fields SubgraphInfo key value old_state items setattr key value contextlib contextmanager create_subgraph_body body_name str clear_cse bool = False assert body_name subgraph_bodies subgraph_bodies body_name = SubgraphInfo IndentedBuffer None None cse=self cse clone clear_cse None set_subgraph_body body_name yield need_numel_args False estimate_kernel_num_bytes Estimate total number bytes kernel takes For out nodes sizes counted twice once reading once writing ninplace_args = len unique args inplace_buffers values num_bytes = i inp enumerate itertools chain input_nodes output_node size = V graph sizevars size_hints inp get_size fallback= numel = functools reduce operator mul size dtype_size = get_dtype_size inp get_dtype num_bytes append numel dtype_size + int i ninplace_args sum num_bytes estimate_flops - int node input_nodes fx_node node _current_origins f = count_flops_fx fx_node f None V graph sizevars size_hint f fallback= jit_lines use_jit triton jit argdefs _ signature _ = args python_argdefs triton_meta dict str Any = signature signature_to_meta signature size_dtype=self index_dtype argdefs=argdefs is_template=True device DeviceProperties create output_node get_device constants triton_meta configs = config_of signature arg_num equal_ _arg_indices signature type ignore index triton_meta constants signature arg_num name = type ignore index union-attr matrix_instr_nonkdim = meta get matrix_instr_nonkdim None waves_per_eu = meta get waves_per_eu None kpack = meta get kpack None matrix_instr_nonkdim triton_meta matrix_instr_nonkdim = matrix_instr_nonkdim waves_per_eu triton_meta waves_per_eu = waves_per_eu kpack triton_meta kpack = kpack triton_meta = triton_meta inductor_meta = kernel_name str Placeholder DESCRIPTIVE_NAME inductor_meta_common FixedGrid setup_grid_as_args config profile_bandwidth config benchmark_kernel num_gb = estimate_kernel_num_bytes e inductor_meta kernel_num_gb = num_gb config benchmark_kernel flops = estimate_flops inductor_meta kernel_flop = flops inductor_meta config_args = meta template_args = f num_stages= num_stages num_warps= num_warps triton_meta= triton_meta r inductor_meta= inductor_meta r HAS_WARP_SPEC template_args += f num_consumer_groups= num_consumer_groups num_buffers_warp_spec= num_buffers_warp_spec f triton_heuristics template template_args triton jit gen_argdefs hook python_argdefs cannot run until after rest template lazily adds more args arg_defs _ = args python_argdefs f join x full_name x arg_defs _register_hook ARGDEFS hook allow_overwriting=True gen_defines defines def_kernel argnames Hook called template code generate function needed args assert all isinstance x str x argnames renames = IndentedBuffer initial_indent= named_args = input_nodes prefix_args len input_nodes - suffix_args assert len argnames == len named_args len argnames len named_args prefix_args len input_nodes input_node input_nodes prefix_args get args correct order args input input_node get_name name input_node zip argnames named_args arg_name = f arg_ name named_input_nodes name = input_node input_node get_name V graph removed_buffers continue input_node get_name prologue_fused_inputs continue args input_buffers input_node get_name = arg_name The args may duplicated so renaming must after args de-duplicated name argnames input_node = named_input_nodes name prologue_loads_all_inputs prologue_supported_inputs add input_node get_name input_node get_name V graph removed_buffers continue input_node get_name prologue_fused_inputs continue arg_name = args input_buffers input_node get_name input_node get_layout offset == renames writeline f name = arg_name offset = texpr rename_indexing input_node get_layout offset renames writeline f name = arg_name + offset input_node input_nodes len input_nodes - suffix_args get args correct order input_node get_name V graph removed_buffers continue input_node get_name prologue_fused_inputs continue args input input_node get_name hook python_argdefs cannot run until after rest template lazily adds more args arg_defs _ = args python_argdefs code = IndentedBuffer code splice gen_common_triton_imports code splice jit_lines code writeline f kernel_name join x full_name x arg_defs code indent code splice defines code splice renames getvalue codegen_prologue code code getvalue _register_hook DEF_KERNEL hook size name Optional str index int Hook called template code get size arg Will add needed args pass dynamic assert isinstance index int name None val = output_node get_size index assert isinstance name str val = named_input_nodes name get_size index texpr rename_indexing val stride name index=None Hook called template code get stride arg Will add needed args pass dynamic name None val = output_node get_stride assert isinstance name str val = named_input_nodes name get_stride isinstance index int texpr rename_indexing val index join texpr rename_indexing i i val _get_subgraph subgraph_number int assert isinstance subgraph_number int assert isinstance subgraphs list assert subgraph_number len subgraphs f Invalid subgraph number provided create_modification subgraph_number must len subgraphs assert body getvalue == Body should clear before adding modification subgraphs subgraph_number _handle_scatter_graph scatter_graph Handle processing single scatter graph Args scatter_graph The scatter graph process assert isinstance scatter_graph ir ComputedBuffer f scatter_graph must instance ComputeBuffer got type scatter_graph contiguous_strides x We always create fresh contiguous grad scattering into sum x_i stride x_i stride zip x scatter_graph get_stride scatter_graph data store_output type ignore attr-defined scatter_graph name contiguous_strides modification subgraph_number int output_name Optional str mask Optional str = None fixed_inputs - str This creates modification function subgraph To use inside template first argument should specify which subgraph codegen Args subgraph_number int The index subgraph subgraphs output_name Optional str The name output variable store result mask Optional str An optional mask use store operation If provided mask will applied store num = out = None scatters = while f mod_ subgraph_number _ num subgraph_bodies num += create_subgraph_body f mod_ subgraph_number _ num subgraph = _get_subgraph subgraph_number modification_handler = ModificationWrapper subgraph_number fixed_inputs mask V set_ops_handler modification_handler assert isinstance subgraph ir ComputedBuffer list f Expected subgraph ComputedBuffer List ComputedBuffer got type subgraph Handle scatter stores isinstance subgraph list scatter_graph subgraph scatters append _handle_scatter_graph scatter_graph isinstance subgraph data ir InputBuffer out = subgraph data make_loader out = subgraph data inner_fn codegen_body output_name None assert isinstance output_name str assert out None body writeline f output_name = out value assert out None scatter scatters body writeline str scatter body_val = body getvalue cse invalidate OrderedSet body_val load_input input_name str output_name str indices Union list Any tuple Any mask Optional str = None other Optional Union float int = indent_width int = index_shape Optional tuple str = None Loads input applies any necessary preprocessing masking Args input_name str The name input load indices Union List Tuple The index each dimension input val str The name variable store loaded value mask Optional str An optional mask use load operation other Optional Union float int The value use masked elements Default indent_width int The number spaces use indentation input_node = named_input_nodes input_name prologue_loads_all_inputs prologue_supported_inputs add input_node get_name tilings = sympy_product input_node get_size sympy Integer groups = x tilings r _ tilings range_trees = construct_range_trees pid_cache=None inside_reduction=False is_reduction=False numels=groups no_x_dim=False load_code = None create_subgraph_body f LOAD_INPUT_ input_name assert isinstance indices list tuple assert isinstance output_name str assert isinstance mask str type None range_trees = range_trees numels = k V graph sizevars simplify v k v groups items indices = list map OpOverrides paren indices index_symbols = sympy Symbol x integer=True x indices lengths = V graph sizevars simplify s s input_node get_size assert len indices == len lengths index_symbols = sympy Symbol x integer=True x indices assert len indices == len lengths glue make generated code use same indexing template TODO reviewers well codegen_template prologue_node codegen kernel split_and_set_ranges prologue_node get_ranges ranges need reflect group prologue input will error sure there any difference between original range_tree_entry new one correct lengths groups both actually seem work name range_tree_entry zip indices range_trees construct_entries lengths range_tree_entry set_name name contiguous_index = sympy_dot ir FlexibleLayout contiguous_strides lengths index_symbols contiguous_index = rename_indexing contiguous_index body writeline xindex = + texpr contiguous_index xindex_range_root = range_trees lookup sympy Integer sympy_product lengths xindex_range_root set_name xindex Note - None override_mask MM Templates work taking out bounds index values wrapping them around so no mask required load offs_a_m = ` rm M ` We should override mask None instead inheriting mask would have been loaded otherwise We using None clarity output code we could alternatively emit ` xmask = tl full xindex shape True tl int ` template_mask = mask mask None None template_out_shape = index_shape index_shape xindex template_indices = indices named_input_nodes input_name data freeze_layout cse invalidate OrderedSet template_mask = template_mask StoreOutputSubstitution V WrapperHandler type ignore name-defined name = StoreOutputSubstitution store name str index sympy Expr value CSEVariable mode StoreMode = None V kernel store_buffer_names add name V kernel cse store_cache name = value name V kernel prologue_fused_inputs We load masked out values then apply prologue The masked out values may necessariliy any more so we need reapply mask value_dtype = value dtype value_str = str value template_mask = None name V kernel prologue_fused_inputs_preserve_zero other = value_str = f tl where template_mask value_str other value_dtype = V graph get_buffer name dtype value_str = f value_str triton_type V graph get_buffer name dtype TODO we should have intermediary var shapes V kernel compute writeline f output_name = value_str broadcast_to xindex shape pyrefly ignore bad-assignment ops_handler = StoreOutputSubstitution input_node = named_input_nodes input_name output_index = input_node make_indexer index_symbols def_kernel above we define inputs storage offset adjusted creating load input_node make_indexer will also adjust storage offset so subtract here double increment V graph sizevars statically_known_equals input_node layout offset output_index = output_index - rename_indexing input_node get_layout offset output_index = rename_indexing output_index output_index == contiguous_index output_index_str = xindex out_indexing = indexing output_index copy_shape=self template_out_shape override_mask=self template_mask codegen triton IndexingOptions assert isinstance out_indexing IndexingOptions output_index_str = f out_indexing index_str broadcast_to xindex shape Generate load code load_code = f output_name = tl load input_name + output_index_str mask load_code += f mask= mask other= other load_code += hook_key = f LOAD_INPUT_ input_name hook set_subgraph_body hook_key cse invalidate OrderedSet codegen_body cse invalidate OrderedSet input_node get_name prologue_fused_inputs assert load_code None body writeline load_code textwrap indent body getvalue indent_width strip _register_hook hook_key hook _generate_index_from_tma_index output_name str offset_name str tma_index sympy Symbol block_size str dim int num_dims int block_name Optional str = None - list str Generate logic compute regular tl load index provided tma index This used ensure variables can support fusions Args output_name str The output variable name offset_name str The name used intermediate offset tma_index sympy Symbol The symbol used original TMA index block_size str The block size index dim int Which dimension project index num_dims int The total number dimensions output block_name Optional str The name block variable If passed then we aren t reusing standard symbol names Returns list str The lines used generate index block_name Generate expected names structure XBLOCK YBLOCK xoffset yoffset We append XBLOCK YBLOCK top kernel so we can safely extract tensor descriptor construction top kernel block_name prologue_cache assert prologue_cache block_name == block_size f Constant block_name must used all stores prologue_cache block_name = block_size prologue writeline f block_name tl constexpr = block_size block_name = block_size line = f offset_name = texpr tma_index expr = f offset_name + tl arange block_name prefix_none = join None dim suffix_none = join None num_dims - dim + line = f output_name = expr prefix_none suffix_none line line _generated_mask_for_tma index_name str shape_val str output_name str - str Generate mask logic feed fusions mask The expectation we have X Y there will variable named xmask ymask Args index_name str The index used mask Should one xindex yindex shape_val str The expression upper bound shape output_name str The expression used output Returns str The mask generation line f output_name = index_name shape_val store_output indices Union list Any tuple Any val str mask Optional str = None indent_width int = val_shape Optional tuple str = None block_indexing bool = False Stores final output appends any epilogue fusions buffer hasn t been optimized away Args indices Union List Tuple The index each dimension output The dot product these indices output strides must match ` val ` val str The value store mask Optional str An optional mask use store operation If provided mask will applied store indent_width int The number spaces use indentation This used when call store_output indented kernel definition block_indexing bool Are input indices presented offsets creating block e g inputs TMA they tensors should passed directly subgraph_name = _get_store_output_subgraph_name next store_output_ctr create_subgraph_body subgraph_name clear_cse=True assert isinstance indices list tuple assert isinstance val str assert isinstance mask str type None assert isinstance val_shape tuple type None assert isinstance block_indexing bool assert template_mask None indices = list map OpOverrides paren indices index_symbols = sympy Symbol x integer=True x indices lengths = V graph sizevars simplify s s output_node get_size assert len indices == len lengths output_layout = output_node get_layout template_out = val block_indexing assert val_shape Blocking indexing requires passing val_shape assert len val_shape == Blocking indexing only supports D data time assert mask Mask supported blocking indexing intermediate_lines list str = epilogue_index_symbols list sympy Symbol = tma_store Generate expected indexing symbols Note TMA indices expected format x y range_tree always yindex xindex index_order = val_shape_copy = list val_shape i range_tree zip index_order range_trees - name = range_tree name symbol = range_tree symbol epilogue_index_symbols append symbol lookup_output = range_tree lookup sympy S One lengths i old_name = lookup_output symbol lookup_output set_name name Update var_list var_range range_tree var_list range_tree var_list index old_name = symbol range_val = range_tree var_ranges old_name del range_tree var_ranges old_name range_tree var_ranges symbol = range_val intermediate_lines extend _generate_index_from_tma_index name xoffset name == xindex yoffset index_symbols i val_shape i i len index_order pyrefly ignore missing-argument block_name=range_tree symt name Generate xmask ymask intermediate_lines append _generated_mask_for_tma name size None i xmask name == xindex ymask Update val_shape information use consistent naming after remapping pyrefly ignore missing-argument val_shape_copy i = range_tree symt name Reverse index symbols because TMA indexed x y whereas variables will naturally indexed y x epilogue_index_symbols reverse val_shape = tuple val_shape_copy mask_vars list str = i index shape enumerate zip index_symbols val_shape index_name = _gen_tmp_var offset_name = _gen_tmp_var intermediate_lines extend _generate_index_from_tma_index index_name offset_name index shape i len index_symbols epilogue_index_symbols append sympy Symbol index_name integer=True mask_name = _gen_tmp_var intermediate_lines append _generated_mask_for_tma index_name size None i mask_name mask_vars append mask_name final_mask_var = _gen_tmp_var final_mask_rhs = join f mask_name mask_name mask_vars intermediate_lines append f final_mask_var = final_mask_rhs template_mask = final_mask_var index_symbols = epilogue_index_symbols contiguous_index = sympy_dot output_layout stride index_symbols tma_store Convert just use xindex contiguous_index = rename_indexing contiguous_index intermediate_lines append f xindex = texpr contiguous_index range_trees lookup sympy S One sympy_product lengths set_name xindex index_symbols = epilogue_index_symbols output_index = contiguous_index Write out intermediate lines line intermediate_lines body writeline line assert tma_store TMA store requires block indexing glue make generated code use same indexing template name range_tree_entry zip indices range_trees construct_entries lengths range_tree_entry set_name name contiguous_index = sympy_dot ir FlexibleLayout contiguous_strides lengths index_symbols contiguous_index = rename_indexing contiguous_index body writeline xindex = + texpr contiguous_index range_trees lookup sympy S One sympy_product lengths set_name xindex template_mask = mask template_indices = indices output_index = output_node get_layout make_indexer index_symbols output_index = rename_indexing output_index output_index == contiguous_index output_index = sympy Symbol xindex integer=True pyrefly ignore bad-assignment template_out_shape = val_shape val_shape val acc_dtype = triton_type_to_torch meta ACC_TYPE ACC_TYPE meta torch float epilogue_args = V kernel cse namedvar val dtype=acc_dtype shape=val_shape input_node itertools chain input_nodes prefix_args input_nodes len input_nodes - suffix_args input_node freeze_layout epilogue_arg = V kernel cse generate compute input_node make_loader index_symbols dtype=acc_dtype shape=input_node get_size epilogue_args append epilogue_arg We update frozen_layouts_cnt order replay function cache hit frozen_layouts_cnt += V ops store output_node get_name output_index epilogue_fn epilogue_args mode= tma tma_store None codegen_body hook set_subgraph_body subgraph_name more stuff might have been added since codegen_body above codegen_body cse invalidate OrderedSet textwrap indent body getvalue indent_width strip _register_hook subgraph_name hook _register_hook hook_name str hook_fn PartialRender HookFn allow_overwriting bool = False - str Register hook function name ` ` hook_name ` ` should match string will replaced via ` ` hook_fn ` ` should already use hook If ` ` allow_overwriting ` ` ` ` False ` ` will assert there isn t currently registered hook same name before registering new one allow_overwriting assert hook_name render_hooks f Tried register hook hook_name multiple times If desired pass allow_overwriting=True _register_hook render_hooks hook_name = hook_fn hook_name _register_extra_template_env_fns fns Callable Any Register some extra functions expose when performing initial template render so they re scope used jinja expressions These can used example implement extra replacement hooks given function Returns name their hook which should also string replace via hook function The convention use format HOOK_NAME Assigns corresponding entry ` ` render_hooks ` ` hook function extra_template_env_fns extend fns render template kwargs record_input_dependent_tracked_event=False record_input_dependent_tracked_event cached_replay_events = template_env = fn __name__ record_input_dependent_tracked_event fn record_input_dependent_tracked_event fn fn def_kernel size stride store_output load_input make_load modification gen_argdefs gen_defines extra_template_env_fns PartialRender template render template_env kwargs render_hooks make_load name indices mask Optional helper called template code generate code needed load tensor assert isinstance indices list tuple assert isinstance name str assert isinstance mask str stride = named_input_nodes name get_stride indices = list map OpOverrides paren indices assert len indices == len stride index = + join f texpr rename_indexing s i s i zip stride indices f tl load name + index mask other= indexing index sympy Expr dense_indexing=False copy_shape=None override_mask=None block_ptr=False tma_compatibility_checker Optional TMACompatibilityChecker = None Override default indexing use our custom mask force dense indexing super indexing index dense_indexing=False We pass template_out shape broadcast indexing mask might broadcast output shape copy_shape=self template_out_shape override_mask=self template_mask block_ptr=block_ptr tma_compatibility_checker=tma_compatibility_checker codegen_range_tree pass ignore default codegen additional_call_args_and_types isinstance grid_fn SymbolicGridFn grid_args = grid_fn sympy_call call_sizes meta assert len grid_args grid_fn should values grid_args map type grid_args all isinstance x int sympy Integer x call_sizes grid_args = grid_fn map int call_sizes meta assert len grid_args grid_fn should values grid_args map type grid_args call_kernel name str node Optional ir IRNode = None deallocate_ws bool = True wrapper = V graph wrapper_code _ call_args _ arg_types = args python_argdefs additional_call_args additional_arg_types = additional_call_args_and_types additional_call_args assert V graph cpp_wrapper cpp_wrapper requires SymbolicGridFn wrapper add_import_once f grid_fn __module__ meta = wrapper add_meta_once meta fn_name = f grid_fn __module__ grid_fn __name__ call_args append f fn_name join map pexpr call_sizes meta arg_types append None call_args extend additional_call_args arg_types extend additional_arg_types workspace_arg None wrapper generate_workspace_allocation workspace_arg wrapper generate_kernel_call name call_args arg_types=arg_types triton_meta=self triton_meta triton=True workspace_arg None wrapper generate_workspace_deallocation workspace_arg kernel_benchmark_extra_args - list str str x x grid_fn V graph sizevars size_hints call_sizes meta functools cache _jinja _env try jinja jinja Environment undefined=jinja StrictUndefined except ImportError None GenerateAndLoadResult NamedTuple Return type TritonTemplate generate_and_load mod ModuleType extra str input_call_args tuple str prologue_supported_inputs OrderedSet str kernel_args_sizevars_keys tuple sympy Expr kernel_options dict str Any GeneratedCodeCacheEntry NamedTuple code str extra str events list Any GeneratedCodeCache Cache generated code The cache key string representation input nodes number stages number warps call sizes The cache value tuple generated code extra code events __init__ args kwargs _cache dict str GeneratedCodeCacheEntry = cache_clear - None _cache clear __repr__ repr _cache make_key input_nodes tuple ir IRNode num_stages int num_warps int call_sizes Sequence sympy core symbol Symbol prefix_args int suffix_args int epilogue_fn Optional Callable Any epilogue_fn_hash Optional str tma_store bool subgraphs Optional list ir Buffer has none cache workspace_arg Optional WorkspaceArg has none cache layout ir Layout num_consumer_groups int num_buffers_warp_spec int kwargs dict str Any hint_override Optional int = None - Optional str layout_key layout ir Layout - str assert isinstance layout ir FlexibleLayout repr layout size layout stride layout dtype layout device layout offset has_flexible_layout - bool isinstance layout ir FlexibleLayout True input input_nodes isinstance input get_layout ir FlexibleLayout True False epilogue_fn identity assert epilogue_fn_hash None epilogue_fn_hash = identity we do cache under those conditions right now has_flexible_layout subgraphs None workspace_arg None epilogue_fn_hash None None repr input_nodes layout_key input get_layout input input_nodes num_stages num_stages num_warps num_warps prefix_args prefix_args suffix_args suffix_args call_sizes call_sizes layout layout_key layout num_consumer_groups num_consumer_groups num_buffers_warp_spec num_buffers_warp_spec epilogue_fn_hash epilogue_fn_hash tma_store tma_store kwargs kwargs hint_override hint_override get_entry cache_key Optional str - Optional GeneratedCodeCacheEntry cache_key None None entry = _cache get cache_key None entry None torch _dynamo utils counters inductor generated_module_cache_miss += torch _dynamo utils counters inductor generated_module_cache_hit += entry put_entry cache_key Optional str code str extra str events list Any - None cache_key None entry = GeneratedCodeCacheEntry code extra events _cache update cache_key entry TritonTemplate KernelTemplate A Triton template template can used generate Triton kernel Allow subclasses override kernel type kernel_type type Any = TritonTemplateKernel index_counter = itertools count all_templates dict str TritonTemplate = __init__ name str grid Any source str debug=False cache_codegen_enabled_for_template=False prologue_loads_all_inputs=False - None super __init__ name hash=hashlib sha source encode utf- hexdigest grid = grid template = _template_from_string source assert name all_templates duplicate template name TritonTemplate all_templates name = debug = debug _cache_codegen_enabled_for_template = cache_codegen_enabled_for_template _generated_code_cache GeneratedCodeCache = GeneratedCodeCache clear_on_fresh_cache _generated_code_cache When prologue_loads_all_inputs true prologue_supported_inputs populated during def_kernel adding all inputs prologue_loads_all_inputs = prologue_loads_all_inputs When flag we ensure cached results generated result cache used same test_cache = False property uid - str unique prefixing triton f triton name maybe_append_choice choices list Any kwargs Any - Optional NotImplementedError Maybe generates new ChoiceCaller appends into existing choices Returns None success otherwise returns error choices A list ChoiceCallers kwargs Additional kwargs passed generate generate new ChoiceCaller try choice = generate generate_with_caching=True kwargs choice None choices append choice None except NotImplementedError e log info noqa G Cannot Append Choice s KernelTemplate type s e type stack_info=log getEffectiveLevel logging INFO e NOTE MAKE SURE THAT ANY ARGUMENT ADDED TO THIS FUNCTION IS PROPERLY HANDLED IN _generated_code_cache make_key generate_and_load input_nodes tuple ir IRNode num_stages int num_warps int call_sizes Sequence sympy core symbol Symbol prefix_args int suffix_args int epilogue_fn Optional Callable Any epilogue_fn_hash Optional str subgraphs Optional list ir Buffer workspace_arg Optional WorkspaceArg num_consumer_groups int num_buffers_warp_spec int layout ir Layout kwargs dict str Any generate_with_caching hint_override Optional int = None tma_store bool = False - Optional GenerateAndLoadResult Generate python code load into current process caching_enabled = generate_with_caching torch _inductor config enable_caching_generated_triton_templates cache_key = None caching_enabled cache_key = _generated_code_cache make_key input_nodes num_stages num_warps call_sizes prefix_args suffix_args epilogue_fn epilogue_fn_hash tma_store subgraphs workspace_arg layout num_consumer_groups num_buffers_warp_spec kwargs assert template requires jinja defines = StringIO name val kwargs items defines write f name tl constexpr = val \n fake_out = ir Buffer name= buf_out layout=layout kernel_name = f triton_ name numel = sympy_product layout size buffers = itertools chain input_nodes fake_out TritonScheduling can_use_ bit_indexing numel buffers index_dtype = tl int index_dtype = tl int Add index dtype defines so s available template defines write f INDEX_DTYPE tl constexpr = index_dtype \n defines = defines getvalue kernel_options = input_nodes input_nodes defines defines num_stages num_stages num_warps num_warps grid_fn grid meta kwargs call_sizes call_sizes prefix_args prefix_args suffix_args suffix_args epilogue_fn epilogue_fn subgraphs subgraphs prologue_loads_all_inputs prologue_loads_all_inputs HAS_WARP_SPEC kernel_options update num_consumer_groups num_consumer_groups num_buffers_warp_spec num_buffers_warp_spec make_kernel kernel_type kernel_name=kernel_name output_node=fake_out workspace_arg=workspace_arg use_jit=False hint_override=hint_override tma_store=tma_store kernel_options generate_code kernel - Optional tuple str str make_extra - str extra_parts = f kwarg = repr kwargs kwarg kwarg sorted kwargs keys extra_parts extend f num_stages= num_stages f num_warps= num_warps HAS_WARP_SPEC extra_parts extend f num_consumer_groups= num_consumer_groups f num_buffers_warp_spec= num_buffers_warp_spec extra = - join extra_parts + - extra try template = kernel render template kwargs caching_enabled code = template finalize_all except ZeroDivisionError TODO nmacchioni fix sympy division zero None debug print Generated Code \n code extra = make_extra code extra maybe_test_cache code str extra str kernel test_cache debug patch object V graph get_dtype _fake_get_dtype fake_out V graph set_current_device layout device make_kernel kernel_test result = generate_code kernel_test assert result None code_test extra_test = result assert code == code_test extra == extra_test kernel args input_buffers == kernel_test args input_buffers kernel prologue_supported_inputs == kernel_test prologue_supported_inputs kernel args sizevars == kernel_test args sizevars Generated code cache results wrong output Generate code extra code Optional str = None extra Optional str = None patch object V graph get_dtype _fake_get_dtype fake_out V graph set_current_device layout device make_kernel kernel cache_entry = _generated_code_cache get_entry cache_key cache_hit = False cache_entry None code extra events = cache_entry kernel replay_cached_events events cache_hit = True result = generate_code kernel result None happens ZeroDivisionError None code extra = result _generated_code_cache put_entry cache_key code extra kernel cached_replay_events assert code None extra None mod = PyCodeCache load code extra input_call_args = tuple kernel args input_buffers keys prologue_supported_inputs = kernel prologue_supported_inputs copy kernel_args_sizevars_keys = tuple kernel args sizevars keys cache_hit maybe_test_cache code extra kernel GenerateAndLoadResult mod extra input_call_args prologue_supported_inputs pyrefly ignore bad-argument-type kernel_args_sizevars_keys kernel_options generate type ignore override input_nodes tuple ir IRNode layout ir Layout num_stages int num_warps int num_consumer_groups int = num_buffers_warp_spec int = prefix_args int = suffix_args int = epilogue_fn Optional Callable Any = identity epilogue_fn_hash Optional str = None subgraphs Optional list ir Buffer = None mutated_inputs Optional list ir IRNode = None call_sizes Optional Sequence sympy core symbol Symbol = None workspace_arg Optional WorkspaceArg = None generate_with_caching=False hint_override Optional int = None tma_store bool = False kwargs This function generates TritonTemplateCaller Args input_nodes List input nodes layout Output layout num_stages Number stages triton launch num_warps Number warps triton launch prefix_args Number input nodes passed arguments suffix_args Number input nodes passed arguments epilogue_fn Optional epilogue function called output subgraphs Optional subgraphs passed arguments these will inlined into triton template string mutated_inputs Optional list input nodes mutated kernel helpful you need multiple outputs You can pass them inputs mark them being mutated kernel HACK Triton currently breaks TF floats requested CUDA capability doesn t support them This bug Triton now we ll patch around here See https github com triton-lang triton issues one example issue problem torch cuda is_available torch cuda is_tf _supported kwargs ALLOW_TF = False call_sizes None call_sizes = layout size result = generate_and_load input_nodes num_stages num_warps call_sizes prefix_args suffix_args epilogue_fn epilogue_fn_hash subgraphs workspace_arg num_consumer_groups num_buffers_warp_spec layout kwargs generate_with_caching _cache_codegen_enabled_for_template hint_override=hint_override tma_store=tma_store May happen result dev result None None We expect input_buffer order input_nodes captured_buffers expected_input_args = tuple unique x get_name x input_nodes assert result input_call_args len expected_input_args == expected_input_args result input_call_args expected_input_args ` kernel_input_nodes ` actual inputs will passed kernel so e g views same input included ` codegen_input_nodes ` includes views inputs preserve kernel semantics The shape strides ` codegen_input_nodes ` will used infer read writes TemplateBuffer extract_read_writes kernel_input_nodes = tuple V graph get_buffer k k result input_call_args Here we have input_nodes captured_buffers codegen_input_nodes = tuple input_nodes + kernel_input_nodes len expected_input_args extra_args = V graph sizevars size_hints map sympy expand result kernel_args_sizevars_keys fallback=config unbacked_symint_fallback hint_override=hint_override kernel_hash_name = f triton_ name _ next index_counter workspace_args = workspace_arg None Create workspace tensor workspace_size = workspace_arg count workspace_tensor = torch empty_strided workspace_size dtype=torch uint device=layout device type Handle zero initialization needed workspace_arg zero_mode = WorkspaceZeroMode UNINITIALIZED workspace_tensor zero_ workspace_args append workspace_tensor options = result kernel_options make_kernel_render out_node hint_override Optional int = None assert result None kernel = kernel_type kernel_name=str Placeholder KERNEL_NAME output_node=out_node workspace_arg=workspace_arg use_jit=False hint_override=hint_override tma_store=tma_store options render = functools partial kernel render template kwargs kernel render create BenchmarkRequest assert result mod __file__ None grid = grid V graph sizevars size_hints call_sizes fallback=config unbacked_symint_fallback hint_override=hint_override kwargs bmreq_cls type TritonBenchmarkRequest layout device type == cpu bmreq_cls = TritonCPUBenchmarkRequest bmreq_cls = TritonGPUBenchmarkRequest bmreq = bmreq_cls module_path=result mod __file__ module_cache_key=result mod key kernel_name=f triton_ name extra_args= extra_args workspace_args grid num_stages=num_stages num_warps=num_warps num_consumer_groups=num_consumer_groups num_buffers_warp_spec=num_buffers_warp_spec matrix_instr_nonkdim=kwargs get matrix_instr_nonkdim waves_per_eu=kwargs get waves_per_eu kpack=kwargs get kpack input_tensor_meta=TensorMeta from_irnodes kernel_input_nodes type ignore arg-type output_tensor_meta=TensorMeta from_irnodes layout TritonTemplateCaller kernel_hash_name codegen_input_nodes layout make_kernel_render result extra strip - replace - bmreq log_info= tile_shape str kwargs get BLOCK_M - kwargs get BLOCK_K - kwargs get BLOCK_N - num_stages num_stages num_warps num_warps GROUP_M kwargs get GROUP_M - allow_tf str kwargs get ALLOW_TF acc_type str kwargs get ACC_TYPE matrix_instr_nonkdim kwargs get matrix_instr_nonkdim waves_per_eu kwargs get waves_per_eu kpack kwargs get kpack k kwargs k k AlgorithmSelectorCache FLEX_ATTENTION_TUNABLE_KEYS k kwargs mutated_inputs=mutated_inputs workspace_arg=workspace_arg allowed_prologue_inps=result prologue_supported_inputs hint_override=hint_override ExternKernelChoice __init__ kernel cpp_kernel=None name=None has_out_variant=True op_overload=None use_fallback_kernel=False kernel_creator=None - None super __init__ name = name kernel __name__ assert callable kernel assert hasattr extern_kernels name f duplicate extern kernel name name = name cpp_kernel_name = cpp_kernel has_out_variant = has_out_variant setattr extern_kernels name kernel op_overload = op_overload use_fallback_kernel = use_fallback_kernel kernel_creator = kernel_creator match API KernelTemplate they can treated same There no src hash ExternKernelChoice traditional sense so we indicate returning None src_hash = None to_callable getattr extern_kernels name call_name f extern_kernels name functools cache noqa B hash_key fn = to_callable parts = name getattr fn __name__ getattr fn __module__ try parts append inspect getsource fn except Exception pass code_hash - join parts bind input_nodes layout ordered_kwargs_for_cpp_kernel= kwargs ordered_kwargs_for_cpp_kernel = ordered_kwargs_for_cpp_kernel ExternKernelCaller input_nodes layout kwargs has_out_variant=self has_out_variant property uid - str unique prefixing aten f aten name choice_or_none kwargs Any - Optional ChoiceCaller Maybe generates new ChoiceCaller returns None generation fails kwargs Additional kwargs passed generate new ChoiceCaller temp_choices list Any = result = maybe_append_choice temp_choices kwargs result None len temp_choices == temp_choices None maybe_append_choice choices list Any kwargs Any - Optional NotImplementedError convenience function match Template interface so templates ExternKernelChoice can treated same when generating choice callers assert input_nodes kwargs input_nodes argument required assert layout kwargs layout argument required input_nodes = kwargs pop input_nodes layout = kwargs pop layout choices append bind input_nodes=input_nodes layout=layout kwargs None TritonTemplateCaller ir TritonTemplateCallerBase __init__ name input_nodes layout make_kernel_render description bmreq log_info Optional dict str Union PrimitiveInfoType list PrimitiveInfoType = None mutated_inputs=None workspace_arg Optional WorkspaceArg = None allowed_prologue_inps Optional OrderedSet str = None hint_override Optional int = None - None super __init__ name input_nodes layout description make_kernel_render = make_kernel_render bmreq TritonBenchmarkRequest = bmreq log_info None log_info = log_info dict str Any = log_info log_info update backend Triton num_stages bmreq num_stages num_warps bmreq num_warps mutated_inputs = mutated_inputs workspace_arg = workspace_arg allowed_prologue_inps = allowed_prologue_inps allowed_prologue_inps None OrderedSet hint_override = hint_override benchmark args out assert bmreq None config profile_bandwidth_with_do_bench_using_profiling algo = bmreq make_run_fn args out=out do_bench_using_profiling algo bmreq benchmark args out=out precompile assert bmreq None bmreq precompile __str__ - str f TritonTemplateCaller bmreq module_path description call_name f template_kernels name hash_key - join name rsplit _ bmreq module_cache_key output_node ir TensorBox create ir TritonTemplateBuffer layout=self layout inputs=self input_nodes make_kernel_render=self make_kernel_render mutated_inputs=self mutated_inputs allowed_prologue_inps=self allowed_prologue_inps info_dict - dict str Union PrimitiveInfoType list PrimitiveInfoType Information returned here logged autotune log file when enabled log_info get_make_kernel_render make_kernel_render autoheuristic_id type_name = triton info = info_dict TODO AlnisM Does tile_shape always exist tile = info tile_shape tile_vals = eval tile type ignore arg-type BLOCK_M = tile_vals BLOCK_K = tile_vals BLOCK_N = tile_vals num_stages = info num_stages num_warps = info num_warps f type= type_name _BLOCK-M= BLOCK_M _BLOCK-K= BLOCK_K _BLOCK-N= BLOCK_N _numstages= num_stages _numwarps= num_warps ExternKernelCaller ChoiceCaller __init__ choice ExternKernelChoice input_nodes layout kwargs=None has_out_variant=True - None super __init__ choice name input_nodes layout description= choice = choice kwargs = kwargs has_out_variant = has_out_variant __str__ - str f ExternKernelCaller choice call_name benchmark args out out numel == no need run kerrnel do benchmarking has_out_variant super benchmark args out=out algo = to_callable out_new = algo args torch _C _dynamo guards assert_size_stride out_new tuple out size tuple out stride out copy_ out_new correctness checking config profile_bandwidth_with_do_bench_using_profiling do_bench_using_profiling lambda algo args benchmarker benchmark algo args to_callable fn = choice to_callable kwargs functools partial fn kwargs fn hash_key - join choice name f kwarg = repr kwargs kwarg kwarg sorted kwargs keys choice hash_key output_node choice use_fallback_kernel assert choice op_overload None Please provide op_overload use ir FallbackKernel inner ir IRNode = ir FallbackKernel create choice op_overload input_nodes kwargs choice kernel_creator None inner = choice kernel_creator input_nodes kwargs cls = ir ExternKernelOut has_out_variant ir ExternKernelAlloc inner = cls layout=self layout inputs=self input_nodes python_kernel_name=self choice call_name cpp_kernel_name=self choice cpp_kernel_name ordered_kwargs_for_cpp_kernel=self choice ordered_kwargs_for_cpp_kernel op_overload=self choice op_overload kwargs=self kwargs ir TensorBox create inner info_dict - dict str Union PrimitiveInfoType list PrimitiveInfoType Information returned here logged autotune log file when enabled backend extern kernel_call_name choice call_name autoheuristic_id f extern_ choice name functools cache get_mm_log_filename - Optional str mm_file_name = os environ get TORCHINDUCTOR_MM_LOGGING_FILE None mm_file_name None json mm_file_name mm_file_name = f mm_file_name json mm_file_name functools cache get_flex_attention_log_filename - Optional str flex_attention_file_name = os environ get TORCHINDUCTOR_FLEX_ATTENTION_LOGGING_FILE None flex_attention_file_name None str Path flex_attention_file_name with_suffix json append_to_log filename data lock_file = filename replace json lock lock = FileLock lock_file lock try open filename f log_data = json load f except FileNotFoundError json JSONDecodeError log_data = log_data append data open filename w f json dump log_data f indent= DataProcessorChoiceCallerWrapper __init__ wrapped preprocessor postprocessor - None _wrapped = wrapped preprocessor None _preprocessor = preprocessor _preprocessor = lambda x y x y postprocessor None _postprocessor = postprocessor _postprocessor = lambda x x __getattr__ name getattr _wrapped name benchmark args out - float new_args new_out = _preprocessor args out result = _wrapped benchmark new_args out=new_out new_out = _postprocessor new_out out new_out out copy_ new_out result output_node - ir TensorBox result = _wrapped output_node _postprocessor result __repr__ - str f DataProcessorChoiceCallerWrapper _wrapped DataProcessorTemplateWrapper A wrapper kernel template This together ` DataProcessorChoiceCallerWrapper ` provides convenient way preprocess postprocess data before after using wrapped template A typical usage reorder filter input nodes order match expected input other kernel choices like ATen kernel A more complicated usage prepack weights See example mod ` cpp_gemm_template ` more details __init__ wrapped_template_cls preprocessor postprocessor kwargs - None preprocessor None _preprocessor = preprocessor _preprocessor = lambda x y x y postprocessor None _postprocessor = postprocessor _postprocessor = lambda x x assert input_nodes kwargs assert layout kwargs pyrefly ignore not-callable kwargs input_nodes kwargs layout = preprocessor kwargs input_nodes kwargs layout _wrapped = wrapped_template_cls kwargs __getattr__ name getattr _wrapped name maybe_append_choice choices kwargs type _wrapped maybe_append_choice choices kwargs generate kwargs choice_caller = _wrapped generate kwargs DataProcessorChoiceCallerWrapper choice_caller _preprocessor _postprocessor __repr__ - str f DataProcessorTemplateWrapper _wrapped ErrorFromChoice RuntimeError __init__ msg choice ChoiceCaller inputs_str - None msg += f \nFrom choice choice \n inputs_str super __init__ msg choice = choice NoValidChoicesError RuntimeError pass functools cache get_num_workers - int TORCHINDUCTOR_COMPILE_THREADS os environ int os environ TORCHINDUCTOR_COMPILE_THREADS cpu_count = len os sched_getaffinity hasattr os sched_getaffinity os cpu_count assert cpu_count Divide number CPUs number GPUs distributed workloads config is_fbcode torch cuda is_available torch cuda device_count cpu_count = cpu_count torch cuda device_count cpu_count create_inputs_key input_nodes - str repr AlgorithmSelectorCache key_of x x input_nodes create_precompile_key name str inputs_key str choices list ChoiceCaller - str join name inputs_key torch get_float _matmul_precision + choice kernel_hash_key choice choices Args FeedbackFunctions timings mapping choices benchmark time name name op input_nodes list input ir py Nodes choices list choices profiled time Callable returns dict mapping choices profiled time FeedbackFunction = Callable dict ChoiceCaller float str list Any list ChoiceCaller Callable dict ChoiceCaller float None Args PreprocessingFunctions choices list ChoiceCaller objects preprocess Returns modified list ChoiceCaller objects PreprocessingFunction = Callable list ChoiceCaller list ChoiceCaller filter_choices_by_name_regex choices list ChoiceCaller - list ChoiceCaller Filter choices based autotune_choice_name_regex config config test_configs autotune_choice_name_regex None c c choices re search config test_configs autotune_choice_name_regex c name choices filter_choices_by_desc_regex choices list ChoiceCaller - list ChoiceCaller Filter choices based autotune_choice_desc_regex config config test_configs autotune_choice_desc_regex None c c choices re search config test_configs autotune_choice_desc_regex c description choices AlgorithmSelectorCache PersistentCache A persistent cache algorithm selection results used autotuning GEMMs convolutions This classes includes precompilation benchmarking kernels The cache keyed input characteristics sizes strides dtypes etc doesn t depend output layout FLEX_ATTENTION_TUNABLE_KEYS = tuple dict fromkeys num_warps num_stages BLOCK_M BLOCK_N BLOCK_M BLOCK_N BLOCK_M BLOCK_N USE_TMA kpack matrix_instr_nonkdim waves_per_eu __init__ args kwargs - None super __init__ args kwargs autotuning will get occur scheduler so there no guarantee first lowering given key will also first benchmark share single precompilation function all lowerings particular key precompile_cache dict str Callable None = cache prescreening results ensure deterministic candidate selection prescreening_cache dict str OrderedSet str = list callbacks called after benchmarking feedback_saver_fns list FeedbackFunction = list callbacks called preprocess choices preprocessing_fns list PreprocessingFunction = _register_default_preprocessing_fns registers ` cache_clear ` called when fresh Inductor cache requested clear_on_fresh_cache _register_default_preprocessing_fns Register default preprocessing functions Note broken out into its own function so we can avoid clearing them i e so we can restore them after clearing user provided ones add_preprocessing_fn filter_choices_by_name_regex add_preprocessing_fn filter_choices_by_desc_regex cache_clear - None precompile_cache clear prescreening_cache clear pick_deterministic_choice choices list ChoiceCaller - ChoiceCaller assert len choices = externs = choice choice choices isinstance choice ExternKernelChoice len externs pyrefly ignore bad-return externs choices __call__ name choices list ChoiceCaller input_nodes layout optional dict mapping arg indices functions generating torch Tensor input corresponding ir Buffer passed given arg function will called instead generating random torch Tensor benchmarking input_gen_fns Optional dict int Callable ir Buffer torch Tensor = None precompilation_timeout_seconds int = return_multi_template=False best_config_future=None codegen cuda cuda_kernel CUDATemplateCaller Run preprocessing functions choices preprocessing_fn preprocessing_fns choices = preprocessing_fn choices Templates selected input_gen_fns require specific input data avoid IMA Passing custom input gen fns benchmark_fusion NYI so skip deferred template selection TODO jgong support multi-template CPU input_gen_fns None layout device type == cpu return_multi_template = False TODO - assert we have mutating kernels here mm_file_name = get_mm_log_filename M K = input_nodes - get_size N = input_nodes - get_size - append_to_log mm_file_name invoke str M K N create_no_valid_choices reason str - NoValidChoicesError backend_config = max_autotune_gemm_backends name = convolution max_autotune_conv_backends NoValidChoicesError f No choices select Provided reason reason f please consider adding ATEN into backend_config config defined torch _inductor config py allow least one choice len choices == raise create_no_valid_choices No choices exist backend log debug Max autotune selects s choices str len choices len choices == isinstance choices CUDATemplateCaller CUDATemplateCaller still needs go through autotuning process retrieve workspace size choices output_node config deterministic pick_deterministic_choice choices output_node inputs_key = create_inputs_key input_nodes TODO nmacchioni remove hacky way tell we ran benchmarking has_autotuned = False benchmark choices hint_override Optional int = None nonlocal has_autotuned TODO nmacchioni remove hacky way tell we ran benchmarking has_autotuned = True counters inductor select_algorithm_autotune += TODO nmacchioni remove layer abstraction construct ` benchmark_fn ` which should pick between in-process sub-process autotuning benchmark_fn = make_benchmark_fn choices input_nodes layout input_gen_fns hint_override=hint_override ` benchmark_fn choices ` will execute each choice dict choice timing which maps each choice its runtime calculated specified benchmarker milliseconds benchmark_fn choices autotune choices hint_override Optional int = None log debug Starting autotuning dynamo_timed f name _template_autotuning log_pt _compile_event=True dynamo_compile_column_us= compile_time_autotune_time_us metadata=_autotune_metadata input_nodes benchmark_results = benchmark choices hint_override=hint_override config max_autotune_report_choices_stats _log_autotune_choices_stats f name _template_autotuning benchmark_results benchmark_results config autotune_in_subproc Initialize suprocess pool so will warmup early torch _inductor autotune_process get_tuning_process_pool do_autotuning choices precompile_fn hint_override Optional int = None precompile_start_ts = time time dynamo_timed f name _template_precompiling log_pt _compile_event=True dynamo_compile_column_us= compile_time_autotune_time_us precompile_fn precompile_elapse = time time - precompile_start_ts log debug Precompilation elapsed time fs precompile_elapse Prune anything failed compile choices = c c choices c failed len choices == raise create_no_valid_choices All choices failed compile backend candidates = prescreen_choices choices name inputs_key prescreening_cache prescreening_elapse Optional float = None candidates prescreening_start_ts = time time timings = lookup candidates name inputs_key lambda choices autotune choices hint_override=hint_override hint_override=hint_override choices = prune_choices_postscreen choices timings name inputs_key prescreening_cache prescreening_elapse = time time - prescreening_start_ts log debug Prescreening elapsed time fs prescreening_elapse autotune_start_ts = time time best_config_future None best_config = await_sync best_config_future important_keys = ACC_TYPE ALLOW_TF BLOCK_K BLOCK_M BLOCK_N EVEN_K GROUP_M USE_FAST_ACCUM num_stages num_warps num_consumer_groups num_buffers_warp_spec choices = choice choice choices all f k = best_config k choice description k important_keys k important_keys log info Filtered d choices based best_config len choices timings = lookup choices name inputs_key lambda choices autotune choices hint_override=hint_override hint_override=hint_override autotune_elapse = time time - autotune_start_ts log debug Autotuning elapsed time fs autotune_elapse timings all math isfinite timing timing timings values raise NoValidChoicesError has_autotuned log getEffectiveLevel == logging DEBUG config trace log_autotuning_results log_results name input_nodes timings autotune_elapse precompile_elapse prescreening_elapse hint_override=hint_override profiler_bench_function we re running through normal caching autotuner method here because we want avoid returning cached value Avoid benchmarking separate process because s easy signal TuningProcess we should use profiler config patch profile_bandwidth_with_do_bench_using_profiling=True autotune_in_subproc=False benchmark choices feedback_fn feedback_saver_fns re-benchmarking same choices profiler bit expensive so pass thunk feedback_fn timings name input_nodes choices profiler_bench_function timings precompile_fn = make_precompile_fn choices name inputs_key precompilation_timeout_seconds=precompilation_timeout_seconds return_multi_template config max_autotune config max_autotune_gemm get_timings hint_override Optional int = None filtered_choices = c c choices hasattr c hint_override c hint_override == hint_override timings = do_autotuning filtered_choices precompile_fn hint_override=hint_override min_extern_choice = float inf choice timing timings items isinstance choice ExternKernelCaller min_extern_choice = min min_extern_choice timing timings = choice time choice time timings items time = min_extern_choice isinstance choice ExternKernelCaller timings We take union allowed prologue inputs all choices within benchmark fusion don t allow prologue fusion choices which don t support whole union allowed_prologue_inps OrderedSet str = OrderedSet c choices isinstance c TritonTemplateCaller allowed_prologue_inps &#124; = c allowed_prologue_inps torch _inductor ir TensorBox create torch _inductor ir MultiTemplateBuffer layout input_nodes get_timings choices allowed_prologue_inps timings = do_autotuning choices precompile_fn timings empty we really have no choice semi-random choice returning first ` ExternKernelCaller ` probably safest bet case since will generally ATen kernel there no ` ExternKernelCaller ` s then returning th kernel our next best option ideally we d fail whenever there no ATen kernel fallback s trivial figure out timings == choice choices isinstance choice ExternKernelCaller node = choice output_node log debug Autotuning returned empty timings falling back first ` ExternKernelCaller ` s node node node = choices output_node log debug Autotuning returned empty timings falling back first choice s node node we got any timings all pick best those choice = min timings key=timings __getitem__ node = choice output_node log debug Autotuning selected choice s node node make_precompile_fn choices name str inputs_key str precompilation_timeout_seconds Optional int = - Callable None Returns function precompiles given choices log debug Starting precompilation no_op args kwargs precompilation_timeout_seconds None precompilation_timeout_seconds = log debug Precompilation timeout None = returning no_op no_op num_workers = min get_num_workers len choices num_workers = no_op https github com python cpython issues sys version_info major == sys version_info minor == sys version_info micro = no_op check local global cache before precompiling timings = lookup choices name inputs_key benchmark=None timings len timings == len choices compilation precompile stage much cheaper than autotuning stage log debug Found all d timings cache returning no_op len timings no_op precompile_key = create_precompile_key name inputs_key choices precompile_func = precompile_cache get precompile_key log debug Precompile function found cache returning precompile_func log info Multithreaded precompilation d choices using d worker threads len choices num_workers In rare circumstances because python threads inherit global state thread pool executor can race leave stdout stderr state different than original values we explicitly restore state here avoid issue precompile_with_captured_stdout choice - tuple None int log debug Precompiling choice captured stdout s choice start_ns = time time_ns restore_stdout_stderr choice precompile elapsed_ns = time time_ns - start_ns Return tuple triton async compile _worker_compile_triton returns tuple CachingAutotuner int None elapsed_ns on_complete future future exception _ precompile_elapsed_us = future result elapsed_seconds = precompile_elapsed_us e elapsed_times future = elapsed_seconds log debug Precompilation complete future s elapsed time fs future elapsed_seconds executor = ThreadPoolExecutor max_workers=num_workers async_compile = torch _inductor async_compile AsyncCompile futures dict concurrent futures Future Any ChoiceCaller = elapsed_times dict concurrent futures Future Any float = Some choices only differ runtime arguments so we skip choice has same hash previously seen choice seen_choices OrderedSet str = OrderedSet c choices Skip choices which we have already issued precompile c kernel_hash_key seen_choices log debug Skipping already seen choice s c continue seen_choices add c kernel_hash_key hasattr c precompile triton_cuda_choice = isinstance c TritonTemplateCaller isinstance c bmreq TritonGPUBenchmarkRequest triton_cuda_choice async_compile use_process_pool open c bmreq module_path file source_code = file read future = async_compile triton kernel_name=c bmreq kernel_name source_code=source_code future log debug Submitted triton async compile choice s c future = executor submit precompile_with_captured_stdout c log debug Submitted precompile choice s c future add_done_callback on_complete futures future = c functools cache restore_stdout_stderr wait_on_futures log debug Waiting futures counters inductor select_algorithm_precompile += exceptions list tuple ChoiceCaller BaseException = future as_completed futures timeout=precompilation_timeout_seconds e = future exception counters inductor select_algorithm_num_precompilation_exceptions += exceptions append futures future e torch _inductor codegen cuda cuda_kernel CUDATemplateCaller isinstance e CUDACompileError isinstance futures future CUDATemplateCaller log debug Exception s benchmark choice s e futures future exc_info=e futures future mark_failed log exception noqa G Exception s benchmark choice s e futures future exc_info=e futures future mark_failed counters inductor select_algorithm_num_precompiles += log info Precompiling benchmark choice s took fs futures get future elapsed_times get future exceptions _log_autotune_exceptions exceptions executor shutdown wait=True precompile_cache precompile_key = wait_on_futures wait_on_futures classmethod get_inputs cls choices Sequence ChoiceCaller input_nodes list ir IRNode layout ir Layout input_gen_fns Optional dict int Callable ir Buffer torch Tensor hint_override Optional int = None - AutotuneArgs Factory method create AutotuneArgs list ChoiceCallers input_gen_fns None input_gen_fns = de-duplicate args unique_example_inputs = x get_name input_gen_fns get i lambda x cls benchmark_example_value x hint_override=hint_override pyrefly ignore bad-argument-type x i x enumerate input_nodes example_inputs = list unique_example_inputs values example_inputs_extern = input_node input_nodes unique_example_inputs input_node get_name is_mkldnn example_inputs_extern append unique_example_inputs input_node get_name base = unique_example_inputs input_node get_name base = base base _base None base _base sizes = tuple V graph sizevars atomically_apply_size_hint size fallback=config unbacked_symint_fallback hint_override=hint_override size input_node get_size strides = tuple V graph sizevars atomically_apply_size_hint stride fallback=config unbacked_symint_fallback hint_override=hint_override stride input_node get_stride storage_offset = V graph sizevars atomically_apply_size_hint input_node get_layout offset fallback=config unbacked_symint_fallback hint_override=hint_override Check required storage size exceeds current storage avoid illegal memory access needed_size = torch _prims_common compute_required_storage_length sizes strides storage_offset current_size = base storage size needed_size current_size Create new base tensor sufficient storage new_base = torch randn needed_size dtype=base dtype device=base device requires_grad=base requires_grad base = new_base as_strided base size base stride base storage_offset example_inputs_extern append torch as_strided base sizes strides storage_offset out = cls benchmark_example_value layout hint_override=hint_override Also check output tensor storage size out_base = out out _base None out _base out_offset = V graph sizevars size_hint layout offset needed_out_size = torch _prims_common compute_required_storage_length out size out stride out_offset current_out_size = out_base storage size needed_out_size current_out_size Create new base tensor sufficient storage new_out_base = torch randn needed_out_size dtype=out_base dtype device=out_base device requires_grad=out_base requires_grad out_base = new_out_base as_strided out_base size out_base stride out_base storage_offset out_extern = torch as_strided out_base out size out stride out_offset expected = None VERIFY choices benchmark example_inputs_extern out=out_extern expected = out_extern clone AutotuneArgs from_choice_args example_inputs example_inputs_extern out out_extern expected staticmethod _is_extern choice ChoiceCaller - bool isinstance choice ExternKernelCaller SubgraphChoiceCaller classmethod benchmark_choice cls choice ChoiceCaller autotune_args AutotuneArgs - float benchmark_tensors = autotune_args get_benchmark_tensors cls _is_extern choice inputs output = benchmark_tensors unpack output zero_ result = choice benchmark inputs out=output device_type = next tensor device type tensor inputs is_gpu tensor device type cuda device_interface = get_interface_for_device device_type device_interface is_available device_interface synchronize shake out any CUDA errors VERIFY autotune_args expected None autotune_args verify VERIFY result classmethod benchmark_choices cls choices Sequence ChoiceCaller autotune_args AutotuneArgs - dict ChoiceCaller float timings = choice choices try timing = cls benchmark_choice choice autotune_args except CUDACompileError torch _inductor codegen cuda cuda_kernel CUDATemplateCaller isinstance choice CUDATemplateCaller log exception CUDA compilation error during autotuning \n s \nIgnoring choice timing = float inf except NotImplementedError log warning Not yet implemented exc_info=True timing = float inf except RuntimeError e torch _inductor codegen cuda cuda_kernel CUDATemplateCaller msg = str e invalid argument msg msg += \n\nThis may mean GPU too small max_autotune mode \n\n illegal memory access msg msg += \n\nEither error template triton bug \n unspecified launch failure msg msg += \n\nAn unrecoverable unspecified launch failure caught during autotuning msg += \nPlease try re-running TORCHINDUCTOR_AUTOTUNE_IN_SUBPROC= \n\n isinstance choice CUDATemplateCaller log debug Runtime error during autotuning \n s \nIgnoring choice msg exc_info=True log error Runtime error during autotuning \n s \nIgnoring choice msg timing = float inf except AssertionError e raise AssertionError noqa B f Incorrect result choice choice \n\n e except Exception e try triton runtime autotuner OutOfResources isinstance e OutOfResources log warning e noqa G timing = float inf raise e except ImportError raise e None timings choice = timing timings classmethod benchmark_in_current_process cls choices Sequence ChoiceCaller input_nodes list ir IRNode layout ir Layout input_gen_fns Optional dict int Callable ir Buffer torch Tensor hint_override Optional int = None - dict ChoiceCaller float inputs = cls get_inputs choices input_nodes layout input_gen_fns hint_override=hint_override cls benchmark_choices choices inputs classmethod benchmark_in_sub_process cls choices Sequence ChoiceCaller input_nodes list ir IRNode layout ir Layout input_gen_fns Optional dict int Callable ir Buffer torch Tensor hint_override Optional int = None autotune_process only benchmark triton kernel sub process now ATen Extern kernel still benchmarked current process extern = c c choices cls _is_extern c triton = c c choices cls _is_extern c timings = cls benchmark_in_current_process extern input_nodes layout input_gen_fns hint_override=hint_override timings update autotune_process benchmark_in_sub_process triton type ignore arg-type timings classmethod make_benchmark_fn cls choices Sequence ChoiceCaller input_nodes list ir IRNode layout ir Layout input_gen_fns Optional dict int Callable ir Buffer torch Tensor hint_override Optional int = None DEBUG print f len choices tuning requests config autotune_in_subproc functools partial cls benchmark_in_sub_process input_nodes=input_nodes layout=layout input_gen_fns=input_gen_fns hint_override=hint_override functools partial cls benchmark_in_current_process input_nodes=input_nodes layout=layout input_gen_fns=input_gen_fns hint_override=hint_override staticmethod prescreen_choices choices list ChoiceCaller name str inputs_key str prescreen_cache dict str OrderedSet str - list ChoiceCaller Figure out what choices need prescreened before autotuning runtime params Prescreening process reducing number autotuning choices runtime params via two stage autotuning process First we fix set runtime params here we use swizzle= run autotuning get set candidates Then we run autotuning again candidates full set runtime params Since have concept runtime params we need differentiate between choice s hash_key choice s kernel_hash_key The former includes information like runtime params while latter does prescreen_cache exists stores set hash_key should win prescreening Right now only CUTLASS choices have runtime params Create cache key prescreening results prescreen_key = f name inputs_key Check we have cached prescreening results prescreen_winners prescreen_key prescreen_cache prescreen_winners = choice choice choices choice hash_key prescreen_cache prescreen_key prescreen_winners prescreen cutlass codegen cuda cuda_kernel CUDATemplateCaller candidates = config cuda cutlass_prescreening len config cuda cutlass_max_profiling_swizzle_options candidates extend c c choices isinstance c CUDATemplateCaller hardcoded only look swizzle= c info_dict get swizzle == skip prescreening number candidates too small len candidates candidates type ignore return-value staticmethod prune_choices_postscreen choices list ChoiceCaller candidate_timings dict ChoiceCaller float name str inputs_key str prescreen_cache dict str OrderedSet str - list ChoiceCaller Prune choices after prescreening codegen cuda cuda_kernel CUDATemplateCaller prescreen_key = f name inputs_key Check we have cached postscreen results prescreen_key prescreen_cache candidate_timings choices have won prescreening already winner_kernel_hashes = candidate kernel_hash_key candidate candidate_timings pruned_choices = choice choice choices isinstance choice CUDATemplateCaller choice kernel_hash_key winner_kernel_hashes pruned_choices log debug Before pruning using prescreening timings d choices len choices sorted_candidates = sorted candidate_timings keys key=lambda choice candidate_timings choice Print prescreening timings candidate_timings PRINT_AUTOTUNE config autotune_num_choices_displayed = n = config autotune_num_choices_displayed top_k = sorted_candidates n best = top_k best_time = candidate_timings best lines = PRESCREENING CANDIDATE TIMINGS choice top_k result = candidate_timings choice result lines append f choice name result f ms best_time result choice description lines append f choice name result f ms DIVIDED BY ZERO ERROR log info \n join lines num_to_keep = max int math sqrt len choices prune choices based prescreening timings candidates_to_prune = OrderedSet candidate kernel_hash_key candidate sorted_candidates num_to_keep winner_hashes OrderedSet str = OrderedSet candidate sorted_candidates num_to_keep candidate_timings candidate == float inf candidates_to_prune add candidate kernel_hash_key winner_hashes add candidate hash_key isinstance candidate CUDATemplateCaller candidate bmreq ensure_dll_loaded pruned_choices = choice choice choices choice kernel_hash_key candidates_to_prune type ignore attr-defined Cache hash_key winners prescreening prescreen_cache prescreen_key = winner_hashes log debug After pruning using prescreening timings d choices len pruned_choices pruned_choices staticmethod get_flex_attention_choice_info choice ChoiceCaller timings dict ChoiceCaller float - dict str Any isinstance choice torch _inductor select_algorithm ExternKernelCaller type extern time timings choice assert isinstance choice torch _inductor select_algorithm TritonTemplateCaller info = choice info_dict result = type triton time timings choice key AlgorithmSelectorCache FLEX_ATTENTION_TUNABLE_KEYS key info pyrefly ignore unsupported-operation result key = info key result staticmethod maybe_log_flex_attention_results name str input_nodes list ir IRNode timings dict ChoiceCaller float - None flex_attention_filename = get_flex_attention_log_filename flex_attention_filename flex_attention name len input_nodes query_size = input_nodes get_size key_size = input_nodes get_size value_size = input_nodes get_size B = query_size Hq = query_size seq_len_q = query_size qk_head_dim = query_size Hkv = key_size seq_len_kv = key_size v_head_dim = value_size kernel_type = backward backward name forward dims_key = str kernel_type B Hq Hkv seq_len_q seq_len_kv qk_head_dim v_head_dim sorted_choices = sorted timings key=timings __getitem__ out_dict = dims_key AlgorithmSelectorCache get_flex_attention_choice_info choice timings choice sorted_choices append_to_log flex_attention_filename out_dict staticmethod log_results name str input_nodes list ir IRNode timings dict ChoiceCaller float elapse float precompile_elapse float prescreening_elapse Optional float = None hint_override Optional int = None Log autotuning results currently only handles mm flex V debug log_autotuning_results name input_nodes timings elapse precompile_elapse config max_autotune config max_autotune_gemm PRINT_AUTOTUNE sizes = join x join map str V graph sizevars size_hints n get_size fallback=config unbacked_symint_fallback type ignore arg-type hint_override=hint_override n input_nodes strides = join str n get_stride n input_nodes dtypes = join str n get_dtype n input_nodes config autotune_num_choices_displayed == when autotune_num_choices_displayed None None means all n = config autotune_num_choices_displayed top_k = sorted timings key=timings __getitem__ n best = top_k get_choice_info choice isinstance choice torch _inductor select_algorithm ExternKernelCaller type cublas time timings choice assert isinstance choice torch _inductor select_algorithm TritonTemplateCaller info = choice info_dict tile = info tile_shape tile_vals = eval tile type ignore arg-type BLOCK_M = tile_vals BLOCK_K = tile_vals BLOCK_N = tile_vals type triton time timings choice BLOCK_M BLOCK_M BLOCK_K BLOCK_K BLOCK_N BLOCK_N num_stages info num_stages num_warps info num_warps mm_filename = get_mm_log_filename mm_filename mm name M K = input_nodes - get_size N = input_nodes - get_size - out_dict = str M K N get_choice_info choice choice timings keys append_to_log mm_filename out_dict AlgorithmSelectorCache maybe_log_flex_attention_results name input_nodes timings best_time = timings best sys stderr write f AUTOTUNE name sizes \n sys stderr write f strides strides \n sys stderr write f dtypes dtypes \n choice top_k result = timings choice result kernel_description = choice description sys stderr write f choice name result f ms best_time result kernel_description \n sys stderr write f choice name result f ms DIVIDED BY ZERO ERROR \n autotune_type_str = SubProcess config autotune_in_subproc SingleProcess prescreening_msg = f prescreening_elapse f seconds prescreening prescreening_elapse None sys stderr write f autotune_type_str AUTOTUNE benchmarking takes elapse f seconds precompile_elapse f f seconds precompiling len timings choices + prescreening_msg + \n staticmethod benchmark_example_value node hint_override Optional int = None Convert ir Buffer into concrete torch Tensor we can use benchmarking isinstance node ir Layout node = ir Buffer name= fake layout=node triton templates want base tensor isinstance node ir BaseView node = node unwrap_view Inplace padding may reinterpret tensor larger tensor stride large enough The V graph get_allocation_size takes into account So we need call as_strided end view tensor correct sizes strides AlgorithmSelectorCache generate_example_value tuple V graph sizevars atomically_apply_size_hint size fallback=config unbacked_symint_fallback hint_override=hint_override size node get_size tuple V graph sizevars atomically_apply_size_hint stride fallback=config unbacked_symint_fallback hint_override=hint_override stride node get_stride node get_device node get_dtype V graph sizevars atomically_apply_size_hint pyrefly ignore missing-attribute node layout offset fallback=config unbacked_symint_fallback hint_override=hint_override tuple V graph sizevars atomically_apply_size_hint size fallback=config unbacked_symint_fallback hint_override=hint_override pyrefly ignore bad-argument-type size V graph get_allocation_size node staticmethod generate_example_value size stride device dtype extra_size allocation_size=None preserve rng states avoid rand_strided call below changes rng states real model code preserve_rng_state allocation_size None allocation_size == size rand_strided size stride device=device dtype=dtype extra_size=extra_size rand_strided allocation_size stride device=device dtype=dtype extra_size=extra_size as_strided size stride staticmethod key_of node Extract pieces ir Buffer we should invalidate cached autotuning results sizevars = V graph sizevars node get_device type str node get_dtype sizevars size_hints node get_size fallback=config unbacked_symint_fallback tuple V graph sizevars atomically_apply_size_hint stride fallback=config unbacked_symint_fallback stride node get_stride sizevars size_hint node get_layout offset fallback=config unbacked_symint_fallback add_feedback_saver fn FeedbackFunction feedback_saver_fns append fn clear_feedback_savers feedback_saver_fns = add_preprocessing_fn fn PreprocessingFunction preprocessing_fns append fn clear_preprocessing_fns clear_defaults bool = False Clear preprocessing functions Args clear_defaults If True clears all functions including defaults If False clears only user-added functions re-registers defaults preprocessing_fns clear clear_defaults _register_default_preprocessing_fns _ALGORITHM_SELECTOR_CACHE Optional AlgorithmSelectorCache = None get_algorithm_selector_cache - AlgorithmSelectorCache Get global algorithm selector cache creating doesn t exist global _ALGORITHM_SELECTOR_CACHE _ALGORITHM_SELECTOR_CACHE None _ALGORITHM_SELECTOR_CACHE = AlgorithmSelectorCache _ALGORITHM_SELECTOR_CACHE autotune_select_algorithm args kwargs cache = get_algorithm_selector_cache return_multi_template kwargs kwargs return_multi_template = torch _inductor config benchmark_epilogue_fusion precompilation_timeout_seconds kwargs kwargs precompilation_timeout_seconds = config precompilation_timeout_seconds cache args kwargs add_feedback_saver fn FeedbackFunction cache = get_algorithm_selector_cache cache add_feedback_saver fn clear_feedback_savers Clear all feedback saver functions cache = get_algorithm_selector_cache cache clear_feedback_savers add_preprocessing_fn fn PreprocessingFunction Add preprocessing function applied choices before autotuning Preprocessing functions called sequentially order they registered each function receiving output previous one They can filter reorder transform modify list choices any way Args fn A function takes list ChoiceCaller objects returns modified list ChoiceCaller objects Example my_filter choices Filter out choices certain names c c choices slow c name lower add_preprocessing_fn my_filter cache = get_algorithm_selector_cache cache add_preprocessing_fn fn clear_preprocessing_fns clear_defaults bool = False Clear preprocessing functions module level Args clear_defaults If True clears all functions including defaults If False clears only user-added functions re-registers defaults cache = get_algorithm_selector_cache cache clear_preprocessing_fns clear_defaults realize_inputs args len args == ir ExternKernel require_stride ir ExternKernel realize_input args realize_inputs x x args SymbolicGridFn Wrapper around grid function allows either int sympy inputs SymbolicGridFn grid x meta cdiv cdiv x meta BLOCK_X __init__ fn Callable tuple Any Any Any fn = fn kwargs_int = kwargs_sym = params = inspect signature fn parameters name fn_sym fn_int cdiv CeilDiv ceildiv min sympy Min min max sympy Max max name params kwargs_int name = fn_int kwargs_sym name = fn_sym __call__ args kwargs - tuple int int int fn args kwargs kwargs_int sympy_call args kwargs fn args kwargs kwargs_sym _autotune_metadata input_nodes Helper function extract autotune metadata input nodes autotune_strides join str n get_stride n input_nodes autotune_dtypes join str n get_dtype n input_nodes autotune_shape join x join map str n get_size n input_nodes autotune_offset join str n get_layout offset n input_nodes TODO coconutruben replace taking KernelInputs argument extracting those out there directly autotune_strides_hinted join str V graph sizevars size_hints n get_stride fallback=config unbacked_symint_fallback n input_nodes autotune_shape_hinted join x join map str V graph sizevars size_hints n get_size fallback=config unbacked_symint_fallback n input_nodes _log_autotune_choices_stats event_name str timings dict ChoiceCaller float - None Helper function extract autotune metadata benchmark results timings None metadata dict str Union int float str = num_choices len timings num_triton_choices len c c timings isinstance c TritonTemplateCaller sorted_choices = sorted timings key=timings __getitem__ best_choice = sorted_choices metadata best_kernel = best_choice name best_choice description metadata best_kernel_desc = best_choice description metadata best_time = timings best_choice best_triton_pos = next i i choice enumerate sorted_choices isinstance choice TritonTemplateCaller None best_triton_pos None metadata best_triton_pos = best_triton_pos best_triton_kernel = sorted_choices best_triton_pos best_triton_pos = metadata best_triton_time = timings best_triton_kernel metadata best_triton_kernel = best_triton_kernel name best_triton_kernel description metadata best_triton_kernel_desc = best_triton_kernel description payload = json dumps metadata get_chromium_event_logger add_event_data event_name autotune_choices_stats=payload sys stderr write f Autotune Choices Stats \n payload \n _log_autotune_exceptions exceptions list tuple ChoiceCaller BaseException - None Log autotune exceptions chromium event logger exceptions try pt _compile_substack = get_chromium_event_logger get_pt _compile_substack pt _compile_substack current_event = pt _compile_substack - current_event endswith _template_precompiling exception_details = choice exc exceptions try choice_type = triton isinstance choice TritonTemplateCaller other data = choice_type choice_type choice choice description exception_message str exc exc_type_match = re search r \w+ str exc exc_type_match data exception = exc_type_match group OutOfMemoryError str exc required_match = re search r Required \d+ str exc required_match data required_memory = required_match group limit_match = re search r Hardware limit \s \d+ str exc limit_match data hardware_limit = limit_match group exception_details append data except Exception Don t let logging errors break main flow continue exception_details metadata = json dumps exceptions exception_details get_chromium_event_logger try_add_event_data current_event metadata=metadata except Exception Silently ignore logging errors avoid breaking autotune pass ensure lowering imported so ` extern_kernels ` populated lowering noqa F