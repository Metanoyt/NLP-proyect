Owner s module inductor torch torch utils _pytree pytree torch _inductor pattern_matcher CallFunctionVarArgs PatternMatcherPass register_graph_pattern torch _inductor test_case run_tests TestCase InductorTestCase torch testing _internal common_utils instantiate_parametrized_tests IS_LINUX parametrize torch testing _internal inductor_utils HAS_CUDA_AND_TRITON TestNeedsExactStrides InductorTestCase parametrize dtype torch float torch float _e m fnu test_custom_op dtype device = cuda float _e m fnu errors cpu x = torch ones device=device dtype=torch float _e m fnu other = torch ones device=device dtype=torch float _e m fnu _CustomPass PatternMatcherPass __init__ - None super __init__ __call__ g torch fx Graph apply g g = _CustomPass called = False register_graph_pattern CallFunctionVarArgs torch ops aten permute pass_dict=g _ match args kwargs flat_args spec = pytree tree_flatten args kwargs decomp flat_args args kwargs = pytree tree_unflatten flat_args spec torch ops mylib force_channels_last torch ops aten permute args kwargs nonlocal called called = True match replace_by_example decomp flat_args torch _inductor config TestPassed RuntimeError pass torch library _scoped_library mylib FRAGMENT lib lib define force_channels_last Tensor x - Tensor tags= torch _C Tag flexible_layout impl x x clone memory_format=torch channels_last lib impl force_channels_last impl CompositeExplicitAutograd lib define add_op Tensor x Tensor y - Tensor impl x y assert x transpose is_contiguous assert y is_contiguous x float + y float meta x y x float + y float lib impl add_op impl CompositeExplicitAutograd lib impl add_op meta Meta f x other torch ops mylib add_op default x transpose other config patch post_grad_custom_post_pass=g try f_compile = torch compile f fullgraph=True f_compile x other except TestPassed pass assert called instantiate_parametrized_tests TestNeedsExactStrides __name__ == __main__ IS_LINUX HAS_CUDA_AND_TRITON run_tests needs= filelock