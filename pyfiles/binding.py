os shutil traceback collections defaultdict collections abc Callable Iterable dataclasses asdict dataclass enum Enum functools wraps logging getLogger typing Optional ParamSpec TypeVar torch torch _utils_internal signpost_event __all__ = AffinityMode maybe_wrap_command_args_with_numa_binding maybe_wrap_with_numa_binding NumaOptions logger = getLogger __name__ AffinityMode str Enum See behavior description each affinity mode torch distributed run NODE = node SOCKET = socket EXCLUSIVE = exclusive CORE_COMPLEX = core-complex dataclass frozen=True NumaOptions affinity_mode AffinityMode If true we will fall back using original command entrypoint we fail compute NUMA bindings You should avoid using option It only intended safety mechanism facilitating mass rollouts numa binding should_fall_back_if_binding_fails bool = False maybe_wrap_command_args_with_numa_binding command_args tuple str gpu_index int numa_options Optional NumaOptions - tuple str Wraps command arguments numactl apply NUMA CPU binding This function prepends numactl appropriate CPU affinity flags provided command arguments binding process CPUs associated specified GPU s NUMA node Args command_args The original command arguments wrap gpu_index The index GPU will used subprocess numa_options Configuration NUMA binding behavior If None returns original command_args unchanged Returns Tuple command arguments potentially wrapped numactl NUMA binding Returns original command_args numa_options None binding fails fallback enabled numa_options None command_args kwargs = command_args command_args gpu_index gpu_index numa_options asdict numa_options try logical_cpu_indices = _get_validated_logical_cpus_to_bind_to gpu_index=gpu_index numa_options=numa_options wrapped_command_args = _assemble_numactl_command_args original_command_args=command_args logical_cpu_indices=logical_cpu_indices signpost_event category= numa_binding name= apply_success parameters= kwargs wrapped_command wrapped_command_args wrapped_command_args except Exception pyrefly ignore bad-argument-type _handle_exception numa_options=numa_options logger_kwargs=kwargs command_args _TParams = ParamSpec _TParams _TReturn = TypeVar _TReturn maybe_wrap_with_numa_binding func Callable _TParams _TReturn gpu_index int numa_options Optional NumaOptions - Callable _TParams _TReturn Wraps function apply NUMA CPU binding before execution This decorator applies NUMA CPU affinity all threads current process before calling wrapped function binding them CPUs associated specified GPU s NUMA node Args func The function wrap NUMA binding gpu_index The index GPU will used numa_options Configuration NUMA binding behavior If None returns original function unchanged Returns A wrapped function applies NUMA binding before execution original function numa_options None numa_options None func wraps func wrapped args _TParams args kwargs _TParams kwargs - _TReturn _maybe_apply_numa_binding_to_current_process gpu_index=gpu_index pyrefly ignore bad-argument-type numa_options=numa_options func args kwargs wrapped _maybe_apply_numa_binding_to_current_process gpu_index int numa_options NumaOptions - None kwargs = gpu_index gpu_index numa_options asdict numa_options try logical_cpu_indices = _get_validated_logical_cpus_to_bind_to gpu_index=gpu_index numa_options=numa_options _bind_all_threads_in_current_process_to_logical_cpus logical_cpu_indices=logical_cpu_indices signpost_event category= numa_binding name= apply_success parameters= kwargs logical_cpu_indices _get_ranges_str_from_ints logical_cpu_indices except Exception pyrefly ignore bad-argument-type _handle_exception numa_options=numa_options logger_kwargs=kwargs _assemble_numactl_command_args original_command_args tuple str logical_cpu_indices set int - tuple str numactl f -- physcpubind= _get_ranges_str_from_ints logical_cpu_indices original_command_args _handle_exception numa_options NumaOptions logger_kwargs dict str object - None signpost_event category= numa_binding name= apply_exception parameters= logger_kwargs traceback traceback format_exc logger exception Failed apply NUMA binding input= r logger_kwargs numa_options should_fall_back_if_binding_fails logger warning Continuing executing without applying NUMA binding despite exception s traceback format_exc This function called within except block so silence warning about raise without exception raise noqa PLE _get_validated_logical_cpus_to_bind_to gpu_index int numa_options NumaOptions - set int logical_cpu_indices = _get_logical_cpus_to_bind_to gpu_index=gpu_index numa_options=numa_options _raise_if_binding_invalid logical_cpu_indices=logical_cpu_indices logical_cpu_indices _raise_if_binding_invalid logical_cpu_indices set int - None NOTE numactl CLI only actually necessary str entrypoint path simplicity we will just check no matter what shutil which numactl None raise RuntimeError numactl CLI required NUMA binding logical_cpu_indices raise RuntimeError Must bind non-empty set CPU indices _bind_all_threads_in_current_process_to_logical_cpus logical_cpu_indices set int - None Save original affinity main thread before changing pyrefly ignore missing-attribute original_main_thread_affinity = os sched_getaffinity type ignore attr-defined represents current thread This outside try except because main thread should always bind successfully pyrefly ignore missing-attribute os sched_setaffinity logical_cpu_indices type ignore attr-defined tid_str os listdir proc task try tid = int tid_str pyrefly ignore missing-attribute tid_affinity = os sched_getaffinity tid type ignore attr-defined Defensive check ensure we do overwrite affinity any threads have already had their affinity set elsewhere tid_affinity == original_main_thread_affinity pyrefly ignore missing-attribute os sched_setaffinity tid logical_cpu_indices type ignore attr-defined except Exception Thread may have exited otherwise become invalid pass _get_logical_cpus_to_bind_to gpu_index int numa_options NumaOptions - set int Args gpu_index The index GPU will used subprocess Example numa_options See NumaOptions details Returns Set logical CPU indices bind numa_options affinity_mode == AffinityMode NODE logical_cpus = _node_get_logical_cpus_to_bind_to gpu_index=gpu_index numa_options affinity_mode == AffinityMode SOCKET logical_cpus = _socket_get_logical_cpus_to_bind_to gpu_index=gpu_index numa_options affinity_mode == AffinityMode EXCLUSIVE logical_cpus = _exclusive_get_logical_cpus_to_bind_to gpu_index=gpu_index numa_options affinity_mode == AffinityMode CORE_COMPLEX logical_cpus = _core_complex_get_logical_cpus_to_bind_to gpu_index=gpu_index raise ValueError f Affinity mode numa_options affinity_mode supported logical_cpus _node_get_logical_cpus_to_bind_to gpu_index int - set int Core logic node numa strategy numa_node_index = _get_numa_node_index_for_gpu_index gpu_index=gpu_index _get_allowed_logical_cpu_indices_for_numa_node numa_node_index=numa_node_index _socket_get_logical_cpus_to_bind_to gpu_index int - set int Core logic socket numa strategy numa_node_index_of_gpu = _get_numa_node_index_for_gpu_index gpu_index=gpu_index socket_index = _get_socket_index_for_numa_node numa_node_index=numa_node_index_of_gpu numa_node_indices = _get_numa_node_indices_for_socket_index socket_index=socket_index logical_cpus = set numa_node_index numa_node_indices logical_cpus update _get_allowed_logical_cpu_indices_for_numa_node numa_node_index=numa_node_index logical_cpus _exclusive_get_logical_cpus_to_bind_to gpu_index int - set int Core logic exclusive numa strategy numa_node_index = _get_numa_node_index_for_gpu_index gpu_index=gpu_index gpu_indices = _get_gpu_indices_for_numa_node numa_node_index=numa_node_index gpu_indices = sorted gpu_indices original_gpu_relative_index = gpu_indices index gpu_index allowed_logical_cpu_indices = _get_allowed_logical_cpu_indices_for_numa_node numa_node_index=numa_node_index Arbitrarily use min logical cpu index physical core represent physical core physical_core_to_allowed_logical_cpu_indices = _group_by allowed_logical_cpu_indices lambda logical_cpu_index min _get_logical_cpu_indices_sharing_same_physical_core_as logical_cpu_index=logical_cpu_index Sort dict consistency dicts maintain order Python physical_core_to_allowed_logical_cpu_indices = dict sorted physical_core_to_allowed_logical_cpu_indices items num_physical_cores_per_gpu = len physical_core_to_allowed_logical_cpu_indices len gpu_indices Often number physical cores will perfectly divisible number GPUs In those cases give lowest GPU indices extra core num_gpus_to_give_one_extra_physical_core = len physical_core_to_allowed_logical_cpu_indices len gpu_indices num_physical_cores_per_gpu raise RuntimeError f There only len physical_core_to_allowed_logical_cpu_indices physical cores numa_node_index= + f there len gpu_indices GPUs associated NUMA node Compute slice indices GPU start = original_gpu_relative_index num_physical_cores_per_gpu + min original_gpu_relative_index num_gpus_to_give_one_extra_physical_core end = start + num_physical_cores_per_gpu + original_gpu_relative_index num_gpus_to_give_one_extra_physical_core Slice flatten logical CPUs selected physical cores logical_cpu_indices_for_original_gpu = logical_cpu_index logical_cpu_indices list physical_core_to_allowed_logical_cpu_indices values start end logical_cpu_index logical_cpu_indices logical_cpu_indices_for_original_gpu _core_complex_get_logical_cpus_to_bind_to gpu_index int - set int Core logic core-complex numa strategy Each GPU assigned full core complex group cores sharing L cache within its affined NUMA node numa_node_index = _get_numa_node_index_for_gpu_index gpu_index=gpu_index gpu_indices = _get_gpu_indices_for_numa_node numa_node_index=numa_node_index gpu_indices = sorted gpu_indices original_gpu_relative_index = gpu_indices index gpu_index allowed_logical_cpu_indices = _get_allowed_logical_cpu_indices_for_numa_node numa_node_index=numa_node_index Arbitrarily use min logical cpu index max level cache represent max level cache max_level_cache_to_allowed_logical_cpu_indices = _group_by allowed_logical_cpu_indices lambda logical_cpu_index min _get_logical_cpus_sharing_same_max_level_cache_as logical_cpu_index=logical_cpu_index max_level_cache_to_allowed_logical_cpu_indices = dict sorted max_level_cache_to_allowed_logical_cpu_indices items First prioritize caches more available cpus Second prioritize lower index cpus just clarity consistency key=lambda item -len item item cache_index_for_original_gpu = original_gpu_relative_index len max_level_cache_to_allowed_logical_cpu_indices logical_cpu_indices_for_original_gpu = list max_level_cache_to_allowed_logical_cpu_indices values cache_index_for_original_gpu logical_cpu_indices_for_original_gpu K = TypeVar K V = TypeVar V _group_by values Iterable V get_key Callable V K - dict K set V Groups elements same key into sets key_to_values defaultdict K set V = defaultdict set value values key = get_key value key_to_values key add value key_to_values _get_logical_cpu_indices_sharing_same_physical_core_as logical_cpu_index int - set int thread_siblings_list_absolute_path = f sys devices system cpu cpu logical_cpu_index topology thread_siblings_list open thread_siblings_list_absolute_path f _get_set_of_int_from_ranges_str f read _get_logical_cpus_sharing_same_max_level_cache_as logical_cpu_index int - set int cpu_cache_dir_absolute_path = f sys devices system cpu cpu logical_cpu_index cache max_level = - logical_cpus_sharing_max_level_cache = set entry os listdir cpu_cache_dir_absolute_path entry startswith index entry isdecimal continue cache_index_absolute_path = os path join cpu_cache_dir_absolute_path entry Filter out other cache types like Instruction type_absolute_path = os path join cache_index_absolute_path type open type_absolute_path type_file type_file read strip Unified Data continue level_absolute_path = os path join cache_index_absolute_path level open level_absolute_path level_file level = int level_file read level = max_level continue max_level = level shared_cpu_list_absolute_path = os path join cache_index_absolute_path shared_cpu_list open shared_cpu_list_absolute_path share_cpu_list_file logical_cpus_sharing_max_level_cache = _get_set_of_int_from_ranges_str share_cpu_list_file read logical_cpus_sharing_max_level_cache _get_allowed_logical_cpu_indices_for_numa_node numa_node_index int - set int all_cpu_indices = _get_cpu_indices_for_numa_node_MAYBE_NOT_ALLOWED numa_node_index=numa_node_index allowed_cpu_indices = _get_allowed_cpu_indices_for_current_thread all_cpu_indices allowed_cpu_indices _get_cpu_indices_for_numa_node_MAYBE_NOT_ALLOWED numa_node_index int - set int Returns Indices all CPUs associated numa_node_index However list filtered based whether thread allowed use them cpulist_absolute_path = f sys devices system node node numa_node_index cpulist try open cpulist_absolute_path f cpu_range_str = f read except FileNotFoundError e raise RuntimeError f Could determine CPUs corresponding numa_node_index= e _get_set_of_int_from_ranges_str cpu_range_str _get_gpu_count - int torch cuda device_count _get_numa_node_index_for_gpu_index gpu_index int - int device_properties = torch cuda get_device_properties gpu_index domain = device_properties pci_domain_id type ignore attr-defined bus = device_properties pci_bus_id type ignore attr-defined device = device_properties pci_device_id type ignore attr-defined Format sysfs PCI address dc pci_addr = f domain x bus x device x pci_numa_node_absolute_path = f sys bus pci devices pci_addr numa_node open pci_numa_node_absolute_path f In systems only one NUMA node will often saved - In those cases there obviously least one numa node so we use max int f read strip _get_gpu_indices_for_numa_node numa_node_index int - set int gpu_index gpu_index range _get_gpu_count _get_numa_node_index_for_gpu_index gpu_index=gpu_index == numa_node_index _get_socket_index_for_numa_node numa_node_index int - int arbitrary_cpu_index = _get_arbitrary_allowed_cpu_index_for_numa_node numa_node_index=numa_node_index _get_socket_index_for_cpu cpu_index=arbitrary_cpu_index _get_socket_index_for_cpu cpu_index int - int package_id_absolute_path = f sys devices system cpu cpu cpu_index topology physical_package_id try open package_id_absolute_path f int f read strip except FileNotFoundError e raise RuntimeError f Could determine socket cpu_index= e _get_arbitrary_allowed_cpu_index_for_numa_node numa_node_index int - int min _get_allowed_logical_cpu_indices_for_numa_node numa_node_index=numa_node_index _get_set_of_int_from_ranges_str ranges_str str - set int Util parsing string int ranges sysfs file Args ranges_str E g - - Returns E g ints set int = set range_str ranges_str split range_str = range_str strip range_str continue - range_str start_str end_str = range_str split - start end = int start_str int end_str ints update range start end + ints add int range_str ints _get_ranges_str_from_ints ints Iterable int - str Convert set integers compact string ranges Args ints E g Returns E g - - ints sorted_ints = sorted ints ranges = start = prev = sorted_ints num sorted_ints num == prev + prev = num start == prev ranges append f start ranges append f start - prev start = prev = num Append last range start == prev ranges append f start ranges append f start - prev join ranges _get_systemwide_numa_node_indices - set int open sys devices system node possible f possible_nodes_str = f read _get_set_of_int_from_ranges_str possible_nodes_str _get_numa_node_indices_for_socket_index socket_index int - set int systemwide_numa_node_indices = _get_systemwide_numa_node_indices matching_numa_node_indices = set numa_node_index systemwide_numa_node_indices arbitrary_cpu_index = _get_arbitrary_allowed_cpu_index_for_numa_node numa_node_index=numa_node_index socket_index == _get_socket_index_for_cpu cpu_index=arbitrary_cpu_index matching_numa_node_indices add numa_node_index matching_numa_node_indices _get_allowed_cpu_indices_for_current_thread - set int denotes current thread pyrefly ignore missing-attribute os sched_getaffinity type ignore attr-defined