Checkpoint reader functionality machine learning models This module provides classes reading checkpoints storage including determining checkpoint layout configuring reader logging os itertools zip_longest pathlib Path typing Any Optional torch torch _subclasses fake_tensor FakeTensorMode types RankInfo STATE_DICT logger = logging getLogger __name__ CheckpointReader Handles reading state dictionaries storage This responsible reading model state dictionaries storage according specified checkpoint layout It supports synchronization barriers ensure all ranks distributed setting complete their checkpoint operations __init__ rank_info RankInfo Initialize CheckpointReader Args rank_info Information about current rank distributed setting _rank_info = rank_info read path str state_dict Optional STATE_DICT = None map_location Any = None kwargs dict str Any - tuple STATE_DICT list str Reads state dictionary storage Args path str The path which read checkpoint map_location Any Device mapping function device name relocating tensors kwargs Additional keyword arguments passed torch load Returns STATE_DICT The loaded state dictionary list str List missing keys logger debug Reading checkpoint s rank s path _rank_info global_rank dir_path = Path path file_path = dir_path f checkpoint_ _rank_info global_rank pt Check file exists os path exists file_path logger error Checkpoint file found s file_path raise FileNotFoundError f Checkpoint file found file_path state_dict None result tuple STATE_DICT list str = torch load file_path map_location=map_location result = _partial_read file_path state_dict map_location=map_location kwargs logger debug Successfully read checkpoint file s file_path result _partial_read file_path Path state_dict STATE_DICT map_location Any = None kwargs dict str Any - tuple STATE_DICT list str Reads only keys present state_dict checkpoint file This method optimizes checkpoint loading only loading tensors actually needed based keys present input state_dict This can significantly reduce memory usage loading time large checkpoints when only subset model needs loaded Args file_path str The path checkpoint file state_dict STATE_DICT The state dictionary containing keys load map_location Any Device mapping function device name relocating tensors kwargs Additional keyword arguments passed torch load Returns tuple STATE_DICT list str The updated state dictionary loaded values list missing keys FakeTensorMode metadata_dict = torch load file_path map_location=map_location missing_keys = open file_path rb file Helper function load tensor data file load_tensor target Optional torch Tensor source torch Tensor full_key str - torch Tensor target None target size = source size target dtype = source dtype raise RuntimeError f Target tensor size= target size dtype= target dtype does match f source tensor size= source size dtype= source dtype key full_key tensor_offset = source untyped_storage _checkpoint_offset tensor_offset None raise AssertionError checkpoint_offset tensor torch serialized file set This could happen checkpoint saved older version Pytorch Please make sure checkpoint saved Pytorch later tensor_len = source nelement source element_size file seek tensor_offset + source element_size int source storage_offset target None target = torch empty source size dtype=source dtype device=source device buffer = file read tensor_len cpu_tensor = torch frombuffer buffer dtype=source dtype tensor = cpu_tensor view source size target copy_ tensor target Helper function recursively process nested structures process_value target_value Any source_value Any key_path str - Any source_type = type source_value source_type torch _subclasses fake_tensor FakeTensor source_type = torch Tensor target_value None isinstance target_value source_type raise RuntimeError f Target value key_path set type target_value source value type source_value isinstance source_value torch Tensor load_tensor target_value source_value key_path isinstance source_value dict target_value None create new map all keys present source_value target_value = dict fromkeys source_value keys pyrefly ignore missing-attribute key list target_value keys current_path = f key_path key key_path key key source_value target_value key = process_value target_value key source_value key current_path missing_keys append current_path target_value isinstance source_value list target_value None target_value = None len source_value result = i target_item source_item enumerate zip_longest target_value source_value fillvalue=None current_path = f key_path i key_path f i result append process_value target_item source_item current_path result source_value Start recursive processing root state dictionary updated_state_dict = process_value state_dict metadata_dict missing_keys len missing_keys logger warning Missing s keys checkpoint s s more len missing_keys missing_keys len missing_keys - logger warning Missing s keys checkpoint s len missing_keys missing_keys updated_state_dict missing_keys