mypy allow-untyped-defs This module provides result classes AOT Autograd compilation Similar how torch _inductor output_code provides OutputCode classes inductor compilation results module provides AOTAutogradResult classes represent compiled artifacts produced AOT Autograd These results - Serializable can saved loaded disk without recompilation - Addressable can stored caches keys later retrieval - Reusable can used both caching ahead-of-time compilation precompile The main result types - GenericAOTAutogradResult Abstract base all AOT Autograd results - AOTAutogradResult Regular result references FxGraphCache entries - BundledAOTAutogradResult Result bundles entire compiled code directly __future__ annotations json logging abc ABC abstractmethod copy copy dataclasses dataclass typing Any Callable Generic Optional TYPE_CHECKING TypeVar torch torch _dynamo precompile_context BackendCacheArtifact torch _inductor codecache FxGraphCache torch _inductor output_code CompiledFxGraph CompiledFxGraphConstants OutputCode torch _inductor utils should_use_remote_fx_graph_cache runtime_wrappers AOTDispatchAutograd AOTDispatchSubclassWrapper CachedAutogradLazyBackwardCompileInfo CompilerWrapper FunctionalizedRngRuntimeWrapper post_compile RuntimeWrapper SerializableCompiledFunction SubclassMeta schemas AOTAutogradCacheInfo noqa F utils simple_wraps TYPE_CHECKING torch _inductor compile_fx _CompileFxKwargs schemas AOTConfig ViewAndMutationMeta log = logging getLogger __name__ TOut = TypeVar TOut bound=OutputCode InductorOutput ABC Generic TOut Class representing single inductor output abstractmethod pre_save - None abstractmethod load example_inputs - TOut abstractmethod post_compile result TOut fx_config _CompileFxKwargs - TOut TOutputCode = TypeVar TOutputCode bound=OutputCode dataclass BundledOutputCodeLoadable InductorOutput TOutputCode Generic TOutputCode A generic wrapper OutputCode objects bundled directly cache rather than looked up via FxGraphCache This works any OutputCode subclass CompiledFxGraph RegionalOutputCode etc result TOutputCode pre_save - None disk_result = copy result disk_result prepare_for_serialization result = disk_result load example_inputs - TOutputCode example_inputs = example_inputs result post_compile result TOutputCode fx_config _CompileFxKwargs - TOutputCode constants = CompiledFxGraphConstants Special handling CompiledFxGraph - needs FxGraphCache cache_hit_post_compile isinstance result CompiledFxGraph graph cache_info = FxGraphCache cache_hit_post_compile result constants graph None raise RuntimeError Failed reload cache entry disk torch _logging trace_structured artifact metadata_fn=lambda name fx_graph_bundled_cache_hit always hit encoding json payload_fn=lambda json dumps cache_info result = graph type ignore assignment Run normal post compile result post_compile example_inputs constants fx_config result Backwards compatibility alias CompiledFxGraphLoadable type BundledOutputCodeLoadable CompiledFxGraph = BundledOutputCodeLoadable CompiledFxGraph dataclass FxGraphCacheLoadable InductorOutput CompiledFxGraph fx_graph_cache_info tuple str list str fx_graph_guard_expr Optional str pre_save _is_backward - bool False load example_inputs - CompiledFxGraph autograd_cache FXGraphCacheMiss Note AOTAutogradCache FXGraphCache Guard interactions As mentioned AOTAutograd takes symint inputs dynamo s list arguments FXGraphCache serializes guards needed shape_env based these symint inputs graph The invariant AOTAutograd uses here sources symints given dynamo exactly same ones passes inductor both forward backward passes This does mean tensor values passed same only their symints That AOTAutograd Inductor never create new guards based symints different sources than those passed inductor We pass post compile function which sets various fx_config boxed values so we can call only after we re sure both forward backward have Clear CompiledTritonKernels before loading FXGraphCache torch _inductor async_compile CompiledTritonKernels cache_clear remote_cache = None constants = CompiledFxGraphConstants should_use_remote_fx_graph_cache remote_cache = FxGraphCache get_remote_cache cache_key debug_lines = fx_graph_cache_info check_exact_guard_match guard_expr _hints AOTAutogradCache tracks its own guards so we just need treat these guard expressions second cache key sorts we just check equality i e FXGraphCache entry exact same guards we originally saved into cache guard_expr == fx_graph_guard_expr result cache_info = FxGraphCache load_with_key cache_key debug_lines example_inputs local=True remote_cache=remote_cache is_backward=self _is_backward constants=constants evaluate_guards=check_exact_guard_match result None log info FXGraphCache cache miss key s fx_graph_cache_info torch _logging trace_structured artifact metadata_fn=lambda name fx_graph_cache_miss always hit encoding json payload_fn=lambda json dumps cache_info raise FXGraphCacheMiss No need log chromium event because AOTAutograd will log immediately us torch _logging trace_structured artifact metadata_fn=lambda name fx_graph_cache_hit always hit encoding json payload_fn=lambda json dumps cache_info example_inputs = example_inputs constants = constants result post_compile result CompiledFxGraph fx_config _CompileFxKwargs - CompiledFxGraph Called after FXGraphCacheLoadable load mutates fx_config result post_compile example_inputs constants fx_config result dataclass CompiledForward FxGraphCacheLoadable Cacheable entry forward function _is_backward - bool False dataclass GenericCompiledBackward InductorOutput TOut Used AOTDispatchAutograd post_compile backward_state_indices list int num_symints_saved_for_bw_ int dataclass CompiledBackward GenericCompiledBackward CompiledFxGraph FxGraphCacheLoadable Cacheable entry forward function _is_backward - bool True post_compile result CompiledFxGraph fx_config _CompileFxKwargs - CompiledFxGraph compiled_bw = super post_compile result fx_config See note Wrapping bw_compiler disable This done _wrapped_bw_compiler torch _dynamo backends common py But since cache hit we do call bw_compiler we need reapply disable torch _dynamo disable type ignore return-value compiled_bw reason= do trace generated backwards pass Generic bundled forward backward classes work any OutputCode type dataclass BundledCompiledForward BundledOutputCodeLoadable TOutputCode Generic TOutputCode Generic forward function bundled compilation Works any OutputCode type CompiledFxGraph RegionalOutputCode etc dataclass BundledCompiledBackward GenericCompiledBackward TOutputCode BundledOutputCodeLoadable TOutputCode Generic TOutputCode Generic backward function bundled compilation Works any OutputCode type CompiledFxGraph RegionalOutputCode etc post_compile result TOutputCode fx_config _CompileFxKwargs - TOutputCode compiled_bw = super post_compile result fx_config See note Wrapping bw_compiler disable This done _wrapped_bw_compiler torch _dynamo backends common py But since cache hit we do call bw_compiler we need reapply disable torch _dynamo disable type ignore return-value compiled_bw reason= do trace generated backwards pass dataclass SerializedGraphModule fn Callable dict Any Any str torch nn Module args tuple Any __init__ gm torch fx GraphModule fn args = gm __reduce__ deserialize - torch fx GraphModule gm = fn args assert isinstance gm torch fx GraphModule gm serialize_graph_module gm torch fx GraphModule - SerializedGraphModule NOTE mutates graph module gm meta = node gm graph nodes node meta = SerializedGraphModule gm TForward = TypeVar TForward bound=InductorOutput TBackward = TypeVar TBackward bound=GenericCompiledBackward dataclass GenericAOTAutogradResult Generic TForward TBackward A single result AOT Autograd compilation genericized Forward Backward types A TForward always InductorOutput some sort which represents forward graph compile A TBackward InductorOutput + metadata about backward useful specific backward-only wrappers This type encapsulated GenericCompiledBackward Each AOTAutogradResult essentially parameterized method loading cache either Bundled UnBundled The type output For now only type output we support Python Wrapper output i e OutputCode CompiledFxGraph same technique works C++ wrapper code we d just add extra InductorOutput type Forward Backward info compiled_fw TForward compiled_bw Optional TBackward Code joint graph using print_readable Used logging purposes aot_joint_graph_str Optional str aot_forward_graph_str Optional str aot_backward_graph_str Optional str Runtime_metadata saved right before compilation runtime_metadata ViewAndMutationMeta Wrappers run after each aot_dispatch_ function dispatch_wrappers list CompilerWrapper Used AOTSubclassWrapper maybe_subclass_meta Optional SubclassMeta num_fw_outs_saved_for_bw Optional int Used RuntimeWrapper indices_of_inps_to_detach list int Time taken trace compile forward forward_time_taken includes AOTAutograd tracing time + inductor compilation time backward_time_taken essentially just time inductor took compile forward_time_taken_ns int backward_time_taken_ns int Used standalone_compile sanitized_aot_config AOTConfig guards_expr Optional str Used Compiled Autograd serialized_bw_module Optional SerializedGraphModule pre_save Perform any preparations make result ready serialization compiled_fw pre_save compiled_bw None compiled_bw pre_save Turn result into original callable wrap_post_compile args list torch Tensor aot_config AOTConfig fx_config _CompileFxKwargs - Callable This function takes result carefully reconstructs original callable AOTAutograd returned first time run It does running various post compile steps AOTAutograd runs its compiled artifact after running fw bw compilers In inference path consists Subclass FunctionalzedRngRuntime RuntimeWrappers In autograd path consists AOTAutogradDispatch post_compile The steps here should match exactly steps run aot_dispatch_base aot_dispatch_autograd Notably absent cached path - DebugAssertWrapper - FakifiedOutWrapper Which we ll handle separately later necessary torch _dynamo utils CompileEventLogger dynamo_timed Log output AOTAutogradCache aot_config enable_log TODO maybe also log aot_graphs_log Unfortunately aot_graphs_log uses slightly different formatting though aot_joint_graph_str None torch _logging trace_structured aot_joint_graph payload_fn=lambda aot_joint_graph_str aot_forward_graph_str None torchgen utils dataclass_repr torch _logging trace_structured artifact metadata_fn=lambda name aot_forward_graph_fw_metadata encoding string payload_fn=lambda dataclass_repr runtime_metadata maybe_subclass_meta None torch _logging trace_structured artifact metadata_fn=lambda name aot_forward_graph_fw_subclass_metadata encoding string payload_fn=lambda dataclass_repr maybe_subclass_meta It s called inference graph running autograd name = aot_forward_graph aot_backward_graph_str None aot_inference_graph torch _logging trace_structured name payload_fn=lambda aot_forward_graph_str aot_backward_graph_str None torch _logging trace_structured aot_backward_graph payload_fn=lambda aot_backward_graph_str dynamo_timed AOTAutogradCache inductor_load compiled_fw_func = compiled_fw load args compiled_bw_func = None compiled_bw None compiled_bw_func = compiled_bw load args needs_autograd = True CompileEventLogger try_add_pt _compile backend_compile dispatch_mode= autograd Now we ve loaded forward backward call post compile both This avoids setting things like BoxedBools fx_config until after both forward backward cache hit fw_fx_config _CompileFxKwargs = fx_config is_backward False bw_fx_config _CompileFxKwargs = fx_config is_backward True compiled_fw_func = compiled_fw post_compile compiled_fw_func fw_fx_config compiled_bw_func = compiled_bw post_compile compiled_bw_func bw_fx_config inference_fx_config _CompileFxKwargs = fx_config is_backward False needs_autograd = False CompileEventLogger try_add_pt _compile backend_compile dispatch_mode= inference compiled_fw_func = compiled_fw post_compile compiled_fw_func inference_fx_config Wrap forward function post compile wrappers compiled_fw_func = AOTDispatchSubclassWrapper trace_joint=needs_autograd fw_only=None maybe_subclass_meta=self maybe_subclass_meta num_fw_outs_saved_for_bw=self num_fw_outs_saved_for_bw post_compile compiled_fw_func aot_config runtime_metadata=self runtime_metadata req_subclass_dispatch = maybe_subclass_meta None CompileEventLogger try_add_pt _compile backend_compile requires_subclass_dispatch=req_subclass_dispatch In autograd case functionalizedRngWrapper should modify outs return_new_outs = needs_autograd compiled_fw_func = FunctionalizedRngRuntimeWrapper return_new_outs=return_new_outs post_compile compiled_fw_func aot_config runtime_metadata=self runtime_metadata disable_amp = torch _C _is_any_autocast_enabled needs_autograd assert compiled_bw None cached_lazy_backward = None serialized_bw_module None cached_lazy_backward = CachedAutogradLazyBackwardCompileInfo serialized_bw_module deserialize This function run both cache miss cache hit either here aot_dispatch_autograd On cache hit bw already compiled we don t need save cache again so those corresponding arguments set None compiled_function = AOTDispatchAutograd post_compile compiled_fw_func compiled_bw_func maybe_subclass_meta compiled_bw num_symints_saved_for_bw_ compiled_bw backward_state_indices disable_amp indices_of_inps_to_detach cached_lazy_backward aot_config fw_metadata=self runtime_metadata try_save_cache_entry=None compiled_function = RuntimeWrapper indices_of_inps_to_detach=self indices_of_inps_to_detach trace_joint=False disable_amp=disable_amp post_compile compiled_fw_func aot_config runtime_metadata=self runtime_metadata Add serialization function back onto object compiled_function _ = post_compile dispatch_wrappers compiled_function aot_config runtime_metadata=self runtime_metadata Now we re pretty sure s successful load add guards existing shape environment cache guards_expr autograd_cache AOTAutogradCache symints = AOTAutogradCache _filter_backed_symints args check = bool AOTAutogradCache evaluate_guards guards_expr symints assert check True compiled_function AOTAutogradResult GenericAOTAutogradResult CompiledForward CompiledBackward Regular AOTAutogradResult saves forward backward FxGraphCache keys looks them up FxGraphCache load BundledAOTAutogradResult GenericAOTAutogradResult BundledCompiledForward TOutputCode BundledCompiledBackward TOutputCode Generic TOutputCode Generic AOTAutogradResult where we bundle entire OutputCode directly rather than looking up via FxGraphCache This works any OutputCode type - CompiledFxGraph Traditional inductor compilation - RegionalOutputCode Regional inductor compilation GraphPickler serialization - Any future OutputCode subclasses Type parameter TOutputCode The OutputCode subclass e g CompiledFxGraph RegionalOutputCode Usage CompiledFxGraph entry = BundledAOTAutogradResult CompiledFxGraph compiled_fw=BundledCompiledForward result=CompiledFxGraph compiled_bw=BundledCompiledBackward result=CompiledFxGraph backward_state_indices= num_symints_saved_for_bw_= Usage RegionalOutputCode entry = BundledAOTAutogradResult RegionalOutputCode compiled_fw=BundledCompiledForward result=RegionalOutputCode gm compiled_bw=BundledCompiledBackward result=RegionalOutputCode gm backward_state_indices= num_symints_saved_for_bw_= deserialize_bundled_cache_entry entry BundledAOTAutogradResult - Callable copy deepcopy torch _inductor cudagraph_utils BoxedDeviceIndex torch _inductor utils BoxedBool In precompile use case guards already serialized dynamo so we don t need add them environment entry guards_expr = None TODO isn t exactly right because cudagraphs needs shared config which set compile_fx But precompile we never actually call compile_fx so we don t have place track cudagraphs here cudagraphs = BoxedBool torch _inductor config triton cudagraphs boxed_forward_device_index = BoxedDeviceIndex None We need make clean copy cache entry case needs serialized again serializable_copy = deepcopy entry torch _subclasses FakeTensorMode torch fx experimental symbolic_shapes ShapeEnv context = torch _guards TracingContext try_get context None Create clean environment when running fx graph post compile one available context = torch _guards TracingContext FakeTensorMode shape_env=ShapeEnv torch _guards tracing context compiled_fn = entry wrap_post_compile entry sanitized_aot_config cudagraphs cudagraphs boxed_forward_device_index boxed_forward_device_index Ensure deserialized cache entry still serializable compiled_fn = SerializableCompiledFunction compiled_fn lambda serializable_copy TODO ignores flat_params which can exist inline_builtin_nn_modules=False simple_wraps compiled_fn forward runtime_args tuple Any compiled_fn list runtime_args assert hasattr compiled_fn serialize forward serialize = compiled_fn serialize type ignore attr-defined forward dataclass BundledAOTAutogradCacheArtifact BackendCacheArtifact Callable after_deserialization - Callable deserialize_bundled_cache_entry content