mypy allow-untyped-defs itertools operator torch torch ao nn intrinsic nni torch ao nn quantized reference nnqr torch nn nn torch nn functional F torch ao quantization fuser_method_mappings _sequential_wrapper torch ao quantization utils MatchAllNode _common_operator_config_utils _get_binary_op_configs _get_bn_configs _get_cat_config _get_conv_configs _get_default_op_configs _get_embedding_op_configs _get_fixed_qparams_op_configs _get_linear_configs _get_ln_configs _get_rnn_op_configs _get_share_qparams_op_configs backend_config BackendConfig BackendPatternConfig DTypeConfig ObservationType =================== &#124; DTYPE CONFIGS &#124; =================== onednn_weighted_op_int _dtype_config = DTypeConfig input_dtype=torch quint output_dtype=torch quint weight_dtype=torch qint bias_dtype=torch float onednn_op_quint _dtype_config = DTypeConfig input_dtype=torch quint output_dtype=torch quint onednn_dynamic_int _dtype_config = DTypeConfig input_dtype=torch quint output_dtype=torch float weight_dtype=torch qint bias_dtype=torch float is_dynamic=True onednn_weight_only_qint _dtype_config = DTypeConfig input_dtype=torch float output_dtype=torch float weight_dtype=torch qint onednn_input_output_only_quint _dtype_config = DTypeConfig input_dtype=torch quint output_dtype=torch quint weight_dtype=torch float bias_dtype=torch float =================== &#124; FUSER METHODS &#124; =================== _fuse_linear_bn_leaky_relu is_qat linear bn leaky_relu r Given linear bn leaky_relu modules fuses them returns fused module Args is_qat flag whether we using quantization aware training fusion post training quantization fusion linear Module instance type Linear bn BatchNorm d instance needs fused linear layer leaky_relu LeakyReLU instance needs fused linear layer Examples xdoctest +SKIP failing m = nn Linear b = nn BatchNorm d lr = nn LeakyReLU m = _fuse_linear_bn_leaky_relu m b lr assert linear training == bn training bn training == leaky_relu training Linear BN LeakyReLU all must same mode train eval is_qat raise NotImplementedError f Cannot fuse train modules linear bn leaky_relu map_to_fused_module_eval = nn Linear nni LinearLeakyReLU fused_module = map_to_fused_module_eval get type linear None fused_module None fused_linear = nn utils fusion fuse_linear_bn_eval linear bn fm = fused_module fused_linear leaky_relu fm raise NotImplementedError f Cannot fuse eval modules linear bn leaky_relu ====================== &#124; CONFIGS FOR CONV &#124; ====================== observation_type = ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT conv_dtype_configs = onednn_weighted_op_int _dtype_config conv_configs = _get_conv_configs conv_dtype_configs Conv d + Add conv d Y \ add include conv d conv d \ add _fuse_conv_add_left is_qat add conv _ nni ConvAdd d conv add _conv_add_root_node_getter_left pattern _ conv _ = pattern conv _conv_add_extra_inputs_getter_left pattern get inputs pattern extra inputs inputs root node assumed copied over root node fused node _ _conv extra_input = pattern extra_input conv d \ bn Y \ add _fuse_conv_bn_add_left is_qat add bn_conv _ bn conv = bn_conv is_qat raise NotImplementedError f Cannot fuse train modules conv bn add fused_conv = nn utils fusion fuse_conv_bn_eval conv bn nni ConvAdd d fused_conv add _conv_bn_add_root_node_getter_left add_pattern _ bn_conv _ = add_pattern _bn conv = bn_conv conv _conv_bn_add_extra_inputs_getter_left add_pattern get inputs pattern extra inputs inputs root node assumed copied over root node fused node _ _bn_conv extra_input = add_pattern extra_input conv_add_left_optioins = itertools product True False with_bn torch add operator add add_op with_bn add_op conv_add_left_optioins with_bn conv_configs append BackendPatternConfig _set_pattern_complex_format add_op nn BatchNorm d nn Conv d MatchAllNode noqa E set_observation_type observation_type set_dtype_configs conv_dtype_configs set_fuser_method _fuse_conv_bn_add_left _set_root_node_getter _conv_bn_add_root_node_getter_left _set_extra_inputs_getter _conv_bn_add_extra_inputs_getter_left set_fused_module nni ConvAdd d conv_configs append BackendPatternConfig _set_pattern_complex_format add_op nn Conv d MatchAllNode noqa E set_observation_type observation_type set_dtype_configs conv_dtype_configs set_fuser_method _fuse_conv_add_left _set_root_node_getter _conv_add_root_node_getter_left _set_extra_inputs_getter _conv_add_extra_inputs_getter_left set_fused_module nni ConvAdd d Y conv d \ add _fuse_conv_add_right is_qat add _ conv nni ConvAdd d conv add _conv_add_root_node_getter_right pattern _add _ conv = pattern conv _conv_add_extra_inputs_getter_right pattern get inputs pattern extra inputs inputs root node assumed copied over root node fused node _ extra_input _conv = pattern extra_input conv d Y bn \ add _fuse_conv_bn_add_right is_qat add _ bn_conv bn conv = bn_conv is_qat raise NotImplementedError f Cannot fuse train modules conv bn add fused_conv = nn utils fusion fuse_conv_bn_eval conv bn nni ConvAdd d fused_conv add _conv_bn_add_root_node_getter_right pattern _add _ bn_conv = pattern _bn conv = bn_conv conv _conv_bn_add_extra_inputs_getter_right pattern get inputs pattern extra inputs inputs root node assumed copied over root node fused node _ extra_input _bn_conv = pattern extra_input conv_add_optioins = itertools product True False with_bn torch add operator add add_op with_bn add_op conv_add_optioins with_bn conv_configs append BackendPatternConfig _set_pattern_complex_format add_op MatchAllNode nn BatchNorm d nn Conv d noqa E set_observation_type observation_type set_dtype_configs conv_dtype_configs set_fuser_method _fuse_conv_bn_add_right _set_root_node_getter _conv_bn_add_root_node_getter_right _set_extra_inputs_getter _conv_bn_add_extra_inputs_getter_right set_fused_module nni ConvAdd d conv_configs append BackendPatternConfig _set_pattern_complex_format add_op MatchAllNode nn Conv d noqa E set_observation_type observation_type set_dtype_configs conv_dtype_configs set_fuser_method _fuse_conv_add_right _set_root_node_getter _conv_add_root_node_getter_right _set_extra_inputs_getter _conv_add_extra_inputs_getter_right set_fused_module nni ConvAdd d conv_configs append BackendPatternConfig nni ConvAdd d set_observation_type observation_type noqa E set_dtype_configs conv_dtype_configs set_root_module nn Conv d set_reference_quantized_module nnqr Conv d Conv d + Add + Relu conv d Y \ add \ relu _fuse_conv_add_relu_left is_qat relu add_pattern add conv _ = add_pattern nni ConvAddReLU d conv add relu _conv_add_relu_root_node_getter_left pattern _relu add_pattern = pattern _ conv _ = add_pattern conv _conv_add_relu_extra_inputs_getter_left pattern get inputs pattern extra inputs inputs root node assumed copied over root node fused node _relu add_pattern = pattern _ _conv extra_input = add_pattern extra_input conv d \ bn Y \ add \ relu _fuse_conv_bn_add_relu_left is_qat relu add_pattern add bn_conv _ = add_pattern bn conv = bn_conv is_qat raise NotImplementedError f Cannot fuse train modules conv bn add relu fused_conv = nn utils fusion fuse_conv_bn_eval conv bn nni ConvAddReLU d fused_conv add relu _conv_bn_add_relu_root_node_getter_left pattern _relu add_pattern = pattern _ bn_conv _ = add_pattern _bn conv = bn_conv conv _conv_bn_add_relu_extra_inputs_getter_left pattern get inputs pattern extra inputs inputs root node assumed copied over root node fused node _relu add_pattern = pattern _ _bn_conv extra_input = add_pattern extra_input conv_add_relu_left_optioins = itertools product True False with_bn torch add operator add add_op with_bn add_op conv_add_relu_left_optioins with_bn conv_configs append BackendPatternConfig _set_pattern_complex_format nn ReLU add_op nn BatchNorm d nn Conv d MatchAllNode noqa E set_observation_type observation_type set_dtype_configs conv_dtype_configs set_fuser_method _fuse_conv_bn_add_relu_left _set_root_node_getter _conv_bn_add_relu_root_node_getter_left _set_extra_inputs_getter _conv_bn_add_relu_extra_inputs_getter_left set_fused_module nni ConvAddReLU d conv_configs append BackendPatternConfig _set_pattern_complex_format nn ReLU add_op nn Conv d MatchAllNode noqa E set_observation_type observation_type set_dtype_configs conv_dtype_configs set_fuser_method _fuse_conv_add_relu_left _set_root_node_getter _conv_add_relu_root_node_getter_left _set_extra_inputs_getter _conv_add_relu_extra_inputs_getter_left set_fused_module nni ConvAddReLU d Y conv d \ add \ relu _fuse_conv_add_relu_right is_qat relu add_pattern add _ conv = add_pattern nni ConvAddReLU d conv add relu _conv_add_relu_root_node_getter_right pattern _relu add_pattern = pattern _ _extra_input conv = add_pattern conv _conv_add_relu_extra_inputs_getter_right pattern get inputs pattern extra inputs inputs root node assumed copied over root node fused node _relu add_pattern = pattern _ extra_input _conv = add_pattern extra_input conv d Y bn \ add \ relu _fuse_conv_bn_add_relu_right is_qat relu add_pattern add _ bn_conv = add_pattern bn conv = bn_conv is_qat raise NotImplementedError f Cannot fuse train modules conv bn add relu fused_conv = nn utils fusion fuse_conv_bn_eval conv bn nni ConvAddReLU d fused_conv add relu _conv_bn_add_relu_root_node_getter_right pattern _relu add_pattern = pattern _ _ bn_conv = add_pattern _bn conv = bn_conv conv _conv_bn_add_relu_extra_inputs_getter_right pattern get inputs pattern extra inputs inputs root node assumed copied over root node fused node _relu add_pattern = pattern _ extra_input _bn_conv = add_pattern extra_input conv_add_relu_left_optioins = itertools product True False with_bn torch add operator add add_op with_bn add_op conv_add_relu_left_optioins with_bn conv_configs append BackendPatternConfig _set_pattern_complex_format nn ReLU add_op MatchAllNode nn BatchNorm d nn Conv d noqa E set_observation_type observation_type set_dtype_configs conv_dtype_configs set_fuser_method _fuse_conv_bn_add_relu_right _set_root_node_getter _conv_bn_add_relu_root_node_getter_right _set_extra_inputs_getter _conv_bn_add_relu_extra_inputs_getter_right set_fused_module nni ConvAddReLU d conv_configs append BackendPatternConfig _set_pattern_complex_format nn ReLU add_op MatchAllNode nn Conv d noqa E set_observation_type observation_type set_dtype_configs conv_dtype_configs set_fuser_method _fuse_conv_add_relu_right _set_root_node_getter _conv_add_relu_root_node_getter_right _set_extra_inputs_getter _conv_add_relu_extra_inputs_getter_right set_fused_module nni ConvAddReLU d conv_configs append BackendPatternConfig nni ConvAddReLU d set_observation_type observation_type noqa E set_dtype_configs conv_dtype_configs set_root_module nn Conv d set_reference_quantized_module nnqr Conv d ======================== &#124; CONFIGS FOR LINEAR &#124; ======================== linear_dtype_configs = onednn_weighted_op_int _dtype_config onednn_dynamic_int _dtype_config linear_configs = _get_linear_configs linear_dtype_configs _add_eltwise_fusion_configs configs root_module root_op post_module post_op dtype_configs fuser_method fused_module observation_type ref_quant_module base module + op module fusion config configs append BackendPatternConfig root_module post_module set_dtype_configs dtype_configs noqa E set_fuser_method fuser_method set_fused_module fused_module base module + functional post op configs append BackendPatternConfig root_module post_op set_dtype_configs dtype_configs noqa E set_fuser_method fuser_method set_fused_module fused_module fused module configs configs append BackendPatternConfig fused_module set_observation_type observation_type noqa E set_dtype_configs dtype_configs set_root_module root_module set_reference_quantized_module ref_quant_module functional base op + post op configs configs append BackendPatternConfig root_op post_module set_observation_type observation_type noqa E set_dtype_configs dtype_configs configs append BackendPatternConfig root_op post_op set_observation_type observation_type noqa E set_dtype_configs dtype_configs Configs linear + leaky_relu fusion _add_eltwise_fusion_configs linear_configs nn Linear F linear nn LeakyReLU F leaky_relu linear_dtype_configs _sequential_wrapper nni LinearLeakyReLU nni LinearLeakyReLU observation_type nnqr Linear Configs linear module + batchnorm + leaky_relu linear_configs append BackendPatternConfig nn Linear nn BatchNorm d nn LeakyReLU set_dtype_configs linear_dtype_configs noqa E set_fuser_method _fuse_linear_bn_leaky_relu set_fused_module nni LinearLeakyReLU Configs linear + tanh fusion _add_eltwise_fusion_configs linear_configs nn Linear F linear nn Tanh torch tanh linear_dtype_configs _sequential_wrapper nni LinearTanh nni LinearTanh observation_type nnqr Linear =========================== &#124; CONFIGS FOR OTHER OPS &#124; =========================== binary_op_dtype_configs = onednn_op_quint _dtype_config default_op_dtype_configs = onednn_op_quint _dtype_config fixed_qparams_op_dtype_configs = onednn_op_quint _dtype_config share_qparams_op_dtype_configs = onednn_op_quint _dtype_config rnn_op_dtype_configs = onednn_dynamic_int _dtype_config embedding_op_dtype_configs = onednn_weight_only_qint _dtype_config layer_norm_op_dtype_configs = onednn_input_output_only_quint _dtype_config ===================== &#124; BACKEND CONFIGS &#124; ===================== get_onednn_backend_config - BackendConfig Return ` BackendConfig ` PyTorch s native ONEDNN backend BackendConfig onednn set_backend_pattern_configs conv_configs set_backend_pattern_configs linear_configs set_backend_pattern_configs _get_binary_op_configs binary_op_dtype_configs set_backend_pattern_config _get_cat_config default_op_dtype_configs set_backend_pattern_configs _get_default_op_configs default_op_dtype_configs set_backend_pattern_configs _get_fixed_qparams_op_configs fixed_qparams_op_dtype_configs set_backend_pattern_configs _get_share_qparams_op_configs share_qparams_op_dtype_configs set_backend_pattern_configs _get_bn_configs default_op_dtype_configs set_backend_pattern_configs _get_ln_configs layer_norm_op_dtype_configs set_backend_pattern_configs _get_rnn_op_configs rnn_op_dtype_configs set_backend_pattern_configs _get_embedding_op_configs embedding_op_dtype_configs __all__ = get_onednn_backend_config