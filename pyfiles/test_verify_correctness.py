Owner s module dynamo operator torch torch _dynamo torch _dynamo config config torch _dynamo test_case torch _dynamo testing same torch fx _lazy_graph_module _force_skip_lazy_graph_module Seq torch nn Module __init__ - None super __init__ layers = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn Sigmoid forward x layers x Conv_Bn_Relu torch nn Module __init__ in_channels out_channels kwargs super __init__ conv = torch nn Conv d in_channels out_channels bias=False kwargs bn = torch nn BatchNorm d out_channels eps= relu = torch nn ReLU forward x relu bn conv x toy_example b x = torch abs + b sum b = b - x b transform gm torch fx GraphModule - torch fx GraphModule node gm graph nodes Checks we re calling function i e operator add node op == call_function The target attribute function call_function calls node target == operator mul node target = operator add gm graph lint Does some checks make sure Graph well-formed gm recompile gm config patch verify_correctness True TestVerifyCorrectness torch _dynamo test_case TestCase test_example_inputs fn bc d b c = bc d - b c compiler_fn graph example_inputs nonlocal r r = graph example_inputs graph forward = torch empty fill_ b = torch empty fill_ c = torch empty fill_ d = r = None r = fn b c d opt_fn = torch _dynamo optimize_assert compiler_fn fn r = opt_fn b c d assertIsNotNone r assertEqual r shape r shape assertEqual r shape r shape assertEqual r device r device assertEqual r device r device _force_skip_lazy_graph_module test_torchscript s = Seq i = torch randn r = s i opt_s = torch compile s backend= ts r = opt_s i assertTrue same r r test_incorrect_verify_true If bad optimization graph functionally equal original graph When config verify_correctness=True will check correctness outputs raise error i = torch randn i = torch randn incorrect_compile_fn gm example_inputs transform gm forward toy_example i i try opt_toy_example = torch compile toy_example backend=incorrect_compile_fn opt_toy_example i i except RuntimeError pass fail expected failure config patch verify_correctness False test_incorrect_verify_false The bad optimization graph functionally equal original graph When config verify_correctness=False wrong outputs will i = torch randn i = torch randn incorrect_compile_fn gm example_inputs transform gm forward r = toy_example i i opt_toy_example = torch compile toy_example backend=incorrect_compile_fn r = opt_toy_example i i assertTrue same r r __name__ == __main__ torch _dynamo test_case run_tests run_tests