numpy np torch torch nn functional F ADAROUND_ZETA float = ADAROUND_GAMMA float = - AdaptiveRoundingLoss torch nn Module Adaptive Rounding Loss functions described https arxiv org pdf pdf rounding regularization eq reconstruction loss eq except regularization term __init__ max_iter int warm_start float = beta_range tuple int int = reg_param float = - None super __init__ max_iter = max_iter warm_start = warm_start beta_range = beta_range reg_param = reg_param rounding_regularization V torch Tensor curr_iter int - torch Tensor Major logics copied official Adaround Implementation Apply rounding regularization input tensor V curr_iter = max_iter raise AssertionError Current iteration strictly les sthan max iteration curr_iter warm_start max_iter torch tensor start_beta end_beta = beta_range warm_start_end_iter = warm_start max_iter compute relative iteration current iteration rel_iter = curr_iter - warm_start_end_iter max_iter - warm_start_end_iter beta = end_beta + start_beta - end_beta + np cos rel_iter np pi A rectified sigmoid soft-quantization formulated https arxiv org pdf pdf h_alpha = torch clamp torch sigmoid V ADAROUND_ZETA - ADAROUND_GAMMA + ADAROUND_GAMMA min= max= Apply rounding regularization This regularization term helps out term converge into binary solution either end optimization inner_term = torch add h_alpha - abs pow beta regularization_term = torch add -inner_term sum regularization_term reg_param reconstruction_loss soft_quantized_output torch Tensor original_output torch Tensor - torch Tensor Compute reconstruction loss between soft quantized output original output F mse_loss soft_quantized_output original_output reduction= none mean forward soft_quantized_output torch Tensor original_output torch Tensor V torch Tensor curr_iter int - tuple torch Tensor torch Tensor Compute asymmetric reconstruction formulation eq regularization_term = rounding_regularization V curr_iter reconstruction_term = reconstruction_loss soft_quantized_output original_output regularization_term reconstruction_term