mypy allow-untyped-defs abc ABC abstractmethod collections abc Sequence dataclasses dataclass typing Optional Union torch torch distributed dist _ReduceOp = Union dist ReduceOp dist ReduceOp RedOpType dataclass frozen=True MixedPrecisionPolicy This configures FSDP s mixed precision Unlike autocast applies mixed precision module level op level which means low-precision activations saved backward high-to-low-precision casts incurred only module boundaries FSDP works well module-level mixed precision since keeps high-precision sharded parameters memory anyway In other words FSDP does require any extra memory keep high-precision copy parameters optimizer step Attributes param_dtype Optional torch dtype This specifies dtype unsharded parameter hence dtype forward backward computation parameter all-gather If ` ` None ` ` then unsharded parameter uses original dtype The optimizer step uses sharded parameter original dtype Default ` ` None ` ` reduce_dtype Optional torch dtype This specifies dtype gradient reduction i e reduce-scatter all-reduce If ` ` None ` ` ` ` param_dtype ` ` ` ` None ` ` then reduction uses compute dtype This can used run gradient reduction full precision while using low precision compute If also gradient reduction disabled via meth ` set_requires_gradient_sync ` then FSDP will accumulate gradients using ` ` reduce_dtype ` ` Default ` ` None ` ` output_dtype Optional torch dtype This specifies dtype casting floating-point forward outputs This can used help implement cases where different modules have different mixed precision policies Default ` ` None ` ` cast_forward_inputs bool This specifies whether FSDP should cast forward s floating-point input tensors ` ` param_dtype ` ` param_dtype Optional torch dtype = None reduce_dtype Optional torch dtype = None output_dtype Optional torch dtype = None cast_forward_inputs bool = True Comm ABC Interface communication primitives A primitive primarily needs handle tasks namely How allocate memory communication Depending goal implementation can choose associate each call temporary buffer best flexibility simplicity b reuse persistent buffer efficiency reasons Where allocate memory e g NCCL mem pool regular cuda caching allocator What do call upon comm called see ` AllGather ` interface example abstractmethod allocate size Sequence Union int torch SymInt dtype torch dtype device torch device - torch Tensor This handles how allocate memory part A default implementation could simply code-block python mem_pool torch empty Args size Sequence Union int torch SymInt size tensor buffer dtype torch dtype dtype tensor buffer device torch device which device allocate tensor onto AllGather Comm Interface all_gather comm primitive abstractmethod __call__ output_tensor torch Tensor input_tensor torch Tensor group dist ProcessGroup async_op bool = False - Optional dist Work ReduceScatter Comm Interface reduce_scatter comm primitive abstractmethod __call__ output_tensor torch Tensor input_tensor torch Tensor group dist ProcessGroup op _ReduceOp async_op bool = False - Optional dist Work dataclass OffloadPolicy This base represents policy no offloading only used default value ` ` offload_policy ` ` arg dataclass CPUOffloadPolicy OffloadPolicy This offload policy offloads parameters gradients optimizer states CPU Sharded parameters copied host-to-device before all-gather The all-gathered parameters freed according ` ` reshard_after_forward ` ` Sharded gradients copied device-to-host backward optimizer step runs CPU CPU optimizer states Attributes pin_memory bool Whether pin sharded parameter gradient memory Pinning memory allows both more efficient H D D H copies copies overlap compute However pinned memory cannot used other processes Set ` ` False ` ` you have insufficient CPU memory Default ` ` True ` ` pin_memory bool = True