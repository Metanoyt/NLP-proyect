Owner s oncall cpu inductor contextlib copy functools itertools math os platform sys unittest collections abc Callable unittest mock patch torch torch nn torch _C FileCheck torch _dynamo testing rand_strided torch _dynamo utils same torch _inductor config cpu_vec_isa metrics test_operators torch _inductor codegen cpp CppOverrides CppVecOverrides torch _inductor compile_fx compile_fx compile_fx_inner complex_memory_overlap torch _inductor exc InductorError torch _inductor graph GraphLowering torch _inductor utils timed torch _prims_common is_float_dtype torch fx experimental proxy_tensor make_fx torch nn functional F torch testing _internal common_utils instantiate_parametrized_tests IS_FBCODE IS_MACOS parametrize skipIfRocm slowTest TEST_MKL xfailIfS X torch utils _python_dispatch TorchDispatchMode try try test_torchinductor except ImportError test_torchinductor manual=fbcode caffe test inductor test_inductor-library except unittest SkipTest __name__ == __main__ sys exit raise vec_dtypes = test_torchinductor vec_dtypes _lowp_fp_dtypes = torch bfloat torch float run_and_get_cpp_code = test_torchinductor run_and_get_cpp_code TestCase = test_torchinductor TestCase aten = torch ops aten check_model = test_torchinductor check_model requires_vectorization = unittest skipUnless cpu_vec_isa valid_vec_isa_list os getenv ATEN_CPU_CAPABILITY = default Does support vectorization _can_check_vec_metrics cpu_vec_isa valid_vec_isa_list os getenv ATEN_CPU_CAPABILITY = default config cpp simdlen = check_metrics_vec_kernel_count num_expected_vec_kernels _can_check_vec_metrics assert metrics generated_cpp_vec_kernel_count == num_expected_vec_kernels simd_lengths_to_test Returns minimal list simd lengths cover common cases simdlens = None valid_isa_list = cpu_vec_isa valid_vec_isa_list valid_isa_list simdlens append valid_isa_list bit_width simdlens contextlib contextmanager set_num_threads num_threads orig_num_threads = torch get_num_threads torch set_num_threads num_threads yield torch set_num_threads orig_num_threads LstmModule torch nn Module __init__ input_size hidden_size num_layers bias=True bidirectional=False batch_first=False super __init__ lstm = torch nn LSTM input_size=input_size hidden_size=hidden_size num_layers=num_layers bias=bias bidirectional=bidirectional batch_first=batch_first forward x h=None x h = lstm x h x h instantiate_parametrized_tests CPUReproTests TestCase common = check_model test_torch_linalg_qr_tuple_slice fn x torch linalg qr x x = torch randn compiled = torch compile fn backend= inductor expected = fn x actual = compiled x assertIsInstance actual tuple assertEqual len actual torch testing assert_close actual expected skipIfRocm test_conv_stride_constraints fmt torch contiguous_format torch channels_last TorchDispatch doesn t work our cuda invocation some reason m = torch nn Conv d fn inp weight F conv d inp weight None m stride m padding m dilation m groups inp = torch randn inps = inp m weight memory_format=fmt fn_fx = make_fx fn inps fn_compiled = compile_fx_inner fn_fx inps test_self = conv_seen = False RecordFunctions TorchDispatchMode __torch_dispatch__ func types args= kwargs=None kwargs = kwargs kwargs func == torch ops aten convolution default For CPU mkldnn enable we always using channels last nonlocal fmt torch backends mkldnn enabled torch backends mkldnn is_available fmt = torch channels_last test_self assertTrue args is_contiguous memory_format=fmt test_self assertTrue args is_contiguous memory_format=fmt nonlocal conv_seen conv_seen = True func args kwargs RecordFunctions fn_compiled inps assertTrue conv_seen patch torch cuda is_available lambda False test_conv d_bn_mixed_dtype Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= padding= bias=False dtype=torch bfloat bn = torch nn BatchNorm d eps= momentum= affine=True track_running_stats=True forward x x = conv x x = bn x x v = torch randn dtype=torch bfloat mod = Model eval torch no_grad common mod v test_complex_cholesky_mh_view_fallback torch manual_seed n = fn inp torch Tensor I = torch eye n dtype=inp dtype device=inp device I = I unsqueeze expand inp shape n n contiguous hermitian = I + inp inp mH chol = torch linalg cholesky hermitian upper=True chol abs sum base = torch randn n n dtype=torch complex run compiled_fn inp = base clone detach requires_grad_ True loss = compiled_fn inp loss backward loss detach inp grad detach expected_loss expected_grad = run fn compiled = torch compile fn backend= inductor actual_loss actual_grad = run compiled torch testing assert_close actual_loss expected_loss torch testing assert_close actual_grad expected_grad test_nn_fold Fix https github com pytorch pytorch issues Model torch nn Module __init__ output_size kernel_size stride - None super __init__ fold = torch nn Fold output_size=output_size kernel_size=kernel_size stride=stride forward x x = fold x x output_sizes = kernel_sizes = strides = input_sizes = idx range len output_sizes output_size = output_sizes idx kernel_size = kernel_sizes idx stride = strides idx input_size = input_sizes idx num_threads None torch _dynamo reset metrics reset v = torch randn input_size mod = Model output_size kernel_size stride eval contextlib nullcontext num_threads = set_num_threads torch no_grad common mod v unittest skipIf torch backends mkldnn is_available MKLDNN enabled patch torch cuda is_available lambda False test_conv d_packed options = itertools product True False x_shape mode_train padding options mod = torch nn Sequential torch nn Conv d padding=padding train mode=mode_train v = torch randn x_shape dtype=torch float torch no_grad common mod v patch torch cuda is_available lambda False test_conv d_autocast v = torch randn dtype=torch float mod = torch nn Sequential torch nn Conv d eval torch no_grad torch cpu amp autocast common mod v test_conv d_strided_weight_torch_compile fn x w wt = w transpose y = F conv d x wt y clone x_eager = torch randn requires_grad=True w_eager = torch randn requires_grad=True out_eager = fn x_eager w_eager grad = torch randn_like out_eager out_eager_val = out_eager detach out_eager backward grad grad_x_eager = x_eager grad detach clone grad_w_eager = w_eager grad detach clone x_comp = x_eager detach requires_grad_ True w_comp = w_eager detach requires_grad_ True compiled = torch compile fn backend= inductor fullgraph=True dynamic=True out_comp = compiled x_comp w_comp out_comp_val = out_comp detach out_comp backward grad torch testing assert_close out_comp_val out_eager_val torch testing assert_close x_comp grad grad_x_eager torch testing assert_close w_comp grad grad_w_eager config patch freezing=True unittest skipIf TEST_MKL Test requires MKL patch torch cuda is_available lambda False test_mkl_linear dtypes = torch float options = itertools product True False dtypes input_shape out_dim bias dtype options mod = torch nn Sequential torch nn Linear input_shape - out_dim bias=bias eval v = torch randn input_shape torch no_grad common mod dtype v dtype unittest skipIf torch backends mkldnn is_available MKLDNN enabled patch torch cuda is_available lambda False test_unsupported_conv_transpose Model torch nn Module __init__ - None super __init__ conv_transpose = torch nn ConvTranspose d stride= padding= output_padding= forward input_tensor x = conv_transpose input_tensor output = torch tanh x output input = torch randn m = Model eval torch no_grad compiled_m = torch compile m The cpp_wrapper C-shim can t utilize Python error API so error messages printed stderr directly intercepted RuntimeError significantly less verbose msg = r aoti_torch_cpu_convolution\ \ API call failed config cpp_wrapper output padding must smaller than either stride dilation assertRaisesRegex RuntimeError msg compiled_m input unittest skipIf torch backends mkldnn is_available MKLDNN enabled patch torch cuda is_available lambda False test_conv_used_from_multiple_places M torch nn Module __init__ conv_in_channel conv_out_channel - None super __init__ conv = torch nn Conv d conv_in_channel conv_out_channel forward x res = conv x res = F relu res res = conv res res torch no_grad mod = M eval x = torch randn common mod x unittest skipIf torch backends mkldnn is_available MKLDNN enabled patch torch cuda is_available lambda False test_linear_used_from_multiple_places M torch nn Module __init__ in_channel out_channel - None super __init__ linear = torch nn Linear in_channel out_channel forward x res = linear x res = F relu res res = linear res res dtypes = torch ops mkldnn _is_mkldnn_bf _supported dtypes append torch bfloat torch ops mkldnn _is_mkldnn_fp _supported dtypes append torch float dtype dtypes torch no_grad m = M dtype eval m_opt = torch compile m x = torch randn dtype=dtype m_opt x assertEqual m x m_opt x config patch implicit_fallbacks=True test_multihead_attention_cpu fn q k v embed_dim num_heads qkv_weight qkv_bias proj_weight proj_bias mask need_weights torch _native_multi_head_attention q k v embed_dim num_heads qkv_weight qkv_bias proj_weight proj_bias mask need_weights B = T = embed_dim = num_heads = q = torch randn B T embed_dim k = torch randn B T embed_dim v = torch randn B T embed_dim qkv_weight = torch randn embed_dim embed_dim qkv_bias = torch randn embed_dim proj_weight = torch randn embed_dim embed_dim proj_bias = torch randn embed_dim mask = None need_weights = False inps = q k v embed_dim num_heads qkv_weight qkv_bias proj_weight proj_bias mask need_weights common fn inps config patch freezing=True test_module_buffer_mutation Model torch nn Module __init__ - None super __init__ foo = torch nn Buffer torch rand forward x lx = x x clone x clone y = i range y append lx i + foo i torch cat y torch no_grad example_inputs = torch rand common Model example_inputs unittest skipIf torch backends mkldnn is_available MKLDNN enabled patch torch cuda is_available lambda False test_linear_packed dtypes = torch ops mkldnn _is_mkldnn_bf _supported dtypes append torch bfloat torch ops mkldnn _is_mkldnn_fp _supported dtypes append torch float options = itertools product True False dtypes input_shape out_dim bias dtype options mod = torch nn Sequential torch nn Linear input_shape - out_dim bias=bias eval v = torch randn input_shape torch no_grad common mod dtype v dtype unittest skipIf torch backends mkldnn is_available MKLDNN enabled patch torch cuda is_available lambda False test_conv_transpose d_packed_cpu options = itertools product x_shape padding options mod = torch nn Sequential torch nn ConvTranspose d padding=padding eval v = torch randn x_shape dtype=torch float torch no_grad common mod v torch _dynamo config patch dynamic_shapes True assume_static_by_default False test_full_boolean_dynamic_shape fn n x = torch full n = x x + common fn common fn config patch freezing=True unittest skipIf torch _C _has_mkldnn MKLDNN enabled torch _dynamo config patch dynamic_shapes=True torch _dynamo config patch assume_static_by_default=False test_conv_in_channel_ _dynamic_shapes M torch nn Module __init__ in_channel out_channel - None super __init__ conv = torch nn Conv d in_channel out_channel forward x res = conv x res = F relu res res test case where channels dim input Reproducer maml_omniglot model Torchbench in_channel = out_channel = amp_enabled_configs = False torch ops mkldnn _is_mkldnn_bf _supported When amp enabled here input Conv FlexibleLayout While s disabled input FixedLayout amp_enabled_configs append True amp_enabled amp_enabled_configs mod = M in_channel out_channel eval v = torch randn in_channel torch no_grad torch cpu amp autocast enabled=amp_enabled common mod v unittest skipIf torch _C _has_mkldnn MKLDNN enabled patch torch cuda is_available lambda False torch _dynamo config patch dynamic_shapes=True torch _dynamo config patch assume_static_by_default=False torch _dynamo config patch allow_rnn=True config patch freezing=True _test_lstm_packed unbatched input_size hidden_size num_layers bidirectional bias empty_state batch_first batch_size seq_len change_input_sizes=False torch _dynamo utils counters dtypes = torch float torch ops mkldnn _is_mkldnn_bf _supported dtypes append torch bfloat torch ops mkldnn _is_mkldnn_fp _supported dtypes append torch float dtype dtypes counters clear num_directions = bidirectional seq_len_var = seq_len + unbatched v = torch randn seq_len input_size v_var = torch randn seq_len_var input_size h = torch randn num_layers num_directions hidden_size c = torch randn num_layers num_directions hidden_size batch_first v = torch randn batch_size seq_len input_size v_var = torch randn batch_size seq_len_var input_size v = torch randn seq_len batch_size input_size v_var = torch randn seq_len_var batch_size input_size h = torch randn num_layers num_directions batch_size hidden_size c = torch randn num_layers num_directions batch_size hidden_size mod = LstmModule input_size hidden_size num_layers bias bidirectional batch_first eval maybe_autocast = torch cpu amp autocast dtype == torch bfloat contextlib nullcontext torch no_grad maybe_autocast inps = v empty_state inps append h c fn_opt = torch compile mod backend= inductor _ code = run_and_get_cpp_code fn_opt inps Check _flat_weights functional_tensor otherwise deepcopy will fail during recompilation fn_opt_copy = copy deepcopy fn_opt _flat_weights = fn_opt_copy lstm _flat_weights _flat_weight _flat_weights assertFalse torch _is_functional_tensor _flat_weight assertTrue aten mkldnn_rnn_layer code assertEqual fn_opt inps mod inps assertEqual counters inductor pattern_matcher_count num_layers num_directions + num mkldnn_rnn_layer call + view call concatenated hy cy Change input sizes change_input_sizes inps_var = v_var assertEqual fn_opt inps_var mod inps_var parametrize unbatched input_size hidden_size num_layers bidirectional bias empty_state batch_first batch_size seq_len itertools product True False False True False True False True True False test_lstm_packed unbatched input_size hidden_size num_layers bidirectional bias empty_state batch_first batch_size seq_len _test_lstm_packed unbatched input_size hidden_size num_layers bidirectional bias empty_state batch_first batch_size seq_len parametrize unbatched input_size hidden_size num_layers bidirectional bias empty_state batch_first batch_size seq_len itertools product False True True False False test_lstm_packed_change_input_sizes_cpu unbatched input_size hidden_size num_layers bidirectional bias empty_state batch_first batch_size seq_len _test_lstm_packed unbatched input_size hidden_size num_layers bidirectional bias empty_state batch_first batch_size seq_len change_input_sizes=True test_set_source_Tensor MaskedConv d torch nn Conv d __init__ in_channels int out_channels int kernel_size int padding int = - None super __init__ in_channels out_channels kernel_size padding=padding mask = torch zeros_like weight mask kernel_size = mask kernel_size kernel_size = register_buffer mask mask forward x torch Tensor - torch Tensor torch no_grad weight data = mask super forward x M torch nn Module __init__ num_channels int num_colors int H int W int - None super __init__ num_channels = num_channels num_colors = num_colors H = H W = W kernel_size = padding = kernel_size - x Mask layers = MaskedConv d in_channels=self num_channels out_channels= kernel_size=kernel_size padding=padding model = nn Sequential layers forward x torch Tensor - torch Tensor x = x permute model x model = M H= W= num_channels= num_colors= fn_opt = torch compile model backend= inductor v = torch rand torch float inp = v clone result code = run_and_get_cpp_code fn_opt inp assertIn aoti_torch_cpu_set__source_Tensor config cpp_wrapper aten set_ source_Tensor code expected = model inp assertEqual expected result test cpp_wrapper_build_separate config patch cpp_wrapper=True cpp_wrapper_build_separate=True result code = run_and_get_cpp_code fn_opt inp assertIn kernel_src code assertEqual expected result config patch cpp_wrapper=True cpp_wrapper_build_separate=False result code = run_and_get_cpp_code fn_opt inp assertNotIn kernel_src code assertEqual expected result torch _dynamo config patch dynamic_shapes=True torch _dynamo config patch assume_static_by_default=False torch _dynamo config patch allow_rnn=True test_pack_padded_sequence_lstm embedding_dim = hidden_dim = batch_size = num_layers = bidirectional = True num_direc = max_lens = sent = torch randn batch_size max_lens embedding_dim hid_ = torch rand num_layers num_direc batch_size hidden_dim hid_ = torch randn num_layers num_direc batch_size hidden_dim sent_lens = torch Tensor assert sent_lens shape == batch_size assert sent_lens max item == max_lens hidden_ = hid_ clone requires_grad_ False hidden_ = hid_ clone requires_grad_ False embeds = torch nn utils rnn pack_padded_sequence sent sent_lens batch_first=True enforce_sorted=False mod = LstmModule embedding_dim hidden_dim num_layers=num_layers bias=True bidirectional=bidirectional batch_first=True eval torch no_grad inps = embeds hidden_ hidden_ fn_opt = torch compile mod backend= inductor _ code = run_and_get_cpp_code fn_opt inps This case unsupported assertFalse torch ops mkldnn _lstm code assertEqual fn_opt inps mod inps patch torch cuda is_available lambda False test_conv_transpose d_has_output_size_input https github com pytorch pytorch issues M torch nn Module __init__ - None super __init__ conv_transpose = torch nn ConvTranspose d in_channels= out_channels= kernel_size= stride= padding= forward x conv_transpose x output_size= mod = M eval v = torch randn dtype=torch float torch no_grad common mod v test_pad_with_nan_value https github com pytorch pytorch issues Model torch nn Module forward x x = F pad x value=float nan x mod = Model eval v = torch randn dtype=torch float torch no_grad common mod v test_masked_fill_with_inf_or_nan_value fn value mask y = torch masked_fill value mask float inf y = torch masked_fill value mask float -inf y = torch masked_fill value mask float nan y y y value = torch randn mask = torch randint size= dtype=torch uint torch bool torch no_grad common fn value mask test_relu_with_inf_value https github com pytorch pytorch issues fn out out = torch sinh input=out out = torch relu input=out out x = torch Tensor - torch no_grad common fn x test_acosh_with_negative_large_input https github com pytorch pytorch issues fn input out = torch acosh input out x = torch Tensor - - - - - repeat dtype torch float torch bfloat torch double torch no_grad torch _dynamo reset metrics reset _x = x dtype common fn _x requires_vectorization test_asinh_with_corner_inputs https github com pytorch pytorch issues fn input out = torch asinh input out x = torch tensor - repeat bit_widths = isa _bit_width isa cpu_vec_isa valid_vec_isa_list dtype torch float torch bfloat torch float torch double simdlen bit_widths torch no_grad config patch cpp simdlen simdlen torch _dynamo reset metrics reset _x = x dtype common fn _x check_metrics_vec_kernel_count config patch fallback_random=True test_require_stride_order_non_owning test_concat_with_conv x = torch randn memory_format=torch channels_last x = torch randn memory_format=torch channels_last First do concatenation cat_result = torch cat x x dim= Then use x which input cat conv conv_weight = torch randn memory_format=torch channels_last x _conv = torch nn functional conv d x conv_weight padding= cat_result x _conv torch manual_seed f_c = torch compile test_concat_with_conv out_result code = run_and_get_cpp_code f_c torch manual_seed assertEqual out_result test_concat_with_conv both inputs conv should channels last config cpp_wrapper FileCheck check L L L L check L L L L check L L L L check L L L L check aoti_torch_empty_strided run code FileCheck check check empty_strided_cpu run code config patch implicit_fallbacks=True test_repeat_interleave fn y torch repeat_interleave y output_size= = torch tensor common fn test_inplace_squeeze_needed mod = torch nn Sequential torch nn Linear torch nn LayerNorm torch nn ReLU eval fn x mod x v = torch randn TODO OMP parallel reduction order deterministic Hence accuracy might vary up down For short term we increase tolerance will fix later using aten parallel common fn v atol= e- rtol= e- test_parallel_reduction_vectorization Fix issue https github com pytorch pytorch issues Model torch nn Module __init__ enable_masked_tail_vec super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= enable_masked_tail_vec = enable_masked_tail_vec forward x weight x = conv x enable_masked_tail_vec x = F hardshrink x lambd= x = x view x size - x = torch mv weight x x enable_masked_tail_vec True False mod = Model enable_masked_tail_vec eval x = torch randn weight = torch randn Use same criterion test_inplace_squeeze_needed parallel reduction common mod x weight atol= e- rtol= e- test_cat_mul https github com pytorch pytorch issues fn p p y = torch cat p p dim= y = torch mul y y y y p = torch randn p = torch randn common fn p p test_pow_cos https github com pytorch pytorch issues fn x t = x pow torch cos t x = torch tensor dtype=torch uint common fn x test_reduce_with_masked https github com pytorch pytorch issues fn b = torch nn functional pad - c = + b c min values = torch randn b = torch randn common fn b test_scalar_sign_with_min https github com pytorch pytorch issues fn t = torch tanh t = torch sign t torch min t t = torch randn common fn test_tanh_atan https github com pytorch pytorch issues Model torch nn Module __init__ super __init__ shrink = nn Tanhshrink forward x x = shrink x x = torch atan x x x x = torch randn common Model x unittest skipIf os getenv ATEN_CPU_CAPABILITY == default Failing periodic nogpu_NO_AVX after added config patch cpp use_decompose_tanh test_tanh_atan _use_decompose_tanh https github com pytorch pytorch issues Model torch nn Module __init__ super __init__ shrink = nn Tanhshrink forward x x = shrink x x = torch atan x x x x = torch randn assertRaises AssertionError common Model x test_index_propagation_issue_ fn x x = torch arange x numel x unsqueeze - x unsqueeze common fn torch randn test_low_fp_index_expr_issue_ https github com pytorch pytorch issues fn start end dtype dim torch sum torch arange start=start end=end dtype=dtype dim=dim common fn torch float test_index_put https github com pytorch pytorch issues fn x y x = x + y x += y x x = torch randint - - dtype=torch int y = torch randn dtype=torch float x_clone = x clone y_clone = y clone torch no_grad fn x y torch compile fn x_clone y_clone assertEqual y y_clone atol= e- rtol= e- test_index_put https github com pytorch pytorch issues fn y index index y index += y index y = torch randn dtype=torch float index = torch tensor index = torch tensor y_clone = y clone index _clone = index clone index _clone = index clone torch no_grad fn y index index torch compile fn y_clone index _clone index _clone assertEqual y y_clone atol= e- rtol= e- test_index_add https github com pytorch pytorch issues fn x y scale_y index values = x index + y scale_y out = x index_add_ dim= source=values index=index out inp = torch randn torch randn torch randn torch randperm device= cpu torch int inp_clones = i range inp_clones append inp clone inp clone inp clone inp clone i == torch zeros device= cpu torch int inp_clone inp_clone inp_clone = inp_clones torch no_grad cfn = torch compile fn ref = fn inp res = cfn inp_clone assertEqual ref res atol= e- rtol= e- ref = fn inp_clone res = cfn inp_clone assertEqual ref res atol= e- rtol= e- test_ModularIndexing_range_issue_ fn q k einsum = torch einsum bcxd bcyd- bcxy q k constant_pad_nd = torch ops aten constant_pad_nd default einsum view = torch ops aten view default constant_pad_nd y = view new_zeros y - = view y common fn torch empty_strided torch empty_strided patch torch cuda is_available lambda False test_max_reduction_lowp_fp fn x torch ops aten max x keepdim=True float dtype _lowp_fp_dtypes common fn torch randn dtype patch torch cuda is_available lambda False test_vec_transpose_lowp_fp dtype _lowp_fp_dtypes fn x x memory_format=torch channels_last dtype common fn torch randn test_load_inf_bf fn x torch where x x math inf fn x torch where x x -math inf fn fn fn common fn torch randn patch torch cuda is_available lambda False test_fp _load_with_to_lowp_fp From llama model Model torch nn Module __init__ - None super __init__ cache_k = torch zeros forward x xk bsz seqlen _ = x shape cache_k = cache_k x cache_k bsz + seqlen = xk cache_k dtype _lowp_fp_dtypes ref_model = Model eval opt_model = torch compile Model eval x = torch randn dtype xk = torch randn dtype assertEqual opt_model x xk ref_model x xk requires_vectorization patch torch cuda is_available lambda False test_sigmoid_with_reduction fn x x = torch ops aten sigmoid default x torch ops aten mean dim x - - True x = torch randn config patch cpp simdlen None torch _dynamo reset metrics reset common fn x test_slice_scatter_default_end_value From HF AllenaiLongformerBase fn query key window_overlap batch_size seq_len num_heads head_dim = query size assert seq_len window_overlap == f Sequence length should multiple window_overlap Given seq_len chunks_count = torch div seq_len window_overlap rounding_mode= trunc - diagonal_chunked_attention_scores = key diagonal_attention_scores = diagonal_chunked_attention_scores new_zeros batch_size num_heads chunks_count + window_overlap window_overlap + diagonal_attention_scores window_overlap = diagonal_chunked_attention_scores window_overlap window_overlap + diagonal_attention_scores common fn torch randn torch randn requires_vectorization patch torch cuda is_available lambda False test_to_uint _rounding_method fn x x torch uint numerical_testsuit = numerical_number numerical_testsuit x = torch ones numerical_number config patch cpp simdlen None torch _dynamo reset metrics reset common fn x check_metrics_vec_kernel_count requires_vectorization _test_decomposed_dequant_relu_quant_helper dtype fn x scale zero_point use_dequant use_quant quant_min quant_max dtype For quantized_decomposed dequantize_per_tensor Refer torch ao quantization fx _decomposed py use_dequant x = x torch float - zero_point scale x = torch relu x For quantized_decomposed quantize_per_tensor Refer torch ao quantization fx _decomposed py use_quant inv_scale = scale x = torch clamp torch round x inv_scale + zero_point quant_min quant_max dtype x assert dtype torch uint torch int quant_min = dtype == torch uint - quant_max = dtype == torch uint use_dequant_list = False True use_quant_list = False True use_dequant use_quant itertools product use_dequant_list use_quant_list x = torch clamp torch randn dtype=torch float quant_min quant_max use_dequant x = x dtype zero_point = scale = config patch cpp simdlen None torch _dynamo reset metrics reset common fn x scale zero_point use_dequant use_quant quant_min quant_max dtype check_metrics_vec_kernel_count requires_vectorization test_decomposed_dequant_relu_quant_uint _test_decomposed_dequant_relu_quant_helper torch uint requires_vectorization test_decomposed_dequant_relu_quant_int _test_decomposed_dequant_relu_quant_helper torch int _test_dequant_quant_lowering_helper dtype dequant_out_dtype=None fn x scale zero_point use_dequant use_quant quant_min quant_max dtype dequant_out_dtype use_dequant x = torch ops quantized_decomposed dequantize_per_tensor x scale zero_point quant_min quant_max dtype out_dtype=dequant_out_dtype x = torch relu x use_quant x = torch ops quantized_decomposed quantize_per_tensor x scale zero_point quant_min quant_max dtype x use_dequant_list = False True use_quant_list = False True use_tensor_overload_list = False True assert dtype torch uint torch int torch float _e m fn torch float _e m quant_min = dtype == torch uint - quant_max = dtype == torch uint dtype torch float _e m fn torch float _e m quant_min = int torch finfo dtype min quant_max = int torch finfo dtype max use_tensor_overload_list = False use_dequant use_quant use_tensor_overload itertools product use_dequant_list use_quant_list use_tensor_overload_list x = torch clamp torch randn dtype=torch float quant_min quant_max use_dequant x = x dtype zero_point = scale = use_tensor_overload zero_point = torch tensor zero_point dtype=torch int scale = torch tensor scale config patch cpp simdlen None torch _dynamo reset metrics reset inputs = x scale zero_point use_dequant use_quant quant_min quant_max dtype dequant_out_dtype common fn inputs check_metrics_vec_kernel_count Check both main tail loops vectorized dtype torch float _e m fn torch float _e m compiled_fn = torch compile fn _ code = run_and_get_cpp_code compiled_fn inputs FileCheck check_count loadu exactly=True run code requires_vectorization test_dequant_quant_lowering_uint _test_dequant_quant_lowering_helper torch uint _test_dequant_quant_lowering_helper torch uint dequant_out_dtype=torch bfloat requires_vectorization test_dequant_quant_lowering_int _test_dequant_quant_lowering_helper torch int _test_dequant_quant_lowering_helper torch int dequant_out_dtype=torch bfloat requires_vectorization test_dequant_quant_lowering_fp _e m _test_dequant_quant_lowering_helper torch float _e m fn requires_vectorization test_dequant_quant_lowering_fp _e m _test_dequant_quant_lowering_helper torch float _e m _test_dequant_maxpool d_lowering_helper dtype fn x scale zero_point quant_min quant_max dtype x = torch ops quantized_decomposed dequantize_per_tensor x scale zero_point quant_min quant_max dtype max_pool d_with_indices_default = torch ops aten max_pool d_with_indices default x max_pool d_with_indices_default assert dtype torch uint torch int quant_min = dtype == torch uint - quant_max = dtype == torch uint use_tensor_overload_list = False True use_tensor_overload use_tensor_overload_list x = torch clamp torch randn dtype=torch float quant_min quant_max dtype contiguous memory_format=torch channels_last zero_point = scale = use_tensor_overload zero_point = torch tensor zero_point dtype=torch int scale = torch tensor scale config patch cpp simdlen None torch _dynamo reset metrics reset common fn x scale zero_point quant_min quant_max dtype check_metrics_vec_kernel_count requires_vectorization test_dequant_maxpool d_lowering_uint _test_dequant_maxpool d_lowering_helper torch uint requires_vectorization test_dequant_maxpool d_lowering_int _test_dequant_maxpool d_lowering_helper torch int _test_tile d_load_decomposed_dequant_add_relu_quant_helper dtype fn x scale zero_point x scale zero_point output_scale output_zero_point use_dequant use_dequant use_quant quant_min quant_max dtype use_dequant x = torch ops quantized_decomposed dequantize_per_tensor x scale zero_point quant_min quant_max dtype use_dequant x = torch ops quantized_decomposed dequantize_per_tensor x scale zero_point quant_min quant_max dtype temp = x + x y = torch relu temp use_quant y = torch ops quantized_decomposed quantize_per_tensor y output_scale output_zero_point quant_min quant_max dtype y contiguous assert dtype torch uint torch int quant_min = dtype == torch uint - quant_max = dtype == torch uint use_dequant_list = False True use_dequant_list = False True use_quant_list = False True use_dequant use_dequant use_quant itertools product use_dequant_list use_dequant_list use_quant_list x = torch clamp torch randn dtype=torch float quant_min quant_max contiguous memory_format=torch channels_last x = torch clamp torch randn dtype=torch float quant_min quant_max contiguous memory_format=torch channels_last use_dequant x = x dtype contiguous memory_format=torch channels_last use_dequant x = x dtype contiguous memory_format=torch channels_last zero_point = scale = zero_point = scale = output_zero_point = output_scale = config patch cpp simdlen None torch _dynamo reset metrics reset common fn x scale zero_point x scale zero_point output_scale output_zero_point use_dequant use_dequant use_quant quant_min quant_max dtype check_metrics_vec_kernel_count requires_vectorization test_tile d_load_decomposed_dequant_add_relu_quant_uint _test_tile d_load_decomposed_dequant_add_relu_quant_helper torch uint requires_vectorization test_tile d_load_decomposed_dequant_add_relu_quant_int _test_tile d_load_decomposed_dequant_add_relu_quant_helper torch int requires_vectorization _test_per_tensor_fake_quant_helper dtype fn input scales zero_points quant_min quant_max dtype input = torch ops quantized_decomposed quantize_per_tensor input scales zero_points quant_min quant_max dtype input = torch ops quantized_decomposed dequantize_per_tensor input scales zero_points quant_min quant_max dtype input use_tensor_overload_list = False True use_tensor_overload use_tensor_overload_list assert dtype torch uint torch int quant_min = dtype == torch uint - quant_max = dtype == torch uint x = torch clamp torch randn dtype=torch float quant_min quant_max zero_point = scale = use_tensor_overload zero_point = torch tensor zero_point dtype=torch int scale = torch tensor scale config patch cpp simdlen None torch _dynamo reset metrics reset common fn x scale zero_point quant_min quant_max dtype assert metrics generated_cpp_vec_kernel_count == requires_vectorization test_per_tensor_fake_quant_uint _test_per_tensor_fake_quant_helper torch uint requires_vectorization test_per_tensor_fake_quant_int _test_per_tensor_fake_quant_helper torch int _test_per_channel_fake_quant_helper dtype input_dtype=torch float output_dtype=None fn input scales zero_points axis quant_min quant_max dtype output_dtype input = torch ops quantized_decomposed quantize_per_channel input scales zero_points axis quant_min quant_max dtype input = torch ops quantized_decomposed dequantize_per_channel input scales zero_points axis quant_min quant_max dtype out_dtype=output_dtype input assert dtype torch uint torch int quant_min = dtype == torch uint - quant_max = dtype == torch uint x = torch clamp torch randn dtype=torch float quant_min quant_max input_dtype = torch float x = x dtype=input_dtype scales = torch ones zero_points = torch zeros axis = config patch cpp simdlen None torch _dynamo reset metrics reset common fn x scales zero_points axis quant_min quant_max dtype output_dtype check_metrics_vec_kernel_count requires_vectorization test_per_channel_fake_quant_uint _test_per_channel_fake_quant_helper torch uint requires_vectorization test_per_channel_fake_quant_module_uint Mod torch nn Module __init__ - None super __init__ scales = torch ones torch float zero_points = torch zeros torch int axis = quant_min = quant_max = dtype = torch uint forward input input = torch ops quantized_decomposed quantize_per_channel input scales zero_points axis quant_min quant_max dtype input = torch ops quantized_decomposed dequantize_per_channel input scales zero_points axis quant_min quant_max dtype input m = Mod eval x = torch clamp torch randn dtype=torch float config patch cpp simdlen None torch _dynamo reset metrics reset common m x assert metrics generated_cpp_vec_kernel_count == requires_vectorization test_per_channel_fake_quant_int _test_per_channel_fake_quant_helper torch int requires_vectorization test_per_channel_fake_quant_uint _bf _input _test_per_channel_fake_quant_helper torch uint input_dtype=torch bfloat _test_per_channel_fake_quant_helper torch uint input_dtype=torch bfloat output_dtype=torch bfloat requires_vectorization test_per_channel_fake_quant_int _bf _input _test_per_channel_fake_quant_helper torch int input_dtype=torch bfloat _test_per_channel_fake_quant_helper torch int input_dtype=torch bfloat output_dtype=torch bfloat _test_non_contiguous_load_buf_quant_helper dtype fn x x groups quant_min quant_max dtype x = torch cat x x dim= batchsize num_channels height width = x size channels_per_group = num_channels groups x = torch ops quantized_decomposed dequantize_per_tensor x quant_min quant_max dtype x = x view batchsize groups channels_per_group height width x = torch ops quantized_decomposed quantize_per_tensor x quant_min quant_max dtype x = torch ops quantized_decomposed dequantize_per_tensor x quant_min quant_max dtype x = torch transpose x contiguous x = x view batchsize num_channels height width x assert dtype torch uint torch int quant_min = dtype == torch uint - quant_max = dtype == torch uint x = torch randint dtype=dtype contiguous memory_format=torch channels_last x = torch randint dtype=dtype contiguous memory_format=torch channels_last config patch cpp simdlen None torch _dynamo reset metrics reset common fn x x quant_min quant_max dtype check_metrics_vec_kernel_count requires_vectorization test_non_contiguous_load_buf_quant_uint _test_non_contiguous_load_buf_quant_helper torch uint requires_vectorization test_non_contiguous_load_buf_quant_int _test_non_contiguous_load_buf_quant_helper torch int _test_tile d_store_channel_shuffle_cl_quant_output_helper dtype channel_shuffle x groups output_scale output_zero_point quant_min quant_max dtype batchsize num_channels height width = x size channels_per_group = num_channels groups x = x view batchsize groups channels_per_group height width x = torch transpose x contiguous x = x view batchsize - height width x = torch ops quantized_decomposed quantize_per_tensor x output_scale output_zero_point quant_min quant_max dtype x contiguous memory_format=torch channels_last assert dtype torch uint torch int quant_min = dtype == torch uint - quant_max = dtype == torch uint config patch cpp simdlen None torch _dynamo reset metrics reset x = torch randn output_zero_point = output_scale = common channel_shuffle x output_scale output_zero_point quant_min quant_max dtype check_metrics_vec_kernel_count requires_vectorization test_tile d_store_channel_shuffle_cl_quant_output_uint _test_tile d_store_channel_shuffle_cl_quant_output_helper torch uint requires_vectorization test_tile d_store_channel_shuffle_cl_quant_output_int _test_tile d_store_channel_shuffle_cl_quant_output_helper torch int _test_dequant_relu_quant_dequant_relu_quant_lowering_helper dtype fn x scale zero_point scale zero_point scale zero_point quant_min quant_max dtype x = torch ops quantized_decomposed dequantize_per_tensor x scale zero_point quant_min quant_max dtype x = torch relu x x = torch ops quantized_decomposed quantize_per_tensor x scale zero_point quant_min quant_max dtype x = torch ops quantized_decomposed dequantize_per_tensor x scale zero_point quant_min quant_max dtype x = torch relu x x = torch ops quantized_decomposed quantize_per_tensor x scale zero_point quant_min quant_max dtype x assert dtype torch uint torch int quant_min = dtype == torch uint - quant_max = dtype == torch uint use_tensor_overload True False x = torch clamp torch randn dtype=torch float quant_min quant_max dtype zero_point_list = scale_list = use_tensor_overload i range len zero_point_list zero_point_list i = torch tensor zero_point_list i dtype=torch int scale_list i = torch tensor scale_list i zero_point zero_point zero_point = zero_point_list scale scale scale = scale_list config patch cpp simdlen None torch _dynamo reset metrics reset common fn x scale zero_point scale zero_point scale zero_point quant_min quant_max dtype rtol= e- atol= e- check_metrics_vec_kernel_count requires_vectorization test_dequant_relu_quant_dequant_relu_quant_lowering_uint _test_dequant_relu_quant_dequant_relu_quant_lowering_helper torch uint requires_vectorization test_dequant_relu_quant_dequant_relu_quant_lowering_int _test_dequant_relu_quant_dequant_relu_quant_lowering_helper torch int test_inplace_add_alpha fn x y aten add_ Tensor x y alpha= x x = torch zeros x = torch zeros x = torch zeros y = torch randn fn_fx = make_fx fn x y fn_compiled = compile_fx_inner fn_fx x y fn x y fn_compiled x y assert same x x test_int_div fn x y s = x size = torch ones + s += y s p = torch randint p = torch randn common fn p p test_no_op_squeeze torch compile backend= inductor forward arg _ torch ops aten squeeze dim arg _ x = torch randn common forward x test_parallel_num_threads torch compile backend= inductor fn x x x + x x = torch randn x = torch randn set_num_threads assert same x + x fn x x set_num_threads assert same x + x fn x x patch torch cuda is_available lambda False test_timed_cpu_only timed lambda torch randn test_complex_memory_overlap dense = torch zeros assertFalse complex_memory_overlap dense assertFalse complex_memory_overlap dense t strided = dense split dim= assertFalse complex_memory_overlap strided assertFalse complex_memory_overlap strided t unsqueezed = dense unsqueeze assertFalse complex_memory_overlap unsqueezed assertFalse complex_memory_overlap unsqueezed permute gathered = dense index_select torch IntTensor assertFalse complex_memory_overlap gathered assertFalse complex_memory_overlap gathered t requires_vectorization test_vec_dynamic_shapes fn x torch softmax x - value = torch randn config patch cpp simdlen None torch _dynamo reset metrics reset common fn value unittest skipIf IS_FBCODE Not yet runnable fbcode unittest skipIf cpu_vec_isa valid_vec_isa_list avx str vec_isa vec_isa cpu_vec_isa valid_vec_isa_list asimd str vec_isa vec_isa cpu_vec_isa valid_vec_isa_list Does support vectorization s x ppc le machine patch torch cuda is_available lambda False test_auto_zvec_vsx_simd vec_zvec_vsx = cpu_vec_isa valid_vec_isa_list assertTrue vec_zvec_vsx bit_width == config patch cpp simdlen isa = cpu_vec_isa pick_vec_isa assertFalse isa config patch cpp simdlen isa = cpu_vec_isa pick_vec_isa assertFalse isa config patch cpp simdlen isa = cpu_vec_isa pick_vec_isa assertFalse isa config patch cpp simdlen isa = cpu_vec_isa pick_vec_isa assertTrue isa == vec_zvec_vsx pre_var = os getenv ATEN_CPU_CAPABILITY pre_var os environ pop ATEN_CPU_CAPABILITY try config patch cpp simdlen None isa = cpu_vec_isa pick_vec_isa assertTrue isa == vec_zvec_vsx config patch cpp simdlen None os environ ATEN_CPU_CAPABILITY = avx isa = cpu_vec_isa pick_vec_isa assertTrue isa == vec_zvec_vsx config patch cpp simdlen None os environ ATEN_CPU_CAPABILITY = avx isa = cpu_vec_isa pick_vec_isa assertTrue isa == vec_zvec_vsx config patch cpp simdlen None os environ ATEN_CPU_CAPABILITY = default isa = cpu_vec_isa pick_vec_isa assertFalse isa config patch cpp simdlen None os environ ATEN_CPU_CAPABILITY = zvector isa = cpu_vec_isa pick_vec_isa assertTrue isa == vec_zvec_vsx config patch cpp simdlen None os environ ATEN_CPU_CAPABILITY = vsx isa = cpu_vec_isa pick_vec_isa assertTrue isa == vec_zvec_vsx finally pre_var os environ ATEN_CPU_CAPABILITY = pre_var os getenv ATEN_CPU_CAPABILITY os environ pop ATEN_CPU_CAPABILITY unittest skipIf IS_FBCODE Not yet runnable fbcode unittest skipIf platform machine = x _ cpu_vec_isa valid_vec_isa_list Does support vectorization x _ machine patch torch cuda is_available lambda False test_auto_simd vec_amx = cpu_vec_isa supported_vec_isa_list vec_avx = cpu_vec_isa supported_vec_isa_list vec_avx = cpu_vec_isa supported_vec_isa_list assertTrue vec_amx bit_width == assertTrue vec_amx nelements == assertTrue vec_amx nelements torch bfloat == assertTrue vec_avx bit_width == assertTrue vec_avx bit_width == assertTrue vec_avx nelements == assertTrue vec_avx nelements == assertTrue vec_avx nelements torch bfloat == assertTrue vec_avx nelements torch bfloat == config patch cpp simdlen isa = cpu_vec_isa pick_vec_isa assertFalse isa config patch cpp simdlen isa = cpu_vec_isa pick_vec_isa assertFalse isa config patch cpp simdlen isa = cpu_vec_isa pick_vec_isa assertFalse isa config patch cpp simdlen isa_list = cpu_vec_isa valid_vec_isa_list vec_avx isa_list assertFalse isa config patch cpp simdlen isa_list = cpu_vec_isa valid_vec_isa_list isa = cpu_vec_isa pick_vec_isa vec_amx isa_list assertTrue isa == vec_amx vec_avx isa_list assertTrue isa == vec_avx config patch cpp simdlen isa_list = cpu_vec_isa valid_vec_isa_list vec_avx isa_list isa = cpu_vec_isa pick_vec_isa assertTrue isa == vec_avx pre_var = os getenv ATEN_CPU_CAPABILITY pre_var os environ pop ATEN_CPU_CAPABILITY try config patch cpp simdlen None isa = cpu_vec_isa pick_vec_isa vec_amx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_amx vec_avx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_avx assertTrue isa == vec_avx config patch cpp simdlen None os environ ATEN_CPU_CAPABILITY = avx isa = cpu_vec_isa pick_vec_isa vec_amx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_avx vec_avx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_avx vec_avx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_avx config patch cpp simdlen None os environ ATEN_CPU_CAPABILITY = avx isa = cpu_vec_isa pick_vec_isa vec_amx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_amx vec_avx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_avx assertTrue isa == vec_avx config patch cpp simdlen None os environ ATEN_CPU_CAPABILITY = default isa = cpu_vec_isa pick_vec_isa assertFalse isa config patch cpp simdlen None os environ ATEN_CPU_CAPABILITY = zvector isa = cpu_vec_isa pick_vec_isa vec_amx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_amx vec_avx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_avx assertTrue isa == vec_avx config patch cpp simdlen None os environ ATEN_CPU_CAPABILITY = vsx isa = cpu_vec_isa pick_vec_isa vec_amx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_amx vec_avx cpu_vec_isa valid_vec_isa_list assertTrue isa == vec_avx assertTrue isa == vec_avx finally pre_var os environ ATEN_CPU_CAPABILITY = pre_var os getenv ATEN_CPU_CAPABILITY os environ pop ATEN_CPU_CAPABILITY requires_vectorization patch torch cuda is_available lambda False test_masked_fill_softmax fn value mask mask = mask torch bool x = torch masked_fill value mask - torch softmax x - dtype vec_dtypes value = torch randn dtype=dtype mask = torch randint size= dtype=torch uint config patch cpp simdlen None cpp_wrapper_flag True False config patch cpp_wrapper cpp_wrapper_flag torch _dynamo reset metrics reset common fn value mask assert metrics generated_cpp_vec_kernel_count = test_channels_last_view_as_complex https github com pytorch pytorch issues #issuecomment- reduce_example x torch Tensor y torch Tensor - torch Tensor Applies rotary embedding query key tensors x_out = torch view_as_complex torch stack x float y float dim=- x_out args = torch randn torch randn expected = reduce_example args actual = torch compile reduce_example fullgraph=True args assertEqual expected actual test_load_same_bool_tensor_twice torch compile backend= inductor fn b x = torch masked_fill b - y = torch masked_fill b - x y value = torch randn mask = torch randint size= dtype=torch uint torch bool fn value mask test_cpu_vec_cosim cpp_vec_op_list = cpp_op_list = k v CppVecOverrides __dict__ items isinstance v staticmethod cpp_vec_op_list append k k v CppOverrides __dict__ items isinstance v staticmethod cpp_op_list append k diff = airy_ai bessel_j bessel_j bessel_y bessel_y modified_bessel_i modified_bessel_i modified_bessel_k modified_bessel_k scaled_modified_bessel_k scaled_modified_bessel_k spherical_bessel_j i i e ndtr ndtri log_ndtr erfcx gammainc gammaincc igamma igammac polygamma zeta shifted_chebyshev_polynomial_u chebyshev_polynomial_u chebyshev_polynomial_t shifted_chebyshev_polynomial_w chebyshev_polynomial_w shifted_chebyshev_polynomial_t chebyshev_polynomial_v shifted_chebyshev_polynomial_v hermite_polynomial_he laguerre_polynomial_l hermite_polynomial_h legendre_polynomial_p constant index_expr signbit isinf frexp mod masked randn isnan rand randint logical_and logical_not logical_or logical_xor bitwise_and bitwise_left_shift bitwise_not bitwise_right_shift bitwise_or bitwise_xor to_dtype_bitcast union = cpp_vec_op_list diff assertTrue set cpp_op_list issubset union f unexpected set cpp_op_list - union test_atomic_add_lowp_fp fn test_args res = torch gather test_args res dtype _lowp_fp_dtypes input_tensor_for_ref = torch tensor - dtype=dtype requires_grad=True input_tensor_for_opt = torch tensor - dtype=dtype requires_grad=True test_args_for_ref = input input_tensor_for_ref dim index torch tensor test_args_for_opt = input input_tensor_for_opt dim index torch tensor opt_fn = torch compile fn ref_fwd = fn test_args_for_ref res_fwd = opt_fn test_args_for_opt assertEqual res_fwd ref_fwd torch manual_seed bwd_tensor_for_ref = torch randn ref_fwd shape dtype=dtype torch manual_seed bwd_tensor_for_opt = torch randn res_fwd shape dtype=dtype assertEqual bwd_tensor_for_ref bwd_tensor_for_opt ref_fwd backward bwd_tensor_for_ref res_fwd backward bwd_tensor_for_opt ref_grad = test_args_for_ref input grad res_grad = test_args_for_opt input grad assertEqual ref_grad res_grad test_meta_device torch compile fullgraph=True fn x = torch ops aten empty memory_format dtype=torch float device= meta pin_memory=False x sin + assertEqual fn shape test_decomposed_fake_quant_per_channel fq input scales zero_points axis quant_min quant_max res = torch fake_quantize_per_channel_affine input scales zero_points axis quant_min quant_max res qdq input scales zero_points axis quant_min quant_max res = torch ops quantized_decomposed fake_quant_per_channel input scales zero_points axis quant_min quant_max res run_eager_aten_fake_quant input scales zero_points axis quant_min quant_max input grad = None res = fq input scales zero_points axis quant_min quant_max res sum backward res input grad run_eager_decomposed_fake_quant input scales zero_points axis quant_min quant_max input grad = None res = qdq input scales zero_points axis quant_min quant_max res sum backward res input grad run_compile_decomposed_fake_quant input scales zero_points axis quant_min quant_max input grad = None compiled_qdq = torch compile qdq res = compiled_qdq input scales zero_points axis quant_min quant_max res sum backward res input grad input = torch randn input = input requires_grad_ scales = torch ones zero_points = torch zeros axis = quant_min = - quant_max = aten_input = copy deepcopy input compiler_input = copy deepcopy input res_aten_eager input_grad_aten_eager = run_eager_aten_fake_quant aten_input scales zero_points axis quant_min quant_max res_decomp_eager input_grad_decomp_eager = run_eager_decomposed_fake_quant input scales zero_points axis quant_min quant_max res input_grad = run_compile_decomposed_fake_quant compiler_input scales zero_points axis quant_min quant_max assertEqual res_aten_eager res assertEqual res_decomp_eager res assertEqual input_grad_aten_eager input_grad assertEqual input_grad_decomp_eager input_grad assertEqual input_grad torch tensor For forward backward kernel check_metrics_vec_kernel_count requires_vectorization test_ops_masked_with_bool_input x = torch zeros dtype=torch bool size = res_aten_eager = torch constant_pad_nd x size cfn = torch compile torch constant_pad_nd res = cfn x size assertEqual res_aten_eager res check_metrics_vec_kernel_count requires_vectorization test_frexp fn x x_frac x_exp = torch frexp x x_frac int x_exp float x = x_frac x_exp x x = torch randn torch _dynamo reset metrics reset common fn x check_metrics_vec_kernel_count test_bitwise_right_shift x = torch randint - device= cpu dtype=torch int bit_num = res_aten_eager = torch bitwise_right_shift x bit_num cfn = torch compile torch bitwise_right_shift res = cfn x bit_num assertEqual res_aten_eager res test_bitwise_shift_corner_inputs Fix https github com pytorch pytorch issues https github com pytorch pytorch issues bitwise_fns = torch bitwise_left_shift torch bitwise_right_shift bitwise_fn bitwise_fns torch _dynamo reset metrics reset x = torch tensor dtype=torch int bit_num = torch tensor dtype=torch int res_aten_eager = bitwise_fn x bit_num cfn = torch compile bitwise_fn res = cfn x bit_num assertEqual res_aten_eager res test_view_dtype f x x view torch int input = torch ones res_aten_eager = f input cfn = torch compile f res = cfn input assertEqual res_aten_eager res patch torch cuda is_available lambda False test_scatter_using_atomic_add fn dim index b aten scatter dim index b reduce= add inps = torch randn torch tensor torch randn _internal_check _fn _inps _target_code_check=None _target_code_check_not=None torch _dynamo reset metrics reset _fn_opt = torch compile _fn _ code = run_and_get_cpp_code _fn_opt inps _target_code_check FileCheck check _target_code_check run code _target_code_check_not FileCheck check_not _target_code_check_not run code Verify output isn t empty FileCheck check Output code run code assertEqual _fn _inps _fn_opt _inps config patch cpp fallback_scatter_reduce_sum False _internal_check fn inps atomic_add scatter_reduce_func = aoti_torch_cpu_scatter_reduce_ config cpp_wrapper aten scatter_reduce_ config patch cpp fallback_scatter_reduce_sum True _internal_check fn inps scatter_reduce_func ATen parallel backend OpenMP torch __config__ parallel_info set_num_threads When running single thread we expect aten scatter will go into cpp backend codegen instead fallback aten scatter_reduce_ Avoid inductor cache so we don t serve entry compiled above config patch fx_graph_cache False fx_graph_remote_cache False _internal_check fn inps _target_code_check_not=scatter_reduce_func config patch cpp dynamic_threads True set_num_threads _internal_check fn inps scatter_reduce_func patch torch cuda is_available lambda False requires_vectorization torch _inductor config patch cpp fallback_scatter_reduce_sum False test_scatter_using_atomic_add_vec fn dim index b aten scatter dim index b reduce= add inps = torch zeros torch tensor torch ones torch _dynamo reset metrics reset common fn inps assert metrics generated_cpp_vec_kernel_count == set_num_threads config patch fx_graph_cache False fx_graph_remote_cache False torch _dynamo reset metrics reset common fn inps assert metrics generated_cpp_vec_kernel_count == test_large_mean size = t = torch rand size dtype=torch float op = torch mean expected = op t actual = torch compile op t assertEqual expected actual set_num_threads expected = op t actual = torch compile op t assertEqual expected actual unittest skipIf IS_FBCODE Not yet runnable fbcode requires_vectorization patch torch cuda is_available lambda False test_new_vec_op_cpu_only fn x torch log p torch expm torch erf x dtype vec_dtypes torch manual_seed x = torch randn dtype=dtype x = torch nan x - = torch nan config patch cpp simdlen None cpp_wrapper_flag True False config patch cpp_wrapper cpp_wrapper_flag torch _dynamo reset metrics reset common fn x check_metrics_vec_kernel_count requires_vectorization patch torch cuda is_available lambda False test_vec_cpu_only_for_all_available_isa fn x torch sin torch cos torch erf x x = torch randn x = torch nan x - = torch nan bit_widths = isa _bit_width isa cpu_vec_isa valid_vec_isa_list + None item bit_widths config patch cpp simdlen item torch _dynamo reset metrics reset common fn x check_metrics_vec_kernel_count slowTest requires_vectorization patch torch cuda is_available lambda False config patch cpp enable_tiling_heuristics False test__adaptive_avg_pool d wrap_fn oh ow fn x torch _adaptive_avg_pool d x oh ow fn bit_widths = isa _bit_width isa cpu_vec_isa valid_vec_isa_list ih = iw = ih oh = ih ow = ih _ih _iw _oh _ow _simd_len dtype itertools product ih iw oh ow bit_widths vec_dtypes x = torch randn _ih _iw dtype=dtype memory_format=torch channels_last _fn = wrap_fn _oh _ow config patch cpp simdlen _simd_len torch _dynamo reset metrics reset common _fn x check_metrics_vec_kernel_count requires_vectorization patch torch cuda is_available lambda False test_vec_logical wrap_fn op Callable fn x torch Tensor torch where op x fn wrap_fn op Callable fn x torch Tensor y torch Tensor torch where op x y fn dtype vec_dtypes x = torch randn dtype=dtype y = torch randn dtype=dtype logical_fns = torch logical_and torch logical_not torch logical_or torch logical_xor logical_fn logical_fns torch _dynamo reset metrics reset logical_fn == torch logical_not _fn = wrap_fn logical_fn _args = x _fn = wrap_fn logical_fn _args = x y common _fn _args check_metrics_vec_kernel_count requires_vectorization test_vec_bitwise dtype torch bool torch uint torch int torch int torch int x = torch randn dtype=torch float y = torch randn dtype=torch float dtype == torch bool x = x y = y x = x dtype y = y dtype bitwise_fns = torch bitwise_and torch bitwise_not torch bitwise_or torch bitwise_xor torch bitwise_left_shift torch bitwise_right_shift bitwise_fn bitwise_fns bitwise_fn torch bitwise_left_shift torch bitwise_right_shift dtype == torch bool Eager doesn t support bool https pytorch org docs stable generated torch bitwise_left_shift html continue torch _dynamo reset metrics reset bitwise_fn == torch bitwise_not _args = x _args = x y common bitwise_fn _args check_metrics_vec_kernel_count requires_vectorization test_vec_randn funcs = torch randn torch rand torch randint float_dtypes = torch bfloat torch float torch float int_dtypes = torch int torch uint torch int torch int dtypes = float_dtypes + int_dtypes rand_func dtype itertools product funcs dtypes rand_func == torch randint dtype int_dtypes rand_func = torch randint dtype float_dtypes Skip invalid combination continue config patch fx_graph_cache False fx_graph_remote_cache False torch _dynamo reset metrics reset func seed torch manual_seed seed rand_func == torch randint rand_func dtype=dtype rand_func dtype=dtype cfn = torch compile func Check result deterministic mauanl seed assertEqual cfn cfn check_metrics_vec_kernel_count res_vec = cfn torch _dynamo reset metrics reset config patch cpp simdlen res_scalar = torch compile func Check same result between scalar vec assertEqual res_vec res_scalar requires_vectorization test_bitwise_logical_op_bool bitwise_fns = torch bitwise_and torch bitwise_or torch bitwise_xor torch logical_and torch logical_or torch logical_xor bitwise_fn bitwise_fns fn b c = bitwise_fn b c = torch ones dtype=torch int b = torch ones dtype=torch uint config patch cpp simdlen None torch _dynamo reset metrics reset common fn b test_torch_logit fix https github com pytorch pytorch issues fn args torch logit args args input = torch tensor dtype=torch float eps = torch tensor dtype=torch float common fn input eps requires_vectorization patch torch cuda is_available lambda False test_vec_compare_op_cpu_only fn x y = torch eq x x = torch where y x -x y = torch ne x x = torch where y x -x y = torch lt x x = torch where y x x - y = torch gt x - x = torch where y x x + y = torch le x x = torch where y x x - y = torch ge x - x = torch where y x x + y = x == x = torch where y x -x y = x = x = torch where y x -x y = x x = torch where y x x - y = x - x = torch where y x x + y = x = x = torch where y x x - y = x = - x = torch where y x x + x dtype vec_dtypes x = torch randn dtype=dtype config patch cpp simdlen None torch _dynamo reset metrics reset common fn x check_metrics_vec_kernel_count assert metrics generated_kernel_count - metrics generated_cpp_vec_kernel_count == requires_vectorization test_vec_remainder dtype torch int torch uint torch int torch int torch bfloat torch float torch float torch float is_float_dtype dtype x = torch randn dtype=dtype y = torch randn dtype=dtype lower = dtype == torch uint - x = torch randint lower dtype=dtype y = torch randint lower dtype=dtype y = torch where y == torch zeros_like y torch ones_like y y torch _dynamo reset metrics reset _args = x y common torch remainder _args check_metrics_vec_kernel_count test_skip_cpp_codegen config patch disable_cpp_codegen True inps = torch ones torch rand f x y x + y + torch tensor f_opt = torch compile f config cpp_wrapper cpp_wrapper CPU doesn t work without CPP codegen assertRaises InductorError f_opt inps _ code = run_and_get_cpp_code f_opt inps inps FileCheck check_not void kernel run code assertEqual f inps f_opt inps constant needs propagated fallback f x x torch tensor f_opt = torch compile f _ code = run_and_get_cpp_code f_opt inps FileCheck check_not void kernel run code assertEqual f_opt inps f inps Model torch nn Module __init__ super __init__ forward v torch Tensor vx = v min dim= values v = torch randn_like vx v model = Model x = torch rand model_f = torch compile model assertEqual model x model_f x test_redundant_to_node_elimination_lowp_fp fn x y res = x + y res = torch mean res res dtype _lowp_fp_dtypes x = torch randn dtype=dtype y = torch randn dtype=dtype torch_compile_debug True False config patch trace enabled torch_compile_debug cpp simdlen None torch _dynamo reset metrics reset common fn x y check_metrics_vec_kernel_count test_do_not_insert_to_dtype_for_memory_copy_only_kernel fn x res = x clone res x = torch randn dtype=torch bfloat torch _dynamo reset metrics reset common fn x assert metrics cpp_to_dtype_count == check_metrics_vec_kernel_count test_insert_to_dtype_count fn x res = x relu res x = torch randn dtype=torch bfloat torch _dynamo reset metrics reset common fn x assert metrics cpp_to_dtype_count == check_metrics_vec_kernel_count test_memory_copy_with_fusion fn x res = x relu x copy_ res res x = torch randn dtype=torch bfloat torch _dynamo reset metrics reset common fn x assert metrics cpp_to_dtype_count == check_metrics_vec_kernel_count requires_vectorization patch torch cuda is_available lambda False test_maxpool d_cpu_only dtype vec_dtypes input = torch randn dtype=dtype memory_format=torch channels_last maxpool = torch nn MaxPool d kernel_size= stride= padding= func x maxpool x patch object config cpp simdlen None torch _dynamo reset metrics reset common func input check_metrics_vec_kernel_count requires_vectorization patch torch cuda is_available lambda False config patch cpp enable_tiling_heuristics False test_maxpool d_with_pre_loop_collapse_cpu_only x = torch randn memory_format=torch channels_last x = torch randn memory_format=torch channels_last maxpool = torch nn MaxPool d kernel_size= stride= ceil_mode=True func x x y = x + x maxpool y patch object config cpp simdlen None torch _dynamo reset metrics reset common func x x check_metrics_vec_kernel_count test_randint_symint_input https github com pytorch pytorch issues torch compile fullgraph=True get_traj_idx lengths torch Tensor num_slices int - torch Tensor torch randint lengths shape num_slices device=lengths device lengths = torch zeros dtype=torch long get_traj_idx lengths num_slices= lengths = torch zeros dtype=torch long get_traj_idx lengths num_slices= test_store_reduction fix https github com pytorch pytorch issues fn x y r = x amax dim= r = y amax dim= r r device = cpu int_dypte float_dtype zip torch int torch int torch int torch int torch float torch float torch float torch bfloat x = torch randint low= high= size= dtype=int_dypte device=device y = torch randn dtype=float_dtype device=device common fn x y requires_vectorization patch torch cuda is_available lambda False test_sign_cpu_only fn x torch sign x dtype vec_dtypes x = torch randn dtype=dtype x = torch nan x - = torch nan config patch cpp simdlen None torch _dynamo reset metrics reset common fn x check_metrics_vec_kernel_count requires_vectorization patch torch cuda is_available lambda False test_reduction_cpu_only fn x torch argmax x - dtype vec_dtypes x = torch randn dtype=dtype config patch cpp simdlen None torch _dynamo reset metrics reset common fn x assert metrics generated_cpp_vec_kernel_count == config patch fx_graph_cache False fx_graph_remote_cache False test_outer_loop_fusion fn x max = torch amax x dim=- keepdim=True x - max x = torch randn config patch cpp simdlen None torch _dynamo reset metrics reset common fn x assertEqual len metrics cpp_outer_loop_fused_inner_counts assertEqual metrics cpp_outer_loop_fused_inner_counts inner_kernel_number test_outer_loop_fusion_buffer_remove https github com pytorch pytorch issues fn x x = x sum dim=- x = torch softmax x - x x = torch randn metrics reset common fn x config patch fx_graph_cache False fx_graph_remote_cache False test_local_buffer_in_outer_loop_fusion fn x max = torch nn functional softmax x dim=- x - max x = torch randn config patch cpp simdlen None torch _dynamo reset metrics reset common fn x assertEqual len metrics cpp_outer_loop_fused_inner_counts assertEqual metrics cpp_outer_loop_fused_inner_counts inner_kernel_number assertEqual metrics cpp_outer_loop_fused_inner_counts local_buffer_number Check number global buffer allocation torch _dynamo reset metrics reset _ code = run_and_get_cpp_code torch compile fn backend= inductor x assertEqual code count aoti_torch_empty_strided config cpp_wrapper empty_strided_cpu config patch fx_graph_cache False fx_graph_remote_cache False test_two_local_buffers_in_outer_loop_fusion fn x softmax = torch nn functional softmax x dim=- sum = torch sum softmax dim=- sum_broadcast = torch broadcast_to sum unsqueeze - sum size sum_exp = torch exp sum_broadcast sum = torch sum sum_exp dim=- sub = sum_exp - sum unsqueeze - x - sub x = torch randn config patch cpp simdlen None torch _dynamo reset metrics reset atol = None rtol = None cpu_vec_isa valid_vec_isa_list os getenv ATEN_CPU_CAPABILITY == default atol = e- rtol = e- common fn x atol=atol rtol=rtol assertEqual len metrics cpp_outer_loop_fused_inner_counts assertEqual metrics cpp_outer_loop_fused_inner_counts inner_kernel_number assertEqual metrics cpp_outer_loop_fused_inner_counts local_buffer_number config patch fx_graph_cache False fx_graph_remote_cache False test_share_local_buffers_in_outer_loop_fusion fn x max = torch nn functional softmax x dim=- max = torch nn functional softmax max dim=- x - max x = torch randn config patch cpp simdlen None torch _dynamo reset metrics reset common fn x assertEqual len metrics cpp_outer_loop_fused_inner_counts assertEqual metrics cpp_outer_loop_fused_inner_counts inner_kernel_number assertEqual metrics cpp_outer_loop_fused_inner_counts local_buffer_number global bufs share local buf config patch fx_graph_cache False fx_graph_remote_cache False test_two_local_buffers_in_outer_loop_fusion_case exp exp should replaced local buffer since exp will used after exp exp can t share same local buffer exp fn x a_max = torch amax x - keepdim=True exp = torch exp x - a_max sum = torch sum exp - keepdim=True exp = torch exp exp - sum sum = torch sum exp - keepdim=True sub = exp - sum sub = exp - sub sub x = torch randn config patch cpp simdlen None torch _dynamo reset metrics reset common fn x assertEqual len metrics cpp_outer_loop_fused_inner_counts assertEqual metrics cpp_outer_loop_fused_inner_counts inner_kernel_number assertEqual metrics cpp_outer_loop_fused_inner_counts local_buffer_number config patch fx_graph_cache False fx_graph_remote_cache False test_local_buffer_with_line_reuse Test Global buffer which inplace buffer replaced local buffer fn x y z = torch matmul x y a_max = torch amax x - keepdim=True Previous inplace buffer now local buffer exp = torch exp z - a_max z sum = torch sum exp - keepdim=True exp - sum inputs = torch rand torch rand config patch cpp simdlen None torch _dynamo reset metrics reset common fn inputs assertEqual len metrics cpp_outer_loop_fused_inner_counts assertEqual metrics cpp_outer_loop_fused_inner_counts local_buffer_number requires_vectorization test_argmin fn x torch argmin x - dtype vec_dtypes x = torch randn dtype=dtype torch _dynamo reset metrics reset common fn x assert metrics generated_cpp_vec_kernel_count == requires_vectorization test_argmax_argmin_with_nan_value fn x torch argmax x fn x torch argmin x inputs = torch Tensor - torch Tensor - torch Tensor - torch Tensor - x inputs x = x repeat x = torch log p x Test argmax torch _dynamo reset metrics reset common fn x assert metrics generated_cpp_vec_kernel_count == Test argmin torch _dynamo reset metrics reset common fn x assert metrics generated_cpp_vec_kernel_count == Currently we enabled AVX AVX vectorization If platform supported vectorization will work skip test case For ARM other platforms support we just need add ISA info supported_vector_isa include proper aten vectorization head file unittest skipIf IS_FBCODE Not yet runnable fbcode requires_vectorization patch torch cuda is_available lambda False test_vec_kernel_cpu_only fn x x Current there some limitations follows rsqrt assert both fallback decomp same kernel aten rsqrt default round couldn t find symbolic meta function decomposition fmod logical_and logic_or vec kernel has support to_type x = torch abs x x = torch sin x x = torch neg x x = torch square x x = torch sigmoid x x = torch relu x x = torch cos x x = torch exp x x = torch sqrt x x = torch add x x x = torch sub x x x = torch mul x x x = torch div x x x = torch pow x x = torch log x x = torch floor x x = torch ceil x x = torch trunc x x = torch lgamma x x = torch fmod x x x = torch sign x res = x + x res dtype vec_dtypes torch manual_seed x = torch randn dtype=dtype x = torch randn dtype=dtype config patch cpp simdlen torch _dynamo reset metrics reset common fn x x assert metrics generated_cpp_vec_kernel_count == config patch cpp simdlen None torch _dynamo reset metrics reset common fn x x check_metrics_vec_kernel_count config patch cpp simdlen None torch _dynamo reset metrics reset x = torch randn permute x = torch randn common fn x x check_metrics_vec_kernel_count torch _dynamo reset metrics reset x = torch randn x = torch randn common fn x x check_metrics_vec_kernel_count unittest skipIf IS_FBCODE Not yet runnable fbcode unittest skipIf sys platform linux win cpp kernel profile only support linux now patch torch cuda is_available lambda False config patch cpp enable_kernel_profile True config patch cpp descriptive_names original_aten test_cpp_kernel_profile torch profiler profile torch compile backend= inductor fullgraph=True fn b + b = torch rand b = torch rand profile prof fn b kernel_profile_events = e prof profiler function_events cpp_fused_add_ e name kernel_profile_events append e name assert len kernel_profile_events xfailIfS X requires_vectorization test_channel_shuffle_cl_output code shape extracted shufflenet_v _x _ channel_shuffle x groups batchsize num_channels height width = x size channels_per_group = num_channels groups x = x view batchsize groups channels_per_group height width x = torch transpose x contiguous x = x view batchsize - height width x contiguous memory_format=torch channels_last simdlen simd_lengths_to_test config patch cpp simdlen simdlen torch _dynamo reset metrics reset x = torch randn common channel_shuffle x check_metrics_vec_kernel_count slowTest requires_vectorization test_transpose_with_norm sub-module TIMM gmlp_s _ Model torch nn Module __init__ - None super __init__ linear = torch nn Linear in_features= out_features= bias=True act = torch nn GELU norm = torch nn LayerNorm proj = torch nn Linear fc = torch nn Linear in_features= out_features= bias=True forward x x = linear x x = act x u v = x chunk dim=- v = norm v v = proj v transpose - - y = u v transpose - - fc y x = torch randn simdlen simd_lengths_to_test config patch cpp simdlen simdlen eval_mode True False torch _dynamo reset metrics reset m = Model eval eval_mode Model common m x check_metrics_vec_kernel_count requires_vectorization config patch cpp enable_tiling_heuristics False test_transpose_copy fn t contiguous simdlen simd_lengths_to_test config patch cpp simdlen simdlen dtype torch float torch bfloat shape torch _dynamo reset metrics reset x = torch randn shape dtype=dtype common fn x check_metrics_vec_kernel_count torch _dynamo config patch specialize_int=False test_slice_scatter_issue torch compile fullgraph=True fn t t_src dim start end step t slice_scatter t_src dim start end step shape = input_tensor = torch zeros shape requires_grad=False device= cpu src_tensor = torch ones shape requires_grad=False device= cpu assertRaisesRegex torch _inductor exc InductorError r shape error scatter op fn input_tensor src_tensor shape shape shape shape test_horizontal_fusion fn b c idx _a = torch index_select dim= index=idx _b = torch index_select b dim= index=idx _c = torch index_select c dim= index=idx _a _b _c config patch cpp max_horizontal_fusion_size metrics reset torch _dynamo reset = torch randn size= dtype=torch bfloat b = torch randn size= dtype=torch bfloat c = torch randn size= dtype=torch bfloat idx = torch zeros size= dtype=torch int opt_fn = torch compile fn backend= inductor opt_fn b c idx assertEqual metrics generated_kernel_count assertTrue same fn b c idx opt_fn b c idx config patch cpp max_horizontal_fusion_size metrics reset torch _dynamo reset = torch randn size= dtype=torch bfloat b = torch randn size= dtype=torch bfloat c = torch randn size= dtype=torch bfloat idx = torch zeros size= dtype=torch int opt_fn = torch compile fn backend= inductor opt_fn b c idx assertEqual metrics generated_kernel_count assertTrue same fn b c idx opt_fn b c idx config patch cpp max_horizontal_fusion_size metrics reset torch _dynamo reset = torch randn size= dtype=torch bfloat b = torch randn size= dtype=torch bfloat c = torch randn size= dtype=torch bfloat idx = torch zeros size= dtype=torch int opt_fn = torch compile fn backend= inductor opt_fn b c idx print metrics generated_kernel_count assertEqual metrics generated_kernel_count assertTrue same fn b c idx opt_fn b c idx config patch cpp max_horizontal_fusion_size metrics reset torch _dynamo reset = torch randn size= dtype=torch bfloat b = torch randn size= dtype=torch bfloat c = torch randn size= dtype=torch bfloat idx = torch zeros size= dtype=torch int opt_fn = torch compile fn backend= inductor opt_fn b c idx assertEqual metrics generated_kernel_count assertTrue same fn b c idx opt_fn b c idx test_lowp_fp_neg_abs fn x x neg abs dtype _lowp_fp_dtypes metrics reset x = torch randn dtype opt_fn = torch compile fn backend= inductor assertTrue same fn x opt_fn x assert metrics cpp_to_dtype_count == check_metrics_vec_kernel_count config patch cpp enable_tiling_heuristics False test_transpose_non_contiguous fn From part timm HaloAttn https github com rwightman pytorch-image-models blob main timm layers halo_attn py#L Fixed https github com pytorch pytorch issues accuracy issue as_strided = torch ops aten as_strided default as_strided_ = torch ops aten as_strided default as_strided clone_ = torch ops aten clone default as_strided_ memory_format=torch contiguous_format _unsafe_view_ = torch ops aten _unsafe_view default clone_ permute_ = torch ops aten permute default _unsafe_view_ split_with_sizes = torch ops aten split_with_sizes default permute_ - getitem = split_with_sizes _getitem_ = split_with_sizes permute_ = torch ops aten permute default getitem expand_ = torch ops aten expand default permute_ clone_ = torch ops aten clone default expand_ memory_format=torch contiguous_format clone_ metrics reset x = torch randn memory_format=torch channels_last common fn x test_issue_ Fix issue https github com pytorch pytorch issues fn x x = F gumbel_softmax x tau= hard=True x = torch where x x torch zeros_like x x = torch scatter x dim= index=torch ones dtype=torch long src=torch ones_like x x metrics reset x = torch randn Only test functionality since output gumbel_softmax has randomness torch compile fn backend= inductor x test_non_contiguous_index_with_constant_stride fn x x = x x = x x = torch stack -x x dim=- x flatten - metrics reset x = torch randn opt_fn = torch compile fn backend= inductor _ code = run_and_get_cpp_code opt_fn x assertTrue same fn x opt_fn x declare use declare same non-cpp_wrapper mode FileCheck check_count cpp_fused config cpp_wrapper exactly=True run code test_invalid_index_of_empty_tensor fn b = b = torch tensor assertRaises RuntimeError torch compile fn torch no_grad torch _inductor config patch freezing=True test_issue func x t = torch unbind x t = torch stack t dim= t = torch tanh t t x = torch randn assertEqual torch compile func x func x config patch fx_graph_cache False fx_graph_remote_cache False test_ir_node_str torch compile fn x torch Tensor - torch Tensor x sin torch nn Softmax dim= x cos run_node_alt args kwargs rv = run_node args kwargs strings append str rv rv strings = run_node = GraphLowering run_node patch object GraphLowering run_node run_node_alt fn torch randn assertGreater len strings test_vertical_sum_cpu_only fn sum dim= fn sum dim= metrics reset x = torch randn common fn x check_metrics_vec_kernel_count metrics reset x = torch randn common fn x check_metrics_vec_kernel_count test_transpose_vertical_sum_cpu_only fn b c = b c sum dim= metrics reset x = torch randn y = torch randn transpose common fn x y check_metrics_vec_kernel_count test_transpose_mxn_ _ _bf _fp fn b c = b c sum dim= dtype torch bfloat torch float metrics reset x = torch randn dtype y = torch randn dtype transpose common fn x y check_metrics_vec_kernel_count test_transpose_mxn_ _ _bf _fp fn permute contiguous dtype torch bfloat torch float metrics reset x = torch randn dtype common fn x check_metrics_vec_kernel_count test_transpose_sum d_cpu_only fn b c = b c sum metrics reset x = torch randn y = torch randn transpose common fn x y check_metrics_vec_kernel_count config patch cpp enable_tiling_heuristics False test_transpose_sum_outer https github com pytorch pytorch issues fn transpose sum dim= contiguous metrics reset x = torch randn common fn x check_metrics_vec_kernel_count test_to_dtype_bool_float https github com pytorch pytorch issues f torch where torch ones_like torch bool torch zeros_like torch ones_like common f torch ones test_to_dtype_float_bool https github com pytorch pytorch issues f = torch tensor = dtype=torch float x = torch rand common f x test_constant_store https github com pytorch pytorch issues f = -float inf x = torch rand common f x test_broadcast_scalar_cpp_tile_ d_kernel Based detectron _maskrcnn backbone conv d - max_pool d s = s = data = torch randn s s weight_one = torch randn requires_grad=True weight_two = torch randn requires_grad=True bias_one = torch randn requires_grad=True bias_two = torch randn requires_grad=True torch compile fn data weight_one weight_two bias_one bias_two conv_result_one = torch ops aten convolution default data weight_one bias_one False conv_result_two = torch ops aten convolution default data weight_two bias_two False max_pool_result = torch nn functional max_pool d conv_result_one False conv_result_one conv_result_two max_pool_result torch _dynamo mark_dynamic data torch _dynamo mark_dynamic data common fn data weight_one weight_two bias_one bias_two test_to_channels_last_lowp_fp f memory_format=torch channels_last dtype _lowp_fp_dtypes x = torch rand dtype common f x test_broadcast_mul_lowp_fp f b b dtype _lowp_fp_dtypes = torch randn dtype b = torch randn dtype common f b test_linear_buffer_reuse M torch nn Module __init__ - None super __init__ linear = torch nn Linear tanh = torch nn Tanh linear = torch nn Linear forward x x = linear x x = tanh x x = linear x x mod = M eval v = torch randn torch no_grad compile_fx_wrapper model_ example_inputs_ compile_fx model_ example_inputs_ run ex kwargs mod ex kwargs run = torch compile run backend=compile_fx_wrapper _ code = run_and_get_cpp_code run v assertFalse = as_strided code assertEqual run v mod v test_invalid_dropout_args MyModel torch nn Module forward x x = x x = torch nn functional dropout x p= x = torch relu x x example_inputs = torch tensor func = MyModel jit_func = torch compile func assertRaises RuntimeError lambda func example_inputs assertRaises RuntimeError lambda jit_func example_inputs test_nn_param_assign https github com pytorch pytorch issues Model nn Module __init__ - None super __init__ conv = nn Conv d in_channels= out_channels= kernel_size= batchnorm = nn BatchNorm d num_features= conv_weight = torch randn conv_bias = torch randn forward x conv weight = nn Parameter conv_weight conv bias = nn Parameter conv_bias requires_grad=False conv eval x = conv x x = batchnorm x x = F relu x x input_tensor = torch randn func = Model cpu torch no_grad func train False v = func input_tensor jit_func = torch compile func fullgraph=True v = jit_func input_tensor assertEqual v v test_nn_param_assign_wrapped Model nn Module __init__ - None super __init__ conv = nn Conv d in_channels= out_channels= kernel_size= batchnorm = nn BatchNorm d num_features= conv_weight = torch randn conv_bias = torch randn forward x conv weight = nn Parameter conv_weight conv bias = nn Parameter conv_bias requires_grad=False conv eval x = conv x x = batchnorm x x = F relu x x input_tensor = torch randn func = Model cpu functools wraps func wrapper args kwargs func args kwargs torch no_grad func train False v = func input_tensor jit_func = torch compile wrapper fullgraph=True v = jit_func input_tensor assertEqual v v config patch inplace_buffers=True test_in_out_buffer fn x y z = torch matmul x y transpose - - z inps = torch randn torch randn fn_opt = torch compile fn backend= inductor _ code = run_and_get_cpp_code fn_opt inps assertTrue in_out_ptr code assertEqual fn_opt inps fn inps test_eliminate_meaningless_copy fn x x permute = torch ops aten permute default x clone = torch ops aten clone default permute memory_format=torch contiguous_format view = torch ops aten view default clone - bmm = torch ops aten bmm default view x permute = torch ops aten permute default view bmm permute metrics reset common fn rand_strided device= cpu dtype=torch float rand_strided device= cpu dtype=torch float assertEqual metrics generated_kernel_count test_relu_permute_reshape_reinterpret_view fn x n c h w = x shape torch relu x permute reshape n h w c x = torch randn memory_format=torch channels_last torch _dynamo reset metrics reset torch no_grad expected = fn x compiled_fn = torch compile fn actual code = run_and_get_cpp_code compiled_fn x assertEqual expected actual generated kernel assertEqual metrics generated_kernel_count check there no transpose FileCheck check_count transpose_mxn exactly=True run code test_attention_size_mismatch Attention torch nn Module __init__ hidden_size num_heads super __init__ hidden_size = hidden_size num_heads = num_heads head_size = hidden_size num_heads query = torch nn Linear hidden_size hidden_size key = torch nn Linear hidden_size hidden_size value = torch nn Linear hidden_size hidden_size inv_scale = torch nn Parameter torch Tensor head_size requires_grad=False forward x query = query x key = key x value = value x batch_size seq_len hidden_size = query size query = query view batch_size seq_len num_heads head_size permute key = key view batch_size seq_len num_heads head_size permute value = value view batch_size seq_len num_heads head_size permute attention_weights = torch matmul query key mul inv_scale softmax dim=- output = torch matmul attention_weights value output torch manual_seed hidden_size = num_heads = seq_len = batch_size = x = torch randn batch_size seq_len hidden_size func = Attention hidden_size num_heads cpu torch no_grad res = func x jit_func = torch compile func res = jit_func x assertEqual res res test_scalar_mul_bfloat f x torch ops aten mul Tensor x metrics reset x = torch randn dtype=torch bfloat common f x check_metrics_vec_kernel_count test_bf _zeros fn x = torch zeros dtype=torch bfloat x common fn test_select_tiliing_with_index_expr fn x y x = torch ops aten view default x x = torch ops aten permute default x y = torch ops aten mul Tensor y x torch ops aten constant_pad_nd default y x = torch randn y = torch randn common fn x y unittest skipIf torch backends mkldnn is_available MKLDNN enabled patch torch cuda is_available lambda False config patch freezing=True test_linear_with_no_default_contiguous_input dtypes = torch float torch ops mkldnn _is_mkldnn_bf _supported dtypes append torch bfloat torch ops mkldnn _is_mkldnn_fp _supported dtypes append torch float mod = torch nn Sequential torch nn Linear eval temp = torch randn v = torch as_strided temp assertTrue v is_contiguous dtype dtypes torch no_grad common mod dtype v dtype patch torch cuda is_available lambda False config patch freezing=True test_linear_with_reshape M torch nn Module __init__ - None super __init__ linear = torch nn Linear bias=False forward x x = linear x x view mod = M eval v = torch randn torch no_grad torch _dynamo reset metrics reset common mod v assert metrics generated_kernel_count == config patch implicit_fallbacks=True test_aten_normal_dtype dtype torch float torch float None fn torch normal dtype=dtype device= cpu assertEqual torch compile fn backend= aot_eager_decomp_partition dtype dtype dtype torch float assertEqual torch compile fn backend= inductor dtype dtype dtype torch float test_group_norm_vec M torch nn Module __init__ - None super __init__ group_norm = torch nn GroupNorm forward x group_norm x options = itertools product vec_dtypes torch contiguous_format torch channels_last True False dtype fmt dynamic options torch _dynamo reset metrics reset mod = M eval x = torch randn dtype=dtype memory_format=fmt torch no_grad expected = mod x compiled_m = torch compile mod dynamic=dynamic actual code = run_and_get_cpp_code compiled_m x assertEqual expected actual generated kernels first one var_mean last two result check_metrics_vec_kernel_count check loop split optimization fmt == torch channels_last check there no non_contiguous loads FileCheck check_count __at_align__ std array exactly=True run code unittest skipIf os getenv ATEN_CPU_CAPABILITY == default Failing periodic nogpu_NO_AVX see example test_group_norm_large_input M torch nn Module __init__ - None super __init__ group_norm = torch nn GroupNorm forward x group_norm x fmt torch contiguous_format torch channels_last torch _dynamo reset metrics reset mod = M eval x = torch randn memory_format=fmt torch no_grad expected = mod x compiled_m = torch compile mod actual = compiled_m x assertEqual expected actual generated kernels first one var_mean last two result check_metrics_vec_kernel_count check there no outer loop fusion assertEqual len metrics cpp_outer_loop_fused_inner_counts check parallel reduction assertEqual metrics parallel_reduction_count unittest skipIf os getenv ATEN_CPU_CAPABILITY == default Failing periodic nogpu_NO_AVX see example test_group_norm_large_size https github com pytorch pytorch issues We using chunk size cascade summation reduction size test case exceeded M torch nn Module __init__ super __init__ gn = torch nn GroupNorm forward x gn x dynamic True False torch _dynamo reset metrics reset mod = M eval x = torch randn torch no_grad expected = mod x compiled_m = torch compile mod dynamic=dynamic actual = compiled_m x assertEqual expected actual torch _dynamo config patch capture_scalar_outputs=True capture_dynamic_output_shape_ops=True config patch emulate_precision_casts=True test_group_norm_backward_symint_divisible_channels fn x weight bias y = torch nn functional group_norm x weight=weight bias=bias torch sigmoid y max dim= values torch _dynamo reset metrics reset shape = x_ref = torch rand shape dtype=torch float requires_grad=True weight_ref = torch rand dtype=torch float requires_grad=True bias_ref = torch rand dtype=torch float requires_grad=True x_cmp = x_ref clone detach requires_grad_ True weight_cmp = weight_ref clone detach requires_grad_ True bias_cmp = bias_ref clone detach requires_grad_ True eager_out = fn x_ref weight_ref bias_ref eager_out sum backward compiled = torch compile fn backend= inductor fullgraph=True dynamic=True compiled_out = compiled x_cmp weight_cmp bias_cmp compiled_out sum backward torch testing assert_close compiled_out eager_out torch testing assert_close x_cmp grad x_ref grad torch testing assert_close weight_cmp grad weight_ref grad torch testing assert_close bias_cmp grad bias_ref grad test_int_div_vec fn x y mode torch div x y rounding_mode=mode dtype torch int torch uint torch int torch int x = torch randint dtype=dtype y = torch randint dtype=dtype mode None trunc floor torch no_grad metrics reset common fn x y mode check_metrics_vec_kernel_count test_uint _add https github com pytorch pytorch issues fn x y torch add x y neg torch int x = torch randint dtype=torch uint y = torch randint dtype=torch uint common fn x y test_uint _sub https github com pytorch pytorch issues fn x y torch sub x y neg torch int x = torch randint dtype=torch uint y = torch randint dtype=torch uint common fn x y test_float _to_uint https github com pytorch pytorch issues torch compile fn x x torch uint x = torch tensor - - - - dtype=torch float device= cpu assertEqual x torch uint fn x msg=f Expected x torch uint got fn x test_non_contiguous_reduction_store https github com pytorch pytorch issues M torch nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= forward x conv x max values m = M x = torch randn common m x test_embedding_vec M torch nn Module __init__ - None super __init__ emb = torch nn Embedding forward idx x emb idx + x idx = torch randint x = torch randn m = M eval torch no_grad metrics reset common m idx x check_metrics_vec_kernel_count requires_vectorization test_embedding_vec_bf M torch nn Module __init__ - None super __init__ emb = torch nn Embedding forward idx x emb idx idx = torch randint x = torch randn torch bfloat m = M eval torch no_grad metrics reset common m idx x check_metrics_vec_kernel_count we doing direct load store make sure we do generate redundant type casts m_opt = torch compile m _ code = run_and_get_cpp_code m_opt idx x assertTrue Vectorized code assertTrue cvt_lowp_fp_to_fp code assertTrue cvt_fp _to_lowp_fp code xfailIfS X test_concat_inner_vec fn x y F relu torch cat x y dim= x = torch randn y = torch randn metrics reset common fn x y check_metrics_vec_kernel_count config patch cpp enable_tiling_heuristics False test_expr_vec_non_contiguous fn x pattern sebotnet ts_ y = torch nn functional pad x reshape - y = y reshape expand - - - - y = y permute clone memory_format=torch contiguous_format y = y view y softmax dim=- x = torch randn opt_fn = torch compile fn metrics reset _ code = run_and_get_cpp_code opt_fn x assertTrue same fn x opt_fn x kernels max exp sum div check_metrics_vec_kernel_count FileCheck check_count Vectorized int loadu tmpbuf data exactly=True run code test_vec_contiguous_ModularIndexing https github com pytorch pytorch issues M torch nn Module __init__ dim super __init__ norm = torch nn LayerNorm dim forward x pattern swin_base_patch _window _ B H W C = x shape x = x reshape B H W C permute flatten x = norm x x x = torch randn m = M opt_m = torch compile m torch no_grad metrics reset _ code = run_and_get_cpp_code opt_m x assertTrue same m x opt_m x Two kernels one reduction one pointwises check_metrics_vec_kernel_count FileCheck check_count Vectorized float loadu tmpbuf data exactly=True run code parametrize dtype torch float torch bfloat torch float parametrize shape test_fp _cast dtype torch dtype shape str fp _cast x y = x dtype=torch float _e m fn dtype y = x dtype=torch float _e m dtype y y shape = int dim dim shape split x = torch rand shape device= cpu dtype=dtype common fp _cast x test_logical_op_store_to_lowp_data_dtype https github com pytorch pytorch issues https github com pytorch pytorch issues fn out out input other o = torch logical_or out=out input=input other=other o = torch logical_xor out=out input=input other=other o o x = torch rand dtype=torch float y = torch rand dtype=torch float dtype _lowp_fp_dtypes o = torch rand dtype=dtype o = torch rand dtype=dtype torch no_grad common fn o o x y test_constant_bool_vec fn x mask = torch zeros dtype=torch bool torch where mask x - x = torch rand metrics reset common fn x check_metrics_vec_kernel_count torch _dynamo config patch dynamic_shapes=True torch _dynamo config patch assume_static_by_default=False test_symbolic_shape_scalar_value_reduction fn x y y + torch ones x sum torch no_grad metrics reset y = torch randn common fn y check_metrics_vec_kernel_count test_int _pointwise_vec fn x x x x = torch randint dtype=torch int metrics reset common fn x check_metrics_vec_kernel_count test_int _reduction_vec fn x x sum dim= x = torch randint dtype=torch int metrics reset common fn x check_metrics_vec_kernel_count test_uint _pointwise_vec fn x x x x = torch randint dtype=torch uint metrics reset common fn x TODO jgong change vectorized uint load assert metrics generated_cpp_vec_kernel_count == test_uint _reduction_vec fn x x sum dim= x = torch randint dtype=torch uint metrics reset common fn x TODO jgong change vectorized uint uint load assert metrics generated_cpp_vec_kernel_count == test_int _pointwise_vec fn x x x x = torch randint dtype=torch int metrics reset common fn x check_metrics_vec_kernel_count test_int _reduction_vec fn x x sum dim= x = torch randint dtype=torch int metrics reset common fn x check_metrics_vec_kernel_count test_uint _pointwise_vec fn x x x x = torch randint dtype=torch uint metrics reset common fn x TODO jgong change vectorized uint load assert metrics generated_cpp_vec_kernel_count == test_uint _reduction_vec fn x x sum dim= x = torch randint dtype=torch uint metrics reset common fn x TODO jgong change vectorized uint load assert metrics generated_cpp_vec_kernel_count == test_convert_int _to_half_vec src_dtypes = torch int torch uint dst_dtypes = torch bfloat torch half _simd_lens = isa _bit_width isa cpu_vec_isa valid_vec_isa_list src_dtype dst_dtype _simd_len itertools product src_dtypes dst_dtypes _simd_lens fn x x dst_dtype low = src_dtype == torch uint - x = torch randint low dtype=src_dtype config patch cpp simdlen _simd_len torch _dynamo reset metrics reset common fn x check_metrics_vec_kernel_count test_convert_int _to_int _vec fn x x torch int x = torch randint dtype=torch int metrics reset common fn x check_metrics_vec_kernel_count test_convert_int _to_int _vec fn x x torch int x = torch randint dtype=torch int metrics reset common fn x check_metrics_vec_kernel_count test_convert_fp _to_int _vec fn x x torch int x = torch rand metrics reset common fn x check_metrics_vec_kernel_count test_convert_int _to_fp _vec fn x x torch float x = torch randint dtype=torch int metrics reset common fn x check_metrics_vec_kernel_count test_double_pointwise_vec fn x x x x = torch randn dtype=torch double metrics reset common fn x check_metrics_vec_kernel_count Tail vectorization case x = torch randn dtype=torch double torch _dynamo reset metrics reset torch no_grad expected = fn x compiled_fn = torch compile fn actual code = run_and_get_cpp_code compiled_fn x assertEqual expected actual generated vec kernel check_metrics_vec_kernel_count Check both main tail loops vectorized _can_check_vec_metrics FileCheck check_count vec VectorizedN double loadu exactly=True run code test_double_reduction_vec fn x x sum dim= x = torch randn dtype=torch double metrics reset common fn x check_metrics_vec_kernel_count Tail vectorization case x = torch randn dtype=torch double torch _dynamo reset metrics reset torch no_grad expected = fn x compiled_fn = torch compile fn actual code = run_and_get_cpp_code compiled_fn x assertEqual expected actual generated vec kernel check_metrics_vec_kernel_count Check both main tail loops vectorized _can_check_vec_metrics FileCheck check_count vec VectorizedN double loadu exactly=True run code test_convert_fp _to_double_vec fn x x torch double x = torch randn metrics reset common fn x check_metrics_vec_kernel_count Tail vectorization case x = torch randn torch _dynamo reset metrics reset torch no_grad expected = fn x compiled_fn = torch compile fn actual code = run_and_get_cpp_code compiled_fn x assertEqual expected actual generated vec kernel check_metrics_vec_kernel_count Check both main tail loops vectorized _can_check_vec_metrics FileCheck check_count vec convert double float exactly=True run code test_convert_double_to_fp _vec fn x x torch float x = torch randn dtype=torch double metrics reset common fn x check_metrics_vec_kernel_count Tail vectorization case x = torch randn dtype=torch double torch _dynamo reset metrics reset torch no_grad expected = fn x compiled_fn = torch compile fn actual code = run_and_get_cpp_code compiled_fn x assertEqual expected actual generated vec kernel check_metrics_vec_kernel_count Check both main tail loops vectorized _can_check_vec_metrics FileCheck check_count vec convert float double exactly=True run code test_no_redundant_to_dtypes_between_fused_scheduler_node https github com pytorch pytorch issues p = torch tensor dtype=torch float Model torch nn Module __init__ - None super __init__ forward args cat = torch cat args args args args dim= max_ = torch max args p mul = torch mul cat max_ tan = torch tan mul mul tan metrics reset m = Model common m torch randn half torch randn half torch randn half torch randn half torch tensor dtype=torch float test_masked_load_int _vec https github com pytorch pytorch issues fn x torch nn functional pad x x = torch randint dtype=torch int metrics reset common fn x check_metrics_vec_kernel_count test_highp_to_lowp_cse_var_cache_with_store Fix issue https github com pytorch pytorch issues input = torch randn dtype=torch float input = torch randint dtype=torch int input = torch randn dtype=torch float Model torch nn Module __init__ - None super __init__ forward x x x x = x torch int temp = test_operators realize x torch float temp = temp torch float temp = temp x torch mm temp x torch float temp metrics reset m = Model common m input input input test_reduction_float_to_int https github com pytorch pytorch issues fn x x max values x = torch randint dtype=torch int metrics reset common fn x check_metrics_vec_kernel_count config patch cpp dynamic_threads True test_reduction_with_dynamic_threads fn b sum b sum common fn torch randn torch rand patch torch cuda is_available lambda False config patch freezing=True test_linear_float M torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch randn dtype=torch float weight = torch nn Parameter torch randn dtype=torch float bias = torch nn Parameter torch randn dtype=torch float forward x v = torch mm x weight v = torch addmm bias x weight v v mod = M eval v = torch randn dtype=torch float torch no_grad common mod v test_fused_attention_conv https github com pytorch pytorch issues Model torch nn Module __init__ - None super __init__ q_conv = torch nn Conv d k_conv = torch nn Conv d v_conv = torch nn Conv d forward x q = q_conv x k = k_conv x v = v_conv x q = q permute k = k permute v = v permute torch nn functional scaled_dot_product_attention q k v dropout_p= is_causal=False fn = Model x = torch randn common fn x parametrize is_inference True False test_disabled_amp is_inference M torch nn Module __init__ super __init__ all_head_size = dense = nn Linear all_head_size all_head_size forward q k v context_layer = F scaled_dot_product_attention q k v attn_mask=None dropout_p= context_layer = context_layer permute contiguous new_context_layer_shape = context_layer size - + all_head_size context_layer = context_layer view new_context_layer_shape dense context_layer mod = M torch bfloat eval q = torch randn dtype=torch bfloat k = torch randn dtype=torch bfloat v = torch randn dtype=torch bfloat inputs = q k v compiler_mode = torch compile mod torch nn attention sdpa_kernel SDPBackend context = contextlib nullcontext is_inference torch no_grad config patch fallback_random True torch cpu amp autocast context sdpa_kernel SDPBackend MATH torch manual_seed eager = mod inputs torch manual_seed assertEqual compiler_mode inputs eager test_fused_node https github com pytorch pytorch issues M torch nn Module __init__ super __init__ forward clone_ gt_scalar div_tensor convert_element_type_default_ convert_element_type_default_ convert_element_type_default_ convert_element_type_default_ = torch ops prims convert_element_type default clone_ torch float clone_ = None view_default_ = torch ops aten view default convert_element_type_default_ convert_element_type_default_ = None convert_element_type_default_ = torch ops prims convert_element_type default view_default_ torch bfloat view_default_ = None mul_tensor = torch ops aten mul Tensor gt_scalar div_tensor mul_tensor_ = torch ops aten mul Tensor mul_tensor mul_tensor = None expand_default_ = torch ops aten expand default mul_tensor_ mul_tensor_ = None view_default_ = torch ops aten view default expand_default_ expand_default_ = None permute_default_ = torch ops aten permute default view_default_ view_default_ = None convert_element_type_default_ = torch ops prims convert_element_type default permute_default_ torch bfloat permute_default_ = None bmm_default_ = torch ops aten bmm default convert_element_type_default_ convert_element_type_default_ convert_element_type_default_ = None convert_element_type_default_ = torch ops prims convert_element_type default bmm_default_ torch float bmm_default_ = None view_default_ = torch ops aten view default convert_element_type_default_ convert_element_type_default_ = None convert_element_type_default_ = torch ops prims convert_element_type default view_default_ torch bfloat view_default_ = None permute_default_ = torch ops aten permute default convert_element_type_default_ convert_element_type_default_ = None bmm_default_ = torch ops aten bmm default convert_element_type_default_ convert_element_type_default_ convert_element_type_default_ = convert_element_type_default_ = None convert_element_type_default_ = torch ops prims convert_element_type default bmm_default_ torch float bmm_default_ = None view_default_ = torch ops aten view default convert_element_type_default_ convert_element_type_default_ = None convert_element_type_default_ = torch ops prims convert_element_type default gt_scalar torch float gt_scalar = None mul_tensor_ = torch ops aten mul Tensor convert_element_type_default_ convert_element_type_default_ = None mul_tensor_ = torch ops aten mul Tensor view_default_ mul_tensor_ view_default_ = mul_tensor_ = None mul_tensor_ = torch ops aten mul Tensor mul_tensor_ div_tensor mul_tensor_ = None sum_dim_int_list_ = torch ops aten sum dim_IntList mul_tensor_ - True neg_default = torch ops aten neg default div_tensor div_tensor = None fma_default = torch ops prims fma default neg_default sum_dim_int_list_ mul_tensor_ neg_default = sum_dim_int_list_ = mul_tensor_ = None view_default_ = torch ops aten view default fma_default fma_default = None convert_element_type_default_ = torch ops prims convert_element_type default view_default_ torch bfloat view_default_ = None bmm_default_ = torch ops aten bmm default convert_element_type_default_ convert_element_type_default_ convert_element_type_default_ = None convert_element_type_default_ = torch ops prims convert_element_type default bmm_default_ torch float bmm_default_ = None view_default_ = torch ops aten view default convert_element_type_default_ convert_element_type_default_ = None mul_scalar_ = torch ops aten mul Scalar view_default_ view_default_ = None permute_default_ = torch ops aten permute default mul_scalar_ mul_scalar_ = None convert_element_type_default_ = torch ops prims convert_element_type default permute_default_ torch bfloat permute_default_ = None _permute_default_ = torch ops aten permute default convert_element_type_default_ convert_element_type_default_ = None bmm_default_ = torch ops aten bmm default convert_element_type_default_ convert_element_type_default_ convert_element_type_default_ = convert_element_type_default_ = None convert_element_type_default_ = torch ops prims convert_element_type default bmm_default_ torch float bmm_default_ = None view_default_ = torch ops aten view default convert_element_type_default_ convert_element_type_default_ = None mul_scalar_ = torch ops aten mul Scalar view_default_ view_default_ = None convert_element_type_default_ = torch ops prims convert_element_type default mul_scalar_ torch bfloat mul_scalar_ = None permute_default_ = torch ops aten permute default convert_element_type_default_ convert_element_type_default_ = None clone_ = torch ops aten clone default permute_default_ memory_format=torch contiguous_format permute_default_ = None view_ = torch ops aten view default clone_ clone_ = None clone_ = torch ops aten clone default permute_default_ memory_format=torch contiguous_format permute_default_ = None view_ = torch ops aten view default clone_ clone_ = None view_ = torch ops aten view default view_ view_ = None view_ view_ clone_ = torch randn dtype=torch bfloat gt_scalar = torch randint dtype=torch bool div_tensor = torch randn dtype=torch float convert_element_type_default_ = torch randn dtype=torch bfloat convert_element_type_default_ = torch randn dtype=torch bfloat convert_element_type_default_ = torch randn dtype=torch bfloat inputs = clone_ gt_scalar div_tensor convert_element_type_default_ convert_element_type_default_ convert_element_type_default_ torch cpu amp autocast mod = M torch bfloat eval common mod inputs atol= e- rtol= e- requires_vectorization test_vec_indirect_load_cse_cache https github com pytorch pytorch issues math inf fn arg _ full_default = torch ops aten full default select = torch ops aten select int arg _ select_ = torch ops aten select int arg _ view = torch ops aten reshape default select_ - expand = torch ops aten expand default view full_default_ = torch ops aten full default scatter_add = torch ops aten scatter_add default full_default_ expand full_default pow_ = torch ops aten pow Tensor_Scalar scatter_add - eq = torch ops aten eq Scalar pow_ inf full_default_ = torch ops aten full default where = torch ops aten where eq full_default_ pow_ index = torch ops aten index Tensor where select index_ = torch ops aten index Tensor where select_ mul_ = torch ops aten mul Tensor index index_ mul_ x = torch zeros torch int opt_fn = torch compile fn backend= inductor _ code = run_and_get_cpp_code opt_fn x FileCheck check_count vec VectorizedN int _t loadu tmpbuf data exactly=True run code test_load_half fn arg _ arg _ arg _ copy_ arg _ config patch cpp simdlen x = torch randn torch half x = torch randn torch half opt_fn = torch compile fn backend= inductor _ code = run_and_get_cpp_code opt_fn x x FileCheck check_count static_cast float exactly=True run code requires_vectorization test_repeated_exp fn x y = x sigmoid y + y sum - x = torch randn opt_fn = torch compile fn _ code = run_and_get_cpp_code opt_fn x FileCheck check_count exp exactly=True run code test_convert_fp _int _oob_vec https github com pytorch pytorch issues fn x float = x torch float float torch int x = torch full - dtype=torch int simdlen simd_lengths_to_test config patch cpp simdlen simdlen torch _dynamo reset metrics reset common fn x check_metrics_vec_kernel_count requires_vectorization test_consistent_remove_buffers fn x z = x + x z = test_operators realize z x + z The shape makes sure we generate both vec scalar kernels x = torch randn dtype=torch bfloat config patch inplace_buffers=False metrics reset common fn x check_metrics_vec_kernel_count _ code = run_and_get_cpp_code torch compile fn x FileCheck check_count tmp + tmp exactly=True run code requires_vectorization test_bool_reduction_vec op torch any torch min torch max fn x x x op x op x op x c = False input = torch Tensor c torch bool c = True input = torch Tensor c torch bool input = torch Tensor True torch bool metrics reset common fn input input input n_veckernel = op torch masked mean check_metrics_vec_kernel_count n_veckernel requires_vectorization test_full_bits_lowp check_use_full_bits func shapes dtype mixed check_vecn example_inputs = torch randn shape dtype=dtype shape shapes mixed example_inputs = example_inputs dtype=torch half dtype == torch bfloat torch bfloat f_opt = torch compile func _ code = run_and_get_cpp_code f_opt example_inputs check_vecn assertTrue vec VectorizedN code vec convert float code assertFalse vec VectorizedN code vec convert float code funcs = func arg arg torch ops aten sum torch ops aten add torch ops aten atanh arg arg funcs append func func arg v = torch ops prims convert_element_type default arg torch float v = torch ops aten add torch ops aten atanh arg v torch ops prims convert_element_type default v arg dtype funcs append func func arg arg v = torch ops aten atanh arg v = torch ops aten add v arg torch ops prims convert_element_type default v arg dtype funcs append func test small shapes funcs append func small_size = cpu_vec_isa pick_vec_isa nelements dtype=torch bfloat example_shapes = test small shapes small_size small_size mixed_types = False False True False check_vecns = True True True False dtype torch bfloat torch float func shapes mixed check_vecn zip funcs example_shapes mixed_types check_vecns check_use_full_bits func shapes dtype mixed check_vecn config patch cpp simdlen requires_vectorization test_avx _bool_constant_pad_nd NOTE I tried using removing cpp simdlen= override didn t repro issue result = torch testing make_tensor dtype=torch bool device=torch device cpu fn arg torch constant_pad_nd arg common fn result config patch unroll_reductions_threshold= requires_vectorization test_unrolled_bool_prod_vectorized result = torch zeros dtype=torch bool dim_select = result narrow dim_select narrow dim_select zero_ result narrow dim_select narrow dim_select zero_ result narrow dim_select narrow dim_select zero_ fn arg torch prod arg dtype=torch bool common fn result requires_vectorization config patch cpp min_chunk_size test_for_loop_collapsed https github com pytorch pytorch issues fn x x transpose contiguous x = torch randn opt_fn = torch compile fn backend= inductor _ code = run_and_get_cpp_code opt_fn x assertTrue same fn x opt_fn x FileCheck check_count #pragma omp collapse exactly=True run code config patch freezing=True test_add_layernorm Model torch nn Module __init__ super __init__ dense = torch nn Linear layernorm = torch nn LayerNorm eps= e- forward context_layer hidden_states attention_output = dense context_layer hidden_states = attention_output + hidden_states layer_output = layernorm hidden_states layer_output model = Model example_batch = torch rand torch rand torch testing _internal common_quantization _generate_qdq_quantized_model torch no_grad converted_model = _generate_qdq_quantized_model model example_batch torch ao quantization move_exported_model_to_eval converted_model metrics reset torch compile converted_model example_batch check_metrics_vec_kernel_count test_dropout Model nn Module __init__ dim super __init__ dropout = eval f nn Dropout dim d p= forward x torch manual_seed x = dropout x x dim model = Model dim torch manual_seed shape = + dim x = torch randn shape output = model x c_model = torch compile model c_output = c_model x assertTrue torch allclose output c_output requires_vectorization test_bool_max torch manual_seed x = torch randn size= ge fn x torch max x False common fn x test_vector_norm_compile x = torch randn dtype=torch float ref = torch linalg vector_norm x ord= dim= keepdim=False dtype=None compiled_vector_norm = torch compile torch linalg vector_norm backend= inductor res = compiled_vector_norm x ord= dim= keepdim=False dtype=None assertEqual ref res test_fractional_max_pool d_ d_input Test https github com pytorch pytorch issues - D input causing assertion error Test various D input shapes ensure compilation crash fixed test_shapes = Original failing case Different channel count Non-square input Larger input shape test_shapes subTest shape=shape torch manual_seed x = torch randn shape Generate explicit samples ensure deterministic correct results n_batch = x dim == x size torch manual_seed samples = torch rand n_batch x size - dtype=x dtype device=x device fn x samples F fractional_max_pool d x kernel_size= output_size= _random_samples=samples Test eager mode works expected = fn x samples Test compiled mode works failing AssertionError before fix compiled_fn = torch compile fn backend= inductor result = compiled_fn x samples Verify correctness explicit samples should match exactly torch testing assert_close result expected rtol= e- atol= e- test_outer_looop_fusion_with_local_buf fn xs torch Tensor Ls torch Tensor arr = -torch einsum i i- i xs Ls temp = torch exp arr Q = torch einsum i - i temp ans = torch einsum i i - i Q temp ans xs = torch ones requires_grad=False Ls = torch ones requires_grad=False expected = fn xs Ls compiled_func = torch compile fn backend= inductor result = compiled_func xs Ls torch testing assert_close result expected __name__ == __main__ torch _inductor test_case run_tests torch testing _internal inductor_utils HAS_CPU HAS_CPU IS_MACOS run_tests needs= filelock