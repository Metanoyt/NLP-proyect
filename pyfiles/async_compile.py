mypy allow-untyped-defs __future__ annotations atexit contextlib functools json logging multiprocessing os re sys concurrent futures Future ThreadPoolExecutor concurrent futures process BrokenProcessPool functools partial time time time_ns typing Any Callable Optional TYPE_CHECKING torch torch _dynamo device_interface get_registered_device_interfaces torch _dynamo utils counters dynamo_timed get_metrics_context set_feature_use torch _inductor config torch _inductor codecache _load_triton_kernel_from_source code_hash CodeCacheFuture CppCodeCache CppPythonBindingsCodeCache CUDACodeCache HalideCodeCache LambdaFuture ROCmCodeCache StaticAutotunerFuture torch_key torch _inductor compile_worker subproc_pool AnyPool SubprocException SubprocPool torch _inductor compile_worker tracked_process_pool TrackedProcessPoolExecutor torch _inductor compile_worker utils _async_compile_initializer torch _inductor runtime compile_tasks _set_triton_ptxas_path _worker_compile_triton torch _inductor utils clear_on_fresh_cache torch _inductor virtualized V torch _utils_internal log_triton_builds torch hub _Faketqdm tqdm torch utils _ordered_set OrderedSet torch utils _triton has_triton_package TYPE_CHECKING torch _inductor runtime hints HalideMeta torch _inductor runtime triton_heuristics CachingAutotuner timing metrics time spent compilation _cumulative_compile_time = _t Optional float = None kernel_code_log = torch _logging getArtifactLogger __name__ kernel_code log = logging getLogger __name__ _triton_kernel_metrics Optional dict str dict str Any = None size_hints_regex = re compile r size_hints= \ \ pre_fork_setup Setup must done prior forking process pool ensure properties have been calculated before processes forked caching_device_properties Computing triton key can slow If we call before fork will cached forked subprocesses torch _inductor runtime triton_compat HAS_TRITON triton_key HAS_TRITON triton_key caching_device_properties _ device_interface get_registered_device_interfaces device_interface is_available device_interface Worker get_device_properties _compile_start - None global _t _triton_kernel_metrics _t None _t = time _triton_kernel_metrics None _triton_kernel_metrics = _compile_end - None global _cumulative_compile_time _t _triton_kernel_metrics _t None t = time _cumulative_compile_time += t - _t _t = None print CUMULATIVE COMPILE TIME _cumulative_compile_time _triton_kernel_metrics Log triton kernel info sorted_info = dict sorted _triton_kernel_metrics items torch _logging trace_structured artifact metadata_fn=lambda name triton_kernel_info encoding json payload_fn=lambda json dumps sorted_info _triton_kernel_metrics = None _add_triton_kernel_info kernel_name str info dict str Any global _triton_kernel_metrics Must called between _compile_start _compile_end _triton_kernel_metrics None _triton_kernel_metrics kernel_name = info _IS_WINDOWS = sys platform == win log = logging getLogger __name__ Used keep track all process pools invoked so far _pool_set = OrderedSet AnyPool shutdown_compile_workers - None Shut down all outstanding compile-worker pools pool _pool_set pool shutdown AsyncCompile _ready_future = None after_fork after_fork Reset pools initial state without shutting them down _pool_set clear AsyncCompile process_pool cache_clear try os register_at_fork after_in_child=after_fork except AttributeError pass register_at_fork does exists windows get_compile_threads - int Temporary internal rollout Assign config compile_threads lazily TODO remove after rollout config compile_threads None config compile_threads = config decide_compile_threads config compile_threads clear_on_fresh_cache CompiledTritonKernels In memory cache storing compiled triton kernels Each triton kernel keyed hash its source code Each value stored cache value AsyncCompile triton Currently cache stores Future objects should generalizable any kernels _cache dict str CodeCacheFuture = staticmethod key kernel_src str Generates cache key given triton kernel s full source code This source includes inductor meta compilation metadata kernel itself etc ` kernel_src ` should exact string passed async_compile triton s first argument Hashes kernel source torch_key into single hash key code_hash kernel_src extra=torch_key staticmethod save kernel_src str future CodeCacheFuture Saves compiled triton kernel cache TODO We store LambdaFuture s callable returned async_compile triton real type we want here actually abstract triton kernel TODO Source code here just kernel s source code also includes inductor preamble etc so could less strict key = CompiledTritonKernels key kernel_src CompiledTritonKernels _cache key = future staticmethod get kernel_src str - Optional CodeCacheFuture key = CompiledTritonKernels key kernel_src CompiledTritonKernels _cache get key None staticmethod cache_clear CompiledTritonKernels _cache = staticmethod remove_future kernel_src str - None key = CompiledTritonKernels key kernel_src Delete LambdaFuture there one key CompiledTritonKernels _cache del CompiledTritonKernels _cache key contextlib contextmanager async_compile_pool_manager Context manager quiesce subproc pool end compilation i e when dynamo done try yield finally AsyncCompile quiesce AsyncCompile Utilities compile thread pools subprocess pools case Triton _ready_future Optional Future Any = None __init__ - None pass staticmethod functools lru_cache pool - ThreadPoolExecutor assert get_compile_threads ThreadPoolExecutor get_compile_threads staticmethod _get_ready No-op function help mark when subprocess pool ready ready staticmethod functools lru_cache process_pool - AnyPool assert get_compile_threads AsyncCompile _ready_future = None log info Creating s pool d workers config worker_start_method get_compile_threads pool AnyPool config worker_start_method == subprocess Wrapper around ProcessPoolExecutor forks new process we control pool = SubprocPool get_compile_threads config worker_start_method == spawn Avoid creating pools spawned subprocs themselves os environ TORCH_WARM_POOL = pre_fork_setup ctx = multiprocessing get_context config worker_start_method pool = TrackedProcessPoolExecutor get_compile_threads mp_context=ctx initializer=partial _async_compile_initializer os getpid when pool created subprocess object normal exit handler doesn t run we need register our own handler exitpriority has high because another one finalizers will kill worker thread sends shutdown message workers multiprocessing util Finalize None pool shutdown exitpriority=sys maxsize _pool_set add pool pool classmethod warm_pool cls - None get_compile_threads = _compile_start Pool created first access Note SubprocPool sidecar process starts its ProcessPoolExecutor does initialize until wakeup call first job submitted cls process_pool _compile_end classmethod wait_pool_ready cls timeout= - None cls use_process_pool cls _ready_future None cls _ready_future result timeout=timeout classmethod submit cls task Callable Any - Any get_compile_threads = task cls pool submit task classmethod use_process_pool cls get_compile_threads = False Create dummy job check pool ready Submit here instead pool creation so we don t launch full pool worker subprocesses until we re sure they re needed cls _ready_future cls _ready_future = cls process_pool submit cls _get_ready cls _ready_future done classmethod quiesce cls - None If using SubprocPool signal sidecar process shut down its ProcessPoolExecutor Don t inadvertently create process pool doesn t already exist cls process_pool cache_info currsize config quiesce_async_compile_pool pool = cls process_pool isinstance pool SubprocPool pool quiesce classmethod wakeup cls - None If using SubprocPool signal sidecar process start up its ProcessPoolExecutor cls use_process_pool pool = cls process_pool isinstance pool SubprocPool pool wakeup triton kernel_name str source_code str device_str str = cuda Async_compile triton more complicated than other backends because we re trying optimize compile time much possible hot callsite First all function cached CompiledTritonKernels there s kernel already compiled we grab directly cache Otherwise we have multiple compile threads we kick off triton compilations each worker process giving kernel source code compile The worker initializes CachingAutotuner runs triton compilation pickles kernel back us We use TritonCompileResult represent objects being pickled back us each worker Some maybe obvious things pickled back us - Most time we can avoid sending back CachingAutotuner fn other metadata do have pay cost loading triton kernel parent But certain cases like coordesc tuning dynamic_scale_rblock require us reload function parent lazily when we require - The AutotuneCache enabled constructed each worker per triton config pickled us via ` CachingAutotuner save_cache_hook ` load_kernel = functools partial _load_triton_kernel_from_source kernel_name source_code reload_kernel_in_parent Benchmark how often happens dynamo_timed reload_kernel_in_parent load_kernel counters inductor async_compile_cache_miss += kernel_code_log info Triton Kernel \n s source_code _compile_start os environ get TRITON_INTERPRET == getattr torch _inductor codecache PyCodeCache load source_code kernel_name is_parallel = use_process_pool set_feature_use parallel_compile_post_warmup is_parallel compile_id = torch _guards CompileContext current_compile_id is_backward = getattr V graph is_backward False future = CompiledTritonKernels get source_code None counters inductor async_compile_cache_hit += Set reload_kernel_from_src properly based source_code isinstance future StaticAutotunerFuture Remove future now we ve cache hit CompiledTritonKernels remove_future source_code future reload_kernel_from_src = reload_kernel_in_parent is_parallel future future result Cache miss is_parallel We want support changing these env vars after while process pool running so pass them subprocess reset env_vars = TORCHINDUCTOR_CACHE_DIR TRITON_CACHE_DIR extra_env = v os environ v v env_vars v os environ extra_config = use_static_cuda_launcher torch _inductor config use_static_cuda_launcher len torch _inductor config autotune_lookup_table m = size_hints_regex search source_code m size_hints_str = m group size_hints_str = str None triton_src = source_code split triton jit\n torch _inductor runtime triton_heuristics generate_lookup_hash_from_source_code fn_hash = generate_lookup_hash_from_source_code size_hints_str triton_src fn_hash torch _inductor config autotune_lookup_table extra_config autotune_lookup_table = type ignore assignment fn_hash torch _inductor config autotune_lookup_table fn_hash task = process_pool submit _worker_compile_triton load_kernel extra_env extra_config get_result - CachingAutotuner try kernel elapsed_us = task result except SubprocException e raise e with_name kernel_name e Now we ve compiled we should clear future so can t used again kernel set_compile_info compile_id is_backward CompiledTritonKernels remove_future source_code kernel restore_after_unpickle old_values=None kernel precompile warm_cache_only=False reload_kernel=reload_kernel_in_parent static_triton_bundle_key=CompiledTritonKernels key source_code info = kernel autotune_cache_info info compile_time_us = elapsed_us _add_triton_kernel_info kernel_name info get_metrics_context add_top_n triton_kernel_compile_times_us kernel_name elapsed_us kernel future = LambdaFuture get_result future=task CompiledTritonKernels save source_code future future dynamo_timed async_compile precompile log_pt _compile_event=True dynamo_compile_column_us= triton_compile_time_us log_waitcounter=True waitcounter_name_override= compile_triton fail = None try start_ns = time_ns _set_triton_ptxas_path kernel = load_kernel kernel set_compile_info compile_id is_backward kernel precompile warm_cache_only=False static_triton_bundle_key=CompiledTritonKernels key source_code elapsed_us = time_ns - start_ns get_metrics_context add_top_n triton_kernel_compile_times_us kernel_name elapsed_us info = kernel autotune_cache_info info compile_time_us = elapsed_us _add_triton_kernel_info kernel_name info kernel except Exception e fail = str e raise finally log_triton_builds fail=fail multi_kernel args kwargs - Any torch _inductor codegen multi_kernel MultiKernelCall no need call parallel since sub-kernels already parallel tasks MultiKernelCall args kwargs size_hint_multi_kernel args kwargs - Any torch _inductor codegen multi_kernel SizeHintMultiKernelCall SizeHintMultiKernelCall args kwargs cpp source_code str kernel_code_log info CPP Kernel \n s source_code get_compile_threads = CppCodeCache load source_code kernel get_result = CppCodeCache load_async source_code submit_fn=self submit LambdaFuture lambda get_result kernel cpp_pybinding argtypes list str source_code str kernel_code_log info CPP+Bindings Kernel \n s source_code get_compile_threads = CppPythonBindingsCodeCache load_pybinding argtypes source_code get_result = CppPythonBindingsCodeCache load_pybinding_async argtypes source_code submit_fn=self submit LambdaFuture get_result cuda source_code dst_file_ext aot_compile=False kernel_code_log info CUDA Kernel \n s source_code task aot_compile We rely JITInductor compile CUDA code so we can load into AOTInductor output_path _ = CUDACodeCache compile source_code o CUDACodeCache aot_kernels_o append output_path CUDACodeCache load source_code dst_file_ext submit task rocm source_code dst_file_ext aot_compile=False kernel_code_log info ROCm Kernel \n s source_code task aot_compile output_path _ = ROCmCodeCache compile source_code dst_file_ext= o ROCmCodeCache aot_kernels_o append output_path config rocm generate_test_runner _ = ROCmCodeCache compile source_code dst_file_ext= exe ROCmCodeCache load source_code dst_file_ext submit task halide meta HalideMeta source_code str kernel_code_log info Halide Kernel \n r\n s meta source_code get_compile_threads = HalideCodeCache generate_halide meta source_code get_result = HalideCodeCache generate_halide_async meta source_code submit_fn=self submit LambdaFuture get_result cutedsl kernel_name str source_code str Compile CuteDSL CUTLASS Python DSL kernels Args kernel_name Name kernel defined source_code Source code CuteDSL kernel string Note CuteDSL currently requires source files do its compilation there we use PyCodeCache write source code file load torch _inductor codegen cutedsl cutedsl_kernel CuteDSLKernelWrapper MAIN_SUFFIX kernel_code_log info CuteDSL Kernel \n s source_code task key path = torch _inductor codecache PyCodeCache write source_code mod = torch _inductor codecache PyCodeCache load_by_key_path key path Find our special entry point named function main_func_name = f kernel_name _ MAIN_SUFFIX hasattr mod main_func_name available = name name dir mod callable getattr mod name raise RuntimeError f Could find CuteDSL main kernel function main_func_name Available callables available CuteDSLKernelWrapper getattr mod main_func_name kernel_path=path get_compile_threads = task future = submit task LambdaFuture lambda future result wait scope dict str Any - None get_compile_threads dynamo_timed async_compile wait log_pt _compile_event=True dynamo_compile_column_us= triton_compile_time_us log_waitcounter=True waitcounter_name_override= compile_triton _wait_futures scope _compile_end _wait_futures scope dict str Any - None kernels = key value key value scope items isinstance value Future CodeCacheFuture pbar = tqdm total=len kernels desc= Inductor Compilation disable=config disable_progress delay= key result kernels items config verbose_progress isinstance pbar _Faketqdm pbar set_postfix_str key try kernel = result result scope key = kernel except BrokenProcessPool e raise RuntimeError A compilation subprocess exited unexpectedly This likely due crash To facilitate debugging you can re-run TORCHINDUCTOR_COMPILE_THREADS= cause compilation occur main process e pbar update maybe_warm_pool - None os environ get TORCH_TNT_IN_USE == os environ get TORCH_WARM_POOL = The subprocess pool only used Triton backend has_triton_package Skip fbcode We have internal reports usages inside multiprocessing pools lead multiplicative number compile subprocesses config is_fbcode AsyncCompile warm_pool TODO This starts SubprocPool s internal process pool early possible expense creating bunch worker processes might needed We could start them lazily we re willing lose small amount compile time AsyncCompile wakeup On exit give workers chance clean themselves up Without resource_tracker can complain about leaked semaphores coming ProcessPoolExecutor UserWarning resource_tracker There appear leaked semaphore objects clean up shutdown atexit register shutdown_compile_workers