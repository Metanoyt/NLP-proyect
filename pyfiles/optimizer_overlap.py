mypy allow-untyped-defs inspect abc ABC abstractmethod torch distributed algorithms ddp_comm_hooks default_hooks allreduce_hook torch distributed algorithms ddp_comm_hooks optimizer_overlap_hooks _hook_then_optimizer _OptimizerHookState torch distributed fsdp FullyShardedDataParallel torch distributed optim as_functional_optim torch nn parallel DistributedDataParallel torch optim Optimizer Contains mappings between regular overlapped optimizer types _registered_overlapped_optims dict type type = register_overlapped optim_cls decorator target_overlapped_optim_cls target_overlapped_optim_cls _registered_overlapped_optims raise ValueError f target_overlapped_optim_cls already registered optim_cls f _registered_overlapped_optims optim_cls optim_cls trying f re-register optim_cls supported _registered_overlapped_optims optim_cls = target_overlapped_optim_cls target_overlapped_optim_cls decorator OverlappedOptimizer ABC __init__ optim_cls type - None Initialize OverlappedOptimizer Overlappedoptimizer base child classes can implement specify how different optimizers will register themselves DDP optim_cls = optim_cls abstractmethod register_ddp ddp DistributedDataParallel - None Registers overlapped optimizer DDP raise NotImplementedError f __class__ __name__ does support overlapped DDP abstractmethod register_fsdp fsdp FullyShardedDataParallel - None Registers overlapped optimizer FSDP raise NotImplementedError f __class__ __name__ does support overlapped FSDP register_overlapped Optimizer _OverlappedStandardOptimizer OverlappedOptimizer Overlaps regular ` ` Optimizer ` ` __init__ optim_cls type params optim_args optim_kwargs - None super __init__ optim_cls f_optim = as_functional_optim optim_cls optim_args optim_kwargs _opt_hook_state = _OptimizerHookState f_optim params register_ddp ddp_inst DistributedDataParallel NOTE using custom communication hook fused optimizer yet supported ddp_inst register_comm_hook type ignore operator None wrapped hook state _hook_then_optimizer allreduce_hook _opt_hook_state TODO register_fsdp once FSDP supports communication hook register_fsdp fsdp FullyShardedDataParallel - None Register overlapped optimizer FSDP raise NotImplementedError f __class__ __name__ does support overlapped FSDP _as_overlapped_optim optim_cls type params args kwargs Return new ` ` OverlappedOptimizer ` ` instance supports ` ` optim_cls ` ` clz inspect getmro optim_cls try _registered_overlapped_optims clz optim_cls params args kwargs except KeyError pass Fallback standard overlapped optimizer which will raise errors user attempting use unsupported optimizer _OverlappedStandardOptimizer optim_cls params args kwargs