__future__ annotations argparse os collections namedtuple pathlib Path typing Any TYPE_CHECKING yaml torchgen dest dest torchgen api lazy setValueT torchgen api types BaseCppType torchgen dest lazy_ir GenLazyIR GenLazyNativeFuncDefinition GenTSLazyIR torchgen gen get_grouped_native_functions parse_native_yaml torchgen gen_backend_stubs error_on_missing_kernels gen_dispatcher_registrations gen_dispatchkey_nativefunc_headers parse_backend_yaml torchgen model NativeFunction NativeFunctionsGroup OperatorName torchgen selective_build selector SelectiveBuilder torchgen utils FileManager NamespaceHelper torchgen yaml_utils YamlLoader TYPE_CHECKING collections abc Callable Iterable Iterator Sequence ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Lazy Tensor Codegen ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Overview ~~~~~~~~ This codegen script builds existing data models helpers used all ATen backends adds new functionality specific lazy tensor backends Inputs - backend _native_functions yaml controls which operators supported backend Outputs all backends DispatchKey Ir h defines Lazy IR classes constructed during tracing - opt-in also generate lowering methods TorchScript backend only DispatchKey NativeFunctions cpp defines implementations native functions which perform lazy tracing - opt-in full_codegen section backend yaml supported section omits these implementations DispatchKey NativeFunctions h declares implementations native functions both supported full_codegen ops Register DispatchKey cpp registers all op implementations dispatcher RegisterAutograd DispatchKey cpp registers all autograd implementations dispatcher Validation Helpers - Shape Inference errs any ops backend yaml require shape inference provided meta kernels implementations torch csrc lazy core shape_inference - native function impls errs any supported ops do have implementation defined backend non-codegen implementation file About Data Model ~~~~~~~~~~~~~~~~~~~~ Modeled after ATen codegen first step parse yaml build data model operators we care about In case backend _native_functions yaml defines subset core operators defined more detail main native_functions yaml which will supported your backend Backends can list ops two categories - ` supported ` ops require hand-implementations still get codegenned declarations registrations - ` full_codegen ` ops get implementations IR classes generated too Each native function modeled object schema each schema has objects representing their arguments Much codegen manipulation arguments their types For example lazy tensor backends need transform Tensor arguments into lazy Value objects well replacing reference types stringref actual string objects done manipulating data model objects - see api lazy py lazy data model Once data model set up rest script processes number templates output CPP file fills template values using helpers ` dest lazy_ir py ` ` dest lazy_ts_lowering py ` These helpers mostly iterate over functions their arguments outputting different c++ snippets ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Parses external backend s yaml adds new BackendIndex backend s dispatch key Returns Tuple backend_key autograd_key cpp_namespace updated BackendIndex mapping full_codegen ParsedExternalYaml = namedtuple ParsedExternalYaml backend_key autograd_key cpp_namespace backend_indices full_codegen parse_native_functions_keys backend_yaml_path str grouped_native_functions Sequence NativeFunction &#124; NativeFunctionsGroup - tuple list OperatorName list Any list OperatorName open backend_yaml_path f yaml_values = yaml load f Loader=YamlLoader assert isinstance yaml_values dict full_codegen = yaml_values pop full_codegen non_native = yaml_values pop non_native ir_gen = yaml_values pop ir_gen assert isinstance full_codegen list assert isinstance non_native list assert isinstance ir_gen list full_codegen_opnames = OperatorName parse name name full_codegen ir_gen_opnames = OperatorName parse name name ir_gen full_codegen_opnames non_native ir_gen_opnames validate_shape_inference_header shape_inference_hdr str expected_shape_infr_decls list str - None try open shape_inference_hdr f shape_infr_decls = f read shape_infr_decl_lines = set shape_infr_decls split \n except OSError e raise AssertionError f Unable read specified shape_inference_hdr file shape_inference_hdr e TODO whc add check shape inference functions have meta kernels implement should retired missing_decls = decl decl expected_shape_infr_decls decl shape_infr_decl_lines missing_decls raise Exception noqa TRY f Missing shape inference function \n Please add declare function shape_inference_hdr \n implement corresponding shape_inference cpp file \n os linesep join missing_decls Some helper functions codegen get_ltc_helper_fns - str \ Tensor to_meta const Tensor tensor undefined tensors can t converted meta device since they don t have sizes strides tensor defined tensor auto out = native empty_strided_meta_symint tensor sym_sizes tensor sym_strides \ dtype= tensor scalar_type layout= tensor layout \ device= c Device c kMeta pin_memory= std nullopt needs handle wrapped numbers so dtype promotion works properly tensor unsafeGetTensorImpl - is_wrapped_number out unsafeGetTensorImpl - set_wrapped_number true out std optional Tensor to_meta const std optional Tensor tensor tensor has_value to_meta tensor std nullopt std vector Tensor to_meta ITensorListRef t_list std vector Tensor outs outs reserve t_list size const auto tensor t_list outs push_back to_meta tensor outs default_args node_base str = Node node_base_hdr str &#124; None = None shape_inference_hdr str = torch csrc lazy core shape_inference h tensor_class str = torch lazy LazyTensor tensor_class_hdr str = torch csrc lazy core tensor h lazy_ir_generator type GenLazyIR = GenLazyIR native_func_definition_generator type GenLazyNativeFuncDefinition = GenLazyNativeFuncDefinition backend_name str = TorchScript main - None parser = argparse ArgumentParser description= Generate Lazy Tensor backend files parser add_argument -s -- source-yaml -- source_yaml help= path source yaml file containing operator external definitions parser add_argument -o -- output-dir -- output_dir help= output directory parser add_argument -- dry-run -- dry_run type=bool default=False help= output directory parser add_argument -- impl-path -- impl_path type=str default=None help= path source C++ file containing kernel definitions parser add_argument -- gen-ts-lowerings -- gen_ts_lowerings action= store_true help= Generate TorchScript lowerings addition Lazy IR NativeFunctions parser add_argument -- node-base -- node_base type=str default=default_args node_base help= Name backend specific custom Lazy IR Node base parser add_argument -- node-base-hdr -- node_base_hdr type=str default=default_args node_base_hdr help= Path header file defining custom Lazy IR Node base parser add_argument -- shape-inference-hdr -- shape_inference_hdr type=str default=default_args shape_inference_hdr help= Path header file defining custom Lazy shape inference functions parser add_argument -- tensor-class -- tensor_class type=str default=default_args tensor_class help= Name backend specific custom Lazy Tensor parser add_argument -- tensor-class-hdr -- tensor_class_hdr type=str default=default_args tensor_class_hdr help= Path header file defining custom Lazy Tensor parser add_argument -- backend-name -- backend_name type=str default=default_args backend_name help= Name backend generate options = parser parse_args Assumes file lives PYTORCH_ROOT torchgen gen_backend_stubs py torch_root = Path __file__ absolute parents aten_path = str torch_root aten src ATen lazy_ir_generator type GenLazyIR = default_args lazy_ir_generator options gen_ts_lowerings lazy_ir_generator = GenTSLazyIR native_func_definition_generator type GenLazyNativeFuncDefinition = default_args native_func_definition_generator run_gen_lazy_tensor aten_path options source_yaml options output_dir options dry_run options impl_path options node_base options node_base_hdr options tensor_class options tensor_class_hdr options shape_inference_hdr lazy_ir_generator native_func_definition_generator options backend_name run_gen_lazy_tensor aten_path str source_yaml str output_dir str dry_run bool impl_path str &#124; None node_base str = default_args node_base node_base_hdr str &#124; None = default_args node_base_hdr tensor_class str = default_args tensor_class tensor_class_hdr str = default_args tensor_class_hdr shape_inference_hdr str = default_args shape_inference_hdr lazy_ir_generator type GenLazyIR = default_args lazy_ir_generator native_func_definition_generator type GenLazyNativeFuncDefinition = default_args native_func_definition_generator build_in_tree true TS backend affects include paths build_in_tree bool = False per_operator_headers changes whether ATen Functions h individual operator headers used must match how ATen built per_operator_headers bool = False backend_name str = default_args backend_name gen_forced_fallback_code bool = False use_lazy_shape bool = True following arguments temporary customization points xla backend migration do rely them otherwise they should removed once migration complete backend_namespace str = torch lazy get_tensorlist str = GetTensorList get_tensor_or_wrap_number str = GetLtcTensorOrCreateForWrappedNumber try_get_tensor str = TryGetLtcTensor metrics_counter str = TORCH_LAZY_FN_COUNTER lazy create_tensor str = LazyTensor Create create_from_first_tensor bool = False create_aten_from_ltc_tensor str = torch lazy CreateAtenFromLtcTensor tuple_aten_from_ltc_tensors str = torch lazy TupleAtenFromLtcTensors lazy_value_class str = torch lazy Value lazy_tensor_ptr str = LazyTensorPtr get_device_fn str = torch lazy GetBackendDevice - None lv_tokens = lazy_value_class split lv_class = lv_tokens - lv_ns = join lv_tokens - setValueT BaseCppType lv_ns lv_class template_dir = os path join aten_path templates make_file_manager install_dir str - FileManager FileManager install_dir=install_dir template_dir=template_dir dry_run=dry_run fm = make_file_manager output_dir native_yaml_path = os path join aten_path native native_functions yaml tags_yaml_path = os path join aten_path native tags yaml parsed_yaml = parse_native_yaml native_yaml_path tags_yaml_path native_functions backend_indices = parsed_yaml native_functions parsed_yaml backend_indices grouped_native_functions = get_grouped_native_functions native_functions sort_native_function f NativeFunctionsGroup &#124; NativeFunction - str We sort native function because note concat_map_codegen TODO alanwaketan Remove sorting hack once all ops grouped properly func = f functional func isinstance f NativeFunctionsGroup f func str func name name grouped_native_functions = sorted grouped_native_functions key=sort_native_function parsed_backend_yaml = parse_backend_yaml source_yaml grouped_native_functions backend_indices backend_key = parsed_backend_yaml backend_key autograd_key = parsed_backend_yaml autograd_key cpp_namespace = parsed_backend_yaml cpp_namespace backend_indices = parsed_backend_yaml backend_indices following keys all processed differently full_codegen we generate IR kernels etc ir_gen we generate only IR non_native used register kernels declared native_functions yaml full_codegen non_native ir_gen = parse_native_functions_keys source_yaml grouped_native_functions concat_map_codegen func Callable NativeFunction Sequence str xs Iterable NativeFunctionsGroup &#124; NativeFunction ops_list list OperatorName = full_codegen - Iterator str We code-gen functional variant which all we need IR classes lowerings shape inferences we only code-gen additional entries inplace variant native functions x xs fs = list x functions isinstance x NativeFunctionsGroup x f fs f func name ops_list yield func f selector = SelectiveBuilder get_nop_selector assert backend_key None class_name = backend_indices backend_key native_function_class_name impl_path None error_on_missing_kernels native_functions backend_indices backend_key autograd_key class_name impl_path full_codegen Validate Shape Inference Definitions Generated lazy native functions all perform shape inference first using meta kernel available op otherwise using compute_shape_ op function instead The generator knows call signature compute_shape_ op because matches nativefunction meta signature so just has check whether op structured generate call one other It s up dev supply missing compute_shape_ op function codegen least warns you about provides expected signature which can copy-pasted into shape_inference h compute_shape_ op functions handwritten should replaced over time ops get ported structured kernels See torch csrc lazy core shape_inference cpp #READ THIS more information shape_inference_hdr None expected_shape_infr_decls = list concat_map_codegen dest GenLazyShapeInferenceDefinition backend_indices backend_key tensor_class grouped_native_functions validate_shape_inference_header shape_inference_hdr expected_shape_infr_decls assert class_name None Generate nativefunction declarations Note eager registrations set False lazy TS backend another LTC backend may want register their own lazy kernels instead registering TS ones The registration will lazily happen when init_ts_backend called gen_dispatchkey_nativefunc_headers fm class_name cpp_namespace backend_indices grouped_native_functions backend_key autograd_key backend_name Generate Dispatcher registrations which hook up nativefunctions dispatch_key backend_key autograd_key None backend_key autograd_key gen_dispatcher_registrations fm output_dir class_name backend_indices grouped_native_functions backend_key dispatch_key selector build_in_tree=build_in_tree per_operator_headers=per_operator_headers backend_name=backend_name eager_registration=False Generate native function impls build IR nodes ns_helper = NamespaceHelper cpp_namespace fm write_with_template f backend_key NativeFunctions cpp DispatchKeyNativeFunctions cpp lambda includes f #include path path tensor_class_hdr shape_inference_hdr ATen Functions h ATen native TensorConversions h ATen NativeFunctions h ATen CompositeExplicitAutogradNonFunctionalFunctions h ATen MetaFunctions h ATen Operators h ATen native CPUFallback h torch csrc lazy core ir_builder h torch csrc lazy core lazy_graph_executor h torch csrc lazy core metrics h torch csrc lazy core shape h f output_dir backend_key NativeFunctions h f output_dir LazyIr h + torch csrc lazy ts_backend ts_eager_fallback h gen_forced_fallback_code helper_fns get_ltc_helper_fns native_functions_include namespace_prologue ns_helper prologue namespace_epilogue ns_helper epilogue native_function_definitions list concat_map_codegen native_func_definition_generator f backend_key NativeFunctions backend_indices backend_key tensor_class gen_forced_fallback_code backend_namespace get_tensorlist get_tensor_or_wrap_number try_get_tensor metrics_counter create_tensor create_from_first_tensor create_aten_from_ltc_tensor tuple_aten_from_ltc_tensors lazy_tensor_ptr get_device_fn grouped_native_functions Generate IR node classes lazy_ir_obj = lazy_ir_generator backend_indices backend_key backend_name node_base use_lazy_shape fm write_with_template LazyIr h LazyIr h lambda lazy_ir_sysinc f #include path path ATen core Formatting h c core ScalarType h torch csrc lazy core hash h torch csrc lazy core ir h torch csrc lazy core shape h optional vector lazy_ir_inc f #include node_base_hdr node_base_hdr None ir_declarations list concat_map_codegen lazy_ir_obj grouped_native_functions full_codegen + ir_gen namespace_prologue ns_helper prologue namespace_epilogue ns_helper epilogue Generate Non Native IR Node classes fm write_with_template LazyNonNativeIr h LazyNonNativeIr h lambda lazy_non_native_ir_inc f #include path path torch csrc lazy core ir h torch csrc lazy core ir_builder h torch csrc lazy core internal_ops ltc_ops h torch csrc lazy core shape_inference h + node_base_hdr node_base_hdr path non_native_ir_nodes dest generate_non_native_lazy_ir_nodes non_native lazy_ir_obj namespace_prologue ns_helper prologue namespace_epilogue ns_helper epilogue __name__ == __main__ main