Owner s module inductor ruff noqa F contextlib os subprocess sys unittest unittest mock patch torch torch _inductor async_compile noqa F required warm up AsyncCompile pools torch _dynamo testing rand_strided torch _inductor config torch _inductor codecache PyCodeCache torch _inductor test_case run_tests TestCase torch _inductor utils fresh_cache torch testing FileCheck torch testing _internal common_cuda xfailIfSM torch testing _internal inductor_utils GPU_TYPE HAS_GPU IS_BIG_GPU TestKernelBenchmark TestCase device_type = GPU_TYPE make sure subprocess runs exact same path parent process we augment PYTHONPATH env var python_path = classmethod setUpClass cls cls exit_stack = contextlib ExitStack cls exit_stack enter_context patch object config benchmark_kernel True setup augmented PYTHONPATH pass subprocess calls augmented_pp = join sys path os environ get PYTHONPATH augmented_pp = f os environ get PYTHONPATH augmented_pp cls python_path = augmented_pp classmethod tearDownClass cls cls exit_stack close setUp super setUp PyCodeCache cache_clear get_compiled_module compiled_module = None v PyCodeCache modules hasattr v benchmark_compiled_module assertTrue compiled_module None Found multiple compiled modules compiled_module = v assertTrue compiled_module None compiled_module verify_compiled_kernels GB_count= compiled_module = get_compiled_module now run compiled module subprocess check its output try bench_out = subprocess check_output f sys executable compiled_module __file__ -kc split stderr=subprocess STDOUT env= os environ PYTHONPATH python_path decode except subprocess CalledProcessError e print Failed when running output code e print e output decode raise e make sure we have bandwidth information output FileCheck check_count GB s GB_count exactly= run bench_out verify_remove_inductor_deps compiled_module try out = subprocess check_output f sys executable compiled_module __file__ split env= os environ copy TORCHINDUCTOR_DUMP_LAUNCH_PARAMS PYTHONPATH python_path stderr=subprocess STDOUT except subprocess CalledProcessError e print Failed when runinng triton code TORCHINDUCTOR_DUMP_LAUNCH_PARAMS= e print e output decode raise e torch utils _get_clean_triton get_clean_triton cleaned_triton = get_clean_triton compiled_module __file__ f compiled_module __file__ cleaned assertTrue triton_heuristics cleaned_triton assertTrue run cleaned_triton try out = subprocess check_output f sys executable compiled_module __file__ cleaned split stderr=subprocess STDOUT env= os environ PYTHONPATH python_path except subprocess CalledProcessError e print Failed when when running cleaned triton e print e output decode print cleaned_triton raise e cleaned_triton check_bandwidth compiled_module num_gb now run compiled module subprocess check its output try bench_out = subprocess check_output f sys executable compiled_module __file__ -k split stderr=subprocess STDOUT env= os environ PYTHONPATH python_path decode except subprocess CalledProcessError e print Failed when running output code e print e output decode raise e make sure we have bandwidth information output FileCheck check_count f num_gb GB exactly= run bench_out test_pw_kernel_benchmark torch compile f x torch sin x + torch cos x inp = torch rand device=GPU_TYPE out = f inp verify_compiled_kernels TODO Currently Triton mm template + relu fusion causes slowdown XPU Need refine template config XPU config patch max_autotune=True max_autotune_gemm_backends= TRITON force_shape_pad=True unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM fresh_cache test_matmul_triton_kernel_benchmark M = N = K = = torch rand M K dtype=torch float device=GPU_TYPE b = torch rand N K dtype=torch float device=GPU_TYPE t torch compile f b torch relu b f b verify_compiled_kernels config patch max_autotune=True max_autotune_gemm_backends= TRITON shape_padding=False fresh_cache test_mm_triton_kernel_benchmark M = N = K = K_ = = rand_strided M K_ K_ device=GPU_TYPE dtype=torch float b = rand_strided K N K device=GPU_TYPE dtype=torch float torch compile f b a_ = torch narrow K c = torch mm a_ b c f b verify_compiled_kernels GB_count= test_matmul_bandwidth_computation The test does matmul then mul Without max-autotune we use matmul aten So there single triton kernel mul The kernel we generated like triton jit triton_ in_out_ptr xnumel XBLOCK tl constexpr Note in_out_ptr argument It s x tensor s inplace updated so when computing bandwidth we should count total memory access = MB This amount what test asserts torch set_float _matmul_precision high suggested warning torch compile f x y z = x y w = z z w M N K = x = torch rand M K device=GPU_TYPE y = torch rand K N device=GPU_TYPE out = f x y compiled_module = get_compiled_module check_bandwidth compiled_module test_unused_input_bandwidth_computation M N = torch compile f b c + c = torch rand M N dtype=torch float device=GPU_TYPE b = torch rand M N dtype=torch float device=GPU_TYPE c = torch rand M N dtype=torch float device=GPU_TYPE torch _dynamo mark_dynamic torch _dynamo mark_dynamic b torch _dynamo mark_dynamic c inputs = b c out = f inputs compiled_module = get_compiled_module num_gb = size_a + size_c + size_out num_gb = + + e = check_bandwidth compiled_module test_reduction_bandwidth_computation torch compile f torch sum dim= = torch rand dtype=torch float device=GPU_TYPE inputs = out = f inputs compiled_module = get_compiled_module num_gb = size_a + size_out num_gb = + e = check_bandwidth compiled_module config patch max_autotune=True test_fused_layernorm_bandwidth_computation M N = torch compile f b c d x = + b x = torch nn functional layer_norm x normalized_shape= N weight=c bias=d eps= e- x = torch sigmoid x x x = torch rand M N dtype=torch float device=GPU_TYPE b = torch rand N dtype=torch float device=GPU_TYPE c = torch rand N dtype=torch float device=GPU_TYPE d = torch rand N dtype=torch float device=GPU_TYPE inputs = b c d out = f inputs compiled_module = get_compiled_module num_gb = size_a + size_b + size_c + size_d + size_out num_gb = + + + + e = check_bandwidth compiled_module test_slice_add_cat_bandwidth_computation M N = torch compile f b c x = torch narrow b N N broadcasting x = x + c torch cat x dim= = torch rand M N dtype=torch float device=GPU_TYPE b = torch rand M N dtype=torch float device=GPU_TYPE c = torch rand N dtype=torch float device=GPU_TYPE torch _dynamo mark_dynamic torch _dynamo mark_dynamic b inputs = b c out = f inputs compiled_module = get_compiled_module we overestimate size slice_b due torch cat num_gp = size_a + size_slice_b + size_c + size_out num_gb = + + + e = check_bandwidth compiled_module test_slice_add_bandwidth_computation M N = torch compile f b c x = torch narrow b N N + x + c = torch rand M N dtype=torch float device=GPU_TYPE b = torch rand M N dtype=torch float device=GPU_TYPE c = torch rand N dtype=torch float device=GPU_TYPE torch _dynamo mark_dynamic torch _dynamo mark_dynamic b inputs = b c out = f inputs compiled_module = get_compiled_module num_gb = size_a + size_slice_b + size_c + out_size num_gb = + + + e = check_bandwidth compiled_module test_mm_slice_add_bandwidth_computation M N K = torch compile f b c x = torch mm b x = torch narrow c N N x = torch narrow c N N x + x + x = torch rand M K dtype=torch float device=GPU_TYPE b = torch rand K N dtype=torch float device=GPU_TYPE c = torch rand N N dtype=torch float device=GPU_TYPE inputs = b c out = f inputs compiled_module = get_compiled_module torch mm becomes extern kernel so we measure nbytes pointwise add kernel num_gb = x + size_slice_c + size_out num_gb = + + e = num_gb = check_bandwidth compiled_module num_gb test_mm_slice_add_bandwidth_computation_ M N K = torch compile f b c x = torch mm b x = torch narrow c N N x = torch narrow c N N x + x + x = torch rand M K dtype=torch float device=GPU_TYPE b = torch rand K N dtype=torch float device=GPU_TYPE c = torch rand N N dtype=torch float device=GPU_TYPE inputs = b c out = f inputs compiled_module = get_compiled_module torch mm becomes extern kernel so we measure nbytes pointwise add kernel num_gb = x + size_slice_c + size_out num_gb = + + e = note we only count one size_slice_c because two accesses have same index check_bandwidth compiled_module xfailIfSM config patch max_autotune=True max_autotune_gemm_backends= TRITON force_shape_pad=True test_slice_mm_bandwidth_computation GPU_TYPE == xpu torch _inductor utils is_big_gpu raise unittest SkipTest unsupported device M N K = torch compile f b x = torch narrow K K torch mm x b = torch rand M K dtype=torch float device=GPU_TYPE b = torch rand K N dtype=torch float device=GPU_TYPE torch _dynamo mark_dynamic inputs = b out = f inputs compiled_module = get_compiled_module c = x b num_gb = + + e = check_bandwidth compiled_module test_star_dep Test bandwidth estimation StarDep torch compile f b b = = torch rand device=GPU_TYPE b = torch randint device=GPU_TYPE dtype=torch int unsqueeze f b compiled_module = get_compiled_module = KB b = MB check_bandwidth compiled_module test_split_scan torch compile f cumsum - = torch rand device=GPU_TYPE f reshape - compiled_module = get_compiled_module = MB Double output well check_bandwidth compiled_module config patch triton unique_kernel_names True config patch benchmark_kernel=False config patch compile_threads= test_remove_inductor_deps torch compile f cos sin = torch randn device=GPU_TYPE f compiled_module = get_compiled_module cleaned_triton = verify_remove_inductor_deps compiled_module config patch triton unique_kernel_names True config patch benchmark_kernel=False config patch compile_threads= test_remove_inductor_deps_multiple_kernels torch compile f = torch mm = cos sin = torch mm = torch softmax dim=- = torch randn device=GPU_TYPE f compiled_module = get_compiled_module verify_remove_inductor_deps compiled_module unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM config patch triton unique_kernel_names True config patch triton unique_kernel_names True config patch benchmark_kernel=False config patch compile_threads= config patch max_autotune=True max_autotune_gemm_backends= TRITON test_remove_inductor_deps_templates torch compile f = torch mm = cos = torch mm = sin = torch randn device=GPU_TYPE f compiled_module = get_compiled_module verify_remove_inductor_deps compiled_module config patch triton unique_kernel_names True config patch benchmark_kernel=False config patch compile_threads= test_remove_inductor_deps_scalar torch compile f b + b = torch tensor device=GPU_TYPE b = torch tensor device=GPU_TYPE f b compiled_module = get_compiled_module verify_remove_inductor_deps compiled_module __name__ == __main__ HAS_GPU run_tests