mypy allow-untyped-defs collections abc Callable dataclasses dataclass functools partial typing Any no_type_check torch torch distributed dist torch autograd Variable __all__ list str = _FUNCTIONAL_OPTIM_STEP_METHOD_NAME = step_param _OptimizerHookState Holds state running optimizer in-line after DDP communication hook Currently contains only optimizer which must have method ` step_param ` __slots__ = functional_optimizer params_to_optimize __init__ functional_optim params=None functional_optimizer = functional_optim _check_valid_functional_optim _set_params_to_optimize params _set_params_to_optimize params params None params_to_optimize = set params _check_valid_functional_optim hasattr functional_optimizer _FUNCTIONAL_OPTIM_STEP_METHOD_NAME raise ValueError f Class type functional_optimizer must implement method f _FUNCTIONAL_OPTIM_STEP_METHOD_NAME dataclass _OptimInBackwardHookState optim_stream torch Stream wait_for_optim_stream_enqueued bool no_type_check _apply_optim_in_backward_hook gradient_is_bucket_view bool - Callable Any dist GradBucket torch futures Future torch Tensor r Register hook apply optimizer backward If torch distributed optim _apply_optimizer_in_backward used overlap optimizer backward pass DDP will run below hook run optimizer step parameters after gradient communication has taken place optim_in_bwd_state = _OptimInBackwardHookState optim_stream=torch Stream wait_for_optim_stream_enqueued=False apply_optim_in_backward_hook hook_state Any bucket dist GradBucket optim_stream_state - torch futures Future torch Tensor Run original hook ddp_weakref = hook_state ddp_inst = ddp_weakref reducer process_group = ddp_inst reducer ddp_inst process_group fut = reducer _run_allreduce_hook bucket optimizer_stream = optim_stream_state optim_stream optimizer_stream fut wait Apply gradient division since C++ side only allreduces does average TODO rohan-varma div factor may different when running join hook bucket buffer div_ process_group size model_params = bucket parameters grads = bucket gradients TODO rohan-varma upcast needed DDP mixed precision once optimizer backward + DDP mixed precision supported p g zip model_params grads hasattr p _in_backward_optimizers Note need set grad bucket s grad because running allreduce results bucket s grad being reduced grad field gradient_is_bucket_view p grad = g optim p _in_backward_optimizers optim step Need Future Tensor obey comm hook API contract ret_fut = torch futures Future ret_fut set_result bucket buffer enqueue callback wait optimizer stream end backward set all DDP managed grads None wait_for_optim_stream_callback torch accelerator current_stream wait_stream optim_stream_state optim_stream Set DDP managed grads None param ddp_inst _get_data_parallel_params ddp_inst module hasattr param _in_backward_optimizers param grad = None reset next backwards pass optim_stream_state wait_for_optim_stream_enqueued = False optim_stream_state wait_for_optim_stream_enqueued Variable _execution_engine queue_callback wait_for_optim_stream_callback mark callback enqueued optim_stream_state wait_for_optim_stream_enqueued = True ret_fut comm_hook = partial apply_optim_in_backward_hook optim_stream_state=optim_in_bwd_state These needed DDP s logging comm hooks comm_hook __name__ = apply_optim_in_backward_hook __name__ comm_hook __qualname__ = apply_optim_in_backward_hook __qualname__ comm_hook _hook_then_optimizer hook Callable Any dist GradBucket torch futures Future torch Tensor optimizer_state _OptimizerHookState - Callable Any dist GradBucket torch futures Future torch Tensor r Run optimizer functional fashion after DDP communication hook has_set_params = hasattr optimizer_state params_to_optimize optimizer_state params_to_optimize None hook_then_optimizer_wrapper hook_state bucket dist GradBucket - torch futures Future torch Tensor Run original hook fut = hook hook_state bucket optimizer_step fut gradient_tensors = bucket gradients model_params = bucket parameters grad_tensor model_param zip gradient_tensors model_params has_set_params model_param optimizer_state params_to_optimize optimizer_state functional_optimizer step_param model_param grad_tensor bucket buffer fut then optimizer_step hook_then_optimizer_wrapper