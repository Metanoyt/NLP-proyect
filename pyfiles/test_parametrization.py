Owner s module nn pickle copy deepcopy itertools product torch torch nn nn torch nn functional F torch nn init init torch nn utils parametrize parametrize torch Tensor torch __future__ get_swap_module_params_on_conversion torch nn Buffer Parameter torch testing _internal common_cuda TEST_MULTIGPU torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_nn NNTestCase torch testing _internal common_utils gradcheck instantiate_parametrized_tests run_tests set_default_dtype skipIfNoLapack skipIfTorchDynamo swap TemporaryFileName torch testing _internal two_tensor TwoTensor TestNNParametrization NNTestCase _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True FIXME Rewrite test using functions depending LAPACK remove ` skipIfNoLapack ` see torch nn utils parametrize skipIfNoLapack swap True False test_register_and_remove_parametrization r Test possible add few parametrizations parameter buffer removing them restores initial state It also tests backpropagating through them works expected Define couple matrix parametrizations Skew nn Module forward X X = X tril - X - X T Orthogonal nn Module forward X Cayley map If X skew-symmetric returns orthogonal matrix Id = torch eye X size device=X device We call contiguous because solve returns tensor strides Fortran-contiguous autograd raises performance warning This happens when we remove parametrization leave_parametrized=True which does set_ non-contiguous tensor while gradient contiguous torch linalg solve Id + X Id - X contiguous Resize nn Module forward X X NoResize nn Module forward X X Define couple vector parametrizations FirstZero nn Module forward x torch cat x new_zeros x LastZero nn Module forward x torch cat x - x new_zeros model = nn Linear initial_weight_id = id model weight initial_bias_id = id model bias initial_model = deepcopy model Test unsafe flag assertRaisesRegex ValueError Registering parametrization may change shape tensor parametrize register_parametrization model weight Resize default unsafe = False model torch ones One parametrization unsafe=True parametrize register_parametrization model weight Resize unsafe=True assertTrue hasattr model parametrizations assertTrue parametrize is_parametrized model assertTrue parametrize is_parametrized model weight assertFalse parametrize is_parametrized model bias assertNotIn weight model _parameters assertTrue model weight shape == parametrize remove_parametrizations model weight leave_parametrized=False assertFalse hasattr model parametrizations assertEqual model weight initial_model weight assertEqual id model weight initial_weight_id assertEqual model __class__ nn Linear Two parametrizations unsafe=True parametrize register_parametrization model weight Resize unsafe=True parametrize register_parametrization model weight NoResize unsafe=False assertTrue hasattr model parametrizations assertTrue parametrize is_parametrized model assertTrue parametrize is_parametrized model weight assertFalse parametrize is_parametrized model bias assertNotIn weight model _parameters assertTrue model weight shape == parametrize remove_parametrizations model weight leave_parametrized=False assertFalse hasattr model parametrizations assertEqual model weight initial_model weight assertEqual id model weight initial_weight_id assertEqual model __class__ nn Linear Test unsafe flag doesn t change expected behavior parametrize register_parametrization model weight Skew unsafe=True assertTrue hasattr model parametrizations assertTrue parametrize is_parametrized model assertTrue parametrize is_parametrized model weight assertFalse parametrize is_parametrized model bias assertNotIn weight model _parameters Result should skew-symmetric A = model weight assertEqual A -A T get_swap_module_params_on_conversion When using swap_tensors path needed so autograd graph alive anymore del A Remove check consistency parametrize remove_parametrizations model weight leave_parametrized=False assertFalse hasattr model parametrizations assertEqual model weight initial_model weight assertEqual id model weight initial_weight_id assertEqual model __class__ nn Linear Test one parametrization parametrize register_parametrization model weight Skew assertTrue hasattr model parametrizations assertTrue parametrize is_parametrized model assertTrue parametrize is_parametrized model weight assertFalse parametrize is_parametrized model bias assertNotIn weight model _parameters Result should skew-symmetric A = model weight assertEqual A -A T get_swap_module_params_on_conversion When using swap_tensors path needed so autograd graph alive anymore del A Remove check consistency parametrize remove_parametrizations model weight leave_parametrized=False assertFalse hasattr model parametrizations assertEqual model weight initial_model weight assertEqual id model weight initial_weight_id assertEqual model __class__ nn Linear Test two parametrizations same time removing them parametrize register_parametrization model weight Skew parametrize register_parametrization model weight Orthogonal Result should orthogonal X = model weight Id = torch eye X size device=X device assertEqual X T X Id get_swap_module_params_on_conversion When using swap_tensors path needed so autograd graph alive anymore del X Structure tests assertTrue hasattr model parametrizations assertTrue parametrize is_parametrized model assertTrue parametrize is_parametrized model weight assertFalse parametrize is_parametrized model bias assertIn weight model parametrizations assertNotIn weight model _parameters Remove parametrize remove_parametrizations model weight leave_parametrized=False assertEqual model weight initial_model weight assertEqual id model weight initial_weight_id assertFalse hasattr model parametrizations assertEqual model __class__ nn Linear Add everything parametrize register_parametrization model weight Skew parametrize register_parametrization model weight Orthogonal parametrize register_parametrization model bias FirstZero parametrize register_parametrization model bias LastZero Basic tests assertTrue parametrize is_parametrized model assertTrue parametrize is_parametrized model weight assertTrue parametrize is_parametrized model bias assertEqual model bias item assertEqual model bias - item assertEqual len list model parameters Nothing weird has happpened Should throw sgd = torch optim SGD model parameters lr= weight_copy = model weight clone bias_copy = model bias clone sgd zero_grad model weight T model bias sum backward sgd step assertNotEqual model weight weight_copy assertNotEqual model bias bias_copy Remove first parametrization Check model still parametrized so second parameter parametrize remove_parametrizations model weight leave_parametrized=False assertTrue parametrize is_parametrized model Still parametrized assertFalse parametrize is_parametrized model weight Parametrization removed assertTrue parametrize is_parametrized model bias Still parametrized assertEqual model bias item Still parametrized assertEqual model bias - item Still parametrized assertNotEqual model weight initial_model weight Has been updated assertEqual id model weight initial_weight_id Keeps same id assertEqual len list model parameters Nothing weird has happened Should throw weight_copy = model weight clone bias_copy = model bias clone sgd zero_grad model weight T model bias sum backward sgd step assertNotEqual model weight weight_copy assertNotEqual model bias bias_copy Remove second parametrization Check module parametrized parametrize remove_parametrizations model bias leave_parametrized=False assertFalse parametrize is_parametrized model Not parametrized assertNotEqual model bias initial_model bias Has been updated assertNotEqual model bias item Not parametrized assertNotEqual model bias - item Not parametrized assertEqual id model bias initial_bias_id Keeps same id assertFalse hasattr model parametrizations Not parametrized module assertEqual model __class__ nn Linear Resores previous assertEqual len list model parameters Nothing weird has happeed Should throw things updated weight_copy = model weight clone bias_copy = model bias clone sgd zero_grad model weight T model bias sum backward sgd step assertNotEqual model weight weight_copy assertNotEqual model bias bias_copy get_swap_module_params_on_conversion When using swap_tensors path needed so autograd graph alive anymore del weight_copy bias_copy Test leave_parametrized=True _ range parametrize register_parametrization model weight Skew parametrize register_parametrization model weight Orthogonal parametrize remove_parametrizations model weight leave_parametrized=True We didn t change dtype nor had multiple inputs so id should same assertEqual id model weight initial_weight_id assertEqual id model bias initial_bias_id Should throw Things updated weight_copy = model weight clone bias_copy = model bias clone sgd zero_grad model weight T model bias sum backward sgd step assertNotEqual model weight weight_copy assertNotEqual model bias bias_copy get_swap_module_params_on_conversion When using swap_tensors path needed so autograd graph alive anymore del weight_copy bias_copy swap True False test_register_and_remove_nested_parametrization r Test possible nest parametrizations meaning original param parametrized again Skew nn Module forward X X = X tril - X - X T model = nn Linear Add top level parametrization parametrize register_parametrization model weight Skew assertTrue hasattr model parametrizations assertTrue parametrize is_parametrized model assertTrue parametrize is_parametrized model weight assertFalse parametrize is_parametrized model bias assertNotIn weight model _parameters Result should skew-symmetric A = model weight assertEqual A -A T get_swap_module_params_on_conversion When using swap_tensors path needed so autograd graph alive anymore del A Add nested parametrization param_mod = model parametrizations weight assertFalse hasattr param_mod parametrizations assertFalse parametrize is_parametrized param_mod assertFalse parametrize is_parametrized param_mod original parametrize register_parametrization param_mod original Skew assertTrue hasattr param_mod parametrizations assertTrue parametrize is_parametrized param_mod assertTrue parametrize is_parametrized param_mod original assertNotIn original param_mod _parameters Result should skew-symmetric A = param_mod original assertEqual A -A T Remove nested param check consistency parametrize remove_parametrizations param_mod original leave_parametrized=False assertFalse hasattr param_mod parametrizations assertEqual param_mod __class__ parametrize ParametrizationList Remove top level check consistency parametrize remove_parametrizations model weight leave_parametrized=False assertFalse hasattr model parametrizations assertEqual model __class__ nn Linear swap True False test_register_and_remove_buffer_parametrization r Test possible add remove parametrizations buffers Define couple vector parametrizations FirstZero nn Module forward x torch cat x new_zeros x LastZero nn Module forward x torch cat x - x new_zeros model = nn Linear Instantiate parametrizations buffers It should work expected delattr model bias model bias = Buffer torch ones parametrize register_parametrization model bias FirstZero parametrize register_parametrization model bias LastZero assertTrue parametrize is_parametrized model assertTrue parametrize is_parametrized model bias assertEqual model bias item assertEqual model bias - item assertTrue model bias - == torch ones all assertEqual len list model parameters Remove parametrizations buffers It should work expected parametrize remove_parametrizations model bias leave_parametrized=True assertFalse parametrize is_parametrized model assertFalse parametrize is_parametrized model bias assertEqual model bias item assertEqual model bias - item assertTrue model bias - == torch ones all assertEqual len list model parameters FIXME Rewrite test using functions depending LAPACK remove ` skipIfNoLapack ` see skipIfNoLapack skipIfTorchDynamo Not applicable see https github com pytorch pytorch issues swap True False test_serialization_parametrization r Test possible serialize parametrized model via state_dict A stateful parametrization Orthogonal nn Module __init__ n super __init__ id = Buffer torch eye n B = Buffer torch empty n n init orthogonal_ B forward X A = X triu A = A - A T B torch linalg solve id + A id - A get_model model = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear parametrize register_parametrization model weight Orthogonal model model = get_model prev_weight = model weight prev_B = model parametrizations weight B new_model = get_model TemporaryFileName fname torch save model state_dict fname new_model load_state_dict torch load fname Integrity tests assertTrue parametrize is_parametrized new_model weight assertEqual prev_weight new_model weight assertEqual prev_B new_model parametrizations weight B Trying save whole parametrized model raises assertRaisesRegex RuntimeError state_dict TemporaryFileName fname torch save model fname FIXME Rewrite test using functions depending LAPACK remove ` skipIfNoLapack ` see skipIfNoLapack swap True False test_initialization_parametrization r Test possible initialize parametrization when implements ` right_inverse ` method Skew nn Module forward X A = X triu A - A T is_skew A torch allclose A -A T atol= e- right_inverse X is_skew X raise ValueError The matrix skew-symmetric X triu Implements Cayley map where right_inverse quite inverse forward Orthogonal nn Module __init__ n super __init__ B = Buffer torch eye n forward X Id = torch eye X size B torch linalg solve Id + X Id - X is_orthogonal X Id = torch eye X size torch allclose X T X Id atol= e- right_inverse X is_orthogonal X raise ValueError The input orthogonal cayley == Id so B cayley == B B = X torch zeros_like X N = model = nn Linear N N Register skew-symmetric constraint The result now skew-symmetric skew = Skew Make weight skew-symmetric before registering parametrization torch no_grad model weight set_ skew model weight parametrize register_parametrization model weight skew X = torch rand N N X skew-symmetric so throws error assertRaises ValueError model weight = X Make X skew-symmetric X = X - X T model weight = X assertEqual model parametrizations weight original X triu assertEqual model weight X Having several parametrizations registered should work same way parametrize register_parametrization model weight Orthogonal N Register now Cayley map The result now orthogonal X = torch rand N N X orthogonal so throws error assertRaises ValueError model weight = X init orthogonal_ X model weight = X assertEqual model weight X assertEqual model parametrizations weight original torch zeros_like X swap True False test_errors_unparametrized_tensor_parametrization Test errors when registering parametrization unparametrized tensor module = nn Linear weight_init = module weight clone Identity nn Module forward x x Register parametrization non-existing parameter throws assertRaisesRegex ValueError does have parameter parametrize register_parametrization module foo Identity assertFalse parametrize is_parametrized module Removing parametrizations unparametrized tensor throws assertRaisesRegex ValueError does have parametrization parametrize remove_parametrizations module bias assertFalse parametrize is_parametrized module A correct parametrization several outputs Sum nn Module forward x y x + y right_inverse z z torch zeros_like z parametrize register_parametrization module weight Sum Cannot remove parametrization several outputs ` leave_parametrized=False ` assertRaisesRegex ValueError leave_parametrized=False parametrize remove_parametrizations module weight leave_parametrized=False parametrize remove_parametrizations module weight leave_parametrized=True A parametrization incorrect number outputs WrongNumberParams nn Module forward x y z x + y + z right_inverse w w torch zeros_like w Makes param param right_inverse X fail assertRaisesRegex TypeError positional argument parametrize register_parametrization module weight WrongNumberParams assertFalse parametrize is_parametrized module A parametrization right_inverse does Tensor Sequence Tensor WrongRightInverse Identity right_inverse z None right_inverse should Tensor Sequence Tensor assertRaisesRegex ValueError Tensor Sequence parametrize register_parametrization module weight WrongRightInverse assertFalse parametrize is_parametrized module If s sequence must sequence tensors WrongRightInverseSequence nn Module forward x y x right_inverse z None z assertRaisesRegex ValueError sequence type parametrize register_parametrization module weight WrongRightInverseSequence assertFalse parametrize is_parametrized module A parametrization one tensor one tensor changes dtype ChangeDtypeInverse nn Module forward x x float right_inverse w w bool For parametrizations one tensor right_inverse may change dtype assertRaisesRegex ValueError outputs one tensor may change dtype parametrize register_parametrization module weight ChangeDtypeInverse assertFalse parametrize is_parametrized module ChangeDeviceInverse nn Module forward x x float right_inverse w w torch device meta For parametrizations one tensor right_inverse may change device assertRaisesRegex ValueError outputs one tensor may change device parametrize register_parametrization module weight ChangeDeviceInverse assertFalse parametrize is_parametrized module Doesn t tensor NotTensor nn Module forward x Forward must tensor assertRaisesRegex ValueError must tensor parametrize register_parametrization module weight NotTensor assertFalse parametrize is_parametrized module A parametrization one tensor one tensor changes dtype ChangeDtype nn Module forward x x bool forward should change initial dtype assertRaisesRegex ValueError may change dtype parametrize register_parametrization module weight ChangeDtype assertFalse parametrize is_parametrized module Change shape ChangeShape nn Module forward x x - forward should change original shape assertRaisesRegex ValueError may change shape parametrize register_parametrization module weight ChangeShape assertFalse parametrize is_parametrized module Many one changes dtype ChangeDtypeMulti nn Module forward x y x + y bool right_inverse w w w + forward should change original shape even parametrizations many inputs assertRaisesRegex ValueError may change dtype parametrize register_parametrization module weight ChangeDtypeMulti assertFalse parametrize is_parametrized module Returning sequence size one although weird s correct SequenceLen nn Module forward x x right_inverse w w parametrize register_parametrization module weight SequenceLen assertTrue hasattr module parametrizations weight original assertFalse hasattr module parametrizations weight original _ = module weight Does throw assertTrue parametrize is_parametrized module parametrize remove_parametrizations module weight leave_parametrized=True None operations above should have altered weight assertFalse parametrize is_parametrized module assertEqual module weight weight_init swap True False test_errors_parametrized_tensor_parametrization Test errors when registering parametrization parametrized tensor Identity nn Module forward x x module = nn Linear parametrize register_parametrization module weight Identity Has tensor WrongReturn nn Module forward x x x assertRaisesRegex ValueError must tensor parametrize register_parametrization module weight WrongReturn assertTrue parametrize is_parametrized module assertEqual len module parametrizations weight assertTrue isinstance module parametrizations weight Identity Cannot change dtype ChangeDtype nn Module forward x x bool assertRaisesRegex ValueError may change dtype parametrize register_parametrization module weight ChangeDtype assertTrue parametrize is_parametrized module assertEqual len module parametrizations weight assertTrue isinstance module parametrizations weight Identity Cannot change shape ChangeShape nn Module forward x x - assertRaisesRegex ValueError may change shape parametrize register_parametrization module weight ChangeShape assertTrue parametrize is_parametrized module assertEqual len module parametrizations weight assertTrue isinstance module parametrizations weight Identity The following checks mostly due bugs code parametrization right_inverse has tensor WrongReturnInverse Identity right_inverse x x x assertRaisesRegex ValueError right_inverse must tensor parametrize register_parametrization module weight WrongReturnInverse assertTrue parametrize is_parametrized module assertEqual len module parametrizations weight assertTrue isinstance module parametrizations weight Identity Cannot change dtype ChangeDtypeInverse Identity right_inverse x x bool assertRaisesRegex ValueError must have same dtype parametrize register_parametrization module weight ChangeDtypeInverse assertTrue parametrize is_parametrized module assertEqual len module parametrizations weight assertTrue isinstance module parametrizations weight Identity Cannot change shape ChangeShapeInverse Identity right_inverse x x - assertRaisesRegex ValueError must have same shape parametrize register_parametrization module weight ChangeShapeInverse assertTrue parametrize is_parametrized module assertEqual len module parametrizations weight assertTrue isinstance module parametrizations weight Identity FIXME Rewrite test using functions depending LAPACK remove ` skipIfNoLapack ` see skipIfNoLapack swap True False test_multiple_inputs_parametrization A parametrization several outputs RankOne nn Module forward x y Form rank- matrix pair vectors x unsqueeze - y unsqueeze - right_inverse Y We project given matrix onto rank matrices U S Vh = torch linalg svd Y full_matrices=False S ordered decreasing way s _sqrt = S sqrt unsqueeze - U s _sqrt Vh s _sqrt Simple parametrisation Double nn Module forward x x right_inverse w w model = nn Linear Test one parametrization parametrize register_parametrization model weight RankOne assertTrue hasattr model parametrizations assertTrue parametrize is_parametrized model assertTrue parametrize is_parametrized model weight assertTrue hasattr model parametrizations weight original assertIn original model parametrizations weight _parameters assertTrue hasattr model parametrizations weight original assertIn original model parametrizations weight _parameters assertFalse parametrize is_parametrized model bias assertNotIn weight model _parameters Result should rank assertEqual torch linalg matrix_rank model weight item assertRaisesRegex ValueError leave_parametrized=False Cannot remove parametrization multiple inputs leave parametrized parametrize remove_parametrizations model weight leave_parametrized=False Remove parametrization check consistency parametrize remove_parametrizations model weight leave_parametrized=True assertFalse hasattr model parametrizations assertEqual model __class__ nn Linear assertFalse parametrize is_parametrized model assertEqual torch linalg matrix_rank model weight item assertIn weight model _parameters Registering parametrizations one input top one multiple inputs should work init_weight = model weight clone parametrize register_parametrization model weight RankOne Projecting rank matrix onto matrices rank one does change matrix assertEqual init_weight model weight parametrize register_parametrization model weight Double The matrix now twice initial matrix assertEqual init_weight model weight Multiplying scalar does change rank assertEqual torch linalg matrix_rank model weight item The model has now three parameters assertEqual len list model parameters sgd = torch optim SGD model parameters lr= Test backward Should throw _ range sgd zero_grad loss = model weight T model bias sum loss backward sgd step Same drill before removing should work expected assertRaisesRegex ValueError leave_parametrized=False Cannot remove parametrization multiple inputs leave parametrized parametrize remove_parametrizations model weight leave_parametrized=False Remove parametrization check consistency parametrize remove_parametrizations model weight leave_parametrized=True assertFalse hasattr model parametrizations assertEqual model __class__ nn Linear assertFalse parametrize is_parametrized model assertEqual torch linalg matrix_rank model weight item assertIn weight model _parameters The model has now two parameters assertEqual len list model parameters Test backward Should throw sgd = torch optim SGD model parameters lr= _ range sgd zero_grad loss = model weight T model bias sum loss backward sgd step FIXME Rewrite test using functions depending LAPACK remove ` skipIfNoLapack ` see skipIfNoLapack swap True False test_caching_parametrization r Test caching system parametrization Define couple matrix parametrizations Skew nn Module forward X X = X tril - X - X T Orthogonal nn Module forward X Id = torch eye X size device=X device torch linalg solve Id + X Id - X model = nn Linear parametrize register_parametrization model weight Skew parametrize register_parametrization model weight Orthogonal Test caching system works parametrize cached X = model weight Y = model weight assertEqual id X id Y FIXME Rewrite test using functions depending LAPACK remove ` skipIfNoLapack ` see skipIfNoLapack swap True False test_caching_parametrization_with_transfer_parametrizations_and_params r Test transferring parametrizations doesn t cause issues caching Skew nn Module forward X X = X tril - X - X T Orthogonal nn Module forward X Id = torch eye X size device=X device torch linalg solve Id + X Id - X model = nn Linear parametrize register_parametrization model weight Skew parametrize register_parametrization model weight Orthogonal to_model = nn Linear parametrize transfer_parametrizations_and_params model to_model parametrize cached X = model weight Y = model weight assertEqual id X id Y A = to_model weight B = to_model weight assertEqual id A id B test results distinct objects each module assertNotEqual id A id X swap True False test_parametrization_same_training_mode r Test training mode updated parametrization registration Identity nn Module forward X X module = nn Linear module eval parametrize register_parametrization module weight Identity assertFalse module parametrizations weight training module train parametrize register_parametrization module weight Identity eval assertTrue module parametrizations weight training assertTrue module parametrizations weight training swap True False test_type_before_parametrizations r Test type_before_parametrizations always retrieves original type Identity nn Module forward X X model = nn Linear original_type = type model assertTrue parametrize type_before_parametrizations model == original_type parametrize register_parametrization model weight Identity assertTrue parametrize type_before_parametrizations model == original_type skipIfTorchDynamo Not applicable see https github com pytorch pytorch issues swap True False test_deepcopy_after_parametrization r Test we able create deepcopy module when s parametrized AddOne nn Module forward x x + ModelWithoutDeepcopy nn Module __init__ - None super __init__ weight = nn Parameter torch tensor requires_grad=True bias = nn Parameter torch tensor requires_grad=True attr = ActualModel ModelWithoutDeepcopy Emulate custom implementation deepcopying __deepcopy__ memo result = __new__ __class__ memo id = result result __dict__ = deepcopy __dict__ memo result check_deepcopy m nn Module m nn Module w = m parametrizations weight original w = m parametrizations weight original b = m parametrizations bias original parametrize is_parametrized m bias m bias b = m parametrizations bias original parametrize is_parametrized m bias m bias Weights biases attributes should equal they must different objects assertEqual m __dict__ keys m __dict__ keys assertIsNot m m assertEqual w w assertIsNot w w assertEqual b b assertIsNot b b assertEqual m attr m attr assertIsNot m attr m attr model ModelWithoutDeepcopy ActualModel General check we able create deepcopy parametrize register_parametrization model weight AddOne check_deepcopy model deepcopy model Check works models several parametrized tensors parametrize register_parametrization model bias AddOne check_deepcopy model deepcopy model Check works models where tensors have more than one parametrization parametrize register_parametrization model weight AddOne check_deepcopy model deepcopy model swap True False test_transfer_parametrizations_and_params r Test all parametrizations their associated parameters transferred AddOne nn Module forward x x + Double nn Module forward x x right_inverse x x MinusOne nn Module forward x x - model = nn Linear parametrize register_parametrization model weight AddOne parametrize register_parametrization model weight Double parametrize register_parametrization model weight MinusOne hold_weight = model weight to_model = torch ao nn qat Linear qconfig=torch ao quantization get_default_qconfig parametrize transfer_parametrizations_and_params model to_model checks final original value correct to_model parametrized assertTrue torch nn utils parametrize is_parametrized to_model weight assertEqual model weight to_model weight assertEqual model parametrizations weight original to_model parametrizations weight original check transfer didn t affect original value assertEqual hold_weight model weight get_swap_module_params_on_conversion When using swap_tensors path needed so autograd graph alive anymore del hold_weight testing changes one set parametrizations do affect other parametrize remove_parametrizations to_model weight assertFalse torch nn utils parametrize is_parametrized to_model weight assertTrue torch nn utils parametrize is_parametrized model weight also test parameters don t exist to_model get transferred model test_param = Parameter torch randn assertTrue hasattr to_model test_param parametrize register_parametrization model test_param Double hold_test_param = model test_param parametrize transfer_parametrizations_and_params model to_model test_param check previously missing params got transferred correctly assertEqual model test_param to_model test_param assertEqual model parametrizations test_param original to_model parametrizations test_param original check new transfer didn t change value from_module assertEqual hold_test_param model test_param swap True False test_transfer_parametrizations_and_params_right_inverse r Test all parametrizations their associated parameters transferred Double nn Module forward x x right_inverse x x model = nn Linear parametrize register_parametrization model weight Double hold_weight = model weight to_model = torch ao nn qat Linear qconfig=torch ao quantization get_default_qconfig parametrize transfer_parametrizations_and_params model to_model check transfer occurs successfully assertEqual model weight to_model weight assertEqual model parametrizations weight original to_model parametrizations weight original check transfer doesn t affect from_model weight assertEqual hold_weight model weight swap True False test_transfer_parametrizations_and_params_single_param r Test all parametrizations their associated parameters transferred AddOne nn Module forward x x + Double nn Module forward x x MinusOne nn Module forward x x - model = nn Linear bias=True parametrize register_parametrization model weight AddOne parametrize register_parametrization model weight Double parametrize register_parametrization model weight MinusOne parametrize register_parametrization model bias AddOne parametrize register_parametrization model bias Double parametrize register_parametrization model bias MinusOne to_model = torch ao nn qat Linear bias=True qconfig=torch ao quantization get_default_qconfig parametrize transfer_parametrizations_and_params model to_model weight check weight only weight transferred assertEqual model weight to_model weight assertEqual model parametrizations weight original to_model parametrizations weight original assertTrue bias to_model parametrizations FIXME Rewrite test using functions depending LAPACK remove ` skipIfNoLapack ` see skipIfNoLapack swap True False test_transfer_parametrizations_and_params_many_to_one A parametrization several outputs RankOne nn Module forward x y Form rank- matrix pair vectors x unsqueeze - y unsqueeze - right_inverse Y We project given matrix onto rank matrices U S Vh = torch linalg svd Y full_matrices=False S ordered decreasing way s _sqrt = S sqrt unsqueeze - U s _sqrt Vh s _sqrt Double nn Module forward x x model = nn Linear parametrize register_parametrization model weight RankOne parametrize register_parametrization model weight Double hold_weight = model weight to_model = torch ao nn qat Linear qconfig=torch ao quantization get_default_qconfig parametrize transfer_parametrizations_and_params model to_model checks final original value correct to_model parametrized assertTrue torch nn utils parametrize is_parametrized to_model weight assertEqual model weight to_model weight assertEqual model parametrizations weight original to_model parametrizations weight original assertEqual model parametrizations weight original to_model parametrizations weight original check transfer didn t affect original value assertEqual hold_weight model weight testing changes one set parametrizations do affect other model test_param = Parameter torch randn assertTrue hasattr to_model test_param parametrize register_parametrization model test_param RankOne hold_test_param = model test_param parametrize transfer_parametrizations_and_params model to_model test_param also check previously missing params got transferred correctly assertEqual model test_param to_model test_param assertEqual model parametrizations test_param original to_model parametrizations test_param original assertEqual model parametrizations test_param original to_model parametrizations test_param original check new transfer didn t change value from_module assertEqual hold_test_param model test_param swap True False test_new_spectral_norm set_default_dtype torch double input = torch randn m = nn Linear m = torch nn utils parametrizations spectral_norm m spectral_norm_m = m parametrizations weight assertEqual spectral_norm_m _u size torch Size m weight size parametrizations weight original should trainable assertTrue hasattr m parametrizations weight original assertTrue original m parametrizations weight _parameters u should just reused buffer assertTrue hasattr spectral_norm_m _u assertTrue _u spectral_norm_m _buffers assertTrue _v spectral_norm_m _buffers weight should plain attribute counted buffer param assertIsNotNone m weight assertFalse weight m _buffers assertFalse weight m _parameters should also sharing storage ` weight_orig ` assertEqual m parametrizations weight original storage m weight storage assertEqual m parametrizations weight original size m weight size assertEqual m parametrizations weight original stride m weight stride m = torch nn utils parametrize remove_parametrizations m weight spectral_norm only parametrization assertFalse hasattr m parametrizations assertTrue weight m _parameters We can register spectral_norm multiple times same parameter multiple parameters same module m = torch nn utils parametrizations spectral_norm m weight m = torch nn utils parametrizations spectral_norm m weight m = torch nn utils parametrizations spectral_norm m bias If we remove parametrization bias weight still parametrized Removing parametrization runs forward eval mode leave_parametrized=True m = torch nn utils parametrize remove_parametrizations m bias assertTrue bias m _parameters assertTrue hasattr m parametrizations assertFalse weight m _parameters m = torch nn utils parametrize remove_parametrizations m weight Neither weight bias parametrized assertFalse hasattr m parametrizations assertTrue weight m _parameters assertFalse torch nn utils parametrize is_parametrized m test correctness training eval modes cpu multi-gpu settings apply_dp True False apply_dp TEST_MULTIGPU continue device = torch device cuda maybe_wrap m torch nn DataParallel m device = torch device cpu maybe_wrap m m requires_grad True False get_modules m = nn Linear device m weight requires_grad_ requires_grad m = torch nn utils parametrizations spectral_norm m wrapped_m = maybe_wrap m spectral_norm_m = m parametrizations weight m wrapped_m spectral_norm_m input = torch randn device=device m wrapped_m spectral_norm_m = get_modules assertTrue hasattr spectral_norm_m _u u = spectral_norm_m _u clone v = spectral_norm_m _v clone TEST TRAINING BEHAVIOR We perform GD first modify initial matrix opt = torch optim SGD wrapped_m parameters lr= opt zero_grad wrapped_m input sum backward opt step out = wrapped_m input requires_grad run forward again assert u v updated assertNotEqual u spectral_norm_m _u assertNotEqual v spectral_norm_m _v assert backprop reaches original weight can t use gradcheck because function changes we activate through training mode requires_grad torch autograd grad out sum m parametrizations weight original test backward works multiple forwards uses training mode so we need reset ` u ` ` v ` vectors same value beginning finite difference test pass saved_u = spectral_norm_m _u clone saved_v = spectral_norm_m _v clone fn input spectral_norm_m _u data copy_ saved_u spectral_norm_m _v data copy_ saved_v out = wrapped_m input out = wrapped_m input out + out Make sure we can compute gradients wrt all parameters case double forward fn input clone requires_grad_ sum backward gradcheck fn input clone requires_grad_ check_batched_grad=False test removing spectral norm module needs eval mode we d like avoid doing another power iteration m wrapped_m _ = get_modules pre_remove_out = wrapped_m input get_swap_module_params_on_conversion When using swap_tensors path needed so autograd graph alive anymore pre_remove_out_ref = pre_remove_out detach del pre_remove_out pre_remove_out_ref = pre_remove_out m eval m = torch nn utils parametrize remove_parametrizations m weight assertEqual wrapped_m input pre_remove_out_ref torch nn utils parametrizations spectral_norm m _ range pre_remove_out = wrapped_m input get_swap_module_params_on_conversion When using swap_tensors path needed so autograd graph alive anymore pre_remove_out_ref = pre_remove_out detach del pre_remove_out pre_remove_out_ref = pre_remove_out m eval m = torch nn utils parametrize remove_parametrizations m weight assertEqual wrapped_m input pre_remove_out_ref TEST EVAL BEHAVIOR m wrapped_m spectral_norm_m = get_modules wrapped_m input last_train_out = wrapped_m input last_train_u = spectral_norm_m _u clone last_train_v = spectral_norm_m _v clone wrapped_m zero_grad wrapped_m eval eval_out = wrapped_m input assert eval gives same result last training iteration assertEqual eval_out last_train_out assert doing more iteration eval don t change things assertEqual eval_out wrapped_m input assertEqual last_train_u spectral_norm_m _u assertEqual last_train_v spectral_norm_m _v FIXME code below flaky when executed DataParallel see https github com pytorch pytorch issues apply_dp continue test backward works multiple forwards mixed training eval modes uses training mode so we need reset ` u ` ` v ` vectors same value beginning finite difference test pass saved_u = spectral_norm_m _u clone saved_v = spectral_norm_m _v clone fn input spectral_norm_m _u data copy_ saved_u spectral_norm_m _v data copy_ saved_v wrapped_m train out = wrapped_m input wrapped_m eval out = wrapped_m input wrapped_m train out = wrapped_m input wrapped_m eval out = wrapped_m input out + out + out + out gradcheck fn input clone requires_grad_ assert backprop reaches weight_orig eval requires_grad fn weight wrapped_m input gradcheck fn m parametrizations weight original test_register_parametrization_no_grad r Test possible register parametrization without gradient SplitAndCat nn Module right_inverse x split tensor two halves torch split x x shape forward x x torch cat x x model = nn Linear model weight requires_grad = False parametrize register_parametrization model weight SplitAndCat making sure parameterized decomposed Tensors both have requires_grad == False assertFalse model weight requires_grad assertFalse model parametrizations weight original requires_grad assertFalse model parametrizations weight original requires_grad swap True False test_new_spectral_norm_load_state_dict activate_times inp = torch randn m = nn Linear snm = torch nn utils parametrizations spectral_norm m snm train _ range activate_times snm inp state_dict = deepcopy snm state_dict assertEqual parametrizations weight original bias parametrizations weight _v parametrizations weight _u set state_dict keys test non-strict loading works non_strict_state_dict = deepcopy state_dict non_strict_state_dict nonsense = nonsense assertRaisesRegex RuntimeError r Unexpected key\ s\ state_dict nonsense snm load_state_dict non_strict_state_dict strict=True snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict parametrizations weight original snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict parametrizations weight _u snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict parametrizations weight _v snm load_state_dict non_strict_state_dict strict=False non_strict_state_dict weight = snm weight detach clone set W buffer snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict _metadata parametrizations weight remove metadata info snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict weight remove W buffer snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict bias snm load_state_dict non_strict_state_dict strict=False normal state_dict test re-wrapping does matter m = torch nn utils parametrize remove_parametrizations snm weight snm = torch nn utils parametrizations spectral_norm m snm load_state_dict state_dict torch no_grad snm eval out _eval = snm inp snm train out _train = snm inp out _train = snm inp snm eval out _eval = snm inp test re-wrapping does matter m = torch nn utils parametrize remove_parametrizations snm weight snm = torch nn utils parametrizations spectral_norm m Test normal loading snm load_state_dict state_dict torch no_grad snm eval assertEqual out _eval snm inp snm train assertEqual out _train snm inp assertEqual out _train snm inp snm eval assertEqual out _eval snm inp swap True False test_new_spectral_norm_dim inp = torch randn m = nn ConvTranspose d m = torch nn utils parametrizations spectral_norm m snm = m parametrizations weight should run into incompatible shapes m inp check u refers same dimension assertEqual snm _u shape m parametrizations weight original shape swap True False test_new_spectral_norm_forward input = torch randn m = nn Linear m = torch nn utils parametrizations spectral_norm m snm = m parametrizations weight naive forward _weight = m parametrizations weight original _bias _v = m bias snm _v _weight_mat = _weight view _weight size - _u = torch mv _weight_mat _v _u = F normalize _u dim= eps= e- _v = torch mv _weight_mat t _u _v = F normalize _v dim= eps= e- _weight data = torch dot _u torch matmul _weight_mat _v out_hat = torch nn functional linear input _weight _bias expect_out = m input assertEqual expect_out out_hat swap True False skipIfTorchDynamo Test does work TorchDynamo test_new_spectral_norm_value test spectral norm = top singular value fact properly calculated using example simple diagonal matrix dtype torch float torch cfloat m = nn Linear dtype=dtype torch no_grad set weight diagonal x = torch diagonal m weight m weight = nn Parameter torch diag x torch nn utils parametrizations spectral_norm m weights should rescaled spectral norm i e largest diagonal element norm expected = torch diag x x abs max assertEqual m weight data expected skipIfNoLapack swap True False test_orthogonal_parametrization Orthogonal implements algorithms x parametrizations times options use_trivialization assert_is_orthogonal X n k = X size - X size - n k X = X mT n k = k n Id = torch eye k dtype=X dtype device=X device expand X size - k k eps = n torch finfo X dtype eps torch testing assert_close X mH X Id atol=eps rtol= assert_weight_allclose_Q weight W Test weight equal Q part QR decomposition W its transpose matrix wide wide_matrix = W size - W size - wide_matrix W = W mT Q R = torch linalg qr W Q = R diagonal dim =- dim =- sgn unsqueeze - wide_matrix Q = Q mT torch testing assert_close Q weight atol= e- rtol= shape dtype use_linear product square tall wide torch float torch complex True False Conv d does support complex yet use_linear continue use_linear input = torch randn shape dtype=dtype input = torch randn shape + shape + dtype=dtype parametrization use_trivialization product matrix_exp cayley householder False True right_inverse Cayley matrix_exp implemented use_trivialization=False See Note right_inverse expm cayley can_initialize = use_trivialization parametrization == householder We generate them every time always start fresh weights use_linear m = nn Linear shape dtype=dtype m = nn Conv d shape dtype=dtype We do support householder complex inputs See Note Householder complex When using swap_tensors path needed so autograd graph alive anymore get_swap_module_params_on_conversion w_init = m weight detach clone w_init = m weight clone parametrization == householder m weight is_complex msg = householder parametrization does support complex tensors assertRaisesRegex ValueError msg torch nn utils parametrizations orthogonal m weight parametrization use_trivialization=use_trivialization continue wide_matrix = w_init size - w_init size - torch nn utils parametrizations orthogonal m weight parametrization use_trivialization=use_trivialization Forwards works expected assertEqual w_init shape m weight shape assert_is_orthogonal m weight can_initialize assert_weight_allclose_Q m weight w_init Initializing given orthogonal matrix works X = torch randn_like m weight wide_matrix X = X mT w_new = torch linalg qr X Q wide_matrix w_new = w_new mT can_initialize m weight = w_new torch testing assert_close w_new m weight atol= e- rtol= msg = assign matrix exponential Cayley parametrization assertRaisesRegex NotImplementedError msg m weight = w_new Initializing non-orthogonal matrix makes m weight Q part given matrix w_new = torch randn_like m weight can_initialize m weight = w_new assert_weight_allclose_Q m weight w_new msg = assign matrix exponential Cayley parametrization assertRaisesRegex NotImplementedError msg m weight = w_new opt = torch optim SGD m parameters lr= _ range opt zero_grad m input norm backward grad = m parametrizations weight original grad assertIsNotNone grad We do update upper triangular part matrix tall tril wide grad size - = grad size - zeros_grad = grad triu zeros_grad = grad tril - assertEqual zeros_grad torch zeros_like zeros_grad The gradient diagonal can only imaginary because skew-Hermitian matrix has imaginary diagonal diag_grad = grad diagonal dim =- dim =- grad is_complex diag_grad = diag_grad real assertEqual diag_grad torch zeros_like diag_grad opt step assert_is_orthogonal m weight skipIfNoLapack swap True False test_orthogonal_errors m = nn Linear assertRaisesRegex ValueError has one torch nn utils parametrizations orthogonal m weight foo assertRaisesRegex ValueError Expected matrix torch nn utils parametrizations orthogonal m bias torch nn utils parametrizations orthogonal m weight assertRaisesRegex ValueError matrices shape m weight = torch randn torch nn utils parametrize remove_parametrizations m weight swap True False test_weight_norm_state_dict_compat m = nn Linear m = torch nn utils weight_norm m old_dict = m state_dict m = nn Linear m = torch nn utils parametrizations weight_norm m m load_state_dict old_dict input = torch randn assertEqual m input m input swap True False test_weight_norm_pickle m = nn Linear m = torch nn utils parametrizations weight_norm m assertRaisesRegex RuntimeError state_dict pickle dumps m swap True False test_weight_norm_deepcopy m = nn Linear m = torch nn utils parametrizations weight_norm m m = deepcopy m input = torch randn assertEqual m input m input swap True test_wrapper_subclass_parametrization Subclassify nn Module forward X TwoTensor X X UnSubclassify nn Module forward X X IdentityWithRightInverse nn Module forward X X right_inverse X TwoTensor X X _check_parametrization parametrization type_before_registration type_after_registration leave_parametrized=False type_after_right_inverse=None model = nn Linear buf = torch randn model buf = torch nn Buffer buf type_before_registration == TwoTensor type_after_registration == Tensor model _apply lambda t TwoTensor t t initial_weight = model weight detach clone initial_weight_id = id model weight initial_buf = model buf detach clone initial_buf_id = id model buf type_original_weight = type_before_registration type_after_right_inverse None type_after_right_inverse type_original_buf = Tensor type_original_weight nn Parameter type_original_weight type_after_removal_buf = type_after_registration leave_parametrized type_original_buf leave_parametrized type_after_registration Tensor type_after_removal_weight = nn Parameter type_after_removal_weight = type_after_registration type_after_removal_weight = type_original_weight parametrize register_parametrization model weight parametrization parametrize register_parametrization model buf parametrization assertTrue hasattr model parametrizations assertTrue parametrize is_parametrized model assertFalse parametrize is_parametrized model bias checks weight assertTrue parametrize is_parametrized model weight assertTrue isinstance model parametrizations weight original nn Parameter assertTrue type model parametrizations weight original type_original_weight assertNotIn weight model _parameters assertTrue type model weight type_after_registration checks buf assertTrue parametrize is_parametrized model buf assertFalse isinstance model parametrizations buf original nn Parameter assertTrue type model parametrizations buf original type_original_buf assertTrue type model buf type_after_registration parametrize remove_parametrizations model weight leave_parametrized=leave_parametrized parametrize remove_parametrizations model buf leave_parametrized=leave_parametrized assertFalse hasattr model parametrizations assertEqual model __class__ nn Linear checks weight assertTrue type model weight type_after_removal_weight assertTrue isinstance model weight nn Parameter assertEqual id model weight initial_weight_id checks buf assertTrue type model buf type_after_removal_buf assertFalse isinstance model buf nn Parameter assertEqual id model buf initial_buf_id leave_parametrized type_after_right_inverse None assertEqual model weight initial_weight assertEqual model buf initial_buf _check_parametrization Subclassify nn Parameter TwoTensor _check_parametrization UnSubclassify TwoTensor Tensor _check_parametrization IdentityWithRightInverse nn Parameter TwoTensor type_after_right_inverse=TwoTensor _check_parametrization Subclassify nn Parameter TwoTensor leave_parametrized=True _check_parametrization UnSubclassify TwoTensor Tensor leave_parametrized=True _check_parametrization IdentityWithRightInverse nn Parameter TwoTensor leave_parametrized=True type_after_right_inverse=TwoTensor TestNNParametrizationDevice NNTestCase swap True False test_weight_norm_parametrization device dtype torch float torch bfloat input = torch randn dtype=dtype device=device m = nn Linear dtype=dtype device=device expected_output = m input add weight normalization m = torch nn utils parametrizations weight_norm m assertEqual m parametrizations weight original size m weight size assertEqual m parametrizations weight original size assertEqual m input expected_output remove weight norm torch nn utils parametrize remove_parametrizations m weight assertFalse hasattr m parametrizations assertEqual m input expected_output test dim= m = torch nn utils parametrizations weight_norm m dim= assertEqual m parametrizations weight original size m weight size assertEqual m parametrizations weight original size assertEqual m input expected_output test dim=None m = nn Linear dtype=dtype device=device expected_output = m input m = torch nn utils parametrizations weight_norm m dim=None assertEqual m input expected_output only_for = cpu cuda instantiate_device_type_tests TestNNParametrizationDevice globals only_for=only_for instantiate_parametrized_tests TestNNParametrization __name__ == __main__ run_tests