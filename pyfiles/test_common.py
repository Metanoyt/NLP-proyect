mypy allow-untyped-defs torch torch nn nn torch distributed _shard sharded_tensor ShardedTensor SimpleMegatronLM nn Module __init__ linear_size rank=None dtype=torch float super __init__ fc = nn Linear linear_size dtype=dtype gelu = nn GELU fc = nn Linear linear_size dtype=dtype rank None fc cuda rank fc cuda rank forward inp fc gelu fc inp get_weights isinstance fc weight ShardedTensor weight = fc weight local_tensor weight = fc weight isinstance fc weight ShardedTensor weight = fc weight local_tensor weight = fc weight weight weight get_biases fc bias fc bias get_weight_grads fc weight grad fc weight grad get_bias_grads fc bias grad fc bias grad