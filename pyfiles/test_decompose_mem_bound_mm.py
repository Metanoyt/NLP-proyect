Owner s module inductor logging unittest torch torch _inductor torch _dynamo utils counters torch _inductor fx_passes decompose_mem_bound_mm check_device torch _inductor test_case run_tests TestCase torch _inductor utils run_and_get_code torch testing FileCheck torch testing _internal common_utils instantiate_parametrized_tests is_navi _arch parametrize patch_test_members TEST_XPU torch testing _internal inductor_utils GPU_TYPE HAS_CUDA_AND_TRITON torch testing _internal triton_utils requires_gpu MyModule torch nn Module __init__ n_input int n_output int has_bias bool device=GPU_TYPE - None super __init__ linear = torch nn Linear n_input n_output bias=has_bias forward x torch Tensor - torch Tensor linear x MyModule torch nn Module __init__ - None super __init__ forward input input output = torch bmm input input output MyModule torch nn Module __init__ - None super __init__ forward input input output = torch mm input input output TestDecomposeAddMM torch nn Module __init__ - None super __init__ forward z torch Tensor x torch Tensor y torch Tensor - torch Tensor torch ops aten addmm default z x y requires_gpu unittest skipIf TEST_XPU Intel GPU has enabled decompose_mem_bound_mm PASS torch _inductor fx_passes decompose_mem_bound_mm py torch _inductor config patch post_grad_fusion_options= decompose_mm_pass instantiate_parametrized_tests TestDecomposeMemMM TestCase __init__ method_name= runTest methodName= runTest super __init__ method_name methodName atol = e- rtol = e- setup_tolerance rtol=None atol=None rtol None rtol = rtol atol None atol = atol rtol = rtol atol = atol compare_dict_tensors ref_dict res_dict rtol=None atol=None setup_tolerance rtol atol len set ref_dict keys = len set res_dict keys False key ref_dict keys key = _orig_mod + key assert key res_dict f key does exist traced module torch allclose ref_dict key res_dict key rtol=self rtol atol=self atol False True compare_pred module traced input rtol=None atol=None setup_tolerance rtol atol ref = module input res = traced input assertEqual ref res rtol=self rtol atol=self atol compare_parameters module traced rtol=None atol=None setup_tolerance rtol atol ref_params = dict module named_parameters res_params = dict traced named_parameters assertTrue compare_dict_tensors ref_params res_params rtol=self rtol atol=self atol compare_gradients module traced rtol=None atol=None setup_tolerance rtol atol ref_grad = key param grad key param module named_parameters res_grad = key param grad key param traced named_parameters assertTrue compare_dict_tensors ref_grad res_grad rtol=self rtol atol=self atol parametrize b m k n should_decompose True False False test_decompose_bmm b m n k should_decompose torch _logging set_logs inductor=logging DEBUG mat = torch randn b m k device=GPU_TYPE requires_grad_ True mat = torch randn b k n device=GPU_TYPE requires_grad_ True counters clear module = MyModule GPU_TYPE traced = torch compile module input = mat mat ref = module input res = traced input compare_pred module traced input expected_val = should_decompose HAS_CUDA_AND_TRITON assertEqual counters inductor decompose_bmm expected_val ref sum backward res sum backward compare_parameters module traced compare_gradients module traced expected_val = should_decompose HAS_CUDA_AND_TRITON assertEqual counters inductor decompose_bmm expected_val counters clear parametrize b m k n should_decompose True False test_decompose_bmm_cpu b m n k should_decompose torch _logging set_logs inductor=logging DEBUG mat = torch randn b m k mat = torch randn b k n counters clear module = MyModule traced = torch compile module input = mat mat compare_pred module traced input expected_val = should_decompose assertEqual counters inductor decompose_bmm expected_val counters clear parametrize m k n should_decompose True False False parametrize has_bias True False test_decompose_linear m n k has_bias should_decompose torch _logging set_logs inductor=logging DEBUG input = torch randn m k device=GPU_TYPE requires_grad_ True counters clear module = MyModule k n has_bias GPU_TYPE traced = torch compile module input = input ref = module input res = traced input compare_pred module traced input expected_val = should_decompose HAS_CUDA_AND_TRITON has_bias assertEqual counters inductor decompose_addmm expected_val assertEqual counters inductor decompose_mm expected_val decompose_mm_fwd = counters inductor decompose_mm ref sum backward res sum backward compare_parameters module traced compare_gradients module traced assertEqual counters inductor decompose_mm - decompose_mm_fwd expected_val counters clear We have increase tolerance navi because all fp bf GEMMs operations have accuracy issue caused hardware limitation patch_test_members atol e- is_navi _arch e- rtol e- is_navi _arch e- parametrize m k n should_decompose True False False parametrize has_bias True False test_decompose_linear_mixed_precision m n k has_bias should_decompose torch amp autocast device_type=GPU_TYPE dtype=torch bfloat torch _logging set_logs inductor=logging DEBUG input = torch randn m k device=GPU_TYPE requires_grad_ True counters clear module = MyModule k n has_bias GPU_TYPE traced = torch compile module input = input ref = module input res = traced input compare_pred module traced input expected_val = should_decompose HAS_CUDA_AND_TRITON has_bias assertEqual counters inductor decompose_addmm expected_val assertEqual counters inductor decompose_mm expected_val decompose_mm_fwd = counters inductor decompose_mm ref sum backward res sum backward compare_parameters module traced compare_gradients module traced assertEqual counters inductor decompose_mm - decompose_mm_fwd expected_val counters clear parametrize m k n should_decompose True False False parametrize has_bias True False test_decompose_mm m n k has_bias should_decompose torch _logging set_logs inductor=logging DEBUG mat = torch randn m k device=GPU_TYPE requires_grad_ True mat = torch randn k n device=GPU_TYPE requires_grad_ True counters clear module = MyModule GPU_TYPE traced = torch compile module input = mat mat ref = module input res = traced input compare_pred module traced input expected_val = should_decompose HAS_CUDA_AND_TRITON assertEqual counters inductor decompose_mm expected_val decompose_mm_fwd = counters inductor decompose_mm ref sum backward res sum backward compare_parameters module traced compare_gradients module traced expected_val = should_decompose HAS_CUDA_AND_TRITON assertEqual counters inductor decompose_mm - decompose_mm_fwd expected_val counters clear parametrize m k n should_decompose True False True test_decompose_mm_cpu m n k should_decompose torch _logging set_logs inductor=logging DEBUG mat = torch randn m k mat = torch randn k n counters clear module = MyModule traced = torch compile module input = mat mat compare_pred module traced input expected_val = should_decompose assertEqual counters inductor decompose_mm expected_val counters clear We have increase tolerance navi because all fp bf GEMMs operations have accuracy issue caused hardware limitation patch_test_members atol e- is_navi _arch e- rtol e- is_navi _arch e- parametrize m k n should_decompose True False False parametrize has_bias True False test_decompose_mm_mixed_precision m n k has_bias should_decompose torch amp autocast device_type=GPU_TYPE dtype=torch bfloat torch _logging set_logs inductor=logging DEBUG mat = torch randn m k device=GPU_TYPE requires_grad_ True mat = torch randn k n device=GPU_TYPE requires_grad_ True counters clear module = MyModule GPU_TYPE traced = torch compile module input = mat mat ref = module input res = traced input compare_pred module traced input expected_val = should_decompose HAS_CUDA_AND_TRITON assertEqual counters inductor decompose_mm expected_val decompose_mm_fwd = counters inductor decompose_mm ref sum backward res sum backward compare_parameters module traced compare_gradients module traced expected_val = should_decompose HAS_CUDA_AND_TRITON assertEqual counters inductor decompose_mm - decompose_mm_fwd expected_val counters clear unittest skip parametrize m k n should_decompose True parametrize has_bias True False test_dynamic_shape m n k has_bias should_decompose torch _logging set_logs inductor=logging DEBUG input = torch randn m k device=GPU_TYPE requires_grad_ True counters clear module = MyModule k n has_bias GPU_TYPE traced = torch compile module dynamic=True input = input ref = module input res = traced input compare_pred module traced input expected_val = should_decompose HAS_CUDA_AND_TRITON has_bias assertEqual counters inductor decompose_addmm expected_val ref sum backward res sum backward compare_parameters module traced compare_gradients module traced expected_val = HAS_CUDA_AND_TRITON expected_val = has_bias assertEqual counters inductor decompose_mm expected_val counters clear test_realize_input m = k = n = torch _logging set_logs inductor=logging DEBUG input = torch randn m k device=GPU_TYPE T contiguous input = torch randn k n device=GPU_TYPE torch compile foo x y x T contiguous y _ code = run_and_get_code foo input input GPU_TYPE == xpu only kernel generated XPU stack FileCheck check_count run exactly=True run code two kernels generated FileCheck check_count run exactly=True run code test_check_device m = k = n = torch _logging set_logs inductor=logging DEBUG input = torch randn m k device=GPU_TYPE input = torch randn k n device=GPU_TYPE assertTrue check_device input input assertFalse check_device input input device= cpu input = torch randn m k input = torch randn k n assertTrue check_device input input device= cpu assertFalse check_device input input input = torch randn m k device=GPU_TYPE input = torch randn k n assertFalse check_device input input device= gpu assertFalse check_device input input device= cpu assertFalse check_device input input device= mtia torch _inductor config patch post_grad_fusion_options= decompose_mm_pass skip_dynamic_shape_dim_check True test_dynamic_shape_decompose_addmm m k n = input = torch randn m k device=GPU_TYPE requires_grad_ False weight = torch randn k n device=GPU_TYPE requires_grad_ False bias = torch randn n device=GPU_TYPE requires_grad_ False counters clear module = TestDecomposeAddMM GPU_TYPE traced = torch compile module dynamic=True input = bias input weight compare_pred module traced input assertEqual counters inductor decompose_addmm counters clear __name__ == __main__ run_tests