Owner s oncall distributed sys typing Optional torch torch nn nn torch nn functional F torch distributed dist torch distributed algorithms _comm_hooks default_hooks torch distributed distributed_c d _get_default_group torch distributed fsdp FullyShardedDataParallel FSDP MixedPrecision torch distributed fsdp fully_sharded_data_parallel ShardingStrategy torch distributed fsdp wrap ModuleWrapPolicy torch testing _internal common_distributed requires_accelerator_dist_backend requires_nccl_version skip_but_pass_in_sandcastle_if skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests dist is_available print Distributed available skipping tests file=sys stderr sys exit device_type = acc type acc = torch accelerator current_accelerator True cpu BFLOAT _AVAILABLE = torch cuda is_bf _supported torch xpu is_bf _supported Net nn Module __init__ has_wrapping sharding_strategy mixed_precision=None ensure determinism torch manual_seed torch get_device_module device_type manual_seed super __init__ has_wrapping net = FSDP nn Sequential nn Linear nn ReLU FSDP nn Linear device_id=torch accelerator current_device_index sharding_strategy=sharding_strategy mixed_precision=mixed_precision device_id=torch accelerator current_device_index sharding_strategy=sharding_strategy mixed_precision=mixed_precision net = nn Sequential nn Linear nn ReLU nn Linear out = nn Linear forward x out F relu net x DummyState __slots__ = process_group noise __init__ process_group dist ProcessGroup noise int process_group = process_group noise = noise DummyHook dummy_hook_for_no_shard_fsdp state DummyState grad torch Tensor This communication hook illustration testing purpose only This communication hook used during FSDP ` ` NO_SHARD ` ` training It adds some noise provided ` ` grad ` ` parameter uses ` ` all_reduce ` ` communicate full flattened unsharded gradient grad add_ state noise dist all_reduce grad group=state process_group custom_reduce_scatter output input group=None This function illustrative purpose only It meant implement custom reduce-scatter flattened tensor all processes group Currently no-op dummy_hook_for_sharded_fsdp state DummyState grad torch Tensor output torch Tensor This communication hook illustration testing purposes only This communication hook used during FSDP ` ` FULL_SHARD ` ` ` ` SHARD_GRAD_OP ` ` training It adds some noise provided ` ` grad ` ` parameter uses ` ` reduce_scatter ` ` gradient communication stores sharded gradient ` ` output ` ` grad add_ state noise custom_reduce_scatter output grad group=state process_group TestCommunicationHooks FSDPTest skip_if_lt_x_gpu parametrize sharding_strategy ShardingStrategy NO_SHARD ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP test_default_communication_hook_behavior sharding_strategy Optional ShardingStrategy Tests FSDP s default communication hook s behavior correctness This test creates simple linear net weight shape ` ` X N ` ` where ` ` N ` ` number workers For sharded cases each worker gets element weight parameter This test checks after backward each worker has proper value its chunk gradient whole gradient every worker equal expected value Arguments sharding_strategy Optional ShardingStrategy Configures FSDP algorithm out_dim = world_size net = torch nn Linear out_dim bias=False inpt = torch tensor rank float rank net_default_hook = FSDP net device_id=torch accelerator current_device_index sharding_strategy=sharding_strategy rank Check default ` _comm_hook ` None entry FSDP fsdp_modules net_default_hook assertEqual entry _comm_hook None _ range Clear gradients net_default_hook zero_grad loss = net_default_hook inpt sum loss backward For each worker gradient weight should worker_rank grad = net_default_hook params grad expected_grad = sum i i range dist get_world_size dist get_world_size Verify default hook produces expected gradients assertEqual grad item expected_grad msg=f Expected hook grad expected_grad got grad item _get_submodules fsdp_net submodule submodule FSDP fsdp_modules fsdp_net submodule check_is_root _init_model core sharding_strategy mixed_precision=None device = torch device device_type FSDP core device_id=torch accelerator current_device_index sharding_strategy=sharding_strategy mixed_precision=mixed_precision device skip_if_lt_x_gpu parametrize has_wrapping True False parametrize sharding_strategy ShardingStrategy NO_SHARD ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP test_default_communication_hook_initialization has_wrapping bool sharding_strategy Optional ShardingStrategy Tests FSDP s communication hook interface behavior Arguments has_wrapping bool Configures wrapping module sharding_strategy Optional ShardingStrategy Configures FSDP algorithm Initialize model fsdp_model_with_hook = _init_model Net has_wrapping=has_wrapping sharding_strategy=sharding_strategy sharding_strategy=sharding_strategy Check default ` _comm_hook ` None fsdp_module FSDP fsdp_modules fsdp_model_with_hook assertEqual fsdp_module _comm_hook None dummy_state = DummyState process_group=None noise= dummy_hook = DummyHook dummy_hook_for_no_shard_fsdp sharding_strategy = ShardingStrategy NO_SHARD DummyHook dummy_hook_for_sharded_fsdp fsdp_model_with_hook register_comm_hook dummy_state dummy_hook Check we can t register comm hook twice assertRaisesRegex AssertionError ^A communication hook already registered$ fsdp_model_with_hook register_comm_hook dummy_state dummy_hook Check dummy hook registered root all submodules any fsdp_module FSDP fsdp_modules fsdp_model_with_hook assertEqual fsdp_module _comm_hook dummy_hook assertEqual fsdp_module _comm_hook_state dummy_state skip_if_lt_x_gpu parametrize sharding_strategy ShardingStrategy NO_SHARD ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP test_registering_hook_non_root sharding_strategy Optional ShardingStrategy Tests FSDP s communication hook registering submodules Make sure can t registered non-root submodules Currently tests only ` ` NO_SHARD ` ` strategy Arguments sharding_strategy Optional ShardingStrategy Configures FSDP algorithm fsdp_model_with_hook = _init_model Net has_wrapping=True sharding_strategy=sharding_strategy sharding_strategy=sharding_strategy dummy_state = DummyState process_group=None noise= dummy_hook = DummyHook dummy_hook_for_no_shard_fsdp sharding_strategy = ShardingStrategy NO_SHARD DummyHook dummy_hook_for_sharded_fsdp Creating list non-root submodules test submodules = _get_submodules fsdp_model_with_hook Check assertion raised registering comm hook non-root assertRaisesRegex AssertionError ^register_comm_hook can only called root instance $ submodules register_comm_hook dummy_state dummy_hook skip_if_lt_x_gpu test_registering_hook_hybrid_strategy sharding_strategy ShardingStrategy HYBRID_SHARD ShardingStrategy _HYBRID_SHARD_ZERO model = Net False None None device=device_type fsdp_model = FSDP model auto_wrap_policy=ModuleWrapPolicy nn Linear sharding_strategy=sharding_strategy dummy_state = DummyState process_group=None noise= dummy_hook = DummyHook dummy_hook_for_sharded_fsdp assertRaisesRegex AssertionError Communication hook supported hybrid strategies fsdp_model register_comm_hook dummy_state dummy_hook skip_if_lt_x_gpu parametrize sharding_strategy ShardingStrategy NO_SHARD ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP test_registering_hook_submodules sharding_strategy Optional ShardingStrategy Tests FSDP s communication hook registering submodules Checks behavior hook registered non-root submodule Currently tests only ` ` NO_SHARD ` ` strategy Arguments sharding_strategy Optional ShardingStrategy Configures FSDP algorithm fsdp_model_with_hook = _init_model Net has_wrapping=True sharding_strategy=sharding_strategy sharding_strategy=sharding_strategy dummy_state = DummyState process_group=None noise= dummy_hook = DummyHook dummy_hook_for_no_shard_fsdp sharding_strategy = ShardingStrategy NO_SHARD DummyHook dummy_hook_for_sharded_fsdp submodules = _get_submodules fsdp_model_with_hook Simulate registration hook submodule submodules _comm_hook = dummy_hook Check error raised when some submodules have non-default hook assigned assertRaisesRegex AssertionError ^A communication hook already registered$ fsdp_model_with_hook register_comm_hook dummy_state dummy_hook _check_low_precision_hook state hook sharding_strategy dtype has_wrapping keep everything deterministic input data torch manual_seed torch get_device_module device_type manual_seed fsdp_with_hook = _init_model Net has_wrapping=has_wrapping sharding_strategy=sharding_strategy sharding_strategy=sharding_strategy fsdp_with_hook register_comm_hook state hook mp_only_grad = MixedPrecision reduce_dtype=dtype fsdp_with_mp = _init_model Net has_wrapping=has_wrapping sharding_strategy=sharding_strategy mixed_precision=mp_only_grad sharding_strategy=sharding_strategy mixed_precision=mp_only_grad optim_hook = torch optim SGD fsdp_with_hook parameters lr= optim_mp = torch optim SGD fsdp_with_mp parameters lr= in_data = torch rand device=device_type fsdp_with_hook train fsdp_with_mp train loss_hook = fsdp_with_hook in_data sum loss_mp = fsdp_with_mp in_data sum loss_hook backward Make sure grads cast parameter s precision assertEqual fsdp_with_hook params grad dtype state parameter_type loss_mp backward optim_hook step optim_mp step dist barrier hook_param mp_param zip fsdp_with_hook parameters fsdp_with_mp parameters assertEqual hook_param grad mp_param grad requires_accelerator_dist_backend nccl xccl skip_if_lt_x_gpu parametrize has_wrapping True False parametrize sharding_strategy ShardingStrategy NO_SHARD ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP test_fp _hook has_wrapping bool sharding_strategy Optional ShardingStrategy state = default_hooks LowPrecisionState process_group=_get_default_group hook = default_hooks fp _compress_hook _check_low_precision_hook state hook sharding_strategy torch float has_wrapping requires_accelerator_dist_backend nccl xccl requires_nccl_version Need NCCL + BF _COMPRESS skip_but_pass_in_sandcastle_if BFLOAT _AVAILABLE BFloat only supported CUDA + XPU skip_if_lt_x_gpu parametrize has_wrapping True False parametrize sharding_strategy ShardingStrategy NO_SHARD ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP test_bf _hook has_wrapping bool sharding_strategy Optional ShardingStrategy state = default_hooks LowPrecisionState process_group=_get_default_group hook = default_hooks bf _compress_hook _check_low_precision_hook state hook sharding_strategy torch bfloat has_wrapping instantiate_parametrized_tests TestCommunicationHooks __name__ == __main__ run_tests