Owner s oncall jit functools itertools os collections abc Sequence pathlib Path unittest skip yaml torch torch _lazy torch _lazy config torch _lazy ir_cache torch _lazy metrics torch _lazy ts_backend torch testing _internal common_device_type instantiate_device_type_tests ops torch testing _internal common_methods_invocations op_db torch testing _internal common_utils run_tests TestCase torch testing _internal jit_utils JitTestCase torch _lazy ts_backend init get_test_device cuda LTC_TS_CUDA os environ cpu remove_suffixes l x split x l init_lists path_to_script = Path os path abspath os path dirname __file__ TS_NATIVE_FUNCTIONS_PATH = path_to_script parent parent aten src ATen native ts_native_functions yaml open TS_NATIVE_FUNCTIONS_PATH f yaml_ts = yaml load f yaml SafeLoader LAZY_OPS_LIST = set remove_suffixes itertools chain yaml_ts full_codegen yaml_ts supported yaml_ts autograd HAS_SYMINT_SUFFIX = yaml_ts symint FALLBACK_LIST = clamp SKIP_RUNTIME_ERROR_LIST = index_select Empty output_sizes supported clone clone decomposed General ASAN Failure due related generating bool values https github com pytorch pytorch issues https github com pytorch pytorch issues nonzero ASAN failure paste P all ASAN failure any ASAN failure logdet ASAN failure SKIP_INCORRECT_RESULTS_LIST = squeeze Value out range t Value out range transpose Value out range bernoulli incorrect results pow incorrect results addcdiv incorrect results CI locally The following ops all show up directly ts_native_functions yaml run functionalized versions composite kernels core This means we don t expect ops show directly LTC metrics FUNCTIONAL_DECOMPOSE_LIST = diag_embed block_diag new_empty_strided narrow_copy pixel_shuffle pixel_unshuffle select_backward _trilinear linalg_inv_ex linalg_pinv atol_rtol_tensor logsumexp For some ops we don t support all variants Here we use formatted_name uniquely identify variant SKIP_VARIANT_LIST = norm_nuc min_reduction_with_dim LAZY_OPS_LIST FALLBACK_LIST SKIP_RUNTIME_ERROR_LIST SKIP_INCORRECT_RESULTS_LIST FUNCTIONAL_DECOMPOSE_LIST HAS_SYMINT_SUFFIX SKIP_VARIANT_LIST LAZY_OPS_LIST FALLBACK_LIST SKIP_RUNTIME_ERROR_LIST SKIP_INCORRECT_RESULTS_LIST FUNCTIONAL_DECOMPOSE_LIST HAS_SYMINT_SUFFIX SKIP_VARIANT_LIST = init_lists torch manual_seed clone_move t dev = lazy copy_t = t detach clone requires_grad_ True device=dev copy_t TestLazyTensor JitTestCase skip Disable until autograd supports symints testConvolutionBackward test_device = get_test_device inp = torch rand device=test_device requires_grad=True inp_copy = clone_move inp grad = torch rand device=test_device no requires_grad grad_copy = clone_move grad weight = torch rand device=test_device requires_grad=True weight_copy = clone_move weight bias = torch rand device=test_device requires_grad=True bias_copy = clone_move bias run eager conv_out = torch nn functional conv d inp weight bias inp_grad weight_grad bias_grad = torch autograd grad conv_out inp weight bias grad run lazy conv_copy_out = torch nn functional conv d inp_copy weight_copy bias_copy inp_copy_grad weight_copy_grad bias_copy_grad = torch autograd grad conv_copy_out inp_copy weight_copy bias_copy grad_copy check numerics torch testing assert_close bias_copy_grad cpu bias_grad cpu torch testing assert_close weight_copy_grad cpu weight_grad cpu torch testing assert_close inp_copy_grad cpu inp_grad cpu test_view_mark_step_preserved test_device = get_test_device inp = torch rand device=test_device inp_lazy = clone_move inp foo x mark_step y = x view y add_ z = x + x noqa F mark_step torch _lazy mark_step y x should continue aliased after mark_step call y add_ x out_ref = foo inp mark_step=False out = foo inp_lazy mark_step=True out will have some pending mutations which will synced cpu call torch testing assert_close out_ref cpu out cpu test_tensor_ctr test_device = get_test_device inp = torch tensor device=test_device inp_lazy = torch tensor device= lazy foo x Calling view op ensure functionalization wrapping occurs x view - out_ref = foo inp out = foo inp_lazy torch testing assert_close out_ref cpu out cpu TestLazyOpInfo TestCase ops op op op_db op name LAZY_OPS_LIST op name SKIP_RUNTIME_ERROR_LIST op name FUNCTIONAL_DECOMPOSE_LIST op formatted_name SKIP_VARIANT_LIST allowed_dtypes= torch float test_dispatched_to_lazy device dtype op get_name op noqa F l = op name op variant_test_name = l append op variant_test_name join l global HAS_SYMINT_SUFFIX FALLBACK_LIST samples = op sample_inputs lazy dtype requires_grad=False sample = next iter samples args = sample input + list sample args kwargs = sample kwargs torch _lazy mark_step torch _lazy wait_device_ops torch _lazy metrics reset op args kwargs torch _lazy mark_step torch _lazy wait_device_ops prefix = aten op name FALLBACK_LIST lazy symint_suffix = _symint op name HAS_SYMINT_SUFFIX found = f prefix op name symint_suffix remove_suffixes torch _lazy metrics counter_names check aliases found alias op aliases alias_found = f prefix alias name symint_suffix remove_suffixes torch _lazy metrics counter_names found = found alias_found found break assertTrue found ops op op op_db op name LAZY_OPS_LIST op name SKIP_RUNTIME_ERROR_LIST &#124; SKIP_INCORRECT_RESULTS_LIST allowed_dtypes= torch float noqa B test_correctness device dtype op test_device = get_test_device clone_to_device input dev isinstance input torch Tensor input detach clone device=dev isinstance input Sequence isinstance input str tuple map functools partial clone_to_device dev=dev input input assert_allclose_rec t b = t assertEqual type type b isinstance torch Tensor assertTrue torch allclose clone_to_device test_device b atol= e- isinstance Sequence map assert_allclose_rec zip b samples = op sample_inputs lazy dtype requires_grad=False sample samples Need run mark step so all random ops computed right order torch _lazy mark_step args = sample input + list sample args kwargs = sample kwargs copy_args = clone_to_device args test_device r_exp = op copy_args kwargs r_actual = op args kwargs torch _lazy mark_step assert_allclose_rec r_actual r_exp ops op op op_db op name LAZY_OPS_LIST op name SKIP_RUNTIME_ERROR_LIST &#124; SKIP_INCORRECT_RESULTS_LIST allowed_dtypes= torch float noqa B test_correctness_with_reusing_ir device dtype op torch _lazy config set_reuse_ir True test_device = get_test_device clone_to_device input dev isinstance input torch Tensor input detach clone device=dev isinstance input Sequence isinstance input str tuple map functools partial clone_to_device dev=dev input input assert_allclose_rec t b = t assertEqual type type b isinstance torch Tensor assertTrue torch allclose clone_to_device test_device b atol= e- isinstance Sequence map assert_allclose_rec zip b samples = op sample_inputs lazy dtype requires_grad=False sample samples Need run mark step so all random ops computed right order torch _lazy mark_step args = sample input + list sample args kwargs = sample kwargs copy_args = clone_to_device args test_device r_exp = op copy_args kwargs r_actual = op args kwargs torch _lazy mark_step assert_allclose_rec r_actual r_exp torch _lazy ir_cache reset torch _lazy config set_reuse_ir False TODO after we move master add Lazy new Device here https github com pytorch pytorch blob master torch testing _internal common_device_type py#L instantiate_device_type_tests TestLazyOpInfo globals only_for= cpu TestLazyDynamicOps TestCase classmethod setUpClass cls - None Setup dynamic shape mode cls old_ssa_mode = torch _C _lazy _get_symbolic_shape_mode torch _C _lazy _set_symbolic_shape_mode True super setUpClass classmethod tearDownClass cls - None torch _C _lazy _set_symbolic_shape_mode cls old_ssa_mode super tearDownClass test_nonzero_dynamic Test nonzero gives upper bounds sizes when symbolic shape mode enabled test_device = get_test_device x = torch tensor device=test_device requires_grad=True x _lazy = clone_move x x _lazy = torch nonzero x _lazy FIXME Add bindings get upper bounds assertEqual tuple x _lazy size We should still able instantiate get actual result x _eager = x _lazy cpu assertEqual tuple x _eager size test_adaptiveavgpool d_dynamic Test adaptive_avg_pool d gives correct shapes lazy backend img_cpu = torch zeros device= cpu out_cpu = torch nn AdaptiveAvgPool d device= cpu img_cpu test_device = get_test_device img_lazy = torch zeros device=test_device out_lazy = torch nn AdaptiveAvgPool d test_device img_lazy assertEqual out_cpu shape out_lazy shape __name__ == __main__ run_tests