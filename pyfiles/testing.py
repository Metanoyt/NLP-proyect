Testing utilities infrastructure Dynamo This module provides comprehensive set testing utilities including - Test result collection validation - Graph manipulation comparison tools - Test case management execution helpers - Specialized test decorators different Python versions features - RNG state management - Compilation counting monitoring - Debug utilities bytecode transformation The utilities module used across Dynamo s test suite ensure consistent testing patterns proper test isolation contextlib dis functools logging os path random re sys types unittest collections abc Callable Sequence typing Any Optional overload TypeVar Union typing_extensions ParamSpec unittest mock patch torch torch fx torch _dynamo backends debugging aot_eager torch _dynamo output_graph OutputGraph config eval_frame optimize_assert reset bytecode_transformation create_instruction debug_checks is_generator transform_code_object guards CheckFunctionManager CompileId GuardedCode types ConvertFrameReturn DynamoFrameType wrap_guarded_code utils CompileCounterInt same np Optional types ModuleType = None try numpy np except ModuleNotFoundError np = None unsupported = eval_frame unsupported three = log = logging getLogger __name__ _P = ParamSpec _P clone_me x Optional torch Tensor - Optional torch Tensor x None None x detach clone requires_grad_ x requires_grad remove_optimized_module_prefix name str - str re sub r ^_orig_mod name extract_graph_and_tracker fn args kwargs type ignore no-untyped-def torch _dynamo symbolic_convert InstructionTranslator gm = None region_tracker = None extract_graph_backend _gm args kwargs type ignore no-untyped-def nonlocal gm nonlocal region_tracker gm = _gm region_tracker = InstructionTranslator current_tx output region_tracker _gm torch compile backend=extract_graph_backend fullgraph=True fn args kwargs gm graph region_tracker type ignore union-attr collect_results model torch nn Module prediction Any loss Any example_inputs Any - list Any results = results append prediction results append loss isinstance loss torch Tensor loss item log warning f High loss value alert - loss f Can result unstable gradients grads = params = name param model named_parameters isinstance model eval_frame OptimizedModule name = remove_optimized_module_prefix name param_copy = param grad = param grad Treat None zero grad same param grad None grad = torch zeros_like param grads name + grad = grad params name = param_copy results append grads results append params buffers = name buffer model named_buffers isinstance model eval_frame OptimizedModule name = remove_optimized_module_prefix name buffers name = buffer results append buffers example example_inputs isinstance example tuple list results extend inp grad inp example isinstance inp torch Tensor isinstance example torch Tensor results append example grad results requires_bwd_pass out Any - bool isinstance out torch Tensor out requires_grad isinstance out list tuple any requires_bwd_pass x x out out None False isinstance out int False raise NotImplementedError Don t know how reduce type out overload reduce_to_scalar_loss out torch Tensor - torch Tensor overload reduce_to_scalar_loss out Union list Any tuple Any dict Any Any - float reduce_to_scalar_loss out Any - Union torch Tensor float Reduce output model get scalar loss isinstance out torch Tensor Mean does work integer tensors out sum out numel isinstance out list tuple sum reduce_to_scalar_loss x x out len out type out __name__ MaskedLMOutput Seq SeqLMOutput CausalLMOutputWithCrossAttentions reduce_to_scalar_loss out logits type out __name__ == SquashedNormal out mean sum isinstance out dict sum reduce_to_scalar_loss value value out values len out keys raise NotImplementedError Don t know how reduce type out debug_dir - str path = os path join os path dirname __file__ debug os path exists path os mkdir path path debug_dump name str code types CodeType extra str = - None open os path join debug_dir name w fd fd write f dis Bytecode code info \n\n dis Bytecode code dis \n\n extra \n debug_insert_nops frame DynamoFrameType cache_size int hooks Any _ Any skip int = - ConvertFrameReturn used debug jump updates insert_nops instructions list Any code_options Any - None instructions insert create_instruction NOP instructions insert create_instruction NOP metrics_context = torch _dynamo utils get_metrics_context torch _dynamo utils dynamo_timed debug_insert_nops metrics_context is_generator frame f_code ConvertFrameReturn debug_checks frame f_code code _ = transform_code_object frame f_code insert_nops graph = OutputGraph code_options= compiler_fn=None root_tx=None type ignore arg-type export=False export_constraints= frame_state= _id TODO shouldn t f_locals f_globals frame local_scope=locals global_scope=globals f_code=frame f_code torch_function_mode_stack= package=None wrap_guarded_code GuardedCode code CheckFunctionManager frame f_code graph guard_manager type ignore arg-type CompileId frame_id= frame_compile_id= CompileCounter __init__ - None frame_count Union int CompileCounterInt = clear __call__ gm torch fx GraphModule example_inputs list torch Tensor - Callable Any frame_count += node gm graph nodes call node op op_count += gm forward clear - None config debug_disable_compile_counter frame_count = CompileCounterInt frame_count = op_count = CompileCounterWithBackend __init__ backend str - None frame_count Union int CompileCounterInt = backend = backend graphs list torch fx GraphModule = clear __call__ gm torch fx GraphModule example_inputs list torch Tensor - Callable Any backends registry lookup_backend frame_count += node gm graph nodes call node op op_count += graphs append gm lookup_backend backend gm example_inputs clear - None config debug_disable_compile_counter frame_count = CompileCounterInt frame_count = op_count = graphs = Equivalent backend= eager also records graphs we can assert EagerAndRecordGraphs __init__ - None graphs list torch fx GraphModule = __call__ gm torch fx GraphModule example_inputs list torch Tensor - Callable Any graphs append gm gm forward AotEagerAndRecordGraphs __init__ - None graphs list torch fx GraphModule = fw_graphs list torch fx GraphModule = bw_graphs list torch fx GraphModule = __call__ gm torch fx GraphModule example_inputs list torch Tensor - Callable Any graphs append gm fw_compiler gm torch fx GraphModule example_inputs list torch Tensor - Callable Any fw_graphs append gm gm forward bw_compiler gm torch fx GraphModule example_inputs list torch Tensor - Callable Any bw_graphs append gm gm forward aot_eager gm example_inputs fw_compiler=fw_compiler bw_compiler=bw_compiler InductorAndRecordGraphs __init__ - None graphs list torch fx GraphModule = inductor_graphs list torch fx GraphModule = __call__ gm example_inputs type ignore no-untyped-def torch _inductor compile_fx compile_fx_mod graphs append gm old_compile_fx_inner = compile_fx_mod _compile_fx_inner patched args kwargs type ignore no-untyped-def inductor_graphs append args old_compile_fx_inner args kwargs patch object compile_fx_mod _compile_fx_inner new=patched compile_fx_mod compile_fx gm example_inputs strip_comment code str - str re sub r m ^ \n code remove_trailing_space code str - str \n join line rstrip line code split \n normalize_gm gm_str str - str strip comments comments have path files which may differ system system remove_trailing_space strip_comment gm_str empty_line_normalizer code str - str Normalize code remove empty lines normal_code = re sub r \r\n + \n code normal_code standard_test Any fn Callable Any nargs int expected_ops Optional int = None expected_ops_dynamic Optional int = None expected_frame_count int = - None config assume_static_by_default expected_ops_dynamic None expected_ops = expected_ops_dynamic actual = CompileCounter args = torch randn _ range nargs args = torch randn _ range nargs correct = fn args correct = fn args reset opt_fn = optimize_assert actual fn val = opt_fn args val = opt_fn args val b = opt_fn args val b = opt_fn args reset assertTrue same val correct assertTrue same val b correct assertTrue same val correct assertTrue same val b correct assertEqual actual frame_count expected_frame_count expected_ops None assertEqual actual op_count expected_ops dummy_fx_compile gm fx GraphModule example_inputs list torch Tensor - Callable Any gm forward format_speedup speedup float pvalue float is_correct bool = True pvalue_threshold float = - str is_correct ERROR pvalue pvalue_threshold f speedup f x SAME f speedup f x p= pvalue f rand_strided size Sequence int stride Sequence int dtype torch dtype = torch float device Union str torch device = cpu extra_size int = - torch Tensor needed_size = extra_size all s s size only need allocate all sizes non-zero needed_size += sum shape - stride shape stride zip size stride + dtype is_floating_point dtype itemsize == normal distribution kernel implemented fp Workaround creating fp tensor then cast buffer = torch randn needed_size dtype=torch float device=device dtype=dtype buffer = torch randn needed_size dtype=dtype device=device buffer = torch zeros size= needed_size dtype=dtype device=device torch as_strided buffer size stride _T = TypeVar _T check_dynamic_shape_capture - bool This also mirrors config ` test dynamo test_dynamic_shapes py make_dynamic_cls ` config assume_static_by_default _make_fn_with_patches fn Callable _P _T patches Any - Callable _P _T functools wraps fn _fn args _P args kwargs _P kwargs - _T contextlib ExitStack stack module attr val patches stack enter_context patch object module attr val fn args kwargs _fn make_test_cls_with_patches cls type cls_prefix str fn_suffix str patches Any xfail_prop Optional str = None decorator Callable Callable Any Callable Any = lambda x x - type DummyTestClass = type f cls_prefix cls __name__ cls __bases__ DummyTestClass __qualname__ = DummyTestClass __name__ name dir cls name startswith test_ fn = getattr cls name callable fn setattr DummyTestClass name getattr cls name continue new_name = f name fn_suffix new_fn = _make_fn_with_patches fn patches new_fn __name__ = new_name xfail_prop None hasattr fn xfail_prop new_fn = unittest expectedFailure new_fn setattr DummyTestClass new_name decorator new_fn NB Doesn t handle slots correctly whatever hasattr DummyTestClass name setattr DummyTestClass name getattr cls name DummyTestClass test Python + specific features skipIfNotPy fn Callable _P _T - Callable _P _T sys version_info = fn pyrefly ignore bad-return bad-argument-type unittest skip fn skipIfNotPy fn Callable _P _T - Callable _P _T sys version_info = fn unittest skip Requires Python + fn skipIfOnlyNotPy fn Callable _P _T - Callable _P _T sys version_info = sys version_info unittest skip Requires Python fn fn xfailIfPy fn Callable _P _T - Callable _P _T sys version_info = unittest expectedFailure fn fn skipIfPy fn Callable _P _T - Callable _P _T sys version_info = unittest skip Not supported Python + fn fn Controls tests generated test inductor test_torchinductor_dynamic_shapes py test dynamo test_dynamic_shapes py expectedFailureDynamic fn Callable _P _T - Callable _P _T fn _expected_failure_dynamic = True type ignore attr-defined fn Controls tests generated test inductor test_torchinductor_codegen_dynamic_shapes py expectedFailureCodegenDynamic fn Callable _P _T - Callable _P _T fn _expected_failure_codegen_dynamic = True type ignore attr-defined fn Controls test generated test inductor test_cpp_wrapper py expectedFailureDynamicWrapper fn Callable _P _T - Callable _P _T fn _expected_failure_dynamic_wrapper = True type ignore attr-defined fn reset_rng_state use_xla bool = False - None torch manual_seed random seed np np random seed use_xla torch_xla core xla_model xm xm set_rng_state str xm xla_device _skipped_function_for_test_reconstruct f Callable _P _T args _P args kwargs _P kwargs - _T f args kwargs