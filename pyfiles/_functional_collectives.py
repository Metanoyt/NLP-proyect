mypy allow-untyped-defs contextlib sys warnings typing Any cast Optional TYPE_CHECKING Union torch torch distributed dist torch distributed distributed_c d c d torch distributed device_mesh DeviceMesh torch fx experimental proxy_tensor get_proxy_mode _functional_collectives_impl fun_col_impl try torch utils _cxx_pytree tree_map_only except ImportError torch utils _pytree tree_map_only type ignore no-redef try torch compiler is_dynamo_compiling is_torchdynamo_compiling except Exception warnings warn Unable torchdynamo util ` is_torchdynamo_compiling ` so won t support torchdynamo correctly stacklevel= is_torchdynamo_compiling type ignore misc False False New traceable functional collectives RFC https github com pytorch pytorch issues compiler trace these ops plain-old-data schemas then choose how lower them eager execute these functional ops which eager AsyncCollectiveTensor subclasses automatically calling wait underlying hidden async work obj only when fed downstream op Issues Where should these ops live Couldn t ` torch ` putting these ops existing torch distributed files Proper support eager requires inplace ops We should explore having option API Functional collectives asynchronous only we perform implicit stream synchronization behalf user We use AsyncCollectiveTensor wrap result tensor collective lets us witness first usage tensor insert cross stream sync right place The above easy bits hard one how we match Work object returned c d tensor AsyncCollectiveTensor wraps We alloc tensor inside collective op implementation see ` ` clone ` ` call ` ` _all_reduce ` ` then s handled dispatcher which might call other implementations allowed change returned tensor - even tensor different shape see ` ` torch vmap ` ` This means caller our ops receives Tensor guaranteed same allocated our implementations makes pairing The AsyncTensor original tensor lot harder This pairing needed so we can lookup Work object use Originally we tried WeakKeyDictionary map Tensor Work because Tensor s identity stable across dispatch op caller would end up different Tensor instance would match any dictionary With Tensor identity out question we decided use tensor data pointer which should stable across all Tensor changes done during dispatch We have dictionary tensor data_ptr - Work we insert right after we call into c d We use dictionary when AsyncCollectiveTensor used invoke Work wait Finally we setup finalizer against tensor wrapper observe getting collected so we can clean up stale entries dictionary To eliminate possibility races we have global version counter used finalizer As wise man said once Don t cross streams https www youtube com watch v=wyKQe_i yyo Functional collectives can accept any these types describe ranks participating collectives The different types will desugared canonical format RANK_TYPES = Union list int list list int dist ProcessGroup DeviceMesh tuple dist tensor DeviceMesh int str User facing APIs functional collectives ------------------------------------------- These apis called user code expected work both eager execution compilation there significant differences how two modes implemented underneath Eager execution optimized using tensor subclass schedules synchronization via wait_tensor op just before tensor first used Compiled tracing currently relies compiler perform optimization cannot yet correctly trace AsyncTensor wrapper In future these paths may unified sufficient subclass support added dynamo Example all_reduce entrypoint API other collectives follow similar pattern Here s how works under torch compile dynamo all_reduce &#124; -- _expand_group - desugars processgroup into canonical traceable format &#124; -- c d_functional all_reduce - dynamo captures op call doesn t trace deeper &#124; -- _maybe_wrap_tensor - wait_tensor op immediately called no AsyncTensor subclass needed And under eager execution all_reduce &#124; -- _expand_group - same above less critical eager &#124; -- c d_functional all_reduce - dispatches real kernel OR records op trace &#124; -- _maybe_wrap_tensor - AsyncTensor wrapper applied returned tensor which issues wait_tensor time first use wait_tensor tensor Wait tensor returned collectives ops Waiting follows device semantics which means blocking CPU synchronizing streams CUDA torch ops _c d_functional wait_tensor tensor type ignore attr-defined broadcast torch Tensor src int group RANK_TYPES tag str = Broadcasts tensor all processes given process group Args src int Source rank group ProcessGroup List int The process group work tag str optional A unique identifier collective Default empty string group_name = _resolve_group_name group tag tensor = torch ops _c d_functional broadcast src group_name _maybe_wrap_tensor tensor all_reduce torch Tensor reduceOp str group RANK_TYPES tag str = Reduces tensor data across all machines such way all get final result The input tensor left unmodified Group can one List int ranks participating collective List List int D mesh ranks taking part collective MPMD ProcessGroup Will perform collective using ranks tag PG DeviceMesh Do SPMD collective over all ranks mesh DeviceMesh int Do MPMD collective over one dimension DeviceMesh N B If you pass PG D list perform MPMD collective compiler won t able recover information perform collective algebraic optimization Use other forms input group_name = _resolve_group_name group tag tensor = torch ops _c d_functional all_reduce reduceOp lower group_name _maybe_wrap_tensor tensor all_gather_tensor torch Tensor gather_dim int group RANK_TYPES tag str = - torch Tensor Gather tensor data across all machines concatenate over ` ` gather_dim ` ` Note currently only supports gather_dim = The input tensor left unmodified Group can one List int ranks participating collective List List int D mesh ranks taking part collective MPMD ProcessGroup Will perform collective using ranks tag PG DeviceMesh Do SPMD collective over all ranks mesh DeviceMesh int Do MPMD collective over one dimension DeviceMesh N B If you pass PG D list perform MPMD collective compiler won t able recover information perform collective algebraic optimization Use other forms input assert is_contiguous group_name = _resolve_group_name group tag group_size = c d _get_group_size_by_name group_name tensor = torch ops _c d_functional all_gather_into_tensor group_size group_name res = _maybe_wrap_tensor tensor TODO should done inside AsyncCollectiveTensor delay wait call gather_dim = torch cat access data so we already need wait here first do wait then chunk + cat avoid us going through ACT dispatching logic again isinstance res AsyncCollectiveTensor res = res wait type ignore attr-defined res = torch cat torch chunk res group_size dim= dim=gather_dim res all_gather_tensor_autograd torch Tensor gather_dim int group RANK_TYPES tag str = Gather tensor data across all machines concatenate over ` ` gather_dim ` ` Note currently only supports gather_dim = This function same all_gather_tensor will propagate backwards gradient across workers See all_gather_tensor more details usage group_name = _resolve_group_name group tag group_size = c d _get_group_size_by_name group_name tensor = torch ops _c d_functional_autograd all_gather_into_tensor group_size group_name res = _FromTorchTensor apply tensor TODO should done inside AsyncCollectiveTensor delay wait call gather_dim = torch cat access data so we already need wait here first do wait then chunk + cat avoid us going through ACT dispatching logic again isinstance res AsyncCollectiveTensor res = res wait type ignore attr-defined res = torch cat torch chunk res group_size dim= dim=gather_dim res reduce_scatter_tensor torch Tensor reduceOp str scatter_dim int group RANK_TYPES tag str = Reduces tensor data across all machines such way all get final result then scatter results corresponding ranks The input tensor left unmodified Group can one List int ranks participating collective List List int D mesh ranks taking part collective MPMD ProcessGroup Will perform collective using ranks tag PG DeviceMesh Do SPMD collective over all ranks mesh DeviceMesh int Do MPMD collective over one dimension DeviceMesh N B If you pass PG D list perform MPMD collective compiler won t able recover information perform collective algebraic optimization Use other forms input group_name = _resolve_group_name group tag group_size = c d _get_group_size_by_name group_name assert size scatter_dim group_size == f input dimension size must multiple group_size group_size scatter_dim = tensor_list = torch chunk group_size dim=scatter_dim = torch cat tensor_list tensor = torch ops _c d_functional reduce_scatter_tensor reduceOp lower group_size group_name type ignore possibly-undefined res = _maybe_wrap_tensor tensor res reduce_scatter_tensor_autograd torch Tensor reduceOp str scatter_dim int group RANK_TYPES tag str = Reduces tensor data across all machines such way all get final result then scatter results corresponding ranks This function same reduce_scatter_tensor will propagate backwards gradient across workers Currently only sum reduceOp supported See reduce_scatter_tensor more details usage group_name = _resolve_group_name group tag group_size = c d _get_group_size_by_name group_name assert size scatter_dim group_size == f input dimension size must multiple group_size group_size scatter_dim = tensor_list = torch chunk group_size dim=scatter_dim = torch cat tensor_list tensor = torch ops _c d_functional_autograd reduce_scatter_tensor reduceOp lower group_size group_name type ignore possibly-undefined res = _FromTorchTensor apply tensor res all_reduce_coalesced list torch Tensor reduceOp str group RANK_TYPES tag str = - list torch Tensor Reduces list tensors across all machines such way all get final result The all tensors input list left unmodified Group can one List int ranks participating collective List List int D mesh ranks taking part collective MPMD ProcessGroup Will perform collective using ranks tag PG DeviceMesh Do SPMD collective over all ranks mesh DeviceMesh int Do MPMD collective over one dimension DeviceMesh N B If you pass PG D list perform MPMD collective compiler won t able recover information perform collective algebraic optimization Use other forms input group_name = _resolve_group_name group tag tensor_list = torch ops _c d_functional all_reduce_coalesced type ignore attr-defined reduceOp lower group_name list map _maybe_wrap_tensor tensor_list all_gather_into_tensor_coalesced list torch Tensor group RANK_TYPES tag str = - list torch Tensor Gather list tensors across all machines Note currently only supports gather_dim = The input tensor left unmodified Group can one List int ranks participating collective List List int D mesh ranks taking part collective MPMD ProcessGroup Will perform collective using ranks tag PG DeviceMesh Do SPMD collective over all ranks mesh DeviceMesh int Do MPMD collective over one dimension DeviceMesh N B If you pass PG D list perform MPMD collective compiler won t able recover information perform collective algebraic optimization Use other forms input group_name = _resolve_group_name group tag group_size = c d _get_group_size_by_name group_name tensor_list = torch ops _c d_functional all_gather_into_tensor_coalesced type ignore attr-defined group_size group_name list map _maybe_wrap_tensor tensor_list reduce_scatter_tensor_coalesced inputs list torch Tensor reduceOp str scatter_dim list int group RANK_TYPES tag str = - list torch Tensor Reduces list tensors across all machines such way all get final result then scatter results corresponding ranks The input tensors left unmodified Group can one List int ranks participating collective List List int D mesh ranks taking part collective MPMD ProcessGroup Will perform collective using ranks tag PG DeviceMesh Do SPMD collective over all ranks mesh DeviceMesh int Do MPMD collective over one dimension DeviceMesh N B If you pass PG D list perform MPMD collective compiler won t able recover information perform collective algebraic optimization Use other forms input group_name = _resolve_group_name group tag group_size = c d _get_group_size_by_name group_name assert len scatter_dim == len inputs idx dim tensor enumerate zip scatter_dim inputs assert tensor size dim group_size == f input dimension dim tensor size dim must multiple group_size group_size tensor index idx dim = tensor_list = torch chunk tensor group_size dim=dim inputs idx = torch cat tensor_list tensor_list = torch ops _c d_functional reduce_scatter_tensor_coalesced type ignore attr-defined inputs reduceOp lower group_size group_name type ignore possibly-undefined list map _maybe_wrap_tensor tensor_list This bit unsafe checks first argument schema reports non-mutable alias Today maps aten ops views _is_view_op tgt assert isinstance tgt torch _ops OpOverload Don t apply view optimization any ` CompositeImplicitAutograd ` ops See issue https github com pytorch pytorch issues torch _C _dispatch_has_kernel_for_dispatch_key tgt name torch DispatchKey CompositeImplicitAutograd False schema = tgt _schema len schema arguments first_arg = schema arguments check op view first_arg alias_info None first_arg alias_info is_write all_to_all_single torch Tensor output_split_sizes Optional list int input_split_sizes Optional list int group RANK_TYPES tag str = - torch Tensor Each process splits input tensor then scatters split list all processes group Then concatenate received tensors all processes group single output tensor Group can one List int ranks participating collective List List int D mesh ranks taking part collective MPMD ProcessGroup Will perform collective using ranks tag PG DeviceMesh Do SPMD collective over all ranks mesh DeviceMesh int Do MPMD collective over one dimension DeviceMesh N B If you pass PG D list perform MPMD collective compiler won t able recover information perform collective algebraic optimization Use other forms input output_split_sizes None assert all isinstance size int torch SymInt size output_split_sizes output_split_sizes input_split_sizes None assert all isinstance size int torch SymInt size input_split_sizes input_split_sizes group_name = _resolve_group_name group tag group_size = c d _get_group_size_by_name group_name output_split_sizes None input_split_sizes None assert output_split_sizes None input_split_sizes None output_split_sizes input_split_sizes must either specified together both set None output_split_sizes = shape group_size group_size input_split_sizes = output_split_sizes tensor = torch ops _c d_functional all_to_all_single type ignore attr-defined output_split_sizes input_split_sizes group_name _maybe_wrap_tensor tensor all_to_all_single_autograd torch Tensor output_split_sizes Optional list int input_split_sizes Optional list int group RANK_TYPES tag str = - torch Tensor Same all_to_all_single supports autograd output_split_sizes None assert all isinstance size int torch SymInt size output_split_sizes output_split_sizes input_split_sizes None assert all isinstance size int torch SymInt size input_split_sizes input_split_sizes group_name = _resolve_group_name group tag group_size = c d _get_group_size_by_name group_name output_split_sizes None input_split_sizes None assert output_split_sizes None input_split_sizes None output_split_sizes input_split_sizes must either specified together both set None output_split_sizes = shape group_size group_size input_split_sizes = output_split_sizes tensor = torch ops _c d_functional_autograd all_to_all_single type ignore attr-defined output_split_sizes input_split_sizes group_name _FromTorchTensor apply tensor permute_tensor torch Tensor src_dst list int group RANK_TYPES tag str = - torch Tensor Permutes elements tensor according given source destination pairs ` src_dst ` should defined such src_dst m == n means m sends n Group can one List int ranks participating collective List List int D mesh ranks taking part collective MPMD ProcessGroup Will perform collective using ranks tag PG DeviceMesh Do SPMD collective over all ranks mesh DeviceMesh int Do MPMD collective over one t rankset group_size = _expand_group group tag local_pg = c d _find_or_create_pg_by_ranks_and_tag t rankset group_size output_split_sizes = group_size input_split_sizes = group_size src dst enumerate src_dst src == dist get_rank local_pg input_split_sizes dst = numel dst == dist get_rank local_pg output_split_sizes src = numel all_to_all_single output_split_sizes input_split_sizes group tag AsyncCollectiveTensor torch Tensor r A Tensor wrapper subclass used trigger call wait prior first use underlying tensor Use inside functional collective pytorch wrappers like following functional_collective group tag tag rankset group_size = _expand_group group tag tensor = torch ops c d_functional collective tag rankset group_size _maybe_wrap_tensor tensor elem torch Tensor completed bool __slots__ = elem completed staticmethod __new__ cls elem torch Tensor r = torch Tensor _make_wrapper_subclass cls elem size strides=elem stride storage_offset=elem storage_offset dtype=elem dtype layout=elem layout device=elem device requires_grad=elem requires_grad r elem = elem r completed = False r __tensor_flatten__ elem None tolist trigger_wait tolist staticmethod __tensor_unflatten__ inner_tensors meta outer_size outer_stride assert meta None elem = inner_tensors elem AsyncCollectiveTensor elem __coerce_same_metadata_as_tangent__ expected_metadata Any expected_type Optional type = None expected_type torch Tensor None trigger_wait __repr__ - str type ignore override f AsyncCollectiveTensor trigger_wait trigger_wait completed out = wait_tensor elem completed = True out elem wait - torch Tensor wait_tensor elem _get_acs_underlying_tensor This method enables _functional_collectives_impl test tensor ACS elem classmethod __torch_dispatch__ cls func types args= kwargs=None type ignore override func torch ops aten view default Fast handle aten view lot view related op goes aten view eventually avoids pytree slowdown pyrefly ignore index-error res = func args elem args wrapper_res = AsyncCollectiveTensor res wrapper_res is_view_op = _is_view_op func unwrap e AsyncCollectiveTensor wait_tensor idepotent will do stream sync only once is_view_op e trigger_wait e elem wrap e torch Tensor wait_tensor idepotent will do stream sync only once assert isinstance e AsyncCollectiveTensor res = AsyncCollectiveTensor e res unwrapped_args = tree_map_only AsyncCollectiveTensor unwrap args unwrapped_kwargs = tree_map_only AsyncCollectiveTensor unwrap kwargs we don t wrap result doesn t need waited out = func unwrapped_args unwrapped_kwargs View ops dont require sync so we should re-wrap outputs is_view_op out = tree_map_only torch Tensor wrap out out numpy type ignore override wait numpy Utils infrastructure tracing support _expand_group group RANK_TYPES tag str = - tuple str list int int _expand_group desugars different RANK_TYPES types into canonical format traceable By having part explicit eager codepath we avoid having specialize behavior inside torchdynamo can still interoperate processgroup objects other untraceable forms had define hack _inside_ expand_group avoid graph_break torch op returned non-Tensor int caused cast_ ` functions being treated torch ops iiuc TYPE_CHECKING cast_listlistint x cast list list int x cast_listint x cast list int x fake cast op use runtime since dynamo doesn t support real cast also dynamo didn t like encountering typing objects NotImplementedError argument type typing _GenericAlias cast_listlistint x x cast_listint x x rankset list int isinstance group list isinstance group list nested_list = cast_listlistint group rankset = group_size = - rs nested_list rankset extend rs group_size = - group_size = len rs raise ValueError f group sizes must identical found group_size len rs group_size = len rs rankset = cast_listint group group_size = len rankset isinstance group dist ProcessGroup rankset = dist get_process_group_ranks group group_size = len rankset tag = tag c d _get_group_tag group isinstance group DeviceMesh assert group ndim == Only D mesh supported pass DeviceMesh int together mesh D TODO should run collective whole mesh instead dim pg = group get_group rankset = dist get_process_group_ranks pg group_size = len rankset tag = tag c d _get_group_tag pg isinstance group tuple len group == isinstance group DeviceMesh isinstance group int dmesh = group dim = group pg = dmesh get_group dim rankset = dist get_process_group_ranks pg group_size = len rankset tag = tag c d _get_group_tag pg raise ValueError Invalid tuple group must DeviceMesh int raise ValueError Invalid type group must one List Processgroup DeviceMesh DeviceMesh int tag rankset group_size _resolve_group_name group RANK_TYPES tag str = - str Given group RANK_TYPES group name ` tag ` will deprecated See details https github com pytorch pytorch issues #issuecomment- isinstance group dist ProcessGroup group group_name isinstance group str group isinstance group DeviceMesh assert group ndim == Only D mesh supported pass DeviceMesh int together mesh D group _dim_group_names isinstance group tuple len group == isinstance group DeviceMesh isinstance group int dmesh = group dim = group dmesh _dim_group_names dim raise ValueError Invalid tuple group must DeviceMesh int isinstance group list is_torchdynamo_compiling warnings warn The combination ranks + tag process group identifier has been deprecated Please switch using ProcessGroup DeviceMesh group name instead FutureWarning stacklevel= pyrefly ignore redundant-cast c d _resolve_group_name_by_ranks_and_tag cast list int group tag raise ValueError f Unsupported group type type group group _FromTorchTensor torch autograd Function _FromTorchTensor allows autograd propagate normal Tensor AsyncCollectiveTensor staticmethod forward type ignore override ctx pyre-ignore Parameter must annotated input torch Tensor - torch Tensor _maybe_wrap_tensor input staticmethod backward ctx grad_output torch Tensor - torch Tensor type ignore override grad_output _are_we_tracing - bool is_torchdynamo_compiling True If fake mode turned we almost definitely compiling tracing torch _C _get_dispatch_mode torch _C _TorchDispatchModeKey FAKE None True See Note enable_python_dispatcher dynamo torch _C _dispatch_tls_is_dispatch_key_included torch _C DispatchKey PythonDispatcher True get_proxy_mode None _maybe_wrap_tensor - torch Tensor _are_we_tracing wait_tensor res = AsyncCollectiveTensor cast torch Tensor res contextlib contextmanager allow_inflight_collective_as_graph_input_ctx value bool = True Context manager temporarily set whether inflight collectives allowed torch compile graph inputs Common use case when collective issued eager ` async_op=True ` waited compiled region ` ` ` all_reduce_eager x y = x x req = dist all_reduce y op=dist ReduceOp SUM async_op=True y torch compile fullgraph=True all_reduce_wait_compiled y torch ops c d_functional wait_tensor y y y x = torch ones device= cuda + rank context manager ensures ` wait_tensor y ` will wait correct work object allow_inflight_collective_as_graph_input_ctx y = all_reduce_eager x z = all_reduce_wait_compiled y ` ` ` With context manager when collective called under hood work object collective will registered work registry wait_tensor compiled region called output tensor collective will wait correct work object previous = torch _C _distributed_c d _allow_inflight_collective_as_graph_input try torch _C _distributed_c d _set_allow_inflight_collective_as_graph_input value yield finally torch _C _distributed_c d _set_allow_inflight_collective_as_graph_input previous _make_all_gather_out_tensor input group_size out_size = list input size len out_size == out_size append group_size out_size = group_size out_tensor = input new_empty out_size out_tensor _all_gather_into_tensor_coalesced_meta tag rankset group_size _make_all_gather_out_tensor t group_size t We now register meta kernels deal tracing _broadcast_meta args torch empty_like _all_reduce_meta args torch empty_like _wait_tensor_meta args torch empty_like _all_gather_into_tensor_meta shard tag rankset group_size _make_all_gather_out_tensor shard group_size _reduce_scatter_tensor_meta input reduce_op tag rankset group_size out_size = list input size out_size = group_size input new_empty out_size _all_reduce_coalesced_meta args torch empty_like t t _all_reduce__meta inp args inp _broadcast__meta inp args inp _all_reduce_coalesced__meta inputs args inputs _reduce_scatter_tensor_coalesced_meta inputs reduceOp tag rankset group_size mk_out_tensor input out_size = list input size out_size = group_size out_tensor = input new_empty out_size out_tensor mk_out_tensor t t inputs NB We often say all_to_all has dynamic output size technically true instead what typically happens you manually communicate output_split_sizes ahead time which dynamic then you pass those sizes explicitly all all itself isn t dynamic just follows specified output splits _all_to_all_single_meta input output_split_sizes input_split_sizes args kwargs output_split_sizes None input new_empty input size s output_split_sizes torch _check s = out_size = list input size out_size = sum output_split_sizes input new_empty out_size _all_gather_into_tensor_out_native_meta input group_size group_name out _make_all_gather_out_tensor input group_size _all_gather_into_tensor_native_meta input group_size group_name _make_all_gather_out_tensor input group_size _all_gather_into_tensor_coalesced_native_meta inputs group_size group_name _all_gather_into_tensor_native_meta input group_size group_name input inputs _reduce_scatter_tensor_native_meta inp reduce_op group_size group_name shape = list inp size shape = group_size inp new_empty shape _reduce_scatter_tensor_coalesced_native_meta inputs reduce_op group_size group_name _reduce_scatter_tensor_native_meta inp reduce_op group_size group_name inp inputs Library MUST defined module scope doesn t work lib_impl = torch library Library _c d_functional IMPL lib_impl impl all_reduce _all_reduce_meta Meta lib_impl impl all_reduce_ _all_reduce__meta Meta lib_impl impl all_reduce_coalesced _all_reduce_coalesced_meta Meta lib_impl impl all_reduce_coalesced_ _all_reduce_coalesced__meta Meta lib_impl impl wait_tensor _wait_tensor_meta Meta lib_impl impl all_gather_into_tensor_out _all_gather_into_tensor_out_native_meta Meta lib_impl impl all_gather_into_tensor _all_gather_into_tensor_native_meta Meta lib_impl impl all_gather_into_tensor_coalesced _all_gather_into_tensor_coalesced_native_meta Meta lib_impl impl reduce_scatter_tensor _reduce_scatter_tensor_native_meta Meta lib_impl impl reduce_scatter_tensor_coalesced _reduce_scatter_tensor_coalesced_native_meta Meta lib_impl impl all_to_all_single _all_to_all_single_meta Meta lib_impl impl broadcast _broadcast_meta Meta lib_impl impl broadcast_ _broadcast__meta Meta mark these ops has side effect so they won t removed DCE torch fx node has_side_effect torch ops _c d_functional wait_tensor default type ignore has-type torch fx node has_side_effect torch ops _c d_functional wait_tensor type ignore has-type Register legacy ops backward compatibility TODO yifu remove these functional collective beta release legacy_lib = torch library Library c d_functional DEF legacy_lib_impl = torch library Library c d_functional IMPL ops_defs = broadcast Tensor int src str tag int ranks int group_size - Tensor all_reduce Tensor str reduceOp str tag int ranks int group_size - Tensor all_reduce_coalesced Tensor str reduceOp str tag int ranks int group_size - Tensor wait_tensor Tensor - Tensor all_gather_into_tensor Tensor shard str tag int ranks int group_size - Tensor all_gather_into_tensor_coalesced Tensor input str tag int ranks int group_size - Tensor reduce_scatter_tensor Tensor input str reduceOp str tag int ranks int group_size - Tensor reduce_scatter_tensor_coalesced Tensor inputs str reduceOp str tag int ranks int group_size - Tensor all_to_all_single Tensor input SymInt output_split_sizes SymInt input_split_sizes str tag int ranks int group_size - Tensor noqa B my_module = sys modules __name__ op_def ops_defs op_name = op_def op_def index backend_impl = getattr fun_col_impl f _ op_name legacy_lib define op_def tags=torch Tag pt _compliant_tag legacy_lib_impl impl op_name backend_impl CompositeImplicitAutograd Dynamo Remappings allow seamless translation non-functional collectives supportable form into functional collective calls followed inplace copy ops allowing them traced into functional graph We implement writing decomposition teaching dynamo how associate corresponding op via mapping dict below These schemas intentionally match torch distributed distributed_c d ops we trying remap all_gather_tensor_inplace output_tensor torch Tensor input_tensor torch Tensor group=None TODO add type async_op bool = False tag str = gather_dim int = assert async_op Can t remap async version inplace op functional collective group = group dist group WORLD assert group None output_tensor copy_ all_gather_tensor input_tensor gather_dim group tag reduce_scatter_tensor_inplace output torch Tensor input torch Tensor op str = sum TODO type actually c d ReduceOp ok group=None TODO add type async_op bool = False scatter_dim int = tag str = assert async_op Can t remap async version inplace op functional collective group = group dist group WORLD assert group None output copy_ reduce_scatter_tensor input op scatter_dim group tag REDUCE_OP_TO_STR = dist ReduceOp SUM sum dist ReduceOp AVG avg dist ReduceOp PRODUCT product dist ReduceOp MIN min dist ReduceOp MAX max dist ReduceOp BAND band dist ReduceOp BOR bor dist ReduceOp BXOR bxor all_reduce_inplace tensor torch Tensor op str = sum group=None async_op bool = False tag str = assert async_op Can t remap async version inplace op functional collective group = group dist group WORLD assert group None tensor copy_ all_reduce tensor op group tag all_to_all_inplace output torch Tensor input torch Tensor output_split_sizes=None input_split_sizes=None group=None async_op=False tag str = assert async_op Can t remap async version inplace op functional collective group = group dist group WORLD assert group None output copy_ all_to_all_single input output_split_sizes input_split_sizes group tag all_gather_inplace tensor_list list torch Tensor tensor torch Tensor group=None async_op=False tag str = assert async_op Can t remap async version inplace op functional collective assert tensor dim == all t size == tensor size t tensor_list Remapping variable size all_gather yet supported group = group dist group WORLD assert group None output = all_gather_tensor tensor group tag Use aten slice instead aten split because latter causes tensor shape unnecessarily baked when s SymInt output_splits = offset = t tensor_list is_scalar = t dim == t_offset = is_scalar t size pyrefly ignore unsupported-operation out = output offset is_scalar output offset offset + t_offset output_splits append out pyrefly ignore unsupported-operation offset += t_offset dst src zip tensor_list output_splits dst copy_ src tensor_list torch distributed distributed_c d pyrefly ignore deprecated _all_gather_base legacy_all_gather_base _reduce_scatter_base legacy_reduce_scatter_base all_gather legacy_all_gather all_gather_into_tensor legacy_allgather all_reduce legacy_allreduce all_to_all_single legacy_all_to_all_single reduce_scatter_tensor legacy_reducescatter This dict should contain sets functions dynamo allowed remap Functions set should accept same args kwargs their mapping traceable_collective_remaps = legacy_allgather all_gather_tensor_inplace type ignore has-type legacy_reducescatter reduce_scatter_tensor_inplace type ignore has-type legacy_allreduce all_reduce_inplace type ignore has-type legacy_all_to_all_single all_to_all_inplace type ignore has-type legacy_all_gather all_gather_inplace type ignore has-type legacy_reduce_scatter_base reduce_scatter_tensor_inplace type ignore has-type legacy_all_gather_base all_gather_tensor_inplace type ignore has-type