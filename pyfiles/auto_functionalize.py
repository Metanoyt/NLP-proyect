mypy allow-untyped-defs warnings abc ABC abstractmethod collections abc Callable Sequence dataclasses dataclass typing Any get_args Optional Union torch torch _library utils library_utils torch utils _pytree pytree torch Tensor torch _C DispatchKey torch _higher_order_ops utils _has_gen_schema call_op HopInstance HopSchema materialize_callable_in_args unique_graph_id torch _ops HigherOrderOperator OperatorBase OpOverload torch _prims_common clone_preserve_strides torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor disable_proxy_modes_tracing ProxyTorchDispatchMode track_tensor_tree SchemaHolder __init__ schema torch FunctionSchema schema = schema __eq__ other schema == other schema __hash__ - int hash schema classmethod from_tree_spec cls tree_spec pytree TreeSpec assert tree_spec None cls pytree tree_unflatten tree_spec schema register_constant allows us get tree_spec pytree tree_flatten SchemaHolder FunctionSchema The tree_spec proxable graph we can get back schema via schema = pytree tree_unflatten tree_spec schema pytree register_constant SchemaHolder get_base tensor torch is_inference_mode_enabled tensor _inference_mode_base tensor _base ViewInfo ABC base_index int __init__ base_index base_index = base_index abstractmethod regenerate_view bases_list list Tensor pass dataclass AsStridedViewInfo ViewInfo size Sequence Union int torch SymInt stride Sequence Union int torch SymInt storage_offset int __init__ base_index size stride storage_offset super __init__ base_index size = size stride = stride storage_offset = storage_offset regenerate_view bases_list list Tensor torch as_strided bases_list base_index size stride storage_offset dataclass SliceViewInfo ViewInfo dim Union int torch SymInt start Union int torch SymInt end Union int torch SymInt __init__ base_index dim start end super __init__ base_index dim = dim start = start end = end regenerate_view bases_list list Tensor torch ops aten slice Tensor bases_list base_index dim start end dataclass AliasViewInfo ViewInfo __init__ base_index super __init__ base_index regenerate_view bases_list list Tensor torch ops aten alias default bases_list base_index dataclass NotView ViewInfo __init__ base_index super __init__ base_index regenerate_view bases_list list Tensor bases_list base_index is_alias base tensor torch fx experimental symbolic_shapes statically_known_true sym_eq all statically_known_true sym_eq base storage_offset tensor storage_offset sym_eq base stride tensor stride sym_eq base size tensor size None dim start end try_use_slice base tensor torch fx experimental symbolic_shapes statically_known_true sym_eq This condition should never triggered is_alias base tensor base size TODO there cases can we use slice even stride len sizes equal statically_known_true sym_eq tensor stride base stride None statically_known_true sym_eq len tensor size len base size None dim = None count = i range len tensor size base size i = tensor size i dim = i count = count + count = None tensor storage_offset tensor stride dim = None start = tensor storage_offset tensor stride dim end = start + tensor size dim dim start end write_view_information_to_args mutable_arg_names list str mutable_arg_types list torch Type kwargs dict str Any arg_to_base_index dict str Any This function writes view information into kwargs It reads mutable_args kwargs uses arg_to_base_index tensor information write ViewInfo into kwargs mutable_arg_names mutable custom operator arg names mutable_arg_types mutable custom operator arg types kwargs original custom operator args arg_to_base_index maps mutable_arg_name int &#124; int refers base tensor corresponds input tensor write_single_view prefix str tensor Tensor base_index int assert f prefix _base_index kwargs assert f prefix _size kwargs assert f prefix _stride kwargs assert f prefix _storage_offset kwargs assert f prefix _slice_dim kwargs assert f prefix _slice_start kwargs assert f prefix _slice_end kwargs use_as_strided tensor kwargs f prefix _size = tensor size kwargs f prefix _stride = tensor stride kwargs f prefix _storage_offset = tensor storage_offset use_slice dim start end kwargs f prefix _slice_dim = dim kwargs f prefix _slice_start = start kwargs f prefix _slice_end = end use_alias kwargs f prefix _alias = True The start function tensor None kwargs f prefix _base_index = None base = get_base tensor kwargs f prefix _base_index = base_index base None no need add anything other than _base_index is_alias base tensor use_alias slice_info = try_use_slice base tensor None use_slice slice_info use_as_strided tensor arg_name arg_type zip mutable_arg_names mutable_arg_types arg = kwargs arg_name library_utils is_tensorlist_like_type arg_type arg None kwargs f _ arg_name _length = None kwargs f _ arg_name _length = len arg i elem enumerate arg write_single_view f _ arg_name _ i elem arg_to_base_index arg_name i library_utils is_tensor_like_type arg_type write_single_view f _ arg_name kwargs arg_name arg_to_base_index get arg_name type ignore arg-type raise RuntimeError f Unsupported type arg_type Returns dict arg_name - ViewInfo &#124; ViewInfo read_view_information_from_args mutable_arg_names list str mutable_arg_types list torch Type kwargs dict str Any all_bases list Tensor This reads view information added ` write_view_information_to_args ` kwargs pop them returns dict arg_name - ViewInfo &#124; ViewInfo input list maps each mutable arg its view information mutable_arg_names mutable custom operator arg names mutable_arg_types mutable custom operator arg types kwargs args auto_functionalize custom_op kwargs get_arg name kwargs pop name read_single_view prefix base_index = get_arg f prefix _base_index base_index None None f prefix _alias kwargs get_arg f prefix _alias AliasViewInfo base_index f prefix _storage_offset kwargs The view regenerated using as_strided size = get_arg f prefix _size stride = get_arg f prefix _stride storage_offset = get_arg f prefix _storage_offset AsStridedViewInfo base_index size stride storage_offset f prefix _slice_dim kwargs dim = get_arg f prefix _slice_dim start = get_arg f prefix _slice_start end = get_arg f prefix _slice_end SliceViewInfo base_index dim start end This means argument base tensor NotView base_index args_view_info dict str Any = arg_name arg_type zip mutable_arg_names mutable_arg_types library_utils is_tensorlist_like_type arg_type length = get_arg f _ arg_name _length length None The whole list None args_view_info arg_name = None args_view_info arg_name = read_single_view f _ arg_name _ i i range length library_utils is_tensor_like_type arg_type args_view_info arg_name = read_single_view f _ arg_name raise RuntimeError f Unsupported type arg_type args_view_info NOTE auto-functionalizing custom ops Users may wish torch compile custom ops mutate their inputs torch compile will automatically support op without anyone needing provide functionalization kernel Here s how Let s say we have hypothetical mylib sin_ Tensor x - op First when FakeTensor sees op - If schema says returns nothing we can generate trivial FakeTensor rule returns nothing - Otherwise user needs provide FakeTensor impl fake impl Next when Python FunctionalTensor sees op will functionalize emitting call auto_functionalize op x x HOP replacing mutated inputs corresponding outputs HOP This HOP effectively runs functional version op when called clones inputs will mutated runs op then returns output Tensors new values auto_functionalize_v improved version auto_functionalize better handle re-inplacing views AutoFunctionalized HigherOrderOperator auto_functionalized _mutable_op kwargs This HOP runs functional version _mutable_op Concretely looks all arguments mutable through _mutable_op s operator schema clones those kwargs runs ` out = _mutable_op kwargs ` cloned values then returns operator output concatenated cloned values mutated We have some restrictions ` _mutable_op ` See ` can_auto_functionalize ` restrictions We can likely lift many these users request The reason why _mutable_op prefixed underscore prevent collisions kwarg names kwargs __init__ - None super __init__ auto_functionalized cacheable=True __call__ _mutable_op OpOverload kwargs Any - tuple Any tuple Tensor assert can_auto_functionalize _mutable_op assert isinstance kwargs dict super __call__ _mutable_op kwargs auto_functionalized = AutoFunctionalized auto_functionalized __module__ = torch ops higher_order auto_functionalized fallthrough DispatchKey AutogradCPU auto_functionalized fallthrough DispatchKey AutogradCUDA _MutableOpType = Union OpOverload HigherOrderOperator AutoFunctionalizedV HigherOrderOperator auto_functionalized_v _mutable_op kwargs This HOP runs functional version _mutable_op Unlike AutoFunctionalized version improved better handle view tensors This version only used non export mode __init__ - None super __init__ auto_functionalized_v cacheable=True __call__ _mutable_op _MutableOpType kwargs Any - tuple Any tuple Tensor _op_to_check Optional Union OpOverload HopInstance = None isinstance _mutable_op HigherOrderOperator _op_to_check = HopInstance _mutable_op SchemaHolder from_tree_spec kwargs get _op_schema schema type ignore arg-type _op_to_check = _mutable_op assert _op_to_check None assert can_auto_functionalize _op_to_check assert isinstance kwargs dict super __call__ _mutable_op kwargs auto_functionalized_v = AutoFunctionalizedV auto_functionalized_v __module__ = torch ops higher_order auto_functionalized_v fallthrough DispatchKey AutogradCPU auto_functionalized_v fallthrough DispatchKey AutogradCUDA can_auto_functionalize op Union OperatorBase HopInstance - bool isinstance op HopInstance HOPs implement gen_schema schema functional auto_functionalizable _has_gen_schema op _op False isinstance op OpOverload False torch _library utils is_builtin op We control built-ins These may rare cases do input metadata mutation which we have banned custom ops False schema = op _schema schema is_mutable False schema = op _schema arg schema arguments arg alias_info None continue arg alias_info is_write continue torch _library utils is_tensor_like_type arg type continue torch _library utils is_tensorlist_like_type arg type continue False len schema returns == isinstance schema returns type torch NoneType Skip schema returns - None True isinstance op OpOverload The returns OpOverload must alias anything ret schema returns ret alias_info None type ret type torch TensorType continue Not yet supported List Tensor False torch _C _dispatch_has_kernel_for_dispatch_key op name Functionalize False True get_mutable_args_from_schema schema torch FunctionSchema - tuple list str list torch Type Returns list argument names get mutated according schema their types mutable_args_names = arg name arg schema arguments arg alias_info None arg alias_info is_write mutable_args_types = arg type arg schema arguments arg alias_info None arg alias_info is_write mutable_args_names mutable_args_types type ignore return-value get_mutable_args op OpOverload - tuple list str list torch Type get_mutable_args_from_schema op _schema do_auto_functionalize mode torch _subclasses functional_tensor FunctionalTensorMode op OpOverload args tuple Any kwargs dict str Any - Any Functionalizes call op args kwargs emitting call ` outs = auto_functionalized op normalized_kwargs ` replacing mutated args kwargs corresponding outputs The normalized_kwargs just args kwargs all kwarg form This makes handling easier auto_functionalized HOP torch _subclasses functional_tensor PythonFunctionalizeAPI ctx = PythonFunctionalizeAPI mode=mode All args kwargs all kwargs The names args come schema This makes easier us work them normalized_kwargs = schema = op _schema idx arg enumerate schema arguments NB torch_dispatch kwargs args defined kwarg-only schema arg name kwargs normalized_kwargs arg name = kwargs arg name idx len args its out bounds we don t need do anything means optional arg passed its default value normalized_kwargs arg name = args idx normalized_kwargs arg name = arg default_value unwrapped_kwargs = ctx unwrap_tensors normalized_kwargs type ignore arg-type unwrapped_kwargs self_ unwrapped_kwargs warnings warn Using ` ` ` self_ ` argument definition custom ops may lead ambiguous parsing Please consider using different name argument avoid potential issues stacklevel= ctx redispatch_to_next unwrapped_outs = auto_functionalized op unwrapped_kwargs type ignore arg-type List name args get mutated according schema mutable_args_names _ = get_mutable_args op unwrapped_actual_out Union Any tuple Any = unwrapped_outs -len mutable_args_names unwrapped_mutable_out = unwrapped_outs -len mutable_args_names len op _schema returns == assert unwrapped_actual_out None unwrapped_actual_out = None len op _schema returns == assert len unwrapped_actual_out == unwrapped_actual_out = unwrapped_actual_out assert len unwrapped_actual_out == len op _schema returns name unwrapped_out zip mutable_args_names unwrapped_mutable_out Can None input ` Tensor ` unwrapped_out None continue We only handle Tensor List Tensor here now sync_update o orig_arg ctx replace orig_arg o ctx commit_update orig_arg ctx sync orig_arg orig_arg = normalized_kwargs name isinstance unwrapped_out torch Tensor sync_update unwrapped_out orig_arg isinstance unwrapped_out list all isinstance o torch Tensor o unwrapped_out assert len orig_arg == len unwrapped_out orig_a o zip orig_arg unwrapped_out sync_update o orig_a raise RuntimeError f unsupported type auto-functionalization unwrapped_out ctx wrap_tensors unwrapped_actual_out type ignore arg-type Wrapper GraphModule applies functionalization during execution enable epilogue graph inlining better fusion opportunities subgraphs When tracing wrapper we ll get graph module epilogue We want hash according original graph module so when we go Functional mode - fake mode multiple invoke_subgraph calls share same inner graph module we can hit cache FunctionalCallableWithEpilogue __init__ orig_callable Callable orig_callable = orig_callable __call__ args kwargs We call torch func functionalize This allows us inline epilogue graph Inlining has benefit allowing easiser fusion inside subgraph Though epilogue graph contains copy_ OK because inductor can handle also how we have been supporting top-level graph input mutation tuple torch func functionalize orig_callable args kwargs __hash__ id orig_callable do_auto_functionalize_v mode torch _subclasses functional_tensor FunctionalTensorMode op Union OpOverload HopInstance args tuple Any kwargs dict str Any - Any torch _subclasses functional_tensor PythonFunctionalizeAPI ctx = PythonFunctionalizeAPI mode=mode All args kwargs all kwargs The names args come schema This makes easier us work them normalized_kwargs = schema = op _schema pyrefly ignore bad-assignment op = op _op isinstance op HopInstance op assert isinstance op get_args _MutableOpType _functionalize_callable arg Any callable arg FunctionalCallableWithEpilogue arg arg args kwargs = pytree tree_map _functionalize_callable args kwargs idx arg enumerate schema arguments NB torch_dispatch kwargs args defined kwarg-only schema arg name kwargs normalized_kwargs arg name = kwargs arg name idx len args its out bounds we don t need do anything means optional arg passed its default value normalized_kwargs arg name = args idx normalized_kwargs arg name = arg default_value List name args get mutated according schema mutable_args_names mutable_args_types = get_mutable_args_from_schema schema A list all bases mutable args without duplication all_bases = all_bases_addresses list int = Map arg_name index its base all_bases arg_to_base_index dict str Any = update_dict tensor arg_name index=None base = tensor get_base tensor None get_base tensor set_result base_index index None arg_to_base_index arg_name = base_index arg_to_base_index arg_name index = base_index all_bases_addresses __contains__ base _cdata all_bases_addresses append base _cdata all_bases append base set_result len all_bases - set_result all_bases_addresses index base _cdata arg_name mutable_args_names arg = normalized_kwargs arg_name arg None continue isinstance arg list arg_to_base_index arg_name = i tensor enumerate arg tensor None arg_to_base_index arg_name append None continue update_dict tensor arg_name i update_dict arg arg_name add view_meta each args into unwrapped_kwargs write_view_information_to_args mutable_args_names mutable_args_types normalized_kwargs arg_to_base_index remove mutated args kwargs its function _all_bases now arg_name mutable_args_names del normalized_kwargs arg_name type ignore arg-type unwrapped_kwargs = ctx unwrap_tensors normalized_kwargs type ignore arg-type unwrapped_kwargs self_ unwrapped_kwargs warnings warn Using ` ` ` self_ ` argument definition custom ops may lead ambiguous parsing Please consider using different name argument avoid potential issues stacklevel= all_basis_unwrapped = ctx unwrap_tensors all_bases assert _all_bases unwrapped_kwargs op unwrapped_kwargs auto_func_kwargs = dict unwrapped_kwargs _all_bases=all_basis_unwrapped isinstance op HigherOrderOperator assert _ops_schema unwrapped_kwargs op unwrapped_kwargs We pass tree_spec tree_flatten SchemaHolder make proxable auto_func_kwargs update _op_schema pytree tree_flatten SchemaHolder schema ctx redispatch_to_next unwrapped_outs = auto_functionalized_v op auto_func_kwargs type ignore arg-type unwrapped_actual_out Union Any tuple Any = unwrapped_outs len all_bases == unwrapped_outs -len all_bases unwrapped_mutable_out = len all_bases == unwrapped_outs -len all_bases isinstance op HigherOrderOperator assert len schema returns f hop expected least one output schema assert len unwrapped_actual_out == len schema returns len schema returns == assert unwrapped_actual_out None unwrapped_actual_out = None len schema returns == assert len unwrapped_actual_out == unwrapped_actual_out = unwrapped_actual_out assert len unwrapped_actual_out == len schema returns orig_arg unwrapped_out zip all_bases unwrapped_mutable_out Can None input ` Tensor ` unwrapped_out None continue We only handle Tensor List Tensor here now sync_update o orig_arg ctx replace orig_arg o ctx commit_update orig_arg ctx sync orig_arg isinstance unwrapped_out torch Tensor sync_update unwrapped_out orig_arg isinstance unwrapped_out list all isinstance o torch Tensor o unwrapped_out assert len orig_arg == len unwrapped_out orig_a o zip orig_arg unwrapped_out sync_update o orig_a raise RuntimeError f unsupported type auto-functionalization unwrapped_out ctx wrap_tensors unwrapped_actual_out type ignore arg-type auto_functionalize functions auto_functionalized py_impl DispatchKey CompositeExplicitAutograd auto_functionalized_dense _mutable_op OpOverload _only_clone_these_tensors Optional tuple str = None kwargs Any - tuple Any tuple Tensor new_kwargs = dict kwargs result = _mutable_args_names _ = get_mutable_args _mutable_op name _mutable_args_names _only_clone_these_tensors None name _only_clone_these_tensors new_kwargs name = kwargs name new_kwargs name = clone_preserve_strides x x kwargs name kwargs name None isinstance kwargs name list clone_preserve_strides kwargs name kwargs name None None result append new_kwargs name out = _mutable_op new_kwargs isinstance out tuple out result type ignore return-value out result type ignore return-value auto_functionalized py_impl FakeTensorMode auto_functionalized_fake mode _mutable_op OpOverload kwargs Any - tuple Any tuple Tensor mode result = auto_functionalized_dense _mutable_op _only_clone_these_tensors=None kwargs result auto_functionalized py_impl ProxyTorchDispatchMode auto_functionalized_proxy mode _mutable_op OpOverload kwargs Any - tuple Any tuple Tensor disable_proxy_modes_tracing out = auto_functionalized _mutable_op kwargs proxy_kwargs = pytree tree_map mode tracer unwrap_proxy kwargs out_proxy = mode tracer create_proxy call_function auto_functionalized _mutable_op proxy_kwargs result = track_tensor_tree out out_proxy constant=None tracer=mode tracer result auto_functionalized py_functionalize_impl auto_functionalized_func ctx _mutable_op kwargs unwrapped_kwargs = ctx unwrap_tensors kwargs ctx redispatch_to_next result = auto_functionalized _mutable_op unwrapped_kwargs ctx wrap_tensors result auto_functionalized_v functions auto_functionalized_v py_impl DispatchKey CompositeExplicitAutograd auto_functionalized_v _dense _mutable_op _MutableOpType _only_clone_these_bases Optional tuple int = None kwargs Any - tuple Any tuple Tensor _all_bases list Tensor = kwargs pop _all_bases _only_clone_these_bases None _only_clone_these_bases = tuple range len _all_bases isinstance _mutable_op OpOverload schema torch _C FunctionSchema = _mutable_op _schema schema = pytree tree_unflatten kwargs pop _op_schema schema isinstance _mutable_op OpOverload _callable_op Union HopInstance OpOverload = _mutable_op assert isinstance schema HopSchema _callable_op = HopInstance _mutable_op schema op_kwargs_new all_bases_new = _generate_new_op_kwargs_from_bases schema kwargs _all_bases _only_clone_these_bases out = call_op _callable_op tuple op_kwargs_new isinstance out tuple out all_bases_new type ignore return-value out all_bases_new type ignore return-value _generate_new_op_kwargs_from_bases schema kwargs all_bases _only_clone_these_bases mutable_args_names mutable_args_types = get_mutable_args_from_schema schema args_view_info = read_view_information_from_args mutable_args_names mutable_args_types kwargs all_bases maybe_copy i t t None None i _only_clone_these_bases clone_preserve_strides t t all_bases_new = maybe_copy i t i t enumerate all_bases create new args new_kwargs = dict kwargs re-generate all inputs all_bases_new using args_view_info add them new_kwargs arg_name mutable_args_names args_view_info arg_name None new_kwargs arg_name = None isinstance args_view_info arg_name list new_kwargs arg_name = i elem enumerate args_view_info arg_name elem None new_kwargs arg_name append None view_info = args_view_info arg_name i new_kwargs arg_name append view_info regenerate_view all_bases_new new_kwargs arg_name = args_view_info arg_name regenerate_view all_bases_new new_kwargs all_bases_new auto_functionalized_v py_impl FakeTensorMode auto_functionalized_v _fake mode _mutable_op _MutableOpType kwargs dict str Any - tuple Any tuple Tensor mode result = auto_functionalized_v _dense _mutable_op _only_clone_these_bases=None kwargs result auto_functionalized_v py_impl ProxyTorchDispatchMode auto_functionalized_v _proxy mode _mutable_op _MutableOpType kwargs Any - tuple Any tuple Tensor isinstance _mutable_op HigherOrderOperator Note materialize callable inputs graph Below code materializes callable inputs hop graph modules kwargs may contain general callables proxable e g FunctionWithNoFreeVars could happen when we auto_functionalize backward hop where backward fn callablle wraps forward graph module This function materialize callable args according schema hop We cannot materialize callables kwargs directly because inputs callable vary hops hop To make materialiation process generic all hops we trace function wraps hop let each hop itself figure out how trace its callable inputs Then we look schema traced hop node replace callable original kwarg traced subgraphs Specifically we first trace wrapped_fn calls into hop Then we look hop node traced graph graph module inputs hop Finally we replace original kwarg s callable graph module all_bases = kwargs get _all_bases _only_clone_these_bases = kwargs get _only_clone_these_bases _only_clone_these_bases None _only_clone_these_bases = tuple range len all_bases schema = pytree tree_unflatten kwargs get _op_schema schema type ignore arg-type new_kwargs _ = _generate_new_op_kwargs_from_bases schema k v k v kwargs items k _all_bases _op_schema all_bases _only_clone_these_bases _ materialized_kwargs = materialize_callable_in_args HopInstance _mutable_op schema tuple new_kwargs Only replace callabes kwargs materialized subgraphs The rest kwargs kept unchanged k v kwargs items callable v assert k materialized_kwargs isinstance materialized_kwargs k torch fx GraphModule kwargs k = materialized_kwargs k disable_proxy_modes_tracing out = auto_functionalized_v _mutable_op kwargs proxy_kwargs = pytree tree_map mode tracer unwrap_proxy kwargs isinstance _mutable_op HigherOrderOperator _maybe_register_subgraph val Any isinstance val torch fx GraphModule _ graph_name = unique_graph_id mode prefix= auto_functionalized_subgraph mode tracer root register_module graph_name val val val proxy_kwargs = pytree tree_map _maybe_register_subgraph proxy_kwargs out_proxy = mode tracer create_proxy call_function auto_functionalized_v _mutable_op proxy_kwargs result = track_tensor_tree out out_proxy constant=None tracer=mode tracer result auto_functionalized_v py_functionalize_impl auto_functionalized_v _func ctx _mutable_op kwargs unwrapped_kwargs = ctx unwrap_tensors kwargs ctx redispatch_to_next result = auto_functionalized_v _mutable_op unwrapped_kwargs ctx wrap_tensors result