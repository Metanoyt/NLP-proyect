Owner s oncall distributed os sys torch torch distributed dist torch distributed checkpoint dist_cp torch nn nn torch distributed checkpoint state_dict get_model_state_dict get_state_dict set_model_state_dict set_state_dict StateDictOptions torch distributed device_mesh init_device_mesh torch distributed fsdp FullyShardedDataParallel FSDP torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN torch testing _internal distributed _tensor common_dtensor DTensorTestBase with_comms torch testing _internal distributed checkpoint_utils with_temp_dir dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit DIM = PreTrainedModel nn Module __init__ - None super __init__ layer = nn Linear DIM DIM layer = nn Linear DIM DIM layer = nn Linear DIM DIM sequential = nn Sequential nn Linear DIM DIM nn ReLU module_list = nn ModuleList nn Linear DIM DIM nn ReLU relu = nn ReLU forward batch x = relu layer batch x = relu layer x x = relu layer x x = sequential x x = module_list module_list x x FineTuningModel nn Module __init__ - None super __init__ pretrain = PreTrainedModel p pretrain parameters p requires_grad = False layer = nn Linear DIM DIM layer = nn Linear DIM DIM layer = nn Linear DIM DIM relu = nn ReLU forward batch x = relu pretrain batch x = relu layer x x = relu layer x x = relu layer x x TestFineTuning DTensorTestBase property world_size - int min torch accelerator device_count property backend curr_backend = dist get_default_backend_for_device device_type f cpu gloo device_type curr_backend pretrain pretrain_dir str - None device_mesh = init_device_mesh device_type world_size model = PreTrainedModel device_type model = FSDP model device_mesh=device_mesh optim = torch optim Adam model parameters lr= e- Training _ range batch = torch rand DIM device=self device_type loss = model batch sum loss backward optim step optim zero_grad Save state_dict model_state_dict optim_state_dict = get_state_dict model optimizers=optim saved_state_dict = model model_state_dict optim optim_state_dict dist_cp save state_dict=saved_state_dict storage_writer=dist_cp FileSystemWriter pretrain_dir finetune pretrain_dir str finetune_dir str - None device_mesh = init_device_mesh device_type world_size model = FineTuningModel device_type TODO make parallelism more complicated e g using D + DDP model = FSDP model use_orig_params=True device_mesh=device_mesh optim = torch optim Adam model parameters lr= e- Simulate fine tuning restart after iterations i range Load pretrain submodules checkpoint pretrain_state_dict = get_model_state_dict model submodules= model pretrain options=StateDictOptions keep_submodule_prefixes=False dist_cp load model pretrain_state_dict storage_reader=dist_cp FileSystemReader pretrain_dir set_model_state_dict model model_state_dict= model pretrain pretrain_state_dict options=StateDictOptions strict=False try Load training submodules checkpoint model_state_dict optim_state_dict = get_state_dict model optimizers=optim options=StateDictOptions ignore_frozen_params=True dist_cp load_state_dict model model_state_dict optim optim_state_dict storage_reader=dist_cp FileSystemReader pretrain_dir set_state_dict model optimizers=optim model_state_dict=model_state_dict optim_state_dict=optim_state_dict options=StateDictOptions strict=False except KeyError If first round fine tuning then nothing saved If restart fine tuning then checkpoint should exit assertEqual i Training _ range batch = torch rand DIM device=self device_type loss = model batch sum loss backward optim step optim zero_grad Save state_dict model_state_dict optim_state_dict = get_state_dict model optimizers=optim options=StateDictOptions ignore_frozen_params=True saved_state_dict = model model_state_dict optim optim_state_dict dist_cp save state_dict=saved_state_dict storage_writer=dist_cp FileSystemWriter finetune_dir skip_if_lt_x_gpu with_comms with_temp_dir test_fine_tuning - None assertTrue os path exists temp_dir pretrain_dir = os path join temp_dir pretrain finetune_dir = os path join temp_dir finetune print pretrain_dir finetune_dir rank == os mkdir pretrain_dir os mkdir finetune_dir dist barrier os sync assertTrue os path exists pretrain_dir assertTrue os path exists finetune_dir pretrain pretrain_dir finetune pretrain_dir finetune_dir __name__ == __main__ run_tests