mypy allow-untyped-defs operator collections abc Callable typing Any Optional Union torch torch ao nn intrinsic nni torch ao nn intrinsic quantized nniq torch ao nn intrinsic quantized dynamic nniqd torch ao nn quantized nnq torch ao nn quantized dynamic nnqd torch ao nn quantized reference nnqr torch nn nn torch nn functional F torch ao nn quantized modules utils WeightedQuantizedModule torch ao quantization qconfig QConfigAny torch ao quantization quantization_mappings get_quantized_operator torch ao quantization utils _parent_name torch fx GraphModule map_arg Node torch fx graph Graph utils collect_producer_nodes create_node_from_old_node_preserve_meta get_linear_prepack_op_for_dtype get_new_attr_name_with_prefix get_qconv_prepack_op graph_module_from_producer_nodes QOP_TO_ARG_NAMES_TO_SKIP dict Callable Any list str = torch _ops ops quantized hardswish inplace torch _ops ops quantized elu inplace torch _ops ops quantized dropout inplace torch _ops ops quantized instance_norm running_mean running_var use_input_stats momentum _is_node_in_list node modules func_list method_list module_type_list is_call_function = node op == call_function node target func_list is_call_method = node op == call_method node target method_list is_call_module = node op == call_module type modules str node target module_type_list is_call_function is_call_method is_call_module is_fixed_qparams_node node modules func_list = torch nn functional hardsigmoid torch nn functional sigmoid torch sigmoid torch tanh method_list = hardsigmoid hardsigmoid_ sigmoid sigmoid_ tanh tanh_ module_type_list = torch nn Hardsigmoid torch nn Sigmoid torch nn Tanh torch nn Softmax _is_node_in_list node modules func_list method_list module_type_list is_default_node node modules func_list = torch nn functional elu torch nn functional hardswish torch nn functional instance_norm torch nn functional layer_norm torch nn functional leaky_relu torch nn functional dropout method_list list Any = module_type_list = nnqr ConvTranspose d nnqr ConvTranspose d nnqr ConvTranspose d torch nn ELU torch nn LeakyReLU torch nn Hardswish torch nn InstanceNorm d torch nn InstanceNorm d torch nn InstanceNorm d torch nn LayerNorm torch nn Dropout torch nn PReLU torch nn BatchNorm d torch nn BatchNorm d torch ao nn intrinsic BNReLU d torch ao nn intrinsic BNReLU d _is_node_in_list node modules func_list method_list module_type_list is_copy_node node modules func_list = torch adaptive_avg_pool d torch nn functional adaptive_avg_pool d torch nn functional adaptive_avg_pool d torch nn functional hardtanh torch nn functional hardtanh_ torch nn functional interpolate torch nn functional max_pool d torch nn functional max_pool d torch nn functional max_pool d torch nn functional relu torch nn functional relu torch avg_pool d torch _C _nn avg_pool d torch _C _nn avg_pool d torch clamp torch flatten torch mean operator floordiv F channel_shuffle torch channel_shuffle essentially same thing so we only need put one them here torch channel_shuffle method_list = clamp mean relu relu_ module_type_list = torch nn AdaptiveAvgPool d torch nn AdaptiveAvgPool d torch nn AdaptiveAvgPool d torch nn AvgPool d torch nn AvgPool d torch nn AvgPool d torch nn Hardtanh torch nn MaxPool d torch nn MaxPool d torch nn MaxPool d torch nn ReLU torch nn ReLU torch nn ChannelShuffle _is_node_in_list node modules func_list method_list module_type_list is_general_tensor_shape_node node modules func_list = torch narrow torch transpose torch repeat_interleave torch squeeze torch stack torch unsqueeze torch nn functional pixel_shuffle torch nn functional pixel_unshuffle method_list = contiguous detach detach_ permute repeat repeat_interleave reshape resize_ shape size squeeze squeeze_ transpose unsqueeze unsqueeze_ view module_type_list = torch nn Identity torch nn PixelShuffle torch nn PixelUnshuffle _is_node_in_list node modules func_list method_list module_type_list is_other_node node modules func_list = torch cat method_list list Any = module_type_list list Any = _is_node_in_list node modules func_list method_list module_type_list is_special_pattern_node node modules res_function res_method res_module = False False False checker is_fixed_qparams_node is_default_node is_copy_node is_general_tensor_shape_node is_other_node is_call_function is_call_method is_call_module = checker node modules res_function = res_function is_call_function res_method = res_method is_call_method res_module = res_module is_call_module res_function res_method res_module is_dequantize_node node isinstance node Node node op == call_method node target == dequantize is_getattr_tensor_metadata_node node node op == call_function node target getattr node args == shape is_get_tensor_info_node node node op == call_method node target shape size should_skip_lowering op torch fx node Node qconfig_map dict str QConfigAny Return True op configured None qconfig False otherwise Note maybe need generalize also check dtype we only lower when dtype matches right now fbgemm qnnpack only support single dtype so OK now op name qconfig_map qconfig_map op name None Mapping reference module replacement static quantized module lowering STATIC_LOWER_MODULE_MAP dict type nn Module type WeightedQuantizedModule = nnqr Linear nnq Linear nnqr Conv d nnq Conv d nnqr Conv d nnq Conv d nnqr Conv d nnq Conv d Mapping reference module replacement dynamic quantized module lowering DYNAMIC_LOWER_MODULE_MAP dict type nn Module type nn Module = nnqr Linear nnqd Linear nnqr GRUCell nnqd GRUCell nnqr LSTMCell nnqd LSTMCell nnqr RNNCell nnqd RNNCell nnqr LSTM nnqd LSTM nnqr GRU nnqd GRU Mapping reference module replacement weight only quantized module lowering TODO correct namespace these modules WEIGHT_ONLY_LOWER_MODULE_MAP dict type nn Module type nn Module = nnqr Embedding nnq Embedding nnqr EmbeddingBag nnq EmbeddingBag TODO merge STATIC_LOWER_MODULE_MAP after we merge _lower_static_weighted_ref_module special_pattern_replacement SPECIAL_PATTERN_LOWER_MODULE_MAP = nn BatchNorm d nnq BatchNorm d nn BatchNorm d nnq BatchNorm d nnqr ConvTranspose d nnq ConvTranspose d nnqr ConvTranspose d nnq ConvTranspose d nnqr ConvTranspose d nnq ConvTranspose d nn ELU nnq ELU nn LeakyReLU nnq LeakyReLU nn Hardswish nnq Hardswish nn InstanceNorm d nnq InstanceNorm d nn InstanceNorm d nnq InstanceNorm d nn InstanceNorm d nnq InstanceNorm d nn LayerNorm nnq LayerNorm nn Dropout nnq Dropout nn Softmax nnq Softmax nn PReLU nnq PReLU nni BNReLU d nniq BNReLU d nni BNReLU d nniq BNReLU d Mapping fused module -tuple The inner reference module The replacement static quantized module lowering STATIC_LOWER_FUSED_MODULE_MAP dict type nn Module tuple type nn Module type WeightedQuantizedModule = nni LinearReLU nnqr Linear nniq LinearReLU TODO LinearLeakyReLU registered global only fused lowered when ondnn s backend config used Maybe need separate registration lowering functions different backends future nni LinearLeakyReLU nnqr Linear nniq LinearLeakyReLU nni LinearTanh nnqr Linear nniq LinearTanh nni ConvReLU d nnqr Conv d nniq ConvReLU d nni ConvReLU d nnqr Conv d nniq ConvReLU d nni ConvReLU d nnqr Conv d nniq ConvReLU d The difference between STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP STATIC_LOWER_FUSED_MODULE_MAP The refer node inside STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP has inputs Mapping fused module -tuple The inner reference module The replacement static quantized module lowering STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP dict type nn Module tuple type nn Module type WeightedQuantizedModule = nni ConvAdd d nnqr Conv d nniq ConvAdd d nni ConvAddReLU d nnqr Conv d nniq ConvAddReLU d Mapping fused module -tuple The inner reference module The replacement dynamic quantized module lowering DYNAMIC_LOWER_FUSED_MODULE_MAP dict type nn Module tuple type nn Module type nn Module = nni LinearReLU nnqr Linear nniqd LinearReLU Mapping functional lower -tuple The quantized version op The quantized version op fused relu exists None STATIC_LOWER_FUNCTIONAL_MAP dict Callable tuple Callable Optional Callable = F linear torch ops quantized linear torch ops quantized linear_relu F conv d torch ops quantized conv d torch ops quantized conv d_relu F conv d torch ops quantized conv d torch ops quantized conv d_relu F conv d torch ops quantized conv d torch ops quantized conv d_relu F conv_transpose d torch ops quantized conv_transpose d None F conv_transpose d torch ops quantized conv_transpose d None F conv_transpose d torch ops quantized conv_transpose d None WEIGHT_PREPACK_OPS set Callable = torch _ops ops quantized linear_prepack torch _ops ops quantized linear_prepack_fp torch _ops ops quantized conv d_prepack torch _ops ops quantized conv d_prepack torch _ops ops quantized conv d_prepack torch ops quantized conv_transpose d_prepack torch ops quantized conv_transpose d_prepack torch ops quantized conv_transpose d_prepack Mapping functional dictionary where key -tuple input_activation_dtype weight_dtype value -tuple The dynamically quantized version op The dynamically quantized version op fused relu exists None DYNAMIC_LOWER_FUNCTIONAL_MAP dict Callable dict tuple torch dtype torch dtype tuple Callable Optional Callable = F linear torch quint torch qint torch ops quantized linear_dynamic torch ops quantized linear_relu_dynamic torch float torch float torch ops quantized linear_dynamic_fp torch ops quantized linear_relu_dynamic_fp dynamic conv + relu available yet F conv d torch quint torch qint torch ops quantized conv d_dynamic None F conv d torch quint torch qint torch ops quantized conv d_dynamic None F conv d torch quint torch qint torch ops quantized conv d_dynamic None CONV_FUNCTIONAL_OPS set Callable = F conv d F conv d F conv d CONV_TRANSPOSE_FUNCTIONAL_OPS set Callable = F conv_transpose d F conv_transpose d F conv_transpose d TODO add tests lowering these ops QBIN_OP_MAPPING dict Union Callable str Callable = operator add torch ops quantized add torch add torch ops quantized add operator mul torch ops quantized mul operator matmul torch ops quantized matmul torch mul torch ops quantized mul torch matmul torch ops quantized matmul QBIN_RELU_OP_MAPPING dict Union Callable str Callable = operator add torch ops quantized add_relu torch add torch ops quantized add_relu operator mul torch ops quantized mul_relu torch mul torch ops quantized mul_relu ORIGINAL_WEIGHTS_LOOKUP = original_weights_lookup _save_packed_weight destination prefix keep_vars attr_name dir _packed_weight attr_name isinstance getattr attr_name torch _C ScriptObject type ignore attr-defined packed_weight = getattr attr_name destination prefix + attr_name = packed_weight _load_packed_weight state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs attrs_to_pop = attr_name state_dict attr_name startswith _packed_weight isinstance state_dict attr_name torch _C ScriptObject type ignore attr-defined noqa B setattr attr_name state_dict attr_name attrs_to_pop append attr_name pop packed param attributesn attr_name attrs_to_pop state_dict pop attr_name fold_weight quantized_model GraphModule node_name_to_scope dict str tuple str type keep_original_weights bool = False - GraphModule Trace back weight node util we hit getattr reconstruct graph module traced nodes run graph module pack weight then replace original chain ops packed weight packed_weights = map folded node name prepacked weight name folded_nodes = original_weights_lookup dict str list = lookup_counter = get packed weights node quantized_model graph nodes node op == call_function node target WEIGHT_PREPACK_OPS nodes_to_fold = collect_producer_nodes node nodes_to_fold None node_to_fold nodes_to_fold folded_nodes node_to_fold name = node prepacking_module = graph_module_from_producer_nodes quantized_model nodes_to_fold packed_weight = prepacking_module packed_weights node name = packed_weight keep_original_weights original_weights = list prepacking_module state_dict values original_weights_lookup str lookup_counter = sorted original_weights key=lambda x x numel reverse=True len original_weights_lookup str lookup_counter == bias None original_weights_lookup str lookup_counter append None lookup_counter += lookup_counter = remove folded nodes replace prepacking node getattr folded_graph = Graph env dict Any Any = load_arg map_arg lambda node env node name node quantized_model graph nodes prepack_node = folded_nodes get node name None prepack_node node packed_weight = packed_weights node name add prepacked attribute root op_node = next iter prepack_node users module_path _ = node_name_to_scope op_node name get_new_packed_weight_name = get_new_attr_name_with_prefix module_path + _packed_weight_ packed_weight_name = get_new_packed_weight_name quantized_model setattr quantized_model packed_weight_name packed_weight replace prepack node getattr node env node name = folded_graph create_node get_attr packed_weight_name keep_original_weights key_name = packed_weight_name replace _ replace _ replace &#124; _ replace lower original_weights_lookup key_name = original_weights_lookup str lookup_counter del original_weights_lookup str lookup_counter lookup_counter += prepack_node None remove fold node continue copy other nodes env node name = folded_graph node_copy node load_arg quantized_model = GraphModule quantized_model folded_graph quantized_model _register_state_dict_hook _save_packed_weight quantized_model register_load_state_dict_pre_hook _load_packed_weight keep_original_weights setattr noqa B quantized_model ORIGINAL_WEIGHTS_LOOKUP original_weights_lookup quantized_model _get_module node Node modules dict str nn Module - Optional nn Module Return ` torch nn Module ` corresponds specified node s target If no such node exists None node op == call_module str node target modules modules str node target None _match_static_pattern node Node modules dict str nn Module qconfig_map dict str QConfigAny matching_modules_or_ops list Callable dequantize_node_arg_indices list int - Union tuple Node Node Node tuple None None None Match pattern dequantize - ref node - quantize against node provided If there match -tuple q_node quantize node relu_node relu node wrapping ref_node ref_node reference module functional node replace its quantized counterpart Otherwise there no match -tuple None None None Parameters node The ` torch fx Node ` match against modules A mapping node names modules model graph used module lookup qconfig_map A mapping node names qconfigs associated nodes If corresponding qconfig reference node None then no match matching_modules_or_ops Either list functions list ` torch nn Module ` s If reference node list then no match dequantize_node_arg_indices A list indices reference node args where dequantize nodes may present An empty list means skipping check dequantize nodes SKIP_LOWERING_VALUE = None None None Match quantize node node op = call_function node target = torch quantize_per_tensor SKIP_LOWERING_VALUE q_node = node ref_node = q_node args isinstance ref_node Node raise AssertionError Expected reference node torch fx Node Handle cases where node wrapped ReLU ref_node op == call_function ref_node target F relu torch relu ref_node op == call_module type _get_module ref_node modules nn ReLU relu_node = ref_node ref_node = relu_node args isinstance ref_node Node raise AssertionError Expected reference node after ReLU torch fx Node relu_node = None should_skip_lowering ref_node qconfig_map SKIP_LOWERING_VALUE Match reference module functional isinstance matching_modules_or_ops type issubclass matching_modules_or_ops nn Module expected_op = call_module match_key = type _get_module ref_node modules expected_op = call_function match_key = ref_node target type ignore assignment ref_node op = expected_op match_key matching_modules_or_ops SKIP_LOWERING_VALUE Match dequantize node s Both following conditions must pass All ` torch fx Node ` s matching indices must dequantize node There must least one dequantize node matched_dequantize = False i dequantize_node_arg_indices i = len ref_node args raise AssertionError f Dequantize index i exceeded reference node s arg length len ref_node args arg = ref_node args i is_dequantize_node arg matched_dequantize = True isinstance arg Node SKIP_LOWERING_VALUE matched_dequantize SKIP_LOWERING_VALUE q_node relu_node ref_node type ignore return-value _match_static_pattern_with_two_inputs node Node modules dict str nn Module qconfig_map dict str QConfigAny matching_modules_or_ops list Callable - Union tuple Node Node tuple None None dequantize \ Match pattern dequantize - ref node - quantize against node provided If there match -tuple q_node quantize node ref_node reference module functional node replace its quantized counterpart Otherwise there no match -tuple None None Parameters node The ` torch fx Node ` match against modules A mapping node names modules model graph used module lookup qconfig_map A mapping node names qconfigs associated nodes If corresponding qconfig reference node None then no match matching_modules_or_ops Either list functions list ` torch nn Module ` s If reference node list then no match SKIP_LOWERING_VALUE = None None Match quantize node node op = call_function node target = torch quantize_per_tensor SKIP_LOWERING_VALUE q_node = node ref_node = q_node args isinstance ref_node Node raise AssertionError Expected reference node torch fx Node should_skip_lowering ref_node qconfig_map SKIP_LOWERING_VALUE Match reference module functional isinstance matching_modules_or_ops type issubclass matching_modules_or_ops nn Module expected_op = call_module match_key = type _get_module ref_node modules This pass only support op call_module SKIP_LOWERING_VALUE ref_node op = expected_op match_key matching_modules_or_ops SKIP_LOWERING_VALUE Check ref_node has input nodes both dq node len ref_node args = SKIP_LOWERING_VALUE i range len ref_node args arg = ref_node args i is_dequantize_node arg SKIP_LOWERING_VALUE q_node ref_node _lower_static_weighted_ref_module model GraphModule qconfig_map dict str QConfigAny Traverse graph find dequantize - ref module - quantize patterns replace them quantized version ref module modules = dict model named_modules remove_duplicate=False n model graph nodes Step Find nodes match pattern dequantize - ref module - quantize matching_modules = list STATIC_LOWER_MODULE_MAP keys + list STATIC_LOWER_FUSED_MODULE_MAP keys q_node _relu_node ref_node = _match_static_pattern n modules qconfig_map matching_modules type ignore arg-type dequantize_node_arg_indices= q_node None continue ref_node None raise AssertionError Expected reference node when matching static pattern _ scale_node zero_point_node _ = q_node args ref_module = _get_module ref_node modules ref_class = type ref_module isinstance scale_node Node raise AssertionError Expected scale_node Node isinstance zero_point_node Node raise AssertionError Expected zero_point_node Node issubclass ref_class nn Module raise AssertionError Expected reference module subclass nn Module Step Change pattern use corresponding quantized module For fused modules we also check whether inner module reference module If so we replace entire fused module corresponding quantized module ref_class STATIC_LOWER_FUSED_MODULE_MAP inner_ref_class q_class = STATIC_LOWER_FUSED_MODULE_MAP ref_class type ref_module inner_ref_class type ignore index continue q_class = STATIC_LOWER_MODULE_MAP ref_class output_scale = getattr model scale_node target type ignore arg-type output_zero_point = getattr model zero_point_node target type ignore arg-type q_module = q_class from_reference ref_module output_scale output_zero_point replace reference module quantized module parent_name module_name = _parent_name ref_node target setattr modules parent_name module_name q_module Step Reroute around dq_node remove q_node its args len ref_node args = raise AssertionError Expected reference node have exactly arg dq_node = ref_node args isinstance dq_node Node raise AssertionError Expected dq_node Node ref_node replace_input_with dq_node dq_node args type ignore arg-type q_node replace_all_uses_with ref_node model graph erase_node q_node model graph erase_node scale_node model graph erase_node zero_point_node _lower_static_weighted_ref_module_with_two_inputs model GraphModule qconfig_map dict str QConfigAny Traverse graph find patterns dequantize dequantize \\ ref module \\ quantize replace them quantized version ref module modules = dict model named_modules remove_duplicate=False n model graph nodes dequantize \ Step Find nodes match pattern dequantize - ref module - quantize matching_modules = list STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP keys q_node ref_node = _match_static_pattern_with_two_inputs n modules qconfig_map matching_modules type ignore arg-type q_node None continue ref_node None raise AssertionError Expected reference node when matching static pattern two inputs _ scale_node zero_point_node _ = q_node args ref_module = _get_module ref_node modules ref_class = type ref_module isinstance scale_node Node raise AssertionError Expected scale_node Node isinstance zero_point_node Node raise AssertionError Expected zero_point_node Node issubclass ref_class nn Module raise AssertionError Expected reference module subclass nn Module Step Change pattern use corresponding quantized module For fused modules we also check whether inner module reference module If so we replace entire fused module corresponding quantized module ref_class STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP inner_ref_class q_class = STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP ref_class type ref_module inner_ref_class type ignore index continue continue output_scale = getattr model scale_node target type ignore arg-type output_zero_point = getattr model zero_point_node target type ignore arg-type q_module = q_class from_reference ref_module output_scale output_zero_point replace reference module quantized module parent_name module_name = _parent_name ref_node target setattr modules parent_name module_name q_module Step Reroute around dq_node remove q_node its args len ref_node args = raise AssertionError Expected reference node have exactly args arg ref_node args is_dequantize_node arg continue dq_node = arg isinstance dq_node Node raise AssertionError Expected dq_node Node ref_node replace_input_with dq_node dq_node args type ignore arg-type q_node replace_all_uses_with ref_node model graph erase_node q_node model graph erase_node scale_node model graph erase_node zero_point_node _lower_dynamic_weighted_ref_module model GraphModule Traverse graph find quantize_per_tensor_dynamic - dequantize - ref_module patterns replace them dynamically quantized version ref module named_modules = dict model named_modules remove_duplicate=False n model graph nodes n op = call_module type named_modules str n target set DYNAMIC_LOWER_MODULE_MAP keys union set DYNAMIC_LOWER_FUSED_MODULE_MAP keys continue ref_node = n dq_node = ref_node args dq_node op = call_method dq_node target = dequantize continue input_dynamic_q_node = dq_node args input_dynamic_q_node op = call_function input_dynamic_q_node target = torch quantize_per_tensor_dynamic continue activation_dtype = input_dynamic_q_node args is_fp = activation_dtype == torch float is_int = activation_dtype torch quint torch qint is_int is_fp continue ref_module = named_modules str ref_node target ref_class = type ref_module ref_class DYNAMIC_LOWER_FUSED_MODULE_MAP inner_ref_class q_class = DYNAMIC_LOWER_FUSED_MODULE_MAP ref_class type ref_module inner_ref_class continue q_class = DYNAMIC_LOWER_MODULE_MAP get ref_class type ignore assignment TODO maybe define WeightedDynamicallyQuantizedModule q_module = q_class from_reference ref_module type ignore attr-defined replace reference module dynamically quantized module parent_name module_name = _parent_name ref_node target setattr named_modules parent_name module_name q_module ref_node replace_input_with dq_node input_dynamic_q_node args _lower_weight_only_weighted_ref_module model GraphModule Traverse graph find ref_module patterns replace them weight only quantized version ref module named_modules = dict model named_modules remove_duplicate=False n model graph nodes n op = call_module type named_modules str n target set WEIGHT_ONLY_LOWER_MODULE_MAP keys continue ref_node = n ref_module = named_modules str ref_node target ref_class = type ref_module q_class = WEIGHT_ONLY_LOWER_MODULE_MAP get ref_class TODO WeightedQuantizedModule currently assuming static quant apis output_scale output_zero_point from_reference we may want relax rename TODO maybe define WeightedWeightOnlyQuantizedModule q_module = q_class from_reference ref_module type ignore union-attr replace reference module dynamically quantized module parent_name module_name = _parent_name ref_node target setattr named_modules parent_name module_name q_module _lower_static_weighted_ref_functional model GraphModule qconfig_map dict str QConfigAny Traverse graph replace functional reference patterns their quantized versions modules = dict model named_modules remove_duplicate=False n model graph nodes Step Find nodes match pattern dequantize - functional op - quantize matching_ops = list STATIC_LOWER_FUNCTIONAL_MAP keys q_node relu_node func_node = _match_static_pattern n modules qconfig_map matching_ops dequantize_node_arg_indices= q_node None continue func_node None raise AssertionError Expected function node when matching static functional pattern _ output_scale_node output_zp_node _ = q_node args input_dq_node weight_dq_node remaining_func_args = func_node args isinstance output_zp_node Node raise AssertionError Expected output_zp_node Node isinstance input_dq_node Node raise AssertionError Expected input_dq_node Node isinstance weight_dq_node Node raise AssertionError Expected weight_dq_node Node quantized_weight = weight_dq_node args isinstance quantized_weight Node raise AssertionError Expected quantized_weight Node quantized_weight op = call_function quantized_weight target torch quantize_per_tensor torch quantize_per_channel continue Step Replace quantized weights packed weights which will folded later Use right prepack op prepare corresponding args Linear prepack args quantized weights bias Conv prepack args quantized weights bias stride padding dilation groups prepack_args = quantized_weight + remaining_func_args func_node target F linear weight_dtype = quantized_weight args - prepack_op = get_linear_prepack_op_for_dtype weight_dtype func_node target CONV_FUNCTIONAL_OPS prepack_op = get_qconv_prepack_op func_node target type ignore arg-type For conv d stride padding dilation args may ints which case we need convert them tuples func_node target F conv d i len prepack_args i isinstance prepack_args i int prepack_args i = prepack_args i func_node target CONV_TRANSPOSE_FUNCTIONAL_OPS prepack_op = get_qconv_prepack_op func_node target type ignore arg-type For conv_transpose d stride padding dilation args may ints which case we need convert them tuples func_node target F conv_transpose d Note prepack_args groups i len prepack_args i isinstance prepack_args i int prepack_args i = prepack_args i swap dilation groups prepack op has arguments w b stride padding output_padding dilation groups transposed conv op has arguments x w b stride padding output_padding groups dilation len prepack_args prepack_args prepack_args = prepack_args prepack_args raise ValueError f Lowering supported op func_node target model graph inserting_before output_scale_node type ignore arg-type kwargs func node needed prepack op i e quantized linear_prepack They needed compute op i e quantized linear kwargs = func_node kwargs F linear uses bias key bias while qlinear_prepack uses B bias func_node target F linear bias kwargs kwargs = kwargs copy kwargs B = kwargs bias del kwargs bias packed_weight = model graph create_node call_function prepack_op tuple prepack_args kwargs Step Replace reference pattern corresponding quantized op q_func q_relu_func = STATIC_LOWER_FUNCTIONAL_MAP func_node target type ignore index conv_transpose does support fusion relu yet q_relu_func None such cases q_relu_func None func_node target = q_relu_func relu_node None q_func func_node target = q_func func_node args = input_dq_node args packed_weight output_scale_node output_zp_node kwargs func_node has been moved kwargs prepack op func_node kwargs = q_node replace_all_uses_with func_node Move func_node after output_zp_node graph output_zp_node append func_node Clean up Remove quantize node relu node exists model graph erase_node q_node relu_node None q_relu_func None model graph erase_node relu_node _lower_dynamic_weighted_ref_functional model GraphModule qconfig_map dict str QConfigAny Traverse graph replace functional reference patterns their dynamically quantized versions Examples quantize_per_tensor_dynamic - dequantize - functional linear -- linear_dynamic torch float - dequantize - functional linear -- linear_dynamic_fp modules = dict model named_modules remove_duplicate=False we want search reserved order so we can match larger patterns first e g we want match linear - relu before linear n reversed model graph nodes Step Find nodes match pattern quantize_per_tensor_dynamic - dequantize - dynamically quantized op We search pattern backwards starting quantize node Quantize node args func scale zp dtype func_node = n Handle cases where functional op wrapped ReLU func_node op == call_function func_node target F relu func_node op == call_module type modules str func_node target torch nn ReLU relu_node = func_node func_node = relu_node args relu_node = None should_skip_lowering func_node qconfig_map continue Linear args dequantized inputs dequantized weights bias Conv args dequantized inputs dequantized weights bias stride padding dilation groups func_node op = call_function func_node target DYNAMIC_LOWER_FUNCTIONAL_MAP continue input_dq_node weight_dq_node remaining_func_args = func_node args input_dq_node op = call_method input_dq_node target = dequantize weight_dq_node op = call_method weight_dq_node target = dequantize continue input_dynamic_q_node = input_dq_node args input_dynamic_q_node op = call_function input_dynamic_q_node target = torch quantize_per_tensor_dynamic continue reduce_range_node = None pattern_input activation_dtype reduce_range_node = input_dynamic_q_node args is_fp = activation_dtype == torch float is_int = activation_dtype torch quint torch qint is_int is_fp continue quantized_weight = weight_dq_node args weight_dtype = quantized_weight args - Step Try select reference pattern corresponding quantized op dynamic_quant_dtype_key = activation_dtype weight_dtype dynamic_quant_dtype_key DYNAMIC_LOWER_FUNCTIONAL_MAP func_node target print f Didn t find dtype combination dynamic_quant_dtype_key during f dynamic quantized op lowering func_node target continue q_func q_relu_func = DYNAMIC_LOWER_FUNCTIONAL_MAP func_node target dynamic_quant_dtype_key q_func None q_relu_func None print Didn t find corresponding quantized function quantized relu function f func_node target dynamic_quant_dtype_key continue Step Replace quantized weights packed weights which will folded later Use right prepack op prepare corresponding args Linear prepack args quantized weights bias Conv prepack args quantized weights bias stride padding dilation groups prepack_args = quantized_weight + remaining_func_args prepack_kwargs = func_node target F linear prepack_op = get_linear_prepack_op_for_dtype weight_dtype kwargs = func_node kwargs copy bias kwargs prepack_kwargs B = kwargs bias del kwargs bias func_node kwargs = kwargs func_node target CONV_FUNCTIONAL_OPS prepack_op = get_qconv_prepack_op func_node target For conv d stride padding dilation args may ints which case we need convert them tuples func_node target F conv d i len prepack_args i isinstance prepack_args i int prepack_args i = prepack_args i raise ValueError f Lowering supported op func_node target model graph inserting_before func_node packed_weight = model graph create_node call_function prepack_op tuple prepack_args prepack_kwargs Step Replace reference pattern corresponding quantized op func_node target = q_relu_func relu_node None q_func is_int func_node args = pattern_input packed_weight reduce_range_node func_node args = pattern_input packed_weight relu_node None relu_node replace_all_uses_with func_node Step Remove relu node exists relu_node None model graph erase_node relu_node _lower_quantized_binary_op model GraphModule qconfig_map dict str QConfigAny binary_ops_to_lower list Callable = operator add torch add operator mul torch mul torch matmul modules = dict model named_modules remove_duplicate=False n model graph nodes Step Find nodes match pattern dequantize - ref module - quantize q_node relu_node bop_node = _match_static_pattern n modules qconfig_map binary_ops_to_lower dequantize_node_arg_indices= q_node None continue bop_node None raise AssertionError Expected binary op node when matching quantized binary op pattern _ scale_node zero_point_node _ = q_node args Step Remove dequant nodes num_dq_nodes = arg bop_node args is_dequantize_node arg continue dq_node = arg isinstance dq_node Node raise AssertionError Expected dq_node Node dn_input = dq_node args bop_node replace_input_with dq_node dn_input type ignore arg-type num_dq_nodes += num_dq_nodes = raise AssertionError Expected least one dequantize node binary op args Step Swap binary op quantized binary op bop_node target QBIN_OP_MAPPING raise AssertionError f Unsupported binary op bop_node target lowering binop_to_qbinop = QBIN_OP_MAPPING relu_node None QBIN_RELU_OP_MAPPING qbin_op = binop_to_qbinop bop_node target prepare args quantized binary op x y qop_node_args = list bop_node args x y scale zero_point add scale zero_point arguments Tensor - Tensor operation num_dq_nodes == qop_node_args extend scale_node zero_point_node insert call quantized binary op remove original binary op model graph inserting_after q_node qop_node = create_node_from_old_node_preserve_meta model graph call_function qbin_op tuple qop_node_args bop_node q_node replace_all_uses_with qop_node Step Remove quantize node binary op node relu node any model graph erase_node q_node relu_node None model graph erase_node relu_node model graph erase_node bop_node special_pattern_replacement model GraphModule modules = dict model named_modules remove_duplicate=False n model graph nodes q_node = n is_quantize = q_node target torch quantize_per_tensor is_to_fp = q_node op == call_method q_node target == len q_node args == q_node args == torch float Only continue when neither quantize nor to_fp is_quantize is_to_fp continue ref_node = q_node args get output scale zero_point dtype quantize node ref_node scale_node zero_point_node dtype = q_node args TODO add safety checks users ref_node dq_node needs one is_call_function is_call_method is_call_module = is_fixed_qparams_node ref_node modules is_to_fp is_call_function is_call_method is_call_module TODO add warning error out here bc-breaking error out warnings warn Only reference patterns currently supported dtype dtype op op format dtype=dtypes op=ref_node continue is_call_function is_call_method is_call_module = is_default_node ref_node modules is_to_fp is_call_function is_call_method is_call_module TODO add warning error out here bc-breaking error out continue This check includes all supported ops is_call_function is_call_method is_call_module = is_special_pattern_node ref_node modules is_call_module is_call_function is_call_method continue len ref_node args = len ref_node kwargs = raise AssertionError Expected ref_node have args kwargs dq_node_or_nodes = ref_node args len ref_node args next iter ref_node kwargs values isinstance dq_node_or_nodes Node tuple list raise AssertionError Expected dq_node_or_nodes Node tuple list is_dequantize = False isinstance dq_node_or_nodes Node is_dequantize = dq_node_or_nodes op == call_method dq_node_or_nodes target == dequantize isinstance dq_node_or_nodes tuple list is_dequantize = all x op == call_method x target == dequantize x dq_node_or_nodes is_dequantize continue TODO enable we have patterns needs swap modules is_call_module ref_module = modules ref_node target type ref_module SPECIAL_PATTERN_LOWER_MODULE_MAP is_quantize qmodule_cls = SPECIAL_PATTERN_LOWER_MODULE_MAP get type ref_module scale_node = q_node args zero_point_node = q_node args output_scale = getattr model scale_node target output_zero_point = getattr model zero_point_node target qmodule = qmodule_cls from_reference type ignore union-attr ref_module output_scale output_zero_point replace reference module quantized module parent_name module_name = _parent_name ref_node target setattr modules parent_name module_name qmodule reroute around dq node dq_nodes list Node = isinstance dq_node_or_nodes Node dq_nodes = dq_node_or_nodes isinstance dq_node_or_nodes tuple list dq_nodes = list dq_node_or_nodes dq_node dq_nodes dn_input = dq_node args ref_node replace_input_with dq_node dn_input store q node args qnode_qparams = list q_node args replace uses q node input remove q node q_node_input = q_node args q_node replace_all_uses_with q_node_input model graph erase_node q_node is_call_function is_call_method is_call_module = is_default_node ref_node modules is_call_function pass scale zer_point arguments quantize_per_tensor default node operator insert op after zero_point node so scale zero_point nodes available qop = get_quantized_operator ref_node target args = list ref_node args kwargs = dict ref_node kwargs qop QOP_TO_ARG_NAMES_TO_SKIP args_to_skip = QOP_TO_ARG_NAMES_TO_SKIP qop arg args_to_skip arg kwargs kwargs pop arg kwargs output_scale = qnode_qparams kwargs output_zero_point = qnode_qparams model graph inserting_after qnode_qparams qop_node = create_node_from_old_node_preserve_meta model graph call_function qop tuple args kwargs ref_node ref_node replace_all_uses_with qop_node model graph erase_node ref_node remove scale zero_point node quantize node n qnode_qparams isinstance n Node model graph erase_node n model _lower_getattr_tensor_metadta_op model GraphModule Modified graph model inplace skip extra dequantize op before general tensor shape ops when possible n model graph nodes is_getattr_tensor_metadata_node n maybe_dq = n args maybe_dq op = call_method maybe_dq target = dequantize continue skip dequantize node args = list n args args = n args args n args = tuple args _lower_get_tensor_info_op model GraphModule Modified graph model inplace skip extra dequantize op before general tensor shape ops when possible n model graph nodes is_get_tensor_info_node n continue maybe_dq = n args maybe_dq op = call_method maybe_dq target = dequantize continue skip dequantize node args = list n args args = n args args n args = tuple args _lower_to_native_backend model GraphModule qconfig_map dict str QConfigAny node_name_to_scope dict str tuple str type keep_original_weights bool = False - GraphModule Lower quantized reference model reference quantized operator patterns native backend PyTorch fbgemm qnnpack both backends shares same operator signature so they can lowered same function _lower_static_weighted_ref_module model qconfig_map _lower_static_weighted_ref_module_with_two_inputs model qconfig_map _lower_dynamic_weighted_ref_module model _lower_weight_only_weighted_ref_module model _lower_static_weighted_ref_functional model qconfig_map _lower_dynamic_weighted_ref_functional model qconfig_map _lower_quantized_binary_op model qconfig_map _lower_getattr_tensor_metadta_op model _lower_get_tensor_info_op model special_pattern_replacement model model graph eliminate_dead_code model = fold_weight model node_name_to_scope keep_original_weights model graph eliminate_dead_code model recompile model graph lint model