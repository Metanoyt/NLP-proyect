Owner s oncall distributed copy deepcopy torch torch distributed checkpoint dcp torch distributed checkpoint default_planner DefaultLoadPlanner DefaultSavePlanner torch distributed device_mesh init_device_mesh torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase MLPModule skip_if_lt_x_gpu with_comms torch testing _internal distributed checkpoint_utils with_temp_dir UnevenShardedModel torch nn Module __init__ device super __init__ torch manual_seed net = torch nn Linear device=device relu = torch nn ReLU net = torch nn Linear device=device net = torch nn Linear device=device forward x net net relu net x TestTpCheckpoint DTensorTestBase with_comms skip_if_lt_x_gpu with_temp_dir test_tp_checkpoint CHECKPOINT_DIR = temp_dir mesh_shpe = world_size tp_mesh = init_device_mesh device_type mesh_shpe create model move GPU id rank model = MLPModule device_type rank Parallelize module based given Parallel Style parallelize_plan = net ColwiseParallel net RowwiseParallel model = parallelize_module model tp_mesh parallelize_plan optimizer = torch optim SGD model parameters lr= original_state_dict = deepcopy model state_dict dcp save state_dict=original_state_dict storage_writer=dcp FileSystemWriter CHECKPOINT_DIR planner=DefaultSavePlanner Update parameters so model state_dict will different original_state_dict torch manual_seed inp = torch rand rank output = model inp output sum backward optimizer step state_dict = model state_dict ensure current model parameters different original_state_dict before loading checkpoint param param zip original_state_dict values state_dict values assertNotEqual param to_local param to_local dcp load state_dict=state_dict storage_reader=dcp FileSystemReader CHECKPOINT_DIR planner=DefaultLoadPlanner now load checkpoint check current model parameters same original_state_dict param param zip original_state_dict values state_dict values assertEqual param to_local param to_local with_comms skip_if_lt_x_gpu with_temp_dir test_tp_checkpoint_load_on_meta_device CHECKPOINT_DIR = temp_dir mesh_shpe = world_size tp_mesh = init_device_mesh device_type mesh_shpe create model move GPU id rank model = UnevenShardedModel device_type rank Parallelize module based given Parallel Style parallelize_plan = net ColwiseParallel net RowwiseParallel net ColwiseParallel model = parallelize_module model tp_mesh parallelize_plan=parallelize_plan original_state_dict = model model state_dict dcp save state_dict=original_state_dict storage_writer=dcp FileSystemWriter CHECKPOINT_DIR model = parallelize_module UnevenShardedModel meta tp_mesh parallelize_plan=parallelize_plan model _sd_before_load = model state_dict state_dict_to_load = model model _sd_before_load dcp load state_dict=state_dict_to_load storage_reader=dcp FileSystemReader CHECKPOINT_DIR We need make sure state_dict_to_load model same state_dict_after_load model since we doing in-place loading assertTrue state_dict_to_load model model _sd_before_load model load_state_dict state_dict_to_load model assign=True state_dict_after_load = model model state_dict assertEqual len original_state_dict model len state_dict_to_load model assertEqual len original_state_dict model len state_dict_after_load model name param original_state_dict model items param_to_load = state_dict_to_load model name param_after_load = state_dict_after_load model name we need explicitly check device meta assertEqual check currently doesn t handle DTensor meta device assertTrue param_to_load is_meta assertTrue param_after_load is_meta assertEqual param to_local param_to_load to_local assertEqual param to_local param_after_load to_local __name__ == __main__ run_tests