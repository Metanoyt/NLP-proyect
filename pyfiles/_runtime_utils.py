mypy allow-untyped-defs functools logging collections abc Callable enum auto Enum typing Any no_type_check Optional torch torch distributed dist torch distributed fsdp _traversal_utils traversal_utils torch nn nn torch nn functional F torch autograd Variable torch autograd graph register_multi_grad_hook torch distributed algorithms _comm_hooks LOW_PRECISION_HOOKS torch distributed fsdp _common_utils _assert_in_training_states _FSDPState _get_module_fsdp_state _is_composable _log_post_backward_hook _no_dispatch_record_stream clean_tensor_name TrainingState torch distributed fsdp _flat_param FlatParameter FlatParamHandle HandleShardingStrategy HandleTrainingState RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES torch distributed fsdp _init_utils HYBRID_SHARDING_STRATEGIES torch distributed fsdp api BackwardPrefetch torch distributed utils _apply_to_tensors _cast_forward_inputs _p_assert _to_kwargs torch utils _pytree pytree logger = logging getLogger __name__ Do include process_group enable hybrid shard MoE cases HOMOGENEOUS_ATTR_NAMES = _use_orig_params limit_all_gathers _use_full_prec_in_eval _PrefetchMode Enum BACKWARD = auto FORWARD = auto _get_fsdp_root_states_with_modules module nn Module - tuple list _FSDPState list nn Module Returns tuple containing A list root ` ` _FSDPState ` ` instances module tree rooted ` ` module ` ` without any duplicates following ` ` module modules ` ` traversal order which assumed depth-first A corresponding list root modules owning states first list This similar func ` _get_fsdp_states_with_modules ` except we must call func ` _is_fsdp_root ` force lazy initialization determine FSDP root case lazy initialization has yet happened fsdp_root_states list _FSDPState = fsdp_root_modules list nn Module = visited_fsdp_states set _FSDPState = set NOTE This function assumes ` module modules ` proceeds top-down submodule module modules optional_state = _get_module_fsdp_state submodule optional_state None optional_state visited_fsdp_states _is_fsdp_root optional_state submodule visited_fsdp_states add optional_state fsdp_root_states append optional_state fsdp_root_modules append submodule fsdp_root_states fsdp_root_modules _get_fsdp_root_states module nn Module - list _FSDPState See func ` _get_fsdp_root_states_with_modules ` fsdp_root_states _ = _get_fsdp_root_states_with_modules module fsdp_root_states _is_fsdp_root state _FSDPState module nn Module - bool Returns ` ` state ` ` corresponds FSDP root For wrapper code path ` ` state ` ` ` ` module ` ` should same For non-wrapper code path ` ` state ` ` should ` ` module ` ` s state Force lazy initialization determine FSDP root _lazy_init state module state _is_root None raise AssertionError Expected _is_root set after lazy init state _is_root no_type_check _lazy_init state _FSDPState root_module nn Module - _FSDPState Performs initialization lazily typically right before first forward pass The laziness needed ensure parameter device dtype FSDP hierarchy have finalized This method s actual logic only runs root FSDP instance which performs initialization all non-root FSDP instances avoid partial initialization For non-composable code path ` ` state ` ` ` ` root_module ` ` should same namely FSDP instance itself state _is_root None no-op already lazily initialized state _device_handle is_available Allow FSDP constructor run even without CUDA check once we start real execution raise RuntimeError FSDP does support CPU only execution The following logic only run root FSDP instance since will set ` _is_root=False ` non-root instances state _is_root = True _assert_in_training_states state TrainingState IDLE _check_flat_params_on_expected_device state root_module state _all_fsdp_states = traversal_utils _get_fsdp_states root_module _init_streams state buffers buffer_dtypes = _get_buffers_and_dtypes_for_computation state root_module _cast_buffers_to_dtype_and_device buffers buffer_dtypes state compute_device state _exec_order_data init state root_module state process_group _share_state_and_init_handle_attrs state root_module state _check_flat_params_on_expected_device state _FSDPState module nn Module Checks all ` ` FlatParameter ` ` s ` ` module ` ` s tree managed ` ` state ` ` expected device lazy initialization cpu_device = torch device cpu handle traversal_utils _get_fsdp_handles module handle _offload_params handle flat_param device = state compute_device raise RuntimeError An FSDP-managed module unexpectedly has parameters f handle flat_param device Make sure move module f state compute_device before training handle _offload_params handle flat_param device = cpu_device raise RuntimeError An FSDP-managed module parameter CPU offloading enabled f has parameters handle flat_param device Make sure f move module CPU when offloading parameters no_type_check _share_state_and_init_handle_attrs root_state _FSDPState root_module nn Module - None Shares data structure state ` ` root_state ` ` all FSDP states ` ` root_module ` ` s module tree initializes handle attributes These done together require single loop over states handle = root_state _handle handle handle init_flat_param_attributes attr_name_to_values dict str set Any = attr_name HOMOGENEOUS_ATTR_NAMES attr_name_to_values attr_name = set root_state _all_handles = root_state _exec_order_data all_handles share reference Update _has_optim_in_backward each handle handle root_state _all_handles flat_param = handle flat_param hasattr flat_param _in_backward_optimizers raise RuntimeError FSDP optimizer backward only supported use_orig_params=True handle _has_optim_in_backward = flat_param _params None any hasattr param _in_backward_optimizers param flat_param _params handle _has_optim_in_backward torch _C _log_api_usage_once fsdp optimizer_in_backward fsdp_state root_state _all_fsdp_states attr_name HOMOGENEOUS_ATTR_NAMES _p_assert hasattr fsdp_state attr_name f FSDP state missing attribute attr_name attr_name_to_values attr_name add getattr fsdp_state attr_name fsdp_state root_state continue Relax assert non-root FSDP instances case nested initialized module wrapped again FSDP later e g after training run inference _p_assert fsdp_state _is_root None fsdp_state _is_root Non-root FSDP instance s ` _is_root ` should have been set yet should have been set ` False ` fsdp_state _is_root = False fsdp_state _unshard_stream = root_state _unshard_stream fsdp_state _post_backward_stream = root_state _post_backward_stream fsdp_state _pre_unshard_stream = root_state _pre_unshard_stream fsdp_state _all_reduce_stream = root_state _all_reduce_stream fsdp_state _default_stream = root_state _default_stream fsdp_state _exec_order_data = root_state _exec_order_data fsdp_state _free_event_queue = root_state _free_event_queue fsdp_state _fsdp_extension None fsdp_state _fsdp_extension compute_stream = root_state _default_stream handle = fsdp_state _handle handle handle init_flat_param_attributes attr_name attr_values attr_name_to_values items len attr_values = raise ValueError f Expects one homogeneous value attr_name got attr_values no_type_check _init_streams state _FSDPState - None Initializes CUDA streams overlapping communication computation data transfers The streams should shared across FSDP instances state _is_root raise AssertionError Expected state root state _device_handle is_available raise AssertionError Expected device handle available uses_hybrid_sharding = any fsdp_state sharding_strategy HYBRID_SHARDING_STRATEGIES fsdp_state state _all_fsdp_states Prioritize all-gathers reduce-scatters over async all-reduce HSDP preserve default priority otherwise high_priority = - state limit_all_gathers uses_hybrid_sharding Default stream computation state _default_stream = state _device_handle current_stream state _fsdp_extension None set compute stream FSDP extension state _fsdp_extension compute_stream = state _default_stream Stream unshard logic including allocating all-gather destination tensors all-gathers themselves state _unshard_stream = state _device_handle Stream priority=high_priority Stream overlapping gradient reduction backward pass gradient computation state _post_backward_stream = state _device_handle Stream priority=high_priority Stream pre-unshard logic namely allocations writes CPU offloading H D copy mixed precision low precision cast state _pre_unshard_stream = state _device_handle Stream priority=high_priority Stream run HSDP s all-reduce async using HSDP state _all_reduce_stream = state _device_handle Stream uses_hybrid_sharding state _default_stream no_type_check _unshard state _FSDPState handle FlatParamHandle unshard_stream torch Stream pre_unshard_stream torch Stream - None Unshards handles ` ` handles ` ` If handles meth ` summon_full_params ` using mixed precision then they forced full precision Postcondition handle s ` ` FlatParameter ` ` s data padded unsharded flat parameter compute device handle state _device_handle stream pre_unshard_stream ran_pre_unshard = handle pre_unshard ran_pre_unshard unshard_stream wait_stream pre_unshard_stream state limit_all_gathers event = state _free_event_queue dequeue_if_needed event torch profiler record_function FullyShardedDataParallel rate_limiter event synchronize state _device_handle stream unshard_stream handle unshard handle post_unshard no_type_check _reshard state _FSDPState handle FlatParamHandle free_unsharded_flat_param bool Reshards handle ` ` free_unsharded_flat_param ` ` indicates whether free handle s padded unsharded flat parameter handle reshard free_unsharded_flat_param state limit_all_gathers free_unsharded_flat_param torch distributed _functional_collectives is_torchdynamo_compiling We don t run even queue freeing under torch compile atm But maybe we need TODO voz Look into free_event = state _device_handle Event free_event record state _free_event_queue enqueue free_event handle post_reshard Flat parameter freed we always have unshard parameter upon next access get its shape correct handle _prefetched = False _unshard_grads handle Optional FlatParamHandle - None handle handle unshard_grad _reshard_grads handle Optional FlatParamHandle - None handle handle reshard_grad no_type_check _pre_forward state _FSDPState handle Optional FlatParamHandle unshard_fn Callable module nn Module args tuple Any kwargs dict str Any - tuple tuple Any dict str Any Runs pre-forward logic This includes opportunity unshard currently sharded parameters such those current forward registering post-backward hooks these current parameters This function also converts forward ` ` args ` ` ` ` kwargs ` ` given precision Args handles List FlatParamHandle Handles giving parameters used current forward unshard_fn Optional Callable A callable unshard any currently sharded parameters ` ` None ` ` do any unsharding module nn Module Module whose forward method runs right before expected hook signature args Tuple Any Module forward ` ` args ` ` kwargs Dict str Any Module forward ` ` kwargs ` ` torch profiler record_function FullyShardedDataParallel _pre_forward For ` fully_shard ` + ` checkpoint ` skip pre-forward logic recomputed forward handle handle _training_state == HandleTrainingState BACKWARD_PRE For both checkpoint implementations we do need re-cast inputs here since they will checkpointed low precision either AC normally autograd long AC region nested within FSDP args kwargs state training_state = TrainingState FORWARD_BACKWARD state _exec_order_data record_pre_forward handle module training handle handle _training_state = HandleTrainingState FORWARD unshard_fn None unshard_fn state handle Register post-backward hooks reshard parameters reduce-scatter their gradients They must re-registered every forward pass case ` grad_fn ` mutated _register_post_backward_hook state handle We have reallocate _cpu_grad optimizer overlap set grad None backward pass handle handle _offload_params handle flat_param _cpu_grad None handle flat_param _cpu_grad = torch zeros_like handle flat_param _local_shard device=torch device cpu pin_memory should_cast_forward_inputs = state _handle state _handle _force_full_precision should_cast_forward_inputs state mixed_precision cast_forward_inputs Recursively convert args kwargs specified precision input_dtype Optional torch dtype = state mixed_precision param_dtype args kwargs = _cast_forward_inputs input_dtype args kwargs _register_post_backward_reshard_only_hook state handle args kwargs args kwargs no_type_check _pre_forward_unshard state _FSDPState handle Optional FlatParamHandle - None Unshards parameters pre-forward handle If handles have been prefetched then there no need call ` _unshard ` again handle _prefetched _unshard state handle state _unshard_stream state _pre_unshard_stream handle _needs_pre_forward_unshard = False Don t wait during trace torch distributed _functional_collectives is_torchdynamo_compiling current_stream = state _device_handle current_stream state _unshard_event None current_stream wait_event state _unshard_event state _unshard_event = None current_stream wait_stream state _unshard_stream torch profiler record_function FullyShardedDataParallel _pre_forward_prefetch _prefetch_handle state handle _PrefetchMode FORWARD no_type_check _post_forward state _FSDPState handle Optional FlatParamHandle reshard_fn Callable module nn Module input Any output Any - Any Runs post-forward logic This includes opportunity reshard currently unsharded parameters such those used current forward registering pre-backward hooks forward outputs Args handles List FlatParamHandle Handles giving parameters used current forward reshard_fn Optional Callable A callable reshard any currently unsharded parameters e g current forward ` ` None ` ` do any resharding module nn Module Module whose forward just ran which should fully sharded module see Note Fully Sharded Module expected hook signature input Any Unused expected hook signature output Any Forward pass output pre-backward hooks registered tensors require gradients output Postcondition Each ` ` FlatParameter ` ` s data points sharded flat parameter torch profiler record_function FullyShardedDataParallel _post_forward For ` fully_shard ` + ` checkpoint ` skip post-forward logic recomputed forward handle handle _training_state == HandleTrainingState BACKWARD_PRE output state _exec_order_data record_post_forward handle reshard_fn None reshard_fn state handle Register pre-backward hooks unshard flat parameters gradient computation needed output = _register_pre_backward_hooks state module output handle state training_state = TrainingState IDLE handle handle _training_state = HandleTrainingState IDLE output no_type_check _post_forward_reshard state _FSDPState handle FlatParamHandle - None Reshards parameters post-forward handle Do free root s parameters post-forward ` FULL_SHARD ` intention they immediately used backward computation though may true free_unsharded_flat_param = state _is_root handle _sharding_strategy RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES _reshard state handle free_unsharded_flat_param no_type_check _root_pre_forward state _FSDPState module nn Module args kwargs - None Runs pre-forward logic specific root FSDP instance which should run before any individual module s pre-forward This starts attempt lazy initialization which only runs non-vacuously once Otherwise called non-root FSDP instance then returns directly Args module nn Module Module which logic tries run It may may root If then method does do anything torch profiler record_function FullyShardedDataParallel _root_pre_forward _lazy_init state module _p_assert state _is_root None Expects root FSDP have been set state _is_root Always cast forward inputs root local FSDP unit mixed precision where mixed precision could configured This more useful auto wrapping recommended composable path For manual wrapping cast forward inputs each local FSDP unit root will increase some overhead so turned model wrapper path right now where manual wrapping more broadly used _is_composable state _root_cast_forward_input state module args kwargs args kwargs We cast buffers back full precision we re forcing full precision Disjointly we check buffers full precision we should cast them back lower precision which happens when exiting eval mode handle = state _handle handle should_cast_buffers_to_full_prec = handle _force_full_precision If root has no handle no managed parameters then we fall back checking any child wants force full precision workaround handles = traversal_utils _get_fsdp_handles module should_cast_buffers_to_full_prec = any handle _force_full_precision handle handles should_cast_buffers_to_full_prec _cast_buffers_to_dtype_and_device buffers=dict module named_buffers values buffer_dtypes=list state _buffer_name_to_orig_dtype values device=state compute_device This flag only set when we cast buffers full precision avoid CPU overhead can stem retrieving all buffers their types following branch state _needs_buffer_dtype_restore_check = True getattr state _needs_buffer_dtype_restore_check False Check buffers full precision we need cast them back down buffers buffer_dtypes_for_computation = _get_buffers_and_dtypes_for_computation state module len buffers len buffer_dtypes_for_computation any buffer dtype = buffer_dtype_for_computation buffer buffer_dtype_for_computation zip buffers buffer_dtypes_for_computation Assume we have cast everything there one mismatch _cast_buffers_to_dtype_and_device buffers buffer_dtypes_for_computation state compute_device We don t have check again until we cast buffers full precision again state _needs_buffer_dtype_restore_check = False state forward_prefetch handles = fsdp_state _handle fsdp_state state _all_fsdp_states fsdp_state _handle handle handles handle _needs_pre_forward_unshard = True handle _prefetched = False _wait_for_computation_stream state _device_handle current_stream state _unshard_stream state _pre_unshard_stream _reset_flat_param_grad_info_if_needed state _all_handles Prepares forward inputs moving them ` ` compute_device ` ` TODO Do use side stream tensor copies now investigate perf without torch profiler record_function FullyShardedDataParallel _to_kwargs args_tuple kwargs_tuple = _to_kwargs args kwargs state compute_device False args = args_tuple args_tuple tuple kwargs = kwargs_tuple kwargs_tuple _root_cast_forward_input state module args kwargs no_type_check _root_cast_forward_input state _FSDPState module torch nn Module args kwargs - tuple Any Any state _handle force_full_precision = state _handle _force_full_precision force_full_precision = True should_cast_forward_inputs = module training state _use_full_prec_in_eval force_full_precision state mixed_precision cast_root_forward_inputs should_cast_forward_inputs input_dtype Optional torch dtype = state mixed_precision param_dtype args kwargs = _cast_forward_inputs input_dtype args kwargs args kwargs no_type_check _pre_backward_hook state _FSDPState module nn Module handle FlatParamHandle grad unused Any - Any Prepares ` ` _handle ` ` s ` ` FlatParameter ` ` s gradient computation Args module nn Module Fully sharded module see Note Fully Sharded Module Only run pre-backward hook once per group handles involved same module forward computation handle hasattr handle _ran_pre_backward_hook handle _ran_pre_backward_hook grad torch profiler record_function FullyShardedDataParallel _pre_backward_hook Queue post-backward callback once root FSDP instance attach outermost backward graph task so called after all backward calls complete state _is_root state _post_backward_callback_queued _register_post_backward_final_callback state module _reset_flat_param_grad_info_if_needed state _all_handles handle allowed_states = TrainingState IDLE _is_composable state allowed_states append TrainingState FORWARD_BACKWARD _assert_in_training_states state allowed_states state training_state = TrainingState FORWARD_BACKWARD Queueing post-backward callback only logic per-handle pre-backward hook so we can early here there no handles handle grad handle _training_state = HandleTrainingState BACKWARD_PRE handle _needs_pre_backward_unshard If handles have been prefetched then there no need call ` _unshard ` again handle _prefetched _unshard state handle state _unshard_stream state _pre_unshard_stream Don t wait during trace torch distributed _functional_collectives is_torchdynamo_compiling state _device_handle current_stream wait_stream state _unshard_stream Set ` False ` ensure mistargeted prefetch does actually unshard these handles handle _needs_pre_backward_unshard = False torch profiler record_function FullyShardedDataParallel _pre_backward_prefetch _prefetch_handle state handle _PrefetchMode BACKWARD handle prepare_gradient_for_backward handle _ran_pre_backward_hook = True grad no_type_check torch no_grad _post_backward_hook state _FSDPState handle FlatParamHandle flat_param unused Any Reduce-scatters gradient ` ` handle ` ` s ` ` FlatParameter ` ` Precondition The ` ` FlatParameter ` ` s ` ` grad ` ` attribute contains unsharded gradient local batch Postcondition - If using ` ` NO_SHARD ` ` then ` ` grad ` ` attribute reduced unsharded gradient - Otherwise ` ` _saved_grad_shard ` ` attribute reduced sharded gradient accumulating any existing gradient _log_post_backward_hook state handle logger flat_param = handle flat_param flat_param _post_backward_called = True torch autograd profiler record_function FullyShardedDataParallel _post_backward_hook _assert_in_training_states state TrainingState FORWARD_BACKWARD For multiple applications reentrant AC across submodules sharing same ` FlatParameter ` post-backward hook may run multiple times one backward which case we permit state already ` BACKWARD_POST ` _p_assert handle _training_state HandleTrainingState BACKWARD_PRE HandleTrainingState BACKWARD_POST f Expects ` BACKWARD_PRE ` ` BACKWARD_POST ` state got handle _training_state handle _training_state = HandleTrainingState BACKWARD_POST flat_param grad None flat_param grad requires_grad raise RuntimeError FSDP does support gradients gradients _post_backward_reshard state handle state _sync_gradients handle _use_orig_params handle _use_unsharded_grad_views Wait all ops current stream e g gradient computation finish before reduce-scattering gradient torch distributed _functional_collectives is_torchdynamo_compiling state _post_backward_stream wait_stream state _device_handle current_stream state _device_handle stream state _post_backward_stream autograd_computed_grad = flat_param grad data _low_precision_hook_enabled state flat_param grad dtype = handle _reduce_dtype If we forcing full precision communicating grads i e model eval + full precision eval configured don t downcast gradient handle _force_full_precision flat_param grad data = flat_param grad handle _reduce_dtype handle uses_sharded_strategy _reduce_grad state handle _reduce_grad_no_shard state handle Since unsharded gradient produced computation stream consumed post-backward stream inform caching allocator before goes out scope _no_dispatch_record_stream autograd_computed_grad state _post_backward_stream _post_backward_reshard_only_hook state _FSDPState handle FlatParamHandle unused Any - None torch profiler record_function FullyShardedDataParallel _post_backward_hook_reshard_only ` _pre_backward_hook ` may get executed forward output does require grad overwrite IDLE state post-backward prefetching state training_state = TrainingState FORWARD_BACKWARD handle _training_state = HandleTrainingState BACKWARD_POST _post_backward_reshard state handle _post_backward_reshard state _FSDPState handle FlatParamHandle unused Any - None free_unsharded_flat_param = _should_free_in_backward state handle _reshard state handle free_unsharded_flat_param TODO Post-backward prefetching does support multiple handles per module case since post-backward hook runs per handle per group handles torch profiler record_function FullyShardedDataParallel _post_backward_prefetch _prefetch_handle state handle _PrefetchMode BACKWARD no_type_check _should_free_in_backward state _FSDPState handle FlatParamHandle - bool Returns whether FSDP should free unsharded flat parameter post-backward handle uses_sharded_strategy False If syncing gradients then we do free strategies do reshard after forward heuristic tradeoff higher memory higher throughput state _sync_gradients handle _sharding_strategy RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES no_type_check _reduce_grad state _FSDPState handle FlatParamHandle - None For sharded strategies runs gradient reduction sharded gradient accumulation needed post-reduction callback flat_param = handle flat_param uses_hybrid_sharded_strategy = handle _sharding_strategy HandleShardingStrategy HYBRID_SHARD HandleShardingStrategy _HYBRID_SHARD_ZERO We clear ` grad ` permit multiple backwards This avoids race where second backward pass computation precedes ahead first backward pass reduction which possible since reduction issued separate stream async would result reducing wrong gradient unsharded_grad = flat_param grad data flat_param grad = None padded_unsharded_grad new_sharded_grad = _get_reduce_scatter_tensors state unsharded_grad state _comm_hook None default path _div_if_needed padded_unsharded_grad state _gradient_predivide_factor pg = handle _fake_process_group handle _use_fake_reduce state process_group dist reduce_scatter_tensor new_sharded_grad padded_unsharded_grad group=pg uses_hybrid_sharded_strategy Don t wait during trace torch distributed _functional_collectives is_torchdynamo_compiling state _all_reduce_stream wait_stream state _post_backward_stream state _device_handle stream state _all_reduce_stream Since new sharded gradient produced post- backward stream consumed all-reduce stream inform caching allocator _no_dispatch_record_stream new_sharded_grad state _all_reduce_stream dist all_reduce new_sharded_grad group=state _inter_node_pg _div_if_needed new_sharded_grad state _gradient_postdivide_factor grad_to_offload = _accumulate_sharded_grad state handle new_sharded_grad _post_reduce_grad_callback state handle grad_to_offload _div_if_needed new_sharded_grad state _gradient_postdivide_factor state _comm_hook state _comm_hook_state padded_unsharded_grad new_sharded_grad NOTE HSDP variants do support communication hook grad_to_offload = _accumulate_sharded_grad state handle new_sharded_grad _post_reduce_grad_callback state handle grad_to_offload no_type_check _get_reduce_scatter_tensors state _FSDPState unsharded_grad torch Tensor - tuple torch Tensor torch Tensor Returns input output tensors reduce-scatter respectively chunks = list unsharded_grad chunk state world_size numel_to_pad = state world_size chunks numel - unsharded_grad numel padded_unsharded_grad = F pad unsharded_grad numel_to_pad numel_to_pad unsharded_grad new_sharded_grad = torch empty_like chunks padded padded_unsharded_grad new_sharded_grad no_type_check _accumulate_sharded_grad state _FSDPState handle FlatParamHandle sharded_grad torch Tensor - torch Tensor Accumulates reduce-scattered sharded gradient any existing sharded gradient needed returning gradient offload CPU offloading enabled flat_param = handle flat_param _cast_grad_to_param_dtype state sharded_grad flat_param Save sharded gradient ` _saved_grad_shard ` support gradient accumulation -- multiple backwards gradient reductions may happen arbitrary order accumulate_grad = hasattr flat_param _saved_grad_shard accumulate_grad _check_grad_to_accumulate sharded_grad flat_param _saved_grad_shard flat_param _saved_grad_shard += sharded_grad flat_param _saved_grad_shard = sharded_grad grad_to_offload = flat_param _saved_grad_shard grad_to_offload no_type_check _reduce_grad_no_shard state _FSDPState handle FlatParamHandle - None For no-shard runs gradient reduction which directly covers any gradient accumulation implicitly post-reduction callback flat_param = handle flat_param state _comm_hook None default path _div_if_needed flat_param grad state _gradient_predivide_factor dist all_reduce flat_param grad group=state process_group _div_if_needed flat_param grad state _gradient_postdivide_factor state _comm_hook state _comm_hook_state flat_param grad For ` NO_SHARD ` we can keep low precision gradients simply omitting cast altogether handle _keep_low_precision_grads _cast_grad_to_param_dtype state flat_param grad flat_param grad_to_offload = flat_param grad data _post_reduce_grad_callback state handle grad_to_offload no_type_check _post_reduce_grad_callback state _FSDPState handle FlatParamHandle Additional arguments needed callback logic grad_to_offload torch Tensor This callback captures any logic run after gradient reduction finishes Currently offloads gradient CPU CPU offloading enabled uses sharded gradient views ` ` use_orig_params=True ` ` _offload_grad state handle grad_to_offload _post_backward_use_sharded_grad_views handle no_type_check _offload_grad state _FSDPState handle FlatParamHandle grad_to_offload torch Tensor handle _offload_params Offload gradient CPU ensure parameters gradients same device required optimizer TODO Investigate why ` NO_SHARD ` breaks correctness when using ` non_blocking=True ` here TODO rohan-varma When CPU offload optimizer overlap non_blocking=True won t work since copy may have finished before optimizer step executes CPU If we want use non-blocking=True here we ll have synchronize before using result CPU non_blocking = handle uses_sharded_strategy handle _has_optim_in_backward handle flat_param _cpu_grad copy_ grad_to_offload detach non_blocking=non_blocking synchronized post-backward callback Since gradient being offloaded may have been produced computation stream being consumed here post-backward stream inform caching allocator _no_dispatch_record_stream grad_to_offload data state _post_backward_stream no_type_check _post_backward_use_sharded_grad_views handle FlatParamHandle handle _use_orig_params Since handle s ` FlatParameter ` completed its gradient computation we should reset gradient noneness mask handle _reset_is_grad_none Delay using sharded gradient views until after reduce-scatter instead immediately after resharding handle _use_sharded_grad_views handle _has_optim_in_backward handle prepare_gradient_for_optim orig_param handle flat_param _params Check ` None ` gradient filter parameters rank orig_param grad None hasattr orig_param _in_backward_optimizers TODO rohan-varma For CPU offload unfortunately operates CPU because parameters gradients have already been offloaded We should run GPU after refactoring optim orig_param _in_backward_optimizers optim step optim zero_grad set_to_none=True handle _reset_flat_param_grad_info_if_needed handle _offload_params handle flat_param _cpu_grad = None _div_if_needed tensor torch Tensor div_factor float - None div_factor tensor div_ div_factor no_type_check _cast_grad_to_param_dtype state _FSDPState sharded_grad torch Tensor param FlatParameter Casts ` ` sharded_grad ` ` back full parameter dtype so optimizer step runs dtype This performs actual cast parameters reduced precision during forward since then gradients would reduced precision parameters reduced precision gradients reduced precision communication However low precision communication hook registered then dtype cast happens hook instead _assert_in_training_states state TrainingState FORWARD_BACKWARD _low_precision_hook_enabled state sharded_grad dtype = param dtype low_prec_grad_data = sharded_grad data sharded_grad data = sharded_grad data dtype=param dtype Since ` NO_SHARD ` gradient produced computation stream consumed here post-backward stream inform caching allocator sharded strategies gradient produced post-backward stream so ` record_stream ` should no-op _no_dispatch_record_stream low_prec_grad_data state _device_handle current_stream _check_grad_to_accumulate new_sharded_grad torch Tensor accumulated_grad torch Tensor - None _p_assert accumulated_grad shape == new_sharded_grad shape Shape mismatch when accumulating gradients f existing gradient shape= accumulated_grad shape f new gradient shape= new_sharded_grad shape _p_assert accumulated_grad device == new_sharded_grad device Device mismatch when accumulating gradients f existing gradient device= accumulated_grad device f new gradient device= new_sharded_grad device no_type_check _low_precision_hook_enabled state _FSDPState - bool state _comm_hook LOW_PRECISION_HOOKS no_type_check torch no_grad _post_backward_final_callback state _FSDPState module nn Module This waits post-backward finish performs some final cleanup This runs end entire backward pass should only called root FSDP instance _p_assert state _is_root The post-backward callback should only called root FSDP instance root_state = state root_state _sync_gradients current_stream = state _device_handle current_stream TODO rohan-varma also waits overlapped optimizer step finish since currently runs post-backward stream That can pushed next forward run different stream current_stream wait_stream root_state _post_backward_stream root_state _all_reduce_stream current_stream uses HSDP current_stream wait_stream root_state _all_reduce_stream root_state cpu_offload offload_params Wait non-blocking GPU - CPU sharded gradient copies post-backward hooks finish explicitly since CPU gradients do automatically synchronize GPU state _device_handle current_stream synchronize root_state _exec_order_data next_iter fsdp_state state _all_fsdp_states _catch_all_reshard fsdp_state _finalize_params fsdp_state fsdp_state training_state = TrainingState IDLE handle = fsdp_state _handle handle handle _ran_pre_backward_hook = False handle _needs_pre_backward_unshard = False handle _post_forward_index = None handle _training_state = HandleTrainingState IDLE handle _prefetched = False Reset cases like one forward multiple backwards root_state _post_backward_callback_queued = False no_type_check _catch_all_reshard state _FSDPState - None Reshards parameters may have been resharded post-backward hook This can happen when module s output used forward pass meaning its pre-backward hook runs unsharding parameter post-backward hook does run because output jused loss computation corresponding backward pass Wrap try-except provide more informative traceback error raised try state _handle TODO This already-resharded check brittle https github com pytorch pytorch issues already_resharded = state _handle flat_param data_ptr == state _handle flat_param _local_shard data_ptr If FSDP skipped using sharded views then flat parameter still points sharded data so we need reshard use sharded views state _handle _skipped_use_sharded_views already_resharded free_unsharded_flat_param = _should_free_in_backward state state _handle _reshard state state _handle free_unsharded_flat_param except Exception e _p_assert False f Got exception catch-all reshard state str e raise_assertion_error=False raise e no_type_check _finalize_params state _FSDPState - None Finalizes parameters before next iteration handle = state _handle handle flat_param = handle flat_param torch distributed _functional_collectives is_torchdynamo_compiling hasattr flat_param _post_backward_hook_handle pbhs_handle = flat_param _post_backward_hook_handle pbhs_handle remove del flat_param _post_backward_hook_handle hasattr flat_param _post_backward_hook_state post_backward_hook_state_len = len flat_param _post_backward_hook_state expected_post_backward_hook_state_len = int flat_param requires_grad + _p_assert post_backward_hook_state_len == expected_post_backward_hook_state_len f Invalid ` ` _post_backward_hook_state ` ` flat_param _post_backward_hook_state flat_param _post_backward_hook_state - remove delattr flat_param _post_backward_hook_state flat_param requires_grad state _sync_gradients Preserve gradient accumulation state synchronizing gradients ` grad ` remains unsharded gradient prior ` no_sync ` iterations ` _saved_grad_shard ` remains sharded gradient last synchronized iteration handle _has_optim_in_backward handle prepare_gradient_for_optim _p_assert hasattr flat_param _post_backward_called Expects ` _post_backward_called ` set ` FlatParameter ` flat_param _post_backward_called = False no_type_check _prefetch_handle state _FSDPState current_handle Optional FlatParamHandle prefetch_mode _PrefetchMode - None Prefetches next handles needed without synchronization An empty handles key cannot prefetch current_handle handle = _get_handle_to_prefetch state current_handle handle Temporarily emulate training state while calling ` _unshard ` ensure correct ` as_params ` ` _use_unsharded_views ` prev_training_state = handle _training_state prefetch_mode == _PrefetchMode BACKWARD handle _training_state = HandleTrainingState BACKWARD_PRE prefetch_mode == _PrefetchMode FORWARD handle _training_state = HandleTrainingState FORWARD raise ValueError f Invalid prefetch mode rank state rank prefetch_mode Prefetch next set handles without synchronizing allow sync happen late possible maximize overlap _unshard state handle state _unshard_stream state _pre_unshard_stream handle _training_state = prev_training_state handle _prefetched = True no_type_check _get_handle_to_prefetch state _FSDPState current_handle FlatParamHandle - FlatParamHandle Returns ` list ` handles keys prefetch next module s where ` ` current_handle ` ` represents current module Prefetching refers running unshard logic early without synchronization next modules depend recorded execution order current training state training_state = _get_training_state current_handle valid_training_states = HandleTrainingState BACKWARD_PRE HandleTrainingState BACKWARD_POST HandleTrainingState FORWARD _p_assert training_state valid_training_states f Prefetching only supported valid_training_states f currently training_state eod = state _exec_order_data target_handle Optional FlatParamHandle = None training_state == HandleTrainingState BACKWARD_PRE state backward_prefetch == BackwardPrefetch BACKWARD_PRE training_state == HandleTrainingState BACKWARD_POST state backward_prefetch == BackwardPrefetch BACKWARD_POST target_handle_candidate = eod get_handle_to_backward_prefetch current_handle target_handle_candidate target_handle_candidate _needs_pre_backward_unshard target_handle_candidate _prefetched target_handle = target_handle_candidate target_handle = None training_state == HandleTrainingState FORWARD state forward_prefetch target_handle_candidate = eod get_handle_to_forward_prefetch current_handle target_handle_candidate target_handle_candidate _needs_pre_forward_unshard target_handle_candidate _prefetched target_handle = target_handle_candidate target_handle = None target_handle _get_training_state handle FlatParamHandle - HandleTrainingState Returns training state handles ` ` handle ` ` _p_assert handle Expects non-empty handle handle _training_state no_type_check _register_pre_forward_hook state _FSDPState module nn Module - None Registers pre-forward hook ` ` module ` ` forward_handle state _pre_forward_handles forward_handle remove state _pre_forward_handles clear module_param_handle = state _fully_sharded_module_to_handle get module None hook = functools partial _pre_forward state module_param_handle _pre_forward_unshard state _pre_forward_handles append module register_forward_pre_hook hook prepend=True with_kwargs=True no_type_check _register_post_forward_hook state _FSDPState module nn Module - None Registers post-forward hook ` ` module ` ` Even module has no handles we should register hook since will register module s pre-backward hook forward_handle state _post_forward_handles forward_handle remove state _post_forward_handles clear module_param_handle = state _fully_sharded_module_to_handle get module None hook = functools partial _post_forward state module_param_handle _post_forward_reshard state _post_forward_handles append module register_forward_hook hook no_type_check _register_root_pre_forward_hook state _FSDPState module nn Module Registers root pre-forward hook ` ` module ` ` which should local FSDP root NOTE For current composable FSDP design we have each application ` ` fully_shard ` ` module indicate module local FSDP root We may remove assumption future which case we will need register root pre-forward hook any candidate module may local FSDP root forward_handle state _root_pre_forward_handles forward_handle remove state _root_pre_forward_handles clear hook = functools partial _root_pre_forward state state _root_pre_forward_handles append module register_forward_pre_hook hook prepend=True with_kwargs=True no_type_check _register_pre_backward_hooks state _FSDPState module nn Module outputs Any handle FlatParamHandle - None Registers pre-backward hooks tensors require gradients forward pass outputs ` ` outputs ` ` which computed using ` ` FlatParameter ` ` s ` ` handles ` ` Args module nn Module Fully sharded module see Note Fully Sharded Module Returns Forward pass outputs pre-backward hooks registered tensors require gradients If there no gradient computation then there no need pre-backward logic torch is_grad_enabled outputs state _is_root state _post_backward_callback_queued = False only defined root handle handle _needs_pre_backward_unshard = False Since these handles ` FlatParameter ` s participated forward we conservatively assume they will used backward handle _ran_pre_backward_hook = False _register_hook t torch Tensor - torch Tensor t requires_grad t register_hook torch utils hooks unserializable_hook functools partial _pre_backward_hook state module handle handle handle _needs_pre_backward_unshard = True t _apply_to_tensors _register_hook outputs _register_post_backward_hook state _FSDPState handle Optional FlatParamHandle - None Registers post-backward hooks ` ` FlatParameter ` ` s ` ` AccumulateGrad ` ` objects reshard reduce-scatter gradients The ` ` AccumulateGrad ` ` object represents last function finalizes ` ` FlatParameter ` ` s gradient so only runs after its entire gradient computation has finished We register post-backward hook only once first forward ` ` FlatParameter ` ` participates This relies ` ` AccumulateGrad ` ` object being preserved through multiple forwards NOTE We follow heuristic prefer first forward target parameter mixed precision case where there separate ` ` AccumulateGrad ` ` objects across different forwards Without parameter mixed precision ` ` AccumulateGrad ` ` objects same If we instead prefer last forward then hook runs early If there no gradient computation then there no need post-backward logic torch is_grad_enabled handle flat_param = handle flat_param torch distributed _functional_collectives is_torchdynamo_compiling already_registered = hasattr flat_param _post_backward_hook_handle already_registered flat_param requires_grad hook = functools partial _post_backward_hook state handle hook_handle = flat_param register_post_accumulate_grad_hook hook flat_param _post_backward_hook_handle = hook_handle type ignore attr-defined already_registered = hasattr flat_param _post_backward_hook_state already_registered flat_param requires_grad Get ` AccumulateGrad ` object temp_flat_param = flat_param expand_as flat_param _p_assert temp_flat_param grad_fn None The ` grad_fn ` needed access ` AccumulateGrad ` register post-backward hook acc_grad = temp_flat_param grad_fn next_functions type ignore union-attr acc_grad None raise AssertionError Expected acc_grad set hook_handle = acc_grad register_hook functools partial _post_backward_hook state handle flat_param _post_backward_hook_state = acc_grad hook_handle type ignore attr-defined _register_post_backward_reshard_only_hook state _FSDPState handle Optional FlatParamHandle args tuple Any kwargs dict str Any - None Registers post-backward hooks reshard flat parameters do require gradient We register these using multi-post-grad hooks input activations ensure all gradients may depend parameters have been computed before resharding If there no gradient computation then there no need post-backward logic torch is_grad_enabled Construct ` inp_tensors ` lazily avoid CPU overhead typical case where each flat parameter requires gradient inp_tensors Optional list torch Tensor = None handle flat_param = handle flat_param torch distributed _functional_collectives is_torchdynamo_compiling already_registered = hasattr flat_param _post_backward_hook_handle already_registered = hasattr flat_param _post_backward_hook_state already_registered flat_param requires_grad inp_tensors None args_flat = pytree arg_tree_leaves args kwargs inp_tensors = obj obj args_flat torch is_tensor obj obj requires_grad inp_tensors None raise AssertionError Expected inp_tensors set hook_handle = register_multi_grad_hook inp_tensors functools partial _post_backward_reshard_only_hook state handle torch distributed _functional_collectives is_torchdynamo_compiling flat_param _post_backward_hook_handle = hook_handle type ignore attr-defined assignment flat_param _post_backward_hook_state = hook_handle type ignore attr-defined assignment no_type_check _register_post_backward_final_callback state _FSDPState module nn Module - None Registers post-backward final callback runs end backward pass This should called root FSDP instance beginning pre-backward _p_assert state _is_root Only root FSDP instance should register post-backward callback state _post_backward_callback_queued _assert_in_training_states state TrainingState IDLE Trace does need callback torch distributed _functional_collectives is_torchdynamo_compiling state _post_backward_callback_queued = True Variable _execution_engine queue_callback functools partial _post_backward_final_callback state module _wait_for_computation_stream computation_stream torch Stream unshard_stream torch Stream pre_unshard_stream torch Stream Has unshard pre-unshard streams wait computation stream For example should called FSDP root s pre-forward respect optimizer step computation Tracing does need wait torch distributed _functional_collectives is_torchdynamo_compiling unshard_stream wait_stream computation_stream type ignore attr-defined Having pre-all-gather stream wait current stream even we do leverage pre-all-gather stream tolerable since only runs once per iteration pre_unshard_stream wait_stream computation_stream type ignore attr-defined _reset_flat_param_grad_info_if_needed handles list FlatParamHandle Clears original parameters gradients needed This method s CPU overhead minimal so we may call throughout FSDP methods which serve callsites free gradient memory earlier isinstance handles list handles = handles handle handles handle _use_orig_params handle _reset_flat_param_grad_info_if_needed no_type_check _get_buffers_and_dtypes_for_computation state _FSDPState root_module nn Module - tuple list torch Tensor list Optional torch dtype Returns all buffers module tree rooted ` ` root_module ` ` corresponding list buffer dtypes computation Each buffer dtype either ` ` None ` ` buffer mixed precision enabled buffer low precision dtype otherwise _p_assert state _is_root Expects root cast buffers buffers list torch Tensor = buffer_dtypes list Optional torch dtype = visited_buffers set torch Tensor = set Traverse FSDP states bottom-up so we prefer owning FSDP instance s mixed precision setting each buffer fsdp_states fsdp_modules = traversal_utils _get_fsdp_states_with_modules root_module fsdp_state fsdp_module zip reversed fsdp_states reversed fsdp_modules buffer_name buffer fsdp_module named_buffers buffer visited_buffers continue visited_buffers add buffer clean_tensor_name buffer_name fsdp_state _ignored_buffer_names continue buffers append buffer buffer_dtypes append fsdp_state mixed_precision buffer_dtype len buffers = len buffer_dtypes raise AssertionError f Expected buffers buffer_dtypes have same length got len buffers len buffer_dtypes buffers buffer_dtypes no_type_check _get_orig_buffer_dtypes state _FSDPState buffer_names list str - list torch dtype Returns original buffer types given buffer names buffer_dtypes list torch dtype = buffer_name buffer_names _p_assert buffer_name state _buffer_name_to_orig_dtype f buffer_name missing pre-computed dict rank f state rank which only has keys f state _buffer_name_to_orig_dtype keys buffer_dtypes append state _buffer_name_to_orig_dtype buffer_name buffer_dtypes _cast_buffers_to_dtype_and_device buffers list torch Tensor buffer_dtypes list Optional torch dtype device torch device - None Casts ` ` buffers ` ` dtypes given ` ` buffer_dtypes ` ` moves them ` ` device ` ` If element ` ` buffer_dtypes ` ` ` ` None ` ` then corresponding buffer only moved ` ` device ` ` _p_assert buffer_dtypes None len buffers == len buffer_dtypes f Expects ` buffers ` ` buffer_dtypes ` have same length f ` buffer_dtypes ` specified got len buffers f len buffer_dtypes buffer buffer_dtype zip buffers buffer_dtypes torch is_floating_point buffer buffer_dtype None buffer data = buffer device=device buffer data = buffer device=device dtype=buffer_dtype