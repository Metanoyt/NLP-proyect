Owner s oncall quantization struct unittest torch torch testing _internal common_device_type dtypes dtypesIfCUDA instantiate_device_type_tests torch testing _internal common_utils DeterministicGuard IS_WINDOWS parametrize run_tests subtest TemporaryFileName TestCase FLOAT _DTYPES = torch float _e m torch float _e m fnuz torch float _e m fn torch float _e m fnuz torch float _e m fnu CUDA_FLOAT _DTYPES = torch float _e m torch float _e m fn torch float _e m fnu The following information yet provided torch finfo MANTISSA_BITS = torch float _e m torch float _e m fnuz torch float _e m fn torch float _e m fnuz torch float _e m fnu As np finfo dtype minexp MINEXP = torch float _e m - torch float _e m fnuz - torch float _e m fn - torch float _e m fnuz - torch float _e m fnu - SPECIAL_NUMBERS = torch float _e m float inf inf - float inf neg_inf float nan nan float nan nan float nan nan float nan nan float nan nan float nan nan zero - neg_zero max_normal - neg_max_normal - min_normal - - neg_min_normal - max_subnorm - - neg_max_subnorm - min_subnorm - - neg_min_subnorm torch float _e m fnuz float nan nan zero - neg_zero max_normal - neg_max_normal - min_normal - - neg_min_normal - max_subnorm - - neg_max_subnorm - min_subnorm - - neg_min_subnorm torch float _e m fn float nan nan float nan nan zero - neg_zero max_normal - neg_max_normal - min_normal - - neg_min_normal - max_subnorm - - neg_max_subnorm - min_subnorm - - neg_min_subnorm torch float _e m fnuz float nan nan zero - neg_zero max_normal - neg_max_normal - min_normal - - neg_min_normal - max_subnorm - - neg_max_subnorm - min_subnorm - - neg_min_subnorm torch float _e m fnu float - smallest_number float largest_number zero_point_five one two float nan nan FLOAT _DTYPES_WITH_INF = torch float _e m _int_bits_to_float x y = struct unpack f struct pack I x y simulate_fp _precision input variant Round input float given float datatype variant Constants dtype = torch float int_type = torch int mbits = MANTISSA_BITS variant minexp = MINEXP variant ml_dtypes finfo variant input = input dtype Extract bitfield components signs = torch sign input input_int = torch abs input view int_type exponent_bits = input_int x F mantissa_bits = input_int x FFFFF exponent_base = exponent_bits - x F Add implicit leading mantissas i e create mmmmmmmm f _is_normal = exponent_bits = mantissa_val_base = f _is_normal x + mantissa_bits Shift mantissa match minimum exponent - denormals lower precision dtype remain normal higher precision dtype denormal_bits = torch maximum minexp - exponent_base torch tensor dtype=int_type mantissa_val = mantissa_val_base denormal_bits exponent = exponent_base + denormal_bits Round off mantissas last_unrounded_bit = - mbits rounding_mask = last_unrounded_bit - mantissa_val_rounded = mantissa_val + rounding_mask ~rounding_mask Round ties nearest even ties = mantissa_val rounding_mask == last_unrounded_bit is_odd = mantissa_val_rounded last_unrounded_bit = mantissa_val_rounded += ties is_odd last_unrounded_bit Re-compose mantissa exponent vals = mantissa_val_rounded - + exponent dtype Replace overflows inf NaN appropriate no saturation have_inf = variant FLOAT _DTYPES_WITH_INF vals vals torch finfo variant max = torch inf have_inf torch nan vals signs _round_e m _rne biased_exponent lsb g r s round_up = False apply g r s rounding rules RNE rounding g == r == s == round_up = True lsb round_up = True round up necessary round_up biased_exponent += biased_exponent ROUND_TRIP_TEST_CASES = A general soak test subtest lambda dtype device torch rand device=device torch finfo dtype max name= soak A range below smallest normal lower precision type ensure these rounded correctly their nearest subnormal type subtest lambda dtype device torch rand device=device torch finfo dtype smallest_normal name= subnormals A range integers exert rounding nearest even subtest lambda dtype device torch arange int torch finfo dtype max dtype=torch int device=device name= rte Values around max subtest lambda dtype device torch finfo dtype max + torch finfo dtype eps torch finfo dtype max torch arange - device=device name= extremes TestFloat Dtype TestCase dtypes FLOAT _DTYPES dtypesIfCUDA CUDA_FLOAT _DTYPES test_creation_with_zeros dtype device Sanity test round-trip casting zeros x = torch zeros dtype=dtype device=device dtype torch float _e m fnu zeros supported dtype values get clamped ^ - x = torch full - dtype=torch float device=device assertEqual x x float atol= rtol= x = torch zeros dtype=torch float device=device assertEqual x x float atol= rtol= dtypes FLOAT _DTYPES dtypesIfCUDA CUDA_FLOAT _DTYPES parametrize get_input ROUND_TRIP_TEST_CASES test_cast_round_trip dtype get_input device Numerical test float conversion performing round-trip cast float dtype back float comparing against simulated lower precision dtype torch float _e m fnu unittest skip numerics e m fnu tested elsewhere x = get_input dtype device x = torch cat x -x x = x dtype x _simulated = simulate_fp _precision x dtype assertEqual x _simulated x float test_float _e m fnu_rne_rounding device For every possible e m exponent options every possible g r s bits float mantissa verify RNE rounding correctly applied when casting float e m Note code morally similar ` test_cast_round_trip ` IMO simpler special case e m here biased_exponent range iterate through all possible options guard round sticky bits current exponent grs range create positive floating point number specified exponent mantissa guard round sticky bits uint _t_start = biased_exponent + grs fp _start = _int_bits_to_float uint _t_start create RNE rounded version exponent biased_exponent == new_biased_exponent = biased_exponent lsb = biased_exponent g = grs r = grs b s = grs b new_biased_exponent = _round_e m _rne biased_exponent lsb g r s create RNE rounded version float fp _e m _fp _emulated = _int_bits_to_float new_biased_exponent now do same PyTorch see results match fp _pt_start = torch full fp _start device=device dtype=torch float fp _pt_e m = fp _pt_start torch float _e m fnu fp _pt_e m _fp = fp _pt_e m torch float expected = fp _e m _fp _emulated biased_exponent == grs = special case rounding up largest representable float exponent which saturates nan expected = float nan biased_exponent == special case inf nan which becomes nan expected = float nan actual = fp _pt_e m _fp item assertEqual expected actual f expected expected actual actual dtypes FLOAT _DTYPES dtypesIfCUDA CUDA_FLOAT _DTYPES test_special_numbers dtype device Test special numbers compare_binary_with_decimal binary decimal number_name dtype device bits_int = int binary tensor_int = torch tensor bits_int dtype=torch uint device=device tensor_fp = tensor_int view dtype number_name == nan assert tensor_fp isnan tensor_fp = tensor_fp float ref_tensor_fp = torch tensor decimal dtype=torch float device=device assertEqual tensor_fp ref_tensor_fp atol= rtol= number SPECIAL_NUMBERS dtype compare_binary_with_decimal number dtype device dtypes FLOAT _DTYPES dtypesIfCUDA CUDA_FLOAT _DTYPES test_type_promotion_fails dtype device Test float promoted higher precision Float Type other_dtype torch float torch bfloat torch float torch float x = torch randn device=device dtype y = torch randn device=device other_dtype assertRaisesRegex RuntimeError Promotion Float Types supported x + y dtypes FLOAT _DTYPES dtypesIfCUDA CUDA_FLOAT _DTYPES test_empty dtype device DeterministicGuard torch are_deterministic_algorithms_enabled use_deterministic True False torch use_deterministic_algorithms use_deterministic torch empty device=device dtype=dtype dtypes FLOAT _DTYPES dtypesIfCUDA CUDA_FLOAT _DTYPES test_to_string dtype device x = torch empty device=device dtype=dtype str x dtypes FLOAT _DTYPES test_finfo dtype device torch finfo dtype dtypes FLOAT _DTYPES dtypesIfCUDA CUDA_FLOAT _DTYPES test_cat dtype device x = torch empty device=device dtype=dtype x = torch empty device=device dtype=dtype torch cat x x dtypes FLOAT _DTYPES dtypesIfCUDA CUDA_FLOAT _DTYPES test_save_load dtype device x = torch randint device=device dtype=torch uint view dtype TemporaryFileName fname torch save x fname x _save_load = torch load fname torch testing assert_close x x _save_load atol= rtol= TestFloat Dtype TestCase TODO make testing generic shell dtypes test_float _e m fn_x device can create tensor dtype float x = torch empty device=device dtype=torch float _e m fn_x can create string so printing will work str x can view float _e m fn_x uint x = x view torch uint can view uint float _e m fn_x x view torch float _e m fn_x test_f _save_load device x = torch randint device=device dtype=torch uint view torch float _e m fn_x TemporaryFileName fname torch save x fname x _save_load = torch load fname TODO make all other shell dtypes support equality comparison torch testing assert_close x view torch uint x _save_load view torch uint atol= rtol= instantiate_device_type_tests TestFloat Dtype globals instantiate_device_type_tests TestFloat Dtype globals TestFloat DtypeCPUOnly TestCase Test mul implementation NOTE CPU-only now because adding CUDA requires adding yet another C++ dtype macro there no use case yet unscaled float multiplication - doesn t seem worth dtypes CUDA_FLOAT _DTYPES test_mul dtype TODO remove arithmetic support all float dtypes dtype torch float _e m fnu unittest skip arithmetic supported torch float _e m fnu shape = = torch randn shape _simulated = simulate_fp _precision dtype = dtype b = torch randn shape b _simulated = simulate_fp _precision b dtype b = b dtype mul = b mul _simulated = _simulated b _simulated dtype assertEqual mul mul _simulated unittest skipIf IS_WINDOWS torch compile supported Windows yet dtypes CUDA_FLOAT _DTYPES test_pt _traceable_aot_eager dtype dtype torch float _e m fnu unittest skip PT support torch float _e m fnu implemented yet torch compile backend= aot_eager fullgraph=True f x x = x dtype x = x float x x = torch randn requires_grad_ f x sum backward instantiate_device_type_tests TestFloat DtypeCPUOnly globals only_for= cpu __name__ == __main__ run_tests