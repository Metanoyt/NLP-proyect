collections OrderedDict typing Any torch torch _C _disabled_torch_function_impl __all__ = Parameter UninitializedParameter is_lazy Buffer UninitializedBuffer UninitializedTensorMixin Metaclass combine _TensorMeta instance check override Parameter _ParameterMeta torch _C _TensorMeta Make ` isinstance t Parameter ` True custom tensor instances have _is_param flag __instancecheck__ instance Parameter isinstance instance torch Tensor getattr instance _is_param False True super __instancecheck__ instance Parameter torch Tensor metaclass=_ParameterMeta r A kind Tensor considered module parameter Parameters ` ~torch Tensor ` subclasses have very special property when used ` Module ` s - when they re assigned Module attributes they automatically added list its parameters will appear e g meth ` ~Module parameters ` iterator Assigning Tensor doesn t have such effect This because one might want cache some temporary state like last hidden state RNN model If there no such ` Parameter ` these temporaries would get registered too Args data Tensor parameter tensor requires_grad bool optional parameter requires gradient Note torch no_grad context does NOT affect default behavior Parameter creation -- Parameter will still have ` requires_grad=True ` ` ~no_grad ` mode See ref ` locally-disable-grad-doc ` more details Default ` True ` __new__ cls data=None requires_grad=True data None data = torch empty type data torch Tensor type data Parameter For ease BC maintenance keep path standard Tensor Eventually tm we should change behavior standard Tensor match torch Tensor _make_subclass cls data requires_grad Path custom tensors set flag instance indicate parameter-ness t = data detach requires_grad_ requires_grad type t type data raise RuntimeError f Creating Parameter instance type type data __name__ requires detach returns instance same type f type type t __name__ found instead To use type Parameter please correct detach semantics defined its __torch_dispatch__ implementation t _is_param = True t Note methods below only apply standard Tensor Parameters custom tensor types still considered custom tensor type these methods will called them __deepcopy__ memo id memo memo id result = type data clone memory_format=torch preserve_format requires_grad memo id = result result pyrefly ignore bad-override __repr__ Parameter containing \n + super __repr__ __reduce_ex__ proto state = torch _utils _get_obj_state See Note Don t serialize hooks hooks = OrderedDict state torch _utils _rebuild_parameter data requires_grad hooks torch _utils _rebuild_parameter_with_state data requires_grad hooks state __torch_function__ = _disabled_torch_function_impl UninitializedTensorMixin _allowed_methods = torch Tensor __hash__ torch Tensor size torch Tensor copy_ torch Tensor is_complex torch Tensor is_floating_point torch Tensor half torch Tensor float torch Tensor double torch Tensor char torch Tensor short torch Tensor int torch Tensor long torch Tensor cuda torch Tensor cpu torch Tensor torch Tensor get_device torch _has_compatible_shallow_copy_type materialize shape device=None dtype=None r Create Parameter Tensor same properties uninitialized one Given shape materializes parameter same device same ` dtype ` current one specified ones arguments Args shape tuple shape materialized tensor device ` torch device ` desired device parameters buffers module Optional dtype ` torch dtype ` desired floating point type floating point parameters buffers module Optional device None device = data device dtype None dtype = data dtype data = torch empty shape device=device dtype=dtype pyrefly ignore bad-override missing-attribute __class__ = cls_to_become property shape raise RuntimeError Can t access shape uninitialized parameter buffer This error usually happens ` load_state_dict ` when trying load uninitialized parameter into initialized one Call ` forward ` initialize parameters before accessing their attributes share_memory_ raise RuntimeError Can t share memory uninitialized parameter buffer Call ` forward ` initialize parameters before calling ` module share_memory ` __repr__ f __class__ __name__ __reduce_ex__ proto See Note Don t serialize hooks pyrefly ignore missing-attribute __class__ requires_grad classmethod __torch_function__ cls func types args= kwargs=None method-wrapper detect access Tensor properties wrapped descriptors func cls _allowed_methods func __class__ __name__ == method-wrapper kwargs None kwargs = pyrefly ignore missing-attribute super __torch_function__ func types args kwargs raise ValueError f Attempted use uninitialized parameter func This error happens when you using ` LazyModule ` f explicitly manipulating ` torch nn parameter cls __name__ ` objects When using LazyModules Call ` forward ` dummy batch initialize parameters before calling torch functions is_lazy param Any - bool Returns whether ` ` param ` ` ` ` UninitializedParameter ` ` ` ` UninitializedBuffer ` ` Args param Any input check isinstance param UninitializedTensorMixin UninitializedParameter UninitializedTensorMixin Parameter r A parameter initialized Uninitialized Parameters special case ` torch nn Parameter ` where shape data still unknown Unlike ` torch nn Parameter ` uninitialized parameters hold no data attempting access some properties like their shape will throw runtime error The only operations can performed uninitialized parameter changing its datatype moving different device converting regular ` torch nn Parameter ` The default device dtype use when parameter materialized can set during construction using e g ` ` device= cuda ` ` cls_to_become = Parameter __new__ cls requires_grad=True device=None dtype=None - None factory_kwargs = device device dtype dtype data = torch empty factory_kwargs pyrefly ignore bad-return torch Tensor _make_subclass cls data requires_grad __deepcopy__ memo id memo memo id result = type requires_grad data device data dtype memo id = result result Metaclass combine _TensorMeta instance check override Buffer _BufferMeta torch _C _TensorMeta Make ` isinstance t Buffer ` True custom tensor instances have _is_buffer flag __instancecheck__ instance Buffer isinstance instance torch Tensor getattr instance _is_buffer False True super __instancecheck__ instance Buffer torch Tensor metaclass=_BufferMeta r A kind Tensor should considered model parameter For example BatchNorm s ` ` running_mean ` ` parameter part module s state Buffers ` ~torch Tensor ` subclasses have very special property when used ` Module ` s -- when they re assigned Module attributes they automatically added list its buffers will appear e g meth ` ~torch nn Module buffers ` iterator Assigning Tensor doesn t have such effect One can still assign Tensor explicitly using meth ` ~torch nn Module register_buffer ` function Args data Tensor buffer tensor persistent bool optional whether buffer part module s attr ` state_dict ` Default ` ` True ` ` __new__ cls data=None persistent=True data None data = torch empty t = data detach requires_grad_ data requires_grad pyrefly ignore missing-attribute t persistent = persistent pyrefly ignore missing-attribute t _is_buffer = True t __torch_function__ = _disabled_torch_function_impl UninitializedBuffer UninitializedTensorMixin torch Tensor r A buffer initialized Uninitialized Buffer special case ` torch Tensor ` where shape data still unknown Unlike ` torch Tensor ` uninitialized parameters hold no data attempting access some properties like their shape will throw runtime error The only operations can performed uninitialized parameter changing its datatype moving different device converting regular ` torch Tensor ` The default device dtype use when buffer materialized can set during construction using e g ` ` device= cuda ` ` cls_to_become = torch Tensor __new__ cls requires_grad=False device=None dtype=None persistent=True - None factory_kwargs = device device dtype dtype data = torch empty factory_kwargs ret = torch Tensor _make_subclass cls data requires_grad pyrefly ignore missing-attribute ret persistent = persistent pyrefly ignore missing-attribute ret _is_buffer = True pyrefly ignore bad-return ret