Script generate baseline values PyTorch optimization algorithms argparse math sys torch torch optim HEADER = #include torch types h #include vector namespace expected_parameters FOOTER = namespace expected_parameters PARAMETERS = inline std vector std vector torch Tensor OPTIMIZERS = LBFGS lambda p torch optim LBFGS p LBFGS_with_line_search lambda p torch optim LBFGS p line_search_fn= strong_wolfe Adam lambda p torch optim Adam p Adam_with_weight_decay lambda p torch optim Adam p weight_decay= e- Adam_with_weight_decay_and_amsgrad lambda p torch optim Adam p weight_decay= e- amsgrad=True AdamW lambda p torch optim AdamW p AdamW_without_weight_decay lambda p torch optim AdamW p weight_decay= AdamW_with_amsgrad lambda p torch optim AdamW p amsgrad=True Adagrad lambda p torch optim Adagrad p Adagrad_with_weight_decay lambda p torch optim Adagrad p weight_decay= e- Adagrad_with_weight_decay_and_lr_decay lambda p torch optim Adagrad p weight_decay= e- lr_decay= e- RMSprop lambda p torch optim RMSprop p RMSprop_with_weight_decay lambda p torch optim RMSprop p weight_decay= e- RMSprop_with_weight_decay_and_centered lambda p torch optim RMSprop p weight_decay= e- centered=True RMSprop_with_weight_decay_and_centered_and_momentum lambda p torch optim RMSprop p weight_decay= e- centered=True momentum= SGD lambda p torch optim SGD p SGD_with_weight_decay lambda p torch optim SGD p weight_decay= e- SGD_with_weight_decay_and_momentum lambda p torch optim SGD p momentum= weight_decay= e- SGD_with_weight_decay_and_nesterov_momentum lambda p torch optim SGD p momentum= weight_decay= e- nesterov=True weight_init module isinstance module torch nn Linear stdev = math sqrt module weight size p module parameters p data uniform_ -stdev stdev run optimizer_name iterations sample_every torch manual_seed model = torch nn Sequential torch nn Linear torch nn Sigmoid torch nn Linear torch nn Sigmoid model = model torch float apply weight_init optimizer = OPTIMIZERS optimizer_name model parameters input = torch tensor dtype=torch float values = i range iterations optimizer zero_grad output = model forward input loss = output sum loss backward closure torch tensor optimizer step closure i sample_every == values append p clone flatten data numpy p model parameters values emit optimizer_parameter_map Don t write generated front file recognized generated print format generated __file__ print HEADER optimizer_name parameters optimizer_parameter_map items print PARAMETERS format optimizer_name print sample parameters print parameter sample parameter_values = format join map str parameter print f torch tensor parameter_values print print print \n print FOOTER main parser = argparse ArgumentParser Produce optimization output baseline PyTorch parser add_argument -i -- iterations default= type=int parser add_argument -s -- sample-every default= type=int options = parser parse_args optimizer_parameter_map = optimizer OPTIMIZERS keys sys stderr write f Evaluating optimizer \n optimizer_parameter_map optimizer = run optimizer options iterations options sample_every emit optimizer_parameter_map __name__ == __main__ main