__future__ annotations argparse json os xml etree ElementTree ET pathlib Path tempfile TemporaryDirectory typing Any TYPE_CHECKING tools stats upload_stats_lib download_s _artifacts is_rerun_disabled_tests unzip upload_workflow_stats_to_s tools stats upload_test_stats process_xml_element TYPE_CHECKING collections abc Generator TESTCASE_TAG = testcase SEPARATOR = process_report report Path - dict str dict str int Return list disabled tests should re-enabled those still flaky failed skipped root = ET parse report All rerun tests report grouped here Success test should re-enable s green after rerunning all platforms where currently disabled Failures pytest because pytest-flakefinder used run same test multiple times some could fails Skipped tests unittest We want keep track how many times test fails num_red passes num_green all_tests dict str dict str int = test_case root iter TESTCASE_TAG Parse test case string values only parsed_test_case = process_xml_element test_case output_numbers=False Under -- rerun-disabled-tests mode test skipped when s skipped explicitly inside PyTorch code s skipped because s normal enabled test s falky num_red num_green s failing num_red num_green == We care only about latter two here skipped = parsed_test_case get skipped None NB Regular ONNX tests could list subskips here where each item list skipped message In context rerunning disabled tests we could ignore case returning list subskips only happens when tests run normally skipped type skipped list num_red skipped get message continue name = parsed_test_case get name classname = parsed_test_case get classname filename = parsed_test_case get file name classname filename continue Check test failure failure = parsed_test_case get failure None disabled_test_id = SEPARATOR join name classname filename disabled_test_id all_tests all_tests disabled_test_id = num_green num_red Under -- rerun-disabled-tests mode test skipped failed s counted success Otherwise s still flaky failing skipped try stats = json loads skipped get message except json JSONDecodeError stats = all_tests disabled_test_id num_green += stats get num_green all_tests disabled_test_id num_red += stats get num_red failure As failure increase failure count all_tests disabled_test_id num_red += all_tests disabled_test_id num_green += all_tests get_test_reports repo str workflow_run_id int workflow_run_attempt int - Generator Path None None Gather all test reports S GHA It currently possible guess which test reports rerun_disabled_tests workflow because name doesn t include test config So all reports will need downloaded examined TemporaryDirectory temp_dir print Using temporary directory temp_dir os chdir temp_dir artifact_paths = download_s _artifacts test-reports workflow_run_id workflow_run_attempt path artifact_paths unzip path yield Path glob xml get_disabled_test_name test_id str - tuple str str str str Follow flaky bot convention here changes will also need updated name classname filename = test_id split SEPARATOR f name __main__ classname name classname filename prepare_record workflow_id int workflow_run_attempt int name str classname str filename str flaky bool num_red int = num_green int = - tuple Any dict str Any Prepare record save onto S key = workflow_id workflow_run_attempt name classname filename record = workflow_id workflow_id workflow_run_attempt workflow_run_attempt name name classname classname filename filename flaky flaky num_green num_green num_red num_red key record save_results workflow_id int workflow_run_attempt int all_tests dict str dict str int - None Save result S which then gets put into HUD backend database should_be_enabled_tests = name stats name stats all_tests items num_green stats stats num_green num_red stats stats num_red == still_flaky_tests = name stats name stats all_tests items name should_be_enabled_tests records = test_id stats all_tests items num_green = stats get num_green num_red = stats get num_red disabled_test_name name classname filename = get_disabled_test_name test_id key record = prepare_record workflow_id=workflow_id workflow_run_attempt=workflow_run_attempt name=name classname=classname filename=filename flaky=test_id still_flaky_tests num_green=num_green num_red=num_red records key = record Log results print f The following len should_be_enabled_tests tests should re-enabled test_id stats should_be_enabled_tests items disabled_test_name name classname filename = get_disabled_test_name test_id print f disabled_test_name filename print f The following len still_flaky_tests still flaky test_id stats still_flaky_tests items num_green = stats get num_green num_red = stats get num_red disabled_test_name name classname filename = get_disabled_test_name test_id print f disabled_test_name filename failing num_red num_red + num_green upload_workflow_stats_to_s workflow_id workflow_run_attempt rerun_disabled_tests list records values main repo str workflow_run_id int workflow_run_attempt int - None Find list all disabled tests should re-enabled Aggregated across all jobs all_tests dict str dict str int = report get_test_reports args repo args workflow_run_id args workflow_run_attempt tests = process_report report The scheduled workflow has both rerun disabled tests memory leak check jobs We only interested former here is_rerun_disabled_tests report workflow_run_id workflow_run_attempt tests continue name stats tests items name all_tests all_tests name = stats copy all_tests name num_green += stats get num_green all_tests name num_red += stats get num_red save_results workflow_run_id workflow_run_attempt all_tests __name__ == __main__ parser = argparse ArgumentParser description= Upload test artifacts GHA S parser add_argument -- workflow-run-id type=int required=True help= id workflow get artifacts parser add_argument -- workflow-run-attempt type=int required=True help= which retry workflow parser add_argument -- repo type=str required=True help= which GitHub repo workflow run belongs args = parser parse_args main args repo args workflow_run_id args workflow_run_attempt