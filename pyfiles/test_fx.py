Owner s module fx ruff noqa F flake noqa E builtins collections contextlib copy gc functools inspect io math numbers operator os pickle sys traceback types typing unittest weakref warnings math sqrt torch multiprocessing Process torch testing FileCheck torch testing _internal common_methods_invocations op_db torch testing _internal common_device_type ops onlyCPU instantiate_device_type_tests torch utils _pytree pytree torch fx _pytree fx_pytree torch fx symbolic_trace Proxy Node GraphModule Interpreter Tracer Transformer Graph wrap PH CodeGen torch fx node Target Argument ArgumentT _format_arg torch fx passes shape_prop torch fx immutable_collections immutable_dict immutable_list torch fx experimental rewriter RewritingTracer torch fx operator_schemas get_signature_for_torch_op copy deepcopy collections namedtuple typing Any NamedTuple Optional Union collections abc Callable torch functorch experimental control_flow fx named_tup MyNamedTup fx test_common_passes TestCommonPass noqa F fx test_cse_pass TestCSEPass noqa F fx test_dce_pass TestDCE noqa F fx test_fx_const_fold TestConstFold noqa F fx test_fx_param_shape_control_flow noqa F TestConstParamShapeInControlFlow fx test_gradual_type noqa F noqa F AnnotationsTest TypeCheckerTest fx test_matcher_utils TestMatcher noqa F fx test_pass_infra TestPassManager noqa F fx test_source_matcher_utils TestSourceMatcher noqa F fx test_subgraph_rewriter TestSubgraphRewriter noqa F torch fx _compatibility _BACK_COMPAT_OBJECTS _MARKED_WITH_COMPATIBILITY torch fx _symbolic_trace PHBase PHWithMeta torch fx proxy TraceError torch testing _internal common_utils find_library_location IS_FBCODE IS_MACOS IS_WINDOWS run_tests skipIfTorchDynamo torch testing _internal jit_utils JitTestCase try torchvision models torchvision_models HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False skipIfNoTorchVision = unittest skipIf HAS_TORCHVISION no torchvision torch testing _internal common_quantization skipIfNoDynamoSupport SimpleTest torch nn Module forward x torch relu x + a_non_torch_leaf b + b Used test_autowrap_function Autowrapped functions need global fx_int x float - int int x fx_int_x x float - int int x used test_pytree It s all way out here because pickling GraphModule uses Point errors out Point local function Point = namedtuple Point x y Test wrap passing both function name well function directly a_lifted_leaf b + + b wrap a_lifted_leaf Test wrapping twice doesn t break anything wrap a_lifted_leaf a_lifted_leaf b + + b wrap a_lifted_leaf wrap len wrap getattr wrapped_named_tup p p p x + p y wrap wrapped_named_tup wrap wrapped_via_decorator + wrap wrapped_with_submodule wrapped_with_submodule x torch Tensor batchnorm d torch nn BatchNorm d batchnorm d x my_decorator f functools wraps f wrapper_inside_decorator args kwargs f args kwargs wrapper_inside_decorator wrap my_decorator wrapped_decorated_fn x x real_wrapped_via_decorator = wrapped_via_decorator real_a_lifed_leaf = a_lifted_leaf real_a_lifed_leaf = a_lifted_leaf _sqrt = sqrt wrap wrapper_fn wrapper_fn x torch foo x Pair NamedTuple x torch Tensor y torch Tensor _custom_fx_repr_fn - str f Pair x= _format_arg x y= _format_arg y testing pytrees Foo __init__ b = b = b Add torch nn Module forward x x + x torch fx has_side_effect torch fx wrap side_effect_func x torch Tensor print x TestFX JitTestCase setUp super setUp Checking mutable operations while tracing feature flagged Enable testing default orig_tracer_mutable_flag = torch fx proxy TracerBase check_mutable_operations torch fx proxy TracerBase check_mutable_operations = True IS_FBCODE IS_WINDOWS IS_MACOS lib_file_path = find_library_location libtorchbind_test so torch ops load_library str lib_file_path tearDown super tearDown torch fx proxy TracerBase check_mutable_operations = orig_tracer_mutable_flag checkGraphModule m torch nn Module args kwargs=None Check nn Module s results match GraphModule version given set args kwargs kwargs = kwargs kwargs ref_outs = m args kwargs gm = symbolic_trace m gm graph lint test_outs = gm args kwargs assertEqual ref_outs test_outs test_graph_module MySub torch nn Module __init__ - None super __init__ w = torch nn Parameter torch rand forward x w + x MyModule torch nn Module __init__ - None super __init__ lin = torch nn Linear sub_mod = MySub w = torch nn Parameter torch rand forward A B c t = torch sigmoid A + lin c sub_mod t data + w + t + - A + B A + -A + A add B alpha= m = MyModule gm = symbolic_trace m ms = torch jit script gm M torch nn Module forward A m idx = torch max A m + idx + m = M gm = symbolic_trace m T torch nn Module forward A b= args c= kwargs x = A + + args + kwargs x t = T symbolic_trace t test issue described https github com pytorch pytorch issues M torch nn Module forward x torch relu x m = M gm = symbolic_trace m new_instance = gm __new__ type gm new_instance __init__ gm gm graph x = torch randn torch testing assert_close new_instance x torch relu x test_informative_co_filename MyModule torch nn Module forward gm = symbolic_trace MyModule assertIn os path basename __file__ gm forward __code__ co_filename test_custom_import graph = torch fx Graph = graph placeholder x b = graph placeholder y c = graph call_function a_non_torch_leaf b d = graph call_function torch sin c graph output d gm = GraphModule torch nn Module graph x y = torch rand torch rand assertEqual torch sin x + y gm x y test_args_kwargs T torch nn Module forward args kwargs x = args + kwargs foo x t = T checkGraphModule t torch rand torch rand foo torch rand test_varargs_concrete T torch nn Module forward args kwargs x = args + args x args = torch rand torch rand t = T ref_outs = t args gm = symbolic_trace t concrete_args= torch fx PH torch fx PH gm graph lint test_outs = gm args assertEqual ref_outs test_outs test_args_kwargs_no_self T torch nn Module forward args kwargs noqa B = args torch relu args t = T assertRaisesRegex RuntimeError r cannot part \ args expansion checkGraphModule t torch rand torch rand foo torch rand test_fx_shifts MyModule torch nn Module forward x x x input = torch LongTensor random_ m = MyModule checkGraphModule m input test_fx_and_or MyModule torch nn Module forward x x x x &#124; x input = torch LongTensor random_ m = MyModule checkGraphModule m input test_dict MyDictMod torch nn Module forward d d relu d neg input_dict = torch rand m = MyDictMod checkGraphModule m input_dict test_matmul_tracing const = torch randn matmul_f x x const mod = symbolic_trace matmul_f inp = torch randn assertEqual mod inp matmul_f inp rmatmul_f x const x mod = symbolic_trace rmatmul_f inp = torch randn assertEqual mod inp rmatmul_f inp skipIfNoDynamoSupport test_control_flow_tracing true x y x + y false x y x - y f x y x = control_flow cond x == true false x y assertRaisesRegex RuntimeError r Expected pred bool tensor got Proxy\ eq\ _ = symbolic_trace f test_disallow_override Custom delegate disallow in-place tensor operations NoMutableCallTracer Tracer create_node kind str target Union str Callable args tuple Argument kwargs dict str Any name Optional str = None type_expr Optional Any = None - Node name = target isinstance target str torch typename target name - == _ raise RuntimeError In-place operations supported super create_node kind target args kwargs name Test method MyInplaceMod torch nn Module forward x x add_ x m = MyInplaceMod assertRaisesRegex RuntimeError In-place operations NoMutableCallTracer trace m Test free function MyInplaceMod torch nn Module forward x torch log_ x x m = MyInplaceMod assertRaisesRegex RuntimeError In-place operations NoMutableCallTracer trace m Test symbolic node arg MyInplaceMod torch nn Module forward x y = torch ones y add_ x x m = MyInplaceMod assertRaisesRegex RuntimeError In-place operations NoMutableCallTracer trace m test_leaf_module Custom delegate make so there no leaf modules everything should get traced through NoLeafModulesTracer Tracer is_leaf_module m qualname False MyReluMod torch nn Module __init__ - None super __init__ relu = torch nn ReLU forward x relu x mrm = MyReluMod sym = NoLeafModulesTracer trace mrm node sym nodes assertNotEqual node op call_module sym lint test_wrap assertEqual + + a_lifted_leaf to_trace y a_lifted_leaf y + a_lifted_leaf + a_lifted_leaf y y y m = symbolic_trace to_trace assertIn a_lifted_leaf m code assertEqual m assertIs a_lifted_leaf real_a_lifed_leaf test_wrap_fn_directly assertEqual + + a_lifted_leaf to_trace y a_lifted_leaf y + a_lifted_leaf + a_lifted_leaf y y y m = symbolic_trace to_trace assertIn a_lifted_leaf m code assertEqual m assertIs a_lifted_leaf real_a_lifed_leaf test_wrapped_via_decorator assertEqual wrapped_via_decorator to_trace y wrapped_via_decorator y m = symbolic_trace to_trace assertIn wrapped_via_decorator m code assertEqual m assertIs wrapped_via_decorator real_wrapped_via_decorator assertFalse hasattr wrapped_via_decorator __fx_already_patched test_wrapped_via_decorator_and_transformed assertEqual wrapped_via_decorator to_trace y wrapped_via_decorator y m = symbolic_trace to_trace assertIn wrapped_via_decorator m code assertEqual m assertIs wrapped_via_decorator real_wrapped_via_decorator assertFalse hasattr wrapped_via_decorator __fx_already_patched transformed = torch fx Transformer m transform assertIn wrapped_via_decorator transformed code assertEqual transformed assertIs wrapped_via_decorator real_wrapped_via_decorator assertFalse hasattr wrapped_via_decorator __fx_already_patched test_wrap_with_submodule M torch nn Module __init__ - None super __init__ batchnorm d = torch nn BatchNorm d affine=False forward x torch Tensor wrapped_with_submodule x batchnorm d m = symbolic_trace M assertIn wrapped_with_submodule m code input = torch rand ref_batchnorm d = torch nn BatchNorm d affine=False assertEqual ref_batchnorm d input m input test_wrapped_retrace to_trace y wrapped_via_decorator y m = symbolic_trace to_trace assertIn wrapped_via_decorator m code assertEqual m retraced = symbolic_trace m assertIn wrapped_via_decorator retraced code assertEqual retraced test_wrap_decorated_function to_trace y wrapped_decorated_fn y m = symbolic_trace to_trace assertIn wrapped_decorated_fn m code assertEqual m test_graph_edit_with_proxy M torch nn Module forward b + b m = M g = symbolic_trace m graph new_g = torch fx Graph val_map dict Node Node = output_val = new_g graph_copy g val_map t = Proxy output_val test we can use proxy objects generate more graph code later things do need work modules new_g output t + t node gm = GraphModule m new_g gm graph lint assertEqual gm test_proxy_deepcopy_without_tracer MyModule torch nn Module __init__ super __init__ forward x x module = MyModule traced = symbolic_trace module node = list traced graph nodes - p = torch fx Proxy node None node proxy = p p = copy deepcopy p assertTrue isinstance p torch fx Proxy assertEqual p node name node name assertEqual p node target node target assertNotEqual id p node id node test_proxy_deepcopy_with_tracer TestTracer Tracer __init__ name super __init__ name = name is_leaf_module module name True MyModule torch nn Module __init__ super __init__ forward x x module = MyModule tracer = TestTracer mytracer traced = symbolic_trace module node = list traced graph nodes - p = torch fx Proxy node tracer node proxy = p p = copy deepcopy p assertTrue isinstance p torch fx Proxy assertTrue isinstance p tracer torch fx _symbolic_trace Tracer assertEqual p tracer name mytracer assertEqual p node name node name assertEqual p node target node target assertNotEqual id p node id node assertNotEqual id p tracer id tracer test_concrete_arg_none_assert Foo torch nn Module forward x val=None x val None x + val f = Foo traced = torch fx symbolic_trace f concrete_args= val None assertRaisesRegex AssertionError val has been specialized have value None traced torch randn torch randn x = torch randn torch testing assert_close traced x f x test_trace_multiple_funcs Foo torch nn Module forward x y x + y minus_forward x y x - y multiply_forward x y x y f = Foo x y = torch randn torch randn print torch __version__ tracer = Tracer torch testing assert_close GraphModule f tracer trace f x y f x y tracer traced_func_name = minus_forward torch testing assert_close GraphModule f tracer trace f x y f minus_forward x y tracer traced_func_name = multiply_forward torch testing assert_close GraphModule f tracer trace f x y f multiply_forward x y tracer traced_func_name = add_forward assertRaisesRegex AssertionError doesn t exist tracer trace f test_graph_unique_names M torch nn Module forward b + b m = M g = symbolic_trace m graph new_g = torch fx Graph val_map dict Node Node = output_val = new_g graph_copy g val_map t = Proxy output_val test we can use proxy objects generate more graph code later things do need work modules new_g output t + t node gm = GraphModule m new_g seen_names set str = set node gm graph nodes assert node name seen_names seen_names add node name test_stack_traces foo b b M torch nn Module __init__ super __init__ forward b c = + b c = foo c c tracer = torch fx Tracer tracer record_stack_traces = True graph = tracer trace M saving original list because we will insert new nodes part test stack_traces = \n join node meta get stack_trace node graph nodes FileCheck check_count c = + b exactly=True run stack_traces strip FileCheck check_count c = foo c exactly=True run stack_traces strip FileCheck check_count b exactly=True run stack_traces strip test_stack_traces_with_transformer M torch nn Module forward b + b tracer = torch fx Tracer tracer record_stack_traces = True graph = tracer trace M gm = GraphModule tracer root graph new_gm = Transformer gm transform nodes after Transformer should still preserve original node s stack trace node new_gm graph nodes node op placeholder output continue assertTrue node stack_trace None assert test_fx py node stack_trace test_lineno_map M torch nn Module forward b = torch sin b = torch cos b + b tracer = torch fx Tracer graph = tracer trace M gm = GraphModule tracer root graph expected = assertTrue set expected items issubset set gm _lineno_map items test custom codegen transform_code code print hello \n code gm graph on_generate_code lambda _ transform_code gm recompile expected = assertTrue set expected items issubset set gm _lineno_map items test_graph_unique_names_manual graph torch fx Graph = torch fx Graph torch fx Node = graph create_node placeholder x b torch fx Node = graph create_node call_module linear_mod args= name= foo_ _ c torch fx Node = graph create_node get_attr y_attr name= foo_ d torch fx Node = graph create_node call_function operator add args= b c graph output d graph = torch fx Graph val_map dict Node Node = graph graph_copy graph val_map seen_names set str = set node graph nodes assert node name seen_names seen_names add node name test_unpack M torch nn Module forward b c d = c + d + b = torch rand torch rand b = torch rand m = M checkGraphModule m b test_native_callable IS_FBCODE IS_WINDOWS IS_MACOS raise unittest SkipTest non-portable load_library call used test This test exercises case where we use FX translate Python code some native callable object For purposes testing we use ElementwiseInterpreter defined test_custom_class cpp We test we can Construct native callable FX IR Construct drop-in replacement module delegates native callable rather than original code Run both original code native callable wrapper equivalent results TorchScript compile native callable wrapper confirm equivalent results reference TorchScript serialize deserialize native callable confirm equivalent results reference We use simple Module reference computation MySimpleMod torch nn Module forward x x + x msm = MySimpleMod This what lowering pass might look like function takes valid nn Module symbolically traces lowers Module some representation wraps representation up into another nn Module instance handles dispatch compiled lowered code lower_to_elementwise_interpreter orig_mod torch nn Module - torch nn Module ===== Stage Symbolic trace module ===== mod = symbolic_trace orig_mod ===== Stage Lower GraphModule representation C++ interpreter s instruction format ====== instructions = constant_idx = constants = fn_input_names = target_to_name = operator add add operator mul mul output_node Optional Node = None For each instruction create triple instruction_name str inputs List str output str feed into C++ interpreter n mod graph nodes target args out_name = n target n args n name assert len n kwargs == kwargs currently supported n op == placeholder Placeholders specify function argument names Save these later when we generate wrapper GraphModule fn_input_names append target n op == call_function assert target target_to_name Unsupported call target + target arg_names = arg args isinstance arg Node Pull out constants These constants will later fed interpreter C++ object via add_constant arg_name = f constant_ constant_idx constants arg_name = torch tensor arg isinstance arg numbers Number arg arg_names append arg_name constant_idx += arg_names append arg name instructions append target_to_name target arg_names out_name n op == output output_node None raise RuntimeError Multiple output nodes output_node = n raise RuntimeError Unsupported opcode + n op interpreter = torch classes _TorchScriptTesting _ElementwiseInterpreter Load constants k v constants items interpreter add_constant k v Specify names positional input arguments interpreter set_input_names fn_input_names Load instructions interpreter set_instructions instructions Specify name single output assert isinstance output_node args torch fx Node interpreter set_output_name output_node args name ===== Stage Create wrapper GraphModule around interpreter ===== WrapperModule torch nn Module __init__ interpreter super __init__ interpreter = interpreter wrapper = WrapperModule interpreter Create graph Takes function arguments Invokes interpreter Returns specified value FIXME The following code could greatly simplified symbolic_trace ing wrapper Tracer considers Wrapper instance root module however I can t get ` __call__ ` exposed TorchBind classes without messing up Python ` hasattr ` some reason More digging into CPython s implementation hasattr probably order graph = torch fx Graph Add placeholders fn inputs placeholder_nodes = name fn_input_names placeholder_nodes append graph create_node placeholder name Get interpreter object interpreter_node = graph create_node get_attr interpreter Add node call interpreter instance output_node = graph create_node op= call_method target= __call__ args= interpreter_node placeholder_nodes Register output graph output output_node graph lint Return final GraphModule GraphModule wrapper graph Lower GraphModule C++ interpreter lowered = lower_to_elementwise_interpreter msm Compare correctness original module x = torch rand ref_out = msm x test_out = lowered x torch testing assert_close test_out ref_out Test TorchScript compilation scripted_lowered = torch jit script lowered script_out = scripted_lowered x torch testing assert_close script_out ref_out Test TorchScript Ser De import_copy = getExportImportCopy scripted_lowered imported_out = import_copy x torch testing assert_close imported_out ref_out test_reserved_getattr Ensure we do name any nodes reserved builtin like ` getattr ` M torch nn Module forward foo bar baz m = M m_g = symbolic_trace m m_g graph lint node m_g graph nodes assertTrue node name = getattr unittest skip Hotfix SEV remediation test_trace_buffer_slice bs d_hid = ExampleCode torch nn Module __init__ - None super __init__ mm_param = torch nn Parameter torch randn d_hid d_hid mm_param = torch nn Parameter torch randn d_hid d_hid lin = torch nn Linear d_hid d_hid buffer = torch nn Buffer torch randn bs + d_hid forward x x = torch mm x mm_param skip_connection = x x = torch relu x x = torch mm x mm_param + buffer x shape x = lin x x = torch relu x x = x + skip_connection x = torch mm x mm_param x = lin x x ec = ExampleCode traced = torch fx symbolic_trace ec x = torch randn bs d_hid torch testing assert_close ec x traced x test_node_tagging TaggingTracer Tracer create_node kind str target Union str Callable args tuple Argument kwargs dict str Any name Optional str = None type_expr Optional Any = None - Node n = super create_node kind target args kwargs name n tag = foo n M torch nn Module forward b + b m = M g = TaggingTracer trace m g lint n g nodes assertTrue hasattr n tag assertEqual n tag foo test_tensor_attribute TensorAttribute torch nn Module __init__ - None super __init__ tensor = torch rand forward x torch nn functional linear x tensor ta = TensorAttribute traced = symbolic_trace ta traced torch rand WrapperForQualname torch nn Module __init__ - None super __init__ ta = TensorAttribute forward x torch nn functional linear x ta tensor wfq = WrapperForQualname traced = symbolic_trace wfq traced graph lint traced torch rand test_tensor_attribute_coalseced count_attrs fx_module targets = set node traced graph nodes node op == get_attr targets add node target len targets val = torch tensor f x x + val + val traced = symbolic_trace f traced graph lint assertEqual count_attrs traced val = torch tensor f x val = torch tensor x + val + val traced = symbolic_trace f traced graph lint assertEqual count_attrs traced test_symbolic_trace_sequential Simple torch nn Module forward x torch neg x seq = torch nn Sequential Simple Simple Simple traced = symbolic_trace seq traced graph lint x = torch rand assertEqual traced x seq x test_tensor_constant ConstTensor torch nn Module forward x torch nn functional linear x torch zeros ct = ConstTensor traced = symbolic_trace ct traced graph lint traced torch rand test_pickle_graphmodule Nested torch nn Module __init__ - None super __init__ st = torch nn Linear forward x st x n = Nested traced = symbolic_trace n traced graph lint pickled = pickle dumps traced loaded = pickle loads pickled loaded graph lint x = torch rand assertEqual loaded x traced x test_pickle_custom_import graph = torch fx Graph = graph placeholder x b = graph placeholder y c = graph call_function a_non_torch_leaf b d = graph call_function torch sin c graph output d gm = GraphModule torch nn Module graph pickled = pickle dumps gm loaded = pickle loads pickled loaded graph lint x y = torch rand torch rand assertEqual loaded x y gm x y test_all_input_nodes graph torch fx Graph = torch fx Graph torch fx Node = graph placeholder x b torch fx Node = graph call_module linear_mod args= c torch fx Node = graph get_attr y_attr d torch fx Node = graph call_function operator add args= b c e torch fx Node = graph call_function torch unsqueeze args= d graph output e graph lint assertEqual b all_input_nodes assertEqual c all_input_nodes assertEqual d all_input_nodes b c assertEqual e all_input_nodes d test_deepcopy_graphmodule_with_transform st = SimpleTest traced = symbolic_trace st traced graph lint transform traced new_graph = torch fx Graph val_map dict Node Node = output_value = new_graph graph_copy traced graph val_map relu_out = new_graph create_node op= call_method target= neg args= output_value kwargs= new_graph output relu_out GraphModule traced new_graph transformed = transform traced transformed graph lint copied = copy deepcopy transformed assertNotEqual id type transformed id type copied x = torch randn assertEqual copied x transformed x test_deepcopy_with_submods_params Bar torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand forward x torch relu x + param Baz torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand bar = Bar forward x bar x - param baz = Baz traced = symbolic_trace baz traced graph lint copied = copy deepcopy traced copied graph lint test_deepcopy_graph_with_tracer_cls TestTracer Tracer is_leaf_module module name True g = Graph tracer_cls=TestTracer x = g placeholder x g output x h = copy deepcopy g assertIsNotNone h _tracer_cls assertTrue g _tracer_cls == h _tracer_cls test_unpack_list_better_error SomeArgs torch nn Module forward b torch rand UnpacksList torch nn Module __init__ - None super __init__ sa = SomeArgs forward x list sa x ul = UnpacksList assertRaisesRegex TraceError Proxy object cannot iterated symbolic_trace ul test_unpack_dict_better_error SomeKwargs torch nn Module forward x= y= torch rand UnpacksDict torch nn Module __init__ - None super __init__ sk = SomeKwargs forward x dict sk x ud = UnpacksDict assertRaisesRegex TraceError Proxy object cannot iterated symbolic_trace ud test_pretty_print_targets Test Graph pretty-print prints friendly name targets ` operator ` ` builtins ` SomeMod torch nn Module forward x torch add x foo + x bar traced = symbolic_trace SomeMod graph_str = str traced graph assertIn builtins getattr graph_str assertIn operator add graph_str assertIn torch add graph_str test_pretty_print_node M torch nn Module __init__ - None super __init__ param torch nn Parameter = torch nn Parameter torch rand linear = torch nn Linear forward x torch Tensor y int = linear x y + param clamp min= max= traced = symbolic_trace M all_formatted = \n join n format_node n traced graph nodes FileCheck check x check placeholder check y check placeholder check getitem check call_function check param check get_attr check add check call_function check linear check call_module check clamp check call_method run all_formatted test_print_graph op torch _ops OpOverload = torch ops aten relu default type_name str = torch typename op graph torch fx Graph = torch fx Graph torch fx Node = graph create_node placeholder x b torch fx Node = graph create_node call_function op type_expr=type_name c torch fx Node = graph create_node call_function op b type_expr=type_name graph output b c gm torch fx GraphModule = torch fx GraphModule torch nn Module graph gm graph lint text = gm print_readable False assert == text count _torch__ops_aten_aten_relu_ test_script_tensor_constant TorchScript seems ignore attributes start ` __ ` We used call anonymous Tensor values ` __tensor_constant ` they getting ignored script Now they re called ` _tensor_constant ` IHaveATensorConstant torch nn Module forward x x + torch rand traced = torch fx symbolic_trace IHaveATensorConstant torch jit script traced test_autowrap_functions AutowrapFnTest torch nn Module forward x fx_int x shape AutowrapFnTest torch nn Module forward x fx_int x shape + fx_int_x x shape Check function s wrapped ` int ` would normally throw TypeError argument can t ` Proxy ` tracer = Tracer autowrap_functions= fx_int graph = tracer trace AutowrapFnTest traced = GraphModule tracer root graph test tracer_ = Tracer autowrap_functions= fx_int fx_int_x tracer_ trace AutowrapFnTest Test scriptability traced_scripted = torch jit script traced assertEqual traced_scripted torch rand test_tuple_no_subscript foo x tuple x traced = torch fx symbolic_trace foo x = torch randn torch testing assert_close traced x x bio = io BytesIO torch save traced bio bio seek weights_only=False loads GraphModule GLOBAL torch fx graph_module reduce_graph_module allowed global default loaded = torch load bio weights_only=False torch testing assert_close loaded x x test_torch_fx_len FXLenTest torch nn Module forward x len x traced = symbolic_trace FXLenTest assertEqual traced torch rand Test scriptability scripted = torch jit script FXLenTest assertEqual scripted torch rand traced_scripted = torch jit script traced assertEqual traced_scripted torch rand Test non-proxy len FXLenTest torch nn Module __init__ - None super __init__ l = forward x x + len l traced = symbolic_trace FXLenTest inp = torch rand assertEqual traced inp inp + assertIs len builtins len test_torch_fx_getattr FXGetattrTest torch nn Module forward x getattr x nonexistent_attr torch Tensor traced = symbolic_trace FXGetattrTest assertEqual traced torch rand torch Tensor test_sqrt Sqrt torch nn Module forward x sqrt x size Sqrt torch nn Module forward x math sqrt x size Sqrt torch nn Module forward x x + math sqrt + sqrt checkGraphModule Sqrt torch zeros checkGraphModule Sqrt torch zeros checkGraphModule Sqrt torch zeros assertIs sqrt _sqrt assertIs math sqrt _sqrt test_torch_custom_ops M torch nn Module forward b = torch ops aten sigmoid c = torch ops aten cat b torch ops aten cat c c m = M input = torch randn ref_out = m input gm = symbolic_trace m gm graph lint out = gm input assertEqual out ref_out test_torch_op_overloads M torch nn Module forward b = torch ops aten add Tensor b m = M input = torch randn ref_out = m input gm = symbolic_trace m gm graph lint out = gm input assertEqual out ref_out node gm graph nodes node op == call_function assert isinstance node target torch _ops OpOverload assert node target __name__ == add Tensor test_pickle_torch_custom_ops M torch nn Module forward b = torch ops aten sigmoid c = torch ops aten cat b torch ops aten cat c c m = M input = torch randn ref_out = m input gm = symbolic_trace m gm graph lint pickled = pickle dumps gm loaded = pickle loads pickled assertEqual loaded input gm input test_pretty_print st = SimpleTest traced = symbolic_trace st traced graph lint printed = str traced assert SimpleTest printed assert torch relu printed test_pretty_print_graph KwargPrintTest torch nn Module forward x torch squeeze x + dim= st = KwargPrintTest traced = symbolic_trace st traced graph lint stringed = str traced graph s args kwargs num_users assert s stringed test_custom_proxy_type TensorPair __init__ left right left right = left right add other l = left + other left r = right + other right TensorPair l r mul other l = left other left r = right other right TensorPair l r use_tensor_pair x TensorPair y TensorPair s = x add y s mul x x = TensorPair torch randn torch randn y = TensorPair torch randn torch randn ref_out = use_tensor_pair x y traced = symbolic_trace use_tensor_pair traced_out = traced x y assertEqual traced_out left ref_out left assertEqual traced_out right ref_out right test_custom_proxy_type_literal TensorPair metaclass=torch fx ProxyableClassMeta __init__ left right left right = left right add other l = left + other left r = right + other right TensorPair l r mul other l = left other left r = right other right TensorPair l r use_tensor_pair_literal x TensorPair s = x add TensorPair torch zeros torch zeros s mul x x = TensorPair torch randn torch randn ref_out = use_tensor_pair_literal x traced = symbolic_trace use_tensor_pair_literal traced_out = traced x assertEqual traced_out left ref_out left assertEqual traced_out right ref_out right test_custom_proxy_dynamic_value TensorPair metaclass=torch fx ProxyableClassMeta __init__ left right left right = left right add other l = left + other left r = right + other right TensorPair l r mul other l = left other left r = right other right TensorPair l r use_tensor_pair_ctor x TensorPair y torch Tensor s = x add TensorPair y y s mul x x = TensorPair torch randn torch randn y = torch randn ref_out = use_tensor_pair_ctor x y traced = symbolic_trace use_tensor_pair_ctor traced_out = traced x y assertEqual traced_out left ref_out left assertEqual traced_out right ref_out right test_custom_proxy_input_dependent_control_flow ZeroTensor metaclass=torch fx ProxyableClassMeta __init__ inp inp sum == is_zero = True tensor = torch tensor is_zero = False tensor = inp add other is_zero ZeroTensor other tensor other is_zero use_zero_tensor x torch Tensor y torch Tensor ZeroTensor x + y x y = torch randn torch randn ref_out = use_zero_tensor x y traced = symbolic_trace use_zero_tensor traced_out = traced x y assertEqual traced_out is_zero ref_out is_zero assertEqual traced_out tensor ref_out tensor test_graph_fns g = Graph = g placeholder b = g call_module linear c = g get_attr bias d = g call_method add b c e = g call_function torch sin d g output e mod = torch nn Module mod linear = torch nn Linear mod bias = torch rand gm = GraphModule mod g gm graph lint input = torch rand r = gm input ref = torch sin mod linear input + mod bias assertEqual r ref test_remove_uses g torch fx Graph = Graph x torch fx Node = g placeholder x relu torch fx Node = g call_function torch relu x neg torch fx Node = g call_function torch neg relu g output neg neg replace_all_uses_with relu g erase_node neg assertTrue neg relu users skipIfTorchDynamo Dynamo does free right away test_prepend_does_not_leak g = Graph x = g placeholder x relu = g call_function torch relu x neg = g call_function torch neg x relu prepend neg ref = weakref ref neg g erase_node neg del g del x del relu del neg gc collect assertIsNone ref test_remove_uses_with_custom_filter g torch fx Graph = Graph x torch fx Node = g placeholder x relu torch fx Node = g call_function torch relu x neg torch fx Node = g call_function torch neg relu g output neg neg replace_all_uses_with relu lambda x x = neg assertTrue neg relu users test_nonetype_annotation eb = torch nn EmbeddingBag symbolic_trace eb test_pickle_nonetype_annotation eb = torch nn EmbeddingBag mode= sum traced = symbolic_trace eb pickled = pickle dumps traced loaded = pickle loads pickled loaded graph lint input = torch LongTensor offsets = torch LongTensor assertEqual loaded input offsets traced input offsets test_return_tuple M torch nn Module forward x torch Tensor - tuple torch Tensor torch Tensor x x + x original = M traced = symbolic_trace original assertEqual traced torch ones original forward torch ones test_construct_root_dict graph torch fx Graph = torch fx Graph torch fx Node = graph create_node placeholder x b torch fx Node = graph create_node call_module foo bar baz args= c torch fx Node = graph create_node get_attr zip zap zam d torch fx Node = graph create_node call_function operator add args= b c graph output d linear_mod torch nn Module = torch nn Linear add_param torch Tensor = torch rand gm torch fx GraphModule = torch fx GraphModule foo bar baz linear_mod zip zap zam add_param graph gm graph lint assert foo bar baz gm code x torch Tensor = torch rand out torch Tensor = gm x ref_out torch Tensor = linear_mod x + add_param assertEqual out ref_out test_symbolic_trace_assert AssertsTensorShape torch nn Module forward x torch _assert x shape assert_foobar x m = AssertsTensorShape verify traceability traced = symbolic_trace m verify assertion traced model works correctly runtime traced torch rand assertRaisesRegex AssertionError assert_foobar traced torch rand verify symbolically traced module scriptable ms = torch jit script m assertRaisesRegex torch jit Error assert_foobar ms torch rand test_fx_create_arg CustomArgObject __init__ x y x = x y = y __fx_create_arg__ tracer torch fx Tracer tracer create_node call_function CustomArgObject args= tracer create_arg x tracer create_arg y kwargs= HasCustomArgObjectWhenLeaf torch nn Module forward o CustomArgObject Not normally traceable good reason make module leaf x o x o y += x o y Root torch nn Module __init__ - None super __init__ inner = HasCustomArgObjectWhenLeaf forward x y o = CustomArgObject x y inner o CreateArgTracer torch fx Tracer is_leaf_module m module_qualified_name type m HasCustomArgObjectWhenLeaf m = Root graph = CreateArgTracer trace m gm = torch fx GraphModule m graph assert CustomArgObject gm code test_trace_fn_constant some_constant = torch rand add_const x some_constant + x traced = symbolic_trace add_const input = torch rand assertEqual traced input add_const input test_copy_no_remap traced = symbolic_trace SimpleTest g = traced graph copied = torch fx Graph node g nodes copied node_copy node assertRaisesRegex RuntimeError does belong Graph copied lint test_wrong_topo graph torch fx Graph = torch fx Graph torch fx Node = graph create_node placeholder x b torch fx Node = graph create_node call_module foo bar baz args= c torch fx Node = graph create_node get_attr zip zap zam d torch fx Node = graph create_node call_function operator add args= b c graph output d nodes = list graph nodes nodes append nodes assertRaisesRegex RuntimeError used before has been defined graph lint test_wrong_target_type graph torch fx Graph = torch fx Graph assertRaises ValueError n = torch fx Node graph=graph name= foo op= call_function target= foo args= kwargs= test_example_shape_prop TestCase torch nn Module __init__ - None super __init__ attr = torch randn submod = torch nn Linear forward x torch neg submod x relu + attr tc = TestCase tc_traced = symbolic_trace tc ref_out = tc_traced torch rand shape_prop ShapeProp tc_traced propagate torch rand Make sure we re testing all opcodes opcodes = set output_shape Optional torch Shape = None output_stride Optional tuple int = None node tc_traced graph nodes opcodes add node op node op == output output_shape = node args meta tensor_meta shape output_stride = node args meta tensor_meta stride assertEqual opcodes placeholder get_attr call_function call_method call_module output Test shape propagation make sure results match actual assertEqual output_shape ref_out shape assertEqual output_stride ref_out stride test_shape_prop_layout ConvTest torch nn Module __init__ - None super __init__ conv_mod = torch nn Conv d forward x conv_mod x contiguous layout test_mod = ConvTest traced = symbolic_trace test_mod x = torch randn shape_prop ShapeProp traced propagate x assert all node meta tensor_meta memory_format torch contiguous_format node traced graph nodes x_channels_last = x contiguous memory_format=torch channels_last traced memory_format=torch channels_last shape_prop ShapeProp traced propagate x_channels_last node traced graph nodes NB implementation conv may preserve memory format unfortunately The best we can do just check placeholder node channels-last node op placeholder assertEqual node meta tensor_meta memory_format torch channels_last test_shape_prop_aggregate ReturnTwo torch nn Module forward x torch sum x UnderTest torch nn Module __init__ - None super __init__ rt = ReturnTwo forward x rt x ut = UnderTest RTTracer torch fx Tracer is_leaf_module m module_qualified_name type m ReturnTwo graph = RTTracer trace ut mod = torch fx GraphModule ut graph shape_prop ShapeProp mod propagate torch rand node mod graph nodes node op == call_module assert tensor_meta node meta tensor_meta = node meta tensor_meta assert tensor_meta == assert tensor_meta shape == torch Size test_shape_prop_layout_ d ConvTest d torch nn Module __init__ - None super __init__ conv_mod = torch nn Conv d forward x conv_mod x test_mod_ d = ConvTest d traced_ d = symbolic_trace test_mod_ d x_ d = torch randn shape_prop ShapeProp traced_ d propagate x_ d assert all node meta tensor_meta memory_format torch contiguous_format node traced_ d graph nodes x_channels_last_ d = x_ d contiguous memory_format=torch channels_last_ d traced_ d memory_format=torch channels_last_ d shape_prop ShapeProp traced_ d propagate x_channels_last_ d node traced_ d graph nodes NB implementation conv may preserve memory format unfortunately The best we can do just check placeholder node channels-last node op placeholder assertEqual node meta tensor_meta memory_format torch channels_last_ d test_shape_prop_unbacked_sym torch _dynamo utils detect_fake_mode M torch nn Module forward x torch Tensor torch nonzero x inp = torch tensor gm = torch export export M inp strict=True module fake_inputs = node meta get val node gm graph nodes node op == placeholder inp = fake_inputs fake_mode = detect_fake_mode inp shape_prop ShapeProp gm=gm fake_mode=fake_mode propagate inp assertEqual len fake_mode shape_env pending_fresh_unbacked_symbols test_nn_module_stack SubModule torch nn Module __init__ - None super __init__ conv_mod = torch nn Conv d padding= bias=False forward x conv_mod x MyModule torch nn Module __init__ - None super __init__ sub_mod = SubModule forward x sub_mod x m = MyModule gm = torch fx symbolic_trace m mod_stack = expected_stack = sub_mod sub_mod type m sub_mod sub_mod conv_mod sub_mod conv_mod type m sub_mod conv_mod node gm graph nodes mod_stack = node meta get nn_module_stack mod_stack break stack_list = list mod_stack items assertEqual stack_list expected_stack test_transformer_preserves_nn_module_stack_for_get_attr M torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch ones forward x weight + x tracer = torch fx Tracer graph = tracer trace M gm = GraphModule tracer root graph node gm graph nodes node op == get_attr node meta nn_module_stack = node meta stack_trace = stack_trace node meta source_fn_stack = source_fn_stack new_gm = Transformer gm transform node new_gm graph nodes node op == get_attr assertEqual node meta nn_module_stack assertEqual node meta stack_trace stack_trace assertEqual node meta source_fn_stack source_fn_stack test_interpreter MyModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand linear = torch nn Linear forward x linear x + param clamp min= max= m = MyModule gm = torch fx symbolic_trace m interpreter = Interpreter gm input = torch randn assertEqual interpreter run input gm input assertEqual interpreter run input m input test_interpreter_other_graph MyModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand linear = torch nn Linear forward x linear x + param clamp min= max= m = MyModule gm = torch fx symbolic_trace m interpreter = Interpreter gm graph=gm graph input = torch randn assertEqual interpreter run input gm input assertEqual interpreter run input m input test_interpreter_run_node_override MyModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand linear = torch nn Linear forward x linear x + param clamp min= max= m = MyModule gm = torch fx symbolic_trace m RunNodeInterpreter Interpreter __init__ module super __init__ module run_node n Node - Any result = super run_node n n cached_value = result result input = torch randn RunNodeInterpreter gm run input node gm graph nodes assert hasattr node cached_value test_interpreter_onthefly_swap fn x torch sigmoid x neg gm = torch fx symbolic_trace fn NegSigmSwapInterpreter Interpreter call_function target Target args tuple kwargs dict - Any target == torch sigmoid torch neg args kwargs super call_function n noqa F call_method target Target args tuple kwargs dict - Any target == neg call_self args_tail = args call_self sigmoid args_tail kwargs super call_method n noqa F input = torch randn result = NegSigmSwapInterpreter gm run input assertEqual result torch neg input sigmoid test_interpreter_partial_eval MyModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand linear = torch nn Linear forward x linear x + param clamp min= max= gm = torch fx symbolic_trace MyModule interp = Interpreter gm env = node gm graph nodes node op == call_module node target == linear env node = torch arange reshape - break assert len env == x = torch randn result = interp run x initial_env=env assertEqual result torch arange reshape - clamp test_interpreter_star_args with_star_args x args x + args gm = torch fx symbolic_trace with_star_args interp = Interpreter gm result = interp run torch ones torch ones torch rand assertEqual result torch ones skipIfNoTorchVision test_interpreter_noop_resnet rn = torchvision_models resnet transformed = torch fx Transformer symbolic_trace rn transform inp = torch randn assertEqual transformed inp rn inp skipIfNoTorchVision test_interpreter_gc_values rn = torchvision_models resnet interp = Interpreter symbolic_trace rn inp = torch rand out = interp run inp env_key_names = n name n interp env keys assertEqual env_key_names output test_interpreter_default_args Model torch nn Module forward x y= x + y model = Model gm = torch fx symbolic_trace model interp = Interpreter gm x = torch randn out = interp run x torch testing assert_close out x + test_interpreter_not_enough_args Model torch nn Module forward x y x + y model = Model gm = torch fx symbolic_trace model interp = Interpreter gm x = torch randn assertRaisesRegex RuntimeError Expected positional argument parameter y one passed out = interp run x test_transformer_noop MyModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand linear = torch nn Linear forward x linear x + param clamp min= max= m = MyModule gm = torch fx symbolic_trace m new_gm = Transformer gm transform input = torch randn assertEqual new_gm input gm input test_transformer_op_swap fn x torch sigmoid x neg gm = torch fx symbolic_trace fn NegSigmSwapXformer Transformer call_function target Target args tuple kwargs dict - Any target == torch sigmoid torch neg args kwargs super call_function n noqa F call_method target Target args tuple kwargs dict - Any target == neg call_self args_tail = args call_self sigmoid args_tail kwargs super call_method n noqa F transformed = NegSigmSwapXformer gm transform input = torch randn assertEqual transformed input torch neg input sigmoid test_transformer_multi_outputs MyModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand linear = torch nn Linear forward x x = x + param out = linear x x out m = MyModule gm = torch fx symbolic_trace m new_gm = Transformer gm transform input = torch randn assertEqual new_gm input gm input test_fn_type_annotations Foo torch nn Module forward p Pair z torch Tensor i int - dict str torch Tensor p x + p y + z + i foo_scripted = torch jit script Foo foo_scripted Pair torch rand torch rand torch rand fixed = symbolic_trace Foo fxed_scripted = torch jit script fixed fxed_scripted Pair torch rand torch rand torch rand test_fn_type_annotation_empty forward list torch Tensor torch jit script symbolic_trace forward test_wrapped_method wrap_with_relu fn functools wraps fn wrapper args kwargs torch relu fn args kwargs wrapper Foo torch nn Module wrap_with_relu forward x w torch matmul x w f = Foo traced = symbolic_trace f x w = torch rand torch rand assertTrue any n target == torch relu n traced graph nodes test_empty_graph_codegen graph = torch fx Graph gm = torch fx GraphModule torch nn Module graph assertEqual gm None test_sequential m = torch nn Sequential torch nn Conv d gm = torch fx symbolic_trace m gm_copy = copy deepcopy gm test_ctx_mgr contextlib contextmanager do_nothing yield M torch nn Module do_nothing forward x torch relu x m = M checkGraphModule m torch rand test_typename_print graph torch fx Graph = torch fx Graph x torch fx Node = graph create_node placeholder x b torch fx Node = graph create_node call_function target=torch relu args= x type_expr=list float output torch fx Node = graph output b assertTrue list float str graph test_typename_print_pre_pep graph torch fx Graph = torch fx Graph x torch fx Node = graph create_node placeholder x b torch fx Node = graph create_node call_function target=torch relu args= x type_expr=typing List float noqa UP output torch fx Node = graph output b assertTrue typing List float str graph test_layout M torch nn Module forward x torch empty_like x layout=torch strided pin_memory=False fill_ traced = symbolic_trace M x = torch rand assertEqual traced x torch zeros_like x test_ellipsis M torch nn Module forward x y x + y traced = symbolic_trace M x y = torch rand torch rand assertEqual traced x y x + y test_inf_nan FooMod torch nn Module forward x x + float inf x + float -inf x + float nan fm = FooMod checkGraphModule fm torch rand test_inf_nan_kwds graph torch fx Graph = torch fx Graph x torch fx Node = graph create_node placeholder x b torch fx Node = graph create_node call_function operator add x float inf name= inf c torch fx Node = graph create_node call_function operator add x float nan name= nan graph output b c gm = torch fx GraphModule torch nn Module graph x = torch rand assertEqual gm x x + float inf x + float nan test_deepcopy_recursion_depth depth = sys getrecursionlimit + g = torch fx Graph x = g placeholder x _ range depth x = g call_function torch relu x g output x copied_graph = copy deepcopy g val_map = dict zip g nodes copied_graph nodes orig_node new_node zip g nodes copied_graph nodes orig_users = set orig_node users keys orig_users_equiv = val_map u u orig_users new_users = set new_node users keys assertEqual orig_users_equiv new_users skipIfNoTorchVision test_replace_uses rn = torchvision_models resnet LowerReluTracer torch fx Tracer is_leaf_module m torch nn Module qualname str isinstance m torch nn ReLU False super is_leaf_module m qualname rn _traced = GraphModule rn LowerReluTracer trace rn to_erase = node rn _traced graph nodes node op == call_function node target torch relu torch nn functional relu kwargs = node kwargs copy Neg doesn t have in-place kwargs pop inplace rn _traced graph inserting_before node new_node = rn _traced graph call_function the_function=torch neg args=node args kwargs=node kwargs node replace_all_uses_with replace_with=new_node to_erase append node node to_erase rn _traced graph erase_node node test_replace_input graph torch fx Graph = torch fx Graph x torch fx Node = graph create_node placeholder x y torch fx Node = graph create_node placeholder y b torch fx Node = graph create_node call_function target=torch relu args= x output torch fx Node = graph output b b replace_input_with x y gm = torch fx GraphModule torch nn Module graph input_x = torch randn input_y = torch randn assertEqual gm input_x input_y torch relu input_y test_insertion_point graph torch fx Graph = torch fx Graph x torch fx Node = graph create_node placeholder x b torch fx Node = graph create_node call_function target=torch relu args= x output torch fx Node = graph output b graph inserting_before b neg torch fx Node = graph call_function the_function=torch neg args= x _ relu_args = b args b args = neg relu_args gm = torch fx GraphModule torch nn Module graph input = torch randn assertEqual gm input torch relu torch neg input test_update_args_api graph torch fx Graph = torch fx Graph x torch fx Node = graph create_node placeholder x y torch fx Node = graph create_node placeholder y b torch fx Node = graph create_node call_function target=torch relu args= x output torch fx Node = graph output b orig_gm = torch fx GraphModule torch nn Module graph inp_x inp_y = torch randn torch randn assertEqual orig_gm inp_x inp_y torch relu inp_x b update_arg y new_gm = torch fx GraphModule torch nn Module graph assertEqual new_gm inp_x inp_y torch relu inp_y test_update_kwargs_api graph torch fx Graph = torch fx Graph x torch fx Node = graph create_node placeholder x y torch fx Node = graph create_node placeholder y b torch fx Node = graph create_node call_function target=torch relu kwargs= input x output torch fx Node = graph output b orig_gm = torch fx GraphModule torch nn Module graph inp_x inp_y = torch randn torch randn assertEqual orig_gm inp_x inp_y torch relu inp_x b update_kwarg input y new_gm = torch fx GraphModule torch nn Module graph assertEqual new_gm inp_x inp_y torch relu inp_y test_immutable_list_pytree_ops rand_tensor = torch randn l = immutable_list rand_tensor flattened spec = pytree tree_flatten l assert flattened == rand_tensor unflattened = pytree tree_unflatten flattened spec assert unflattened == l assert isinstance unflattened immutable_list test_immutable_dict_pytree_ops rand_tensor = torch randn d = immutable_dict b rand_tensor flattened spec = pytree tree_flatten d assert flattened == rand_tensor unflattened = pytree tree_unflatten flattened spec assert unflattened == d assert isinstance unflattened immutable_dict test_move_before graph torch fx Graph = torch fx Graph x torch fx Node = graph create_node placeholder x b torch fx Node = graph create_node call_function target=torch relu args= x output torch fx Node = graph output b neg torch fx Node = graph call_function the_function=torch neg args= x _ relu_args = b args b args = neg relu_args b prepend neg gm = torch fx GraphModule torch nn Module graph input = torch randn assertEqual gm input torch relu torch neg input test_prepend_self graph torch fx Graph = torch fx Graph x torch fx Node = graph create_node placeholder x b torch fx Node = graph create_node call_function target=torch relu args= x output torch fx Node = graph output b b prepend b x append b assertEqual len graph nodes test_erase_node_error st = SimpleTest traced = symbolic_trace st node traced graph nodes Test deleting uses both another Node output node target operator add torch relu assertRaisesRegex RuntimeError still had users graph traced graph erase_node node test_copy_it d = immutable_dict l = immutable_list assertEqual d deepcopy d assertEqual l deepcopy l test_get_torch_func_signature key dir torch obj = getattr torch key callable obj schemas = get_signature_for_torch_op obj test_find_uses graph = torch fx Graph x = torch fx Proxy graph placeholder x y = torch relu x z = x + x u = torch neg x graph output y + z + u node graph lint users_of_x = x node users assertEqual len users_of_x expected_ops = relu add neg use users_of_x assert any use name startswith prefix prefix expected_ops test_inline_graph InlineInto torch nn Module forward x torch relu x ToInline torch nn Module forward x torch neg x inline_into = symbolic_trace InlineInto to_inline = symbolic_trace ToInline combined_graph = torch fx Graph output_node = combined_graph graph_copy inline_into graph input_node = next iter to_inline graph nodes assert input_node input_node op == placeholder val_map = input_node output_node output = combined_graph graph_copy to_inline graph val_map combined_graph output output combined_module = torch fx GraphModule torch nn Module combined_graph input = torch rand assertEqual combined_module input input relu neg test_multi_insert_point graph = torch fx Graph x = torch fx Proxy graph placeholder x relu = torch relu x graph inserting_before relu node y = torch neg x z = torch tanh y graph output relu node z node graph lint expected_ops = x neg tanh relu node expected zip graph nodes expected_ops assert expected node name test_reassign_args_kwargs_uses graph = torch fx Graph x y = Proxy graph placeholder x Proxy graph placeholder y z = x + y zed = z + z + z graph output zed node graph lint zed = z + z + z - zed = z + z + x zed node args = zed node args x node assertEqual list x node users keys z node zed node z = x + y - z = y + y z node args = y node y node assertEqual list x node users keys zed node test_trace_function foo x y torch relu x + y x y = torch randn torch randn checkGraphModule foo x y test_trace_return_dataclass Test case Module dataclass dataclasses dataclass dataclass MyOutput foo torch Tensor bar torch Tensor ModuleReturnDataclass torch nn Module forward d torch Tensor MyOutput foo=d + d bar=d module = ModuleReturnDataclass traced_graph = symbolic_trace module graph print traced_graph gm = GraphModule module traced_graph x = torch rand assertEqual module x gm x test_trace_return_dataclass_nested Test case Module dataclass dataclasses dataclass dataclass MyOutput foo torch Tensor bar torch Tensor ModuleReturnDataclass torch nn Module forward d torch Tensor MyOutput foo=d + d bar=d CallsModule torch nn Module __init__ - None super __init__ m = ModuleReturnDataclass forward x tmp = m x MyOutput foo=tmp foo bar=tmp bar module = CallsModule traced_graph = symbolic_trace module graph print traced_graph gm = GraphModule module traced_graph x = torch rand assertEqual module x gm x test_trace_return_namedtuple Test case Module namedtuple MyOutput NamedTuple foo torch Tensor bar torch Tensor ModuleReturnNamedTuple torch nn Module forward d torch Tensor MyOutput foo=d bar=d module = ModuleReturnNamedTuple traced_graph = symbolic_trace module graph print traced_graph gm = GraphModule module traced_graph x = torch rand assertEqual module x gm x test_trace_dict_int_keys ModWithDictArg torch nn Module forward d dict int torch Tensor d CallsModWithDict torch nn Module __init__ - None super __init__ m = ModWithDictArg forward x m x MyTracer torch fx Tracer is_leaf_module m torch nn Module module_qualified_name str - bool isinstance m ModWithDictArg traced_graph = MyTracer trace CallsModWithDict test_trace_dict_proxy_keys ModWithDictArg torch nn Module forward d dict torch Tensor torch Tensor d CallsModWithDict torch nn Module __init__ - None super __init__ m = ModWithDictArg forward x m x x MyTracer torch fx Tracer is_leaf_module m torch nn Module module_qualified_name str - bool isinstance m ModWithDictArg assertRaisesRegex RuntimeError cannot contain Node traced_graph = MyTracer trace CallsModWithDict test_module_deepcopy_edit_nodes Foo torch nn Module forward x torch relu x traced = symbolic_trace Foo copied = copy deepcopy traced node copied graph nodes node target == torch relu node target = torch neg copied recompile traced recompile x = torch randn torch testing assert_close traced x torch relu x torch testing assert_close copied x torch neg x test_direct_param_use TransposeTest torch nn Module __init__ - None super __init__ b = torch nn Parameter torch rand forward x b Foo torch nn Module __init__ - None super __init__ = TransposeTest forward x b b t b view traced = torch fx symbolic_trace Foo assert all constant node target node traced graph nodes test_single_default_arg M torch nn Module forward y= y m = M checkGraphModule m checkGraphModule m test_multiple_default_args M torch nn Module forward y= z= y + z m = M checkGraphModule m checkGraphModule m checkGraphModule m test_regular_and_default_args M torch nn Module forward x y= x + y m = M checkGraphModule m checkGraphModule m test_string_literal_return M torch nn Module forward foo m = M checkGraphModule m test_namedtuple_return_qualname NamedTupReturn torch nn Module forward x MyNamedTup x x traced = symbolic_trace NamedTupReturn input = torch rand assertEqual traced input MyNamedTup input input test_update_args_kwargs_yells_at_you symtraced = symbolic_trace SimpleTest node = next iter symtraced graph nodes assertRaisesRegex AttributeError __update_args_kwargs node __update_args_kwargs test_torchbind_class_attribute_in_fx IS_FBCODE IS_WINDOWS IS_MACOS skipTest torch classes _TorchScriptTesting _StackString registered skipping FooBar torch nn Module __init__ - None super __init__ f = torch classes _TorchScriptTesting _StackString forward f top m = FooBar checkGraphModule m test_torchbind_class_attribute_in_fx_tensor_arg IS_FBCODE IS_WINDOWS IS_MACOS skipTest torch classes _TorchScriptTesting _ReLUClass registered skipping FooBar torch nn Module __init__ - None super __init__ f = torch classes _TorchScriptTesting _ReLUClass forward x f run x m = FooBar traced = symbolic_trace m input = torch randn assertEqual traced input m input assertTrue any n op == call_method n traced graph nodes test_script_method_trace Scripted torch nn Module forward x torch relu x Holder torch nn Module __init__ - None super __init__ s = torch jit script Scripted forward x s x h = Holder traced = symbolic_trace h input = torch randn assertEqual traced input h input assertTrue any n op == call_method n traced graph nodes test_namedtuple_return_trace NamedTupReturn torch nn Module forward x Pair x x traced = symbolic_trace NamedTupReturn input = torch rand assertEqual traced input Pair input input test_named_tuple_inlined NamedTupMod torch nn Module forward inp wrapped_named_tup Pair inp p =Pair inp m = NamedTupMod input = torch rand ref = m input traced = symbolic_trace m res = traced input assertEqual ref res Check Pair NamedTuple works when inlined into function call ph = call_func = None node traced graph nodes node op == placeholder ph = node node op == call_function node target == wrapped_named_tup node update_arg Pair ph node update_kwarg p Pair ph call_func = node break assertTrue call_func None assertTrue isinstance call_func args Pair assertTrue isinstance call_func kwargs p Pair assertEqual _format_arg call_func args Pair x= inp y= assertEqual _format_arg call_func kwargs p Pair x= y= inp traced graph eliminate_dead_code traced recompile res = traced input assertEqual ref res test_return_type_exists ReturnTypeModule torch nn Module other x list str - list str x forward x list str - list str other x traced = symbolic_trace ReturnTypeModule assertIn - list str traced _code scripted = torch jit script traced assertIn - List str scripted code test_return_type_exists_pre_pep ReturnTypeModule torch nn Module other x typing List str - typing List str noqa UP x forward x typing List str - typing List str noqa UP other x traced = symbolic_trace ReturnTypeModule assertIn - typing_List str traced _code scripted = torch jit script traced assertIn - List str scripted code getitem_inner GetItemBase torch nn Module __init__ - None super __init__ pe = torch nn Buffer torch randn GetItem GetItemBase forward x pe x size GetItem GetItemBase forward x pe x size GetItem GetItemBase forward x pe fx creates ` _tensor_constant ` here checkGraphModule GetItem torch zeros checkGraphModule GetItem torch zeros checkGraphModule GetItem torch zeros unittest skipUnless os environ get FX_PATCH_GETITEM == Will checked test_getitem_subproc test_getitem getitem_inner test_getitem_subproc need run test subproc work around https github com pytorch pytorch issues proc = Process target=run_getitem_target proc start proc join assertEqual proc exitcode test_user_friendly_call_provenance_with_function fn x wrapper_fn x traced = torch fx symbolic_trace fn assertRaisesRegex RuntimeError wrapper_fn being compiled since called fn forward scripted = torch jit script traced test_user_friendly_call_provenance_with_module M torch nn Module forward x wrapper_fn x traced = torch fx symbolic_trace M assertRaisesRegex RuntimeError wrapper_fn being compiled since called M forward scripted = torch jit script traced test_snake_case M torch nn Module __init__ - None super __init__ activations = torch nn ModuleDict snake_case torch nn ReLU PascalCase torch nn LeakyReLU ALL_CAPS torch nn PReLU forward x = activations snake_case x b = activations PascalCase x c = activations ALL_CAPS x b c traced = symbolic_trace M check = activations_snake_case activations snake_case activations_pascal_case activations PascalCase activations_all_caps activations ALL_CAPS i = node traced graph nodes node op == placeholder node op == output continue name = check i target = check i assertEqual name node name assertEqual target node target i += assertEqual i test_no_mutation torch fx immutable_collections immutable_list x = immutable_list assertRaisesRegex TypeError new_args x = test_partial_trace Foo torch nn Module forward x y y x x mod = Foo mod_true = symbolic_trace mod concrete_args= y True mod_false = symbolic_trace mod concrete_args= y False assertEqual mod_true True print mod_true code assert any i target == torch _assert i mod_true graph nodes assertRaises AssertionError mod_true False assertEqual mod_false False assertRaises AssertionError mod_false True f_higher f f nf = symbolic_trace f_higher concrete_args= f lambda x x assertEqual nf lambda x x test_custom_traceback_raised_when_exception_source_is_graphmodule M torch nn Module __init__ - None super __init__ W = torch nn Parameter torch randn forward x torch dot W x traced = torch fx symbolic_trace M out = n n traced graph nodes n op == output - traced graph inserting_before out relu_out = traced graph call_method method_name= relu args= out args out args = relu_out traced recompile capture_stderr captured assertRaises TypeError traced assertRegex captured r Call using FX-traced Module line r traced Module s generated forward function test_custom_traceback_not_raised_when_exception_source_is_submodule M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x traced = torch fx symbolic_trace M Do change ` capture_stderr ` another context manager without ensuring output expected try traced torch rand except RuntimeError captured = traceback format_exc assertNotRegex captured r Call using FX-traced Module line r traced Module s generated forward function test_graph_module_replicate_for_dp Foo torch nn Module forward x torch relu x gm = torch fx symbolic_trace Foo x = torch randn out = gm x replica = gm _replicate_for_data_parallel out_replica = replica x torch testing assert_close out_replica out test_ast_rewriter_rewrites_assert M torch nn Module forward x torch Tensor y int z int assert y == z torch add x x ast_rewriter = RewritingTracer graph = ast_rewriter trace M traced = GraphModule ast_rewriter root graph gm traced graph lint test_ast_rewriter_rewrites_assert_with_message M torch nn Module forward x torch Tensor y int z int assert y == z msg torch add x x ast_rewriter = RewritingTracer graph = ast_rewriter trace M traced = GraphModule ast_rewriter root graph gm traced graph lint test_throw_out_variant foo x y = torch rand_like x torch sigmoid x out=y y MyTracer torch fx Tracer check_mutable_operations = True tracer = MyTracer assertRaisesRegex RuntimeError mutable operation aten sigmoid out traced_graph = tracer trace foo test_ast_rewriter_reassigns_submodules M torch nn Module __init__ - None super __init__ bn = torch nn BatchNorm d forward x torch Tensor torch add x x ast_rewriter = RewritingTracer graph = ast_rewriter trace M traced = GraphModule ast_rewriter root graph gm traced graph lint test_ast_rewriter_wrap assertEqual + + a_lifted_leaf to_trace y a_lifted_leaf y + a_lifted_leaf + a_lifted_leaf y y y ast_rewriter = RewritingTracer graph = ast_rewriter trace to_trace traced = GraphModule ast_rewriter root graph gm assertIn a_lifted_leaf traced code assertEqual traced assertIs a_lifted_leaf real_a_lifed_leaf test_ast_rewriter_wrap_fn_directly assertEqual + + a_lifted_leaf to_trace y a_lifted_leaf y + a_lifted_leaf + a_lifted_leaf y y y ast_rewriter = RewritingTracer graph = ast_rewriter trace to_trace traced = GraphModule ast_rewriter root graph gm assertIn a_lifted_leaf traced code assertEqual traced assertIs a_lifted_leaf real_a_lifed_leaf test_profiler_ranges_side_effect g = torch fx Graph handle = g call_function torch ops profiler _record_function_enter_new test_range g call_function torch ops profiler _record_function_exit handle g output None found_targets = node g nodes node op == call_function found_targets setdefault node target assertEqual list found_targets keys torch ops profiler _record_function_enter_new torch ops profiler _record_function_exit g eliminate_dead_code found_targets = node g nodes node op == call_function found_targets setdefault node target assertEqual list found_targets keys torch ops profiler _record_function_enter_new torch ops profiler _record_function_exit test_ast_rewriter_wrapped_via_decorator F torch nn Module forward x wrapped_via_decorator x ast_rewriter = RewritingTracer graph = ast_rewriter trace F traced = GraphModule ast_rewriter root graph gm assertIn wrapped_via_decorator traced code assertEqual traced assertIs wrapped_via_decorator real_wrapped_via_decorator assertFalse hasattr wrapped_via_decorator __fx_already_patched test_ast_rewriter_wrapped_via_decorator_and_transformed assertEqual wrapped_via_decorator to_trace y wrapped_via_decorator y ast_rewriter = RewritingTracer graph = ast_rewriter trace to_trace traced = GraphModule ast_rewriter root graph gm assertIn wrapped_via_decorator traced code assertEqual traced assertIs wrapped_via_decorator real_wrapped_via_decorator assertFalse hasattr wrapped_via_decorator __fx_already_patched transformed = torch fx Transformer traced transform assertIn wrapped_via_decorator transformed code assertEqual transformed assertIs wrapped_via_decorator real_wrapped_via_decorator assertFalse hasattr wrapped_via_decorator __fx_already_patched test_ast_rewriter_wrap_with_submodule M torch nn Module __init__ - None super __init__ batchnorm d = torch nn BatchNorm d affine=False forward x torch Tensor wrapped_with_submodule x batchnorm d ast_rewriter = RewritingTracer graph = ast_rewriter trace M traced = GraphModule ast_rewriter root graph gm assertIn wrapped_with_submodule traced code input = torch rand ref_batchnorm d = torch nn BatchNorm d affine=False assertEqual ref_batchnorm d input traced input test_submodule_manipulation_API C torch nn Module __init__ - None super __init__ conv = torch nn Conv d stride= param = torch nn Parameter torch rand forward x conv torch cat param x B torch nn Module __init__ - None super __init__ linear = torch nn Linear buf = torch nn Buffer torch randn net_c = C forward x linear torch cat buf net_c x A torch nn Module __init__ - None super __init__ net_b = B param = torch nn Parameter torch rand forward x net_b x + param = symbolic_trace A add_submodule net_b net_c dropout torch nn Dropout p= conv = n n graph nodes n target == net_b net_c conv - graph inserting_before conv warnings catch_warnings record=True w dropout = graph call_module module_name= net_b net_c dropout args=conv args assertEqual len w conv replace_all_uses_with dropout graph erase_node conv recompile module_exists gm GraphModule path str - bool any path == name name _ gm named_modules parameter_exists gm GraphModule path str - bool any path == name name _ gm named_parameters any path == name name gm state_dict keys buffer_exists gm GraphModule path str - bool any path == name name _ gm named_buffers any path == name name gm state_dict keys Test we added dropout submodule assertTrue module_exists net_b net_c dropout Test ` get_submodule ` added submodule assertIsNotNone get_submodule net_b net_c dropout Test conv submodule still there assertTrue module_exists net_b net_c conv Test ` get_submodule ` original module assertIsNotNone get_submodule net_b net_c conv Test conv node NOT still there conv = n n graph nodes n target == net_b net_c conv assertEqual conv delete_submodule net_b net_c conv Test conv submodule now gone assertFalse module_exists net_b net_c conv Test ` get_submodule ` deleted submodule assertRaisesRegex AttributeError has no attribute ` conv ` assertIsNone get_submodule net_b net_c conv Test ` get_attr ` warnings cat = n n graph nodes n target == torch cat - graph inserting_before cat warnings catch_warnings record=True w param = graph get_attr qualified_name= net_b net_c param assertEqual len w assertWarnsRegex UserWarning Attempted insert get_attr Node no underlying reference owning GraphModule bad_param = graph get_attr qualified_name= net_b param graph erase_node bad_param cat args = cat args param recompile graph lint Test ` get_parameter ` get_parameter net_b net_c param assertRaisesRegex AttributeError nn Parameter get_parameter net_b buf assertRaisesRegex AttributeError has no attribute ` param ` get_parameter net_b param Test ` get_buffer ` get_buffer net_b buf assertRaisesRegex AttributeError buffer get_buffer net_b net_c param assertRaisesRegex AttributeError has no attribute ` buf ` get_buffer net_b net_c buf Test non-nested attributes get_submodule get_parameter param Insert some unused submodules add_submodule net_b embedding torch nn Embedding add_submodule net_b net_c embedding torch nn Embedding add_submodule net_b net_c rnn torch nn RNN add_submodule batch_norm_ d torch nn BatchNorm d Garbage collection delete_all_unused_submodules Test all unused submodules gone assertFalse module_exists net_b embedding assertFalse module_exists net_b net_c embedding assertFalse module_exists net_b net_c rnn assertFalse module_exists batch_norm_ d Test we didn t delete any unused Parameters buffers assertTrue parameter_exists net_b net_c param assertTrue buffer_exists net_b buf graph lint test_delete_unused_submodules_leaf SubModule torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU forward x x = linear x x = relu x x Model torch nn Module __init__ - None super __init__ submod = SubModule forward x x = submod x x model = Model MyCustomTracer torch fx Tracer is_leaf_module m torch nn Module module_qualified_name str - bool module_qualified_name == submod inputs = torch randn traced_graph = MyCustomTracer trace model gm = torch fx GraphModule model traced_graph gm delete_all_unused_submodules torch testing assert_close gm inputs model inputs test_fx_stateless MockModule torch nn Module __init__ - None super __init__ l = torch nn Linear buffer = torch nn Buffer torch ones forward x l x + buffer module = MockModule x = torch rand weight = torch tensor requires_grad=True bias = torch tensor requires_grad=True buffer = torch tensor parameters = l weight weight l bias bias buffer buffer fx_module = torch fx symbolic_trace module res = torch func functional_call fx_module parameters x res backward assertIsNotNone weight grad assertIsNotNone bias grad assertIsNone buffer grad Gradient calculated module stated buffers assertIsNone module l weight grad assertIsNone module l bias grad assertIsNone module buffer grad test_tracing_graphmodules_as_leaf_submodules A torch nn Module forward t t + t B torch nn Module __init__ - None super type __init__ calling = False called = False forward t calling t - t t + t __call__ args called = True calling = True super type __call__ args calling = False M torch nn Module __init__ b super __init__ = b = b forward t x = t y = b t x + y LeafTracer Tracer is_leaf_module module name True LeafTracerNotB Tracer is_leaf_module module name b name Recompile calls added fun since they chain __call__ wrappers Test B regular non-leaf module = symbolic_trace A recompile m = M B graph = LeafTracerNotB trace m gm = GraphModule m graph gm recompile Test graphmodule submodule inlined assertTrue isinstance gm get_submodule GraphModule match = n n gm graph nodes n op == call_module n target == assertTrue len match == Test submodule b treated leaf assertFalse hasattr gm b Test assert custom __call__ submodule b honored match = n n gm graph nodes n op == call_function n target == operator sub assertTrue len match == Test B regular leaf module symbolic_trace should only patch torch nn Module __call__ which means B __call__ should still execute = symbolic_trace A recompile b = B m = M b graph = LeafTracer trace m gm = GraphModule m graph gm recompile Test graphmodule submodule inlined assertTrue isinstance gm get_submodule GraphModule match = n n gm graph nodes n op == call_module n target == assertTrue len match == Test submodule b leaf assertTrue isinstance gm get_submodule b torch nn Module match = n n gm graph nodes n op == call_module n target == b assertTrue len match == Test b __call__ run assertTrue b called assertTrue gm get_submodule b called Test B GraphModule leaf __call__ honored since symbolic_trace directly invokes forward = symbolic_trace A recompile b = symbolic_trace B b recompile m = M b graph = LeafTracer trace m gm = GraphModule m graph gm recompile assertTrue isinstance gm get_submodule GraphModule match = n n gm graph nodes n op == call_module n target == assertTrue len match == assertTrue isinstance gm get_submodule b torch nn Module match = n n gm graph nodes n op == call_module n target == b assertTrue len match == _test_graph_module_init_buffer_param_copied use_dict_init bool MyModule torch nn Module __init__ - None super __init__ my_buff = torch nn Buffer torch rand register_parameter my_param torch nn Parameter torch rand forward x x + my_buff + my_param mod = MyModule mod_traced = symbolic_trace mod Create new GraphModule based original either w dict root module orig_buff = mod_traced get_buffer my_buff orig_param = mod_traced get_parameter my_param mod_traced_new = GraphModule my_buff orig_buff my_param orig_param use_dict_init mod mod_traced graph Check both my_buff my_param found same try new_buff = mod_traced_new get_buffer my_buff except Exception fail Did find my_buff assertEqual orig_buff new_buff try new_param = mod_traced_new get_parameter my_param except Exception fail Did find my_param assertEqual orig_param new_param x = torch rand orig_out = mod_traced x submodules_out = mod_traced_new x assertEqual orig_out submodules_out test_graph_module_init_buffer_param_copied_dict_init _test_graph_module_init_buffer_param_copied use_dict_init=True test_graph_module_init_buffer_param_copied_mod_init _test_graph_module_init_buffer_param_copied use_dict_init=False test_annotations_with_no_forward_references A __call__ x torch Tensor torch add x x M torch nn Module forward x torch Tensor A - torch Tensor x checkGraphModule M torch rand A kwargs=None test_annotations_with_forward_references A __call__ x torch Tensor torch add x x M torch nn Module forward x torch Tensor A - torch Tensor x checkGraphModule M torch rand A kwargs=None test_annotations_with_non_torch_reference_and_no_internal_forward_references A __call__ x torch Tensor torch add x x M torch nn Module forward x list torch Tensor A - torch Tensor x checkGraphModule M torch rand A kwargs=None test_annotations_with_non_torch_reference_and_internal_forward_references A __call__ x torch Tensor torch add x x M torch nn Module forward x list torch Tensor A - torch Tensor x checkGraphModule M torch rand A kwargs=None test_annotation_with_future try fx test_future noqa F finally del sys modules __future__ unittest skipIf sys version_info Does work test_annotations_empty_tuple Foo torch nn Module forward x typing Tuple y typing Tuple str typing Tuple noqa UP foo traced = torch fx symbolic_trace Foo x = y = bar traced x y FileCheck check typing_Tuple check typing_Tuple str typing_Tuple run traced code scripted = torch jit script traced scripted x y FileCheck check Tuple check Tuple str Tuple run scripted code test_pytree Used test you can use your own placeholder PHTest PHBase pass f_sum x sum x f_sum_dict x out = v x values out += v out f_dict_list_map x new_dict = k v x items new_dict k = i + i v new_dict f_dict_add x x + sum x z f_namedtuple_add x x x + x y pytree register_pytree_node Foo lambda x x x b None lambda x _ Foo x x fx_pytree register_pytree_flatten_spec Foo lambda x _ x x b f_custom x x + x b f_custom_dict x f_sum_dict x + x b f_return_custom x Foo x b x tests = f_sum PH PH PH f_sum f_sum PHTest PHTest PHTest f_sum_dict PH b PH c PH f_dict_list_map PH PH b PH c f_dict_list_map PH PH PH f_dict_add PH z PH PH PH f_dict_add PH z f_custom Foo PH PH f_custom Foo PH f_custom_dict Foo PH b PH PH f_return_custom Foo PH PH Don t currently support output pytrees f_namedtuple_add Point PH PH verify_pytree f inp val = pytree tree_map lambda x torch randn isinstance x PHBase x inp num_flat_args = len pytree tree_leaves inp orig_out = f val nf = symbolic_trace f concrete_args= x inp assertEqual nf val orig_out bare_fx = GraphModule copy deepcopy nf graph bare_fx graph set_codegen CodeGen bare_fx recompile assertEqual nf graph process_outputs bare_fx nf graph process_inputs val orig_out assert num_flat_args == tree_flatten_spec nf code assert sum i op == placeholder i nf graph nodes == num_flat_args nf = symbolic_trace nf assertEqual nf val orig_out assert tree_flatten_spec nf code assert sum i op == placeholder i nf graph nodes == nf = symbolic_trace nf concrete_args= x inp assertEqual nf val orig_out assert num_flat_args == tree_flatten_spec nf code assert sum i op == placeholder i nf graph nodes == num_flat_args pickled = pickle dumps nf nf = pickle loads pickled assertEqual nf val orig_out f inp tests verify_pytree f inp test_pytree_concrete f b b z inp = PH z PH b True nf = symbolic_trace f concrete_args=inp val = pytree tree_map lambda x torch randn x == PH x inp assertEqual nf val f val nf = symbolic_trace nf assertEqual nf val f val test_metadata_on_ph f_sum int b int - int + b Due unflattening dict batch argument will split into two separate nodes names batch_ batch_ referring keys f f respectively dict f_dict dict str str - bool f == f verify_metadata gm GraphModule arg_names list str metadata list str node gm graph nodes node op == placeholder assertTrue node name arg_names assertTrue node ph_key metadata verify_metadata gm=symbolic_trace f_sum concrete_args= PHWithMeta ph_key= b PHWithMeta ph_key= b arg_names= a_ b_ metadata= b verify_metadata gm=symbolic_trace f_dict concrete_args= f PHWithMeta ph_key= f f PHWithMeta ph_key= f arg_names= a_ a_ metadata= f f Ensures tags nodes NOT overwritten PH attributes same attr name tag TaggingTracer Tracer create_node kind str target Union str Callable args tuple Argument kwargs dict str Any name Optional str = None type_expr Optional Any = None - Node n = super create_node kind target args kwargs name n tag = foo n PHWithTag PHBase __init__ tag str super __init__ tag = tag g = TaggingTracer trace f_sum concrete_args= PHWithTag tag= bar b PHWithTag tag= bar n g nodes assertTrue hasattr n tag Ensure tag still foo bar PHWithTag assertEqual n tag foo test_custom_codegen ListCodeGen CodeGen gen_fn_def free_vars maybe_return_annotation lst_unpack = f forward args_list List torch Tensor maybe_return_annotation join free_vars = args_list lst_unpack additional_globals List list process_inputs inputs assert len inputs == inputs f b + b nf = symbolic_trace f vals = torch randn torch randn assertEqual nf vals f vals nf graph set_codegen ListCodeGen nf recompile bare_fx = GraphModule copy deepcopy nf graph bare_fx graph set_codegen CodeGen bare_fx recompile assertEqual nf vals f vals assertEqual nf graph process_outputs bare_fx nf graph process_inputs vals f vals ts_f = torch jit script nf assertEqual nf vals ts_f vals test_custom_codegen_with_transformer ListCodeGen CodeGen gen_fn_def free_vars maybe_return_annotation lst_unpack = f forward args_list List torch Tensor maybe_return_annotation join free_vars = args_list lst_unpack additional_globals List list process_inputs inputs assert len inputs == inputs f b + b nf = symbolic_trace f vals = torch randn torch randn assertEqual nf vals f vals nf graph set_codegen ListCodeGen nf recompile assertEqual nf vals f vals transformed_gm = Transformer nf transform assertEqual nf vals transformed_gm vals test_interpreter_with_codegen ListCodeGen CodeGen gen_fn_def free_vars maybe_return_annotation lst_unpack = f forward args_list List torch Tensor maybe_return_annotation join free_vars = args_list lst_unpack additional_globals List list process_inputs inputs assert len inputs == inputs generate_output output_args f list repr output_args process_outputs outputs list outputs f b = + b b = + b b nf = symbolic_trace f vals = torch randn torch randn nf graph set_codegen ListCodeGen nf recompile assertEqual Interpreter nf run vals nf vals test_imul_code_print graph = torch fx Graph = graph placeholder b = graph placeholder b graph call_function operator imul b graph output gm = torch fx GraphModule graph gm recompile assertEqual gm assertIn = b gm code test_deepcopy_tracer fn x y x + y relu sin tracer = Tracer tracer_before = copy deepcopy tracer tracer trace fn tracer_after = copy deepcopy tracer assertEqual str tracer graph str tracer_after graph assertTrue hasattr tracer_before graph str tracer graph = str tracer_before graph test_deepcopy_graphmodule m = symbolic_trace SimpleTest m meta hello = world copy_m = copy deepcopy m assertEqual copy_m meta hello world test_deepcopy_no_recursion m = symbolic_trace SimpleTest m meta hello = m circular reference copy_m = copy deepcopy m finishes assertEqual id copy_m id copy_m meta hello test_enum enum Enum Foo Enum A = B = leaf_fn arr enum_val Use raw enum arr append enum_val arr - value foo x Pass enum argument leaf_fn x Foo A traced = torch fx symbolic_trace foo assertEqual foo traced test_insert_arg m = symbolic_trace SimpleTest m buf = torch nn Buffer torch tensor output_node = next iter reversed m graph nodes m graph inserting_before output_node = m graph get_attr buf r = len output_node args output_node insert_arg assertEqual len output_node args r + assertEqual len users assertIs output_node args assertIs next iter users keys output_node output_node insert_arg assertEqual len output_node args r + assertEqual len users assertIs output_node args assertIs next iter users keys output_node m graph lint test_delete_unused_values torch fx experimental proxy_tensor make_fx disable mutable checking temporarily orig_tracer_mutable_flag = torch fx proxy TracerBase check_mutable_operations torch fx proxy TracerBase check_mutable_operations = False fn b c d x = + b y = c + d y copy_ x x = torch relu x x b c d = torch randn requires_grad=False _ range fx_fn = make_fx fn b c d print fx_fn fx_fn graph eliminate_dead_code py_code = fx_fn recompile assertTrue copy_ = torch ops aten copy_ default py_code src assertTrue copy_ = None py_code src recorver mutable checking flag torch fx proxy TracerBase check_mutable_operations = orig_tracer_mutable_flag run_getitem_target torch fx _symbolic_trace _wrapped_methods_to_patch _wrapped_methods_to_patch append torch Tensor __getitem__ try TestFX getitem_inner finally _wrapped_methods_to_patch pop TestOperatorSignatures JitTestCase setUp Checking mutable operations while tracing feature flagged Enable testing default orig_tracer_mutable_flag = torch fx proxy TracerBase check_mutable_operations torch fx proxy TracerBase check_mutable_operations = True tearDown torch fx proxy TracerBase check_mutable_operations = orig_tracer_mutable_flag onlyCPU ops op_db allowed_dtypes= torch float test_get_torch_func_signature_exhaustive device dtype op isinstance op op types BuiltinFunctionType raise unittest SkipTest This path doesn t work Python functions sample_inputs_itr = op sample_inputs device dtype requires_grad=False schemas = get_signature_for_torch_op op op schemas raise RuntimeError No Schemas Returned sample_input sample_inputs_itr Iterate through overloads until we hit match If we exit loop via ` ` we haven t found match schema schemas try bound_args = schema bind sample_input input sample_input args sample_input kwargs bound_args apply_defaults op bound_args args bound_args kwargs break except TypeError e pass raise RuntimeError f Did match any schemas op op name TestFXAPIBackwardCompatibility JitTestCase setUp super setUp maxDiff = None Checking mutable operations while tracing feature flagged Enable testing default orig_tracer_mutable_flag = torch fx proxy TracerBase check_mutable_operations torch fx proxy TracerBase check_mutable_operations = True tearDown super tearDown torch fx proxy TracerBase check_mutable_operations = orig_tracer_mutable_flag _fn_to_stable_annotation_str obj Unfortunately we have serialize function signatures manually since serialization ` inspect Signature ` objects stable across python versions fn_name = torch typename obj signature = inspect signature obj sig_str = f fn_name signature arg_strs = k v signature parameters items maybe_type_annotation = f _annotation_type_to_stable_str v annotation sig_str v annotation inspect Signature empty default_val_str val isinstance val tuple list str_pieces = isinstance val tuple str_pieces append join default_val_str v v val isinstance val tuple len str_pieces == str_pieces append str_pieces append isinstance val tuple join str_pieces Need fix up some default value strings First case modules Default module ` repr ` contains FS path module Don t leak isinstance val types ModuleType f module val __name__ Second case callables Callables such lambdas encode their address their string repr Don t do callable val f function val __name__ str val v default inspect Signature empty default_val_str = default_val_str v default isinstance v default str f v default maybe_default = f = default_val_str maybe_default = maybe_stars = v kind == inspect Parameter VAR_POSITIONAL maybe_stars = v kind == inspect Parameter VAR_KEYWORD maybe_stars = arg_strs append f maybe_stars k maybe_type_annotation maybe_default return_annot = f - _annotation_type_to_stable_str signature return_annotation sig_str signature return_annotation inspect Signature empty f fn_name join arg_strs return_annot _trivial_mappings = str str int int float float bool bool torch dtype torch dtype torch Tensor torch Tensor torch device torch device torch memory_format torch memory_format slice slice torch nn Module torch nn modules module Module torch fx Graph torch fx graph Graph torch fx Node torch fx node Node torch fx Proxy torch fx proxy Proxy torch fx node Target torch fx node Target torch fx node Argument torch fx node Argument torch fx graph PythonCode torch fx graph PythonCode torch fx graph_module GraphModule torch fx graph_module GraphModule torch fx subgraph_rewriter Match torch fx subgraph_rewriter Match Ellipsis typing Any Any type None NoneType None None typing Iterator Iterator collections abc Iterator Iterator _UNBOUND_TYPES = dict list tuple type typing Callable typing Dict noqa UP typing List noqa UP typing Tuple noqa UP typing Type noqa UP typing Union _annotation_type_to_stable_str t sig_str recursive bool = False t inspect Signature empty Forward ref isinstance t str recursive t f t hasattr typing ForwardRef isinstance t typing ForwardRef t __forward_arg__ hasattr typing _ForwardRef isinstance t typing _ForwardRef t __forward_arg__ mapping = _trivial_mappings get t None mapping mapping Handle types contained types contained = getattr t __args__ None Callables contain bare List arguments contained = t isinstance t list contained Python puts type vars into __args__ unbound types such Dict all isinstance ct typing TypeVar ct contained contained = contained_type_annots = _annotation_type_to_stable_str ct sig_str True ct contained contained_type_str = f join contained_type_annots len contained_type_annots origin = getattr t __origin__ None origin None Unbound types don t have ` __origin__ ` some Python versions so fix up here origin = t t _UNBOUND_TYPES origin origin tuple tuple f Tuple contained_type_str origin typing Union Annoying hack detect Optional len contained == contained type None ^ contained type None not_none_param = contained contained type None contained f Optional _annotation_type_to_stable_str not_none_param sig_str True f Union contained_type_str origin dict dict f Dict contained_type_str origin list list f List contained_type_str origin type type f Type contained_type_str isinstance t typing Callable len contained contained Ellipsis f Callable join contained_type_annots - contained_type_annots - f Callable contained_type_str t ArgumentT ArgumentT TypeVar bound torch fx node Argument f torch fx node Argument contained_type_str raise RuntimeError f Unrecognized type t used BC-compatible type signature sig_str f Please add support type confirm f FX team your signature change valid raise RuntimeError f Unrecognized type t used BC-compatible type signature sig_str f Please add support type confirm f FX team your signature change valid test_function_back_compat Test backward compatibility function signatures compatibility is_backward_compatible=True Currently checks exact signature matches which may lead false positives If becomes too annoying we can refine check actually parse out saved schema strings check change truly backward- incompatible signature_strs = obj _BACK_COMPAT_OBJECTS isinstance obj type signature_strs append _fn_to_stable_annotation_str obj signature_strs sort try assertExpected \n join signature_strs + \n fx_backcompat_function_signatures except AssertionError e msg = f e \n ERROR \nAn FX function has been marked f backwards-compatible has experienced signature change See f above exception context more information If change f unintended please revert If intended check FX f team ensure proper deprecation protocols have been followed f subsequently -- accept change raise AssertionError msg noqa B test_class_member_back_compat Test backward compatibility members classes compatibility is_backward_compatible=True Currently checks exact matches publicly visible members class_method_strs = obj _BACK_COMPAT_OBJECTS isinstance obj type public_members = name name obj __dict__ name startswith _ class_method_strs append f torch typename obj sorted public_members class_method_strs sort try assertExpected \n join class_method_strs fx_backcompat_class_members except AssertionError e msg = f e \n ERROR \nAn FX has been marked f backwards-compatible has experienced change its public members See f above exception context more information If change f unintended please revert If intended check FX f team ensure proper deprecation protocols have been followed f subsequently -- accept change raise AssertionError msg e test_public_api_surface non_back_compat_objects = check_symbols_have_bc_designation m seen m __name__ startswith torch fx m __name__ startswith torch fx experimental It s really common inner functions point random modules - make sure we don t recurse into modules we ve already checked seen add m __name__ k v m __dict__ items hasattr v __name__ v __name__ seen continue v m continue k startswith _ continue isinstance v types ModuleType check_symbols_have_bc_designation v seen isinstance v type types FunctionType v _MARKED_WITH_COMPATIBILITY non_back_compat_objects setdefault v check_symbols_have_bc_designation torch fx set check_symbols_have_bc_designation torch fx passes set non_back_compat_strs = torch typename obj obj non_back_compat_objects keys Only want objects torch fx non_back_compat_strs = s s non_back_compat_strs s startswith torch fx s startswith torch fx experimental Only want objects public namespaces non_back_compat_strs = s s non_back_compat_strs all atom startswith _ atom s split non_back_compat_strs sort len non_back_compat_strs = raise AssertionError f Public FX API s non_back_compat_strs introduced given f backwards-compatibility classification Please decorate these f API s ` torch fx _compatibility compatibility ` specify f BC guarantees test_adding_side_effect_function TestModule torch nn Module forward x side_effect_func x x gm = torch fx symbolic_trace TestModule assertEqual len gm graph nodes gm graph eliminate_dead_code gm recompile assertEqual len gm graph nodes found = False node gm graph nodes node op == call_function node target == side_effect_func found = True assertTrue found test_preserve_unused_attr_after_unpickle gm = torch fx symbolic_trace Add gm add_submodule foo Add gm dummy_buffer = torch nn Buffer torch empty gm register_parameter dummy_parameter torch nn Parameter torch empty b = io BytesIO torch save gm b b seek weights_only=False loads GraphModule GLOBAL torch fx graph_module reduce_graph_module allowed global default reload_gm = torch load b weights_only=False assertTrue hasattr reload_gm foo assertTrue hasattr reload_gm dummy_buffer assertTrue hasattr reload_gm dummy_parameter This failing Python https github com pytorch pytorch issues unittest skipIf sys version_info = Failing python + TestFunctionalTracing JitTestCase setUp super setUp Checking mutable operations while tracing feature flagged Enable testing default orig_tracer_mutable_flag = torch fx proxy TracerBase check_mutable_operations torch fx proxy TracerBase check_mutable_operations = True tearDown super tearDown torch fx proxy TracerBase check_mutable_operations = orig_tracer_mutable_flag IGNORE_FUNCS = has_torch_function has_torch_function_unary has_torch_function_variadic handle_torch_function boolean_dispatch TO_PATCH = has_torch_function None has_torch_function_unary None has_torch_function_variadic None BUILT_IN_FUNC = AssertionError PROXY_ITERABLE = TypeError r argument type Proxy iterable PROXY_ITERATED = TraceError r Proxy object cannot iterated LEN_ERROR = RuntimeError r len supported symbolic tracing default ARG_TYPE_MISMATCH = TypeError r Proxy$ CONTROL_FLOW = TraceError r symbolically traced variables cannot used inputs control flow INTERPOLATE_ARGS_CONFLICT = ValueError r only one size scale_factor should defined MUTABLE = RuntimeError r Tried trace mutable operation UNTRACEABLE_FUNCTIONALS = adaptive_avg_pool d BUILT_IN_FUNC avg_pool d BUILT_IN_FUNC avg_pool d BUILT_IN_FUNC avg_pool d BUILT_IN_FUNC bilinear BUILT_IN_FUNC celu_ BUILT_IN_FUNC channel_shuffle BUILT_IN_FUNC native_channel_shuffle BUILT_IN_FUNC conv d BUILT_IN_FUNC conv d BUILT_IN_FUNC conv d BUILT_IN_FUNC conv_tbc BUILT_IN_FUNC conv_transpose d BUILT_IN_FUNC conv_transpose d BUILT_IN_FUNC conv_transpose d BUILT_IN_FUNC cosine_similarity BUILT_IN_FUNC elu_ BUILT_IN_FUNC gelu BUILT_IN_FUNC hardshrink BUILT_IN_FUNC hardtanh_ BUILT_IN_FUNC leaky_relu_ BUILT_IN_FUNC linear BUILT_IN_FUNC logsigmoid BUILT_IN_FUNC one_hot BUILT_IN_FUNC pairwise_distance BUILT_IN_FUNC pdist BUILT_IN_FUNC pixel_shuffle BUILT_IN_FUNC pixel_unshuffle BUILT_IN_FUNC prelu BUILT_IN_FUNC relu_ BUILT_IN_FUNC rrelu_ BUILT_IN_FUNC selu_ BUILT_IN_FUNC scaled_dot_product_attention BUILT_IN_FUNC softplus BUILT_IN_FUNC softshrink BUILT_IN_FUNC threshold_ BUILT_IN_FUNC adaptive_avg_pool d LEN_ERROR adaptive_avg_pool d LEN_ERROR adaptive_max_pool d_with_indices LEN_ERROR adaptive_max_pool d_with_indices LEN_ERROR instance_norm CONTROL_FLOW adaptive_max_pool d PROXY_ITERABLE adaptive_max_pool d PROXY_ITERABLE adaptive_max_pool d PROXY_ITERABLE fractional_max_pool d PROXY_ITERABLE fractional_max_pool d PROXY_ITERABLE max_pool d PROXY_ITERABLE max_pool d PROXY_ITERABLE max_pool d PROXY_ITERABLE lp_pool d PROXY_ITERATED lp_pool d PROXY_ITERATED max_unpool d PROXY_ITERATED max_unpool d PROXY_ITERATED max_unpool d PROXY_ITERATED fold PROXY_ITERATED unfold PROXY_ITERATED affine_grid CONTROL_FLOW alpha_dropout CONTROL_FLOW batch_norm CONTROL_FLOW binary_cross_entropy CONTROL_FLOW binary_cross_entropy_with_logits CONTROL_FLOW celu CONTROL_FLOW cosine_embedding_loss CONTROL_FLOW cross_entropy CONTROL_FLOW ctc_loss CONTROL_FLOW dropout CONTROL_FLOW dropout d CONTROL_FLOW dropout d CONTROL_FLOW dropout d CONTROL_FLOW elu CONTROL_FLOW embedding CONTROL_FLOW embedding_bag CONTROL_FLOW feature_alpha_dropout CONTROL_FLOW gaussian_nll_loss CONTROL_FLOW glu CONTROL_FLOW grid_sample CONTROL_FLOW group_norm CONTROL_FLOW gumbel_softmax CONTROL_FLOW hardsigmoid CONTROL_FLOW hardswish CONTROL_FLOW hardtanh CONTROL_FLOW hinge_embedding_loss CONTROL_FLOW huber_loss CONTROL_FLOW interpolate CONTROL_FLOW kl_div CONTROL_FLOW l _loss CONTROL_FLOW leaky_relu CONTROL_FLOW local_response_norm CONTROL_FLOW margin_ranking_loss CONTROL_FLOW mse_loss CONTROL_FLOW multi_head_attention_forward CONTROL_FLOW multi_margin_loss CONTROL_FLOW multilabel_margin_loss CONTROL_FLOW multilabel_soft_margin_loss CONTROL_FLOW nll_loss CONTROL_FLOW poisson_nll_loss CONTROL_FLOW relu CONTROL_FLOW relu CONTROL_FLOW rrelu CONTROL_FLOW selu CONTROL_FLOW silu CONTROL_FLOW mish CONTROL_FLOW smooth_l _loss CONTROL_FLOW soft_margin_loss CONTROL_FLOW threshold CONTROL_FLOW triplet_margin_loss CONTROL_FLOW triplet_margin_with_distance_loss CONTROL_FLOW upsample CONTROL_FLOW upsample_bilinear INTERPOLATE_ARGS_CONFLICT upsample_nearest INTERPOLATE_ARGS_CONFLICT List nn functionals Tensor inputs type annotation FUNCTIONALS_WITHOUT_ANNOTATION = adaptive_max_pool d adaptive_max_pool d adaptive_max_pool d fractional_max_pool d fractional_max_pool d max_pool d max_pool d max_pool d gaussian_nll_loss upsample upsample_bilinear upsample_nearest Inconsistent behavior between Python other Python versions - Python + Re-raise internal exception like ` PROXY_ITERATED ` - Other Python Raise ` argument type Proxy iterable ` due same internal exception above Use following map override expected exception Python UNTRACEABLE_FUNCTIONALS_PY = adaptive_max_pool d PROXY_ITERATED adaptive_max_pool d PROXY_ITERATED adaptive_max_pool d PROXY_ITERATED fractional_max_pool d PROXY_ITERATED fractional_max_pool d PROXY_ITERATED max_pool d PROXY_ITERATED max_pool d PROXY_ITERATED max_pool d PROXY_ITERATED group_norm CONTROL_FLOW classmethod _get_functional cls functional_list = f dir torch nn functional f islower continue Ignore internal functions f startswith _ continue Ignore supporting functions f cls IGNORE_FUNCS continue fn = getattr torch nn functional f Ignore non-callable object like modules isinstance fn Callable continue f cls FUNCTIONALS_WITHOUT_ANNOTATION try sig = inspect signature fn has_tensor_arg = False param sig parameters values isinstance param annotation type issubclass param annotation torch Tensor has_tensor_arg = True has_tensor_arg continue No signature Object supported except ValueError pass functional_list append f fn functional_list classmethod generate_test_func cls func_name fn functional_test func_name UNTRACEABLE_FUNCTIONALS_PY sys version_info exc err = UNTRACEABLE_FUNCTIONALS_PY func_name assertRaisesRegex exc err symbolic_trace fn func_name UNTRACEABLE_FUNCTIONALS exc err = UNTRACEABLE_FUNCTIONALS func_name assertRaisesRegex exc err symbolic_trace fn symbolic_trace fn functional_test classmethod generate_tests cls functional_list = cls _get_functional func_name fn functional_list test_name = test_nn_functional_ + func_name functional_test = cls generate_test_func func_name fn setattr cls test_name functional_test classmethod setUpClass cls no args kwargs False name cls TO_PATCH keys cls TO_PATCH name = getattr torch nn functional name setattr torch nn functional name no classmethod tearDownClass cls name cls TO_PATCH keys setattr torch nn functional name cls TO_PATCH name TestFunctionalTracing generate_tests instantiate_device_type_tests TestOperatorSignatures globals skipIfTorchDynamo too slow skipIfNoTorchVision TestVisionTracing JitTestCase setUp Checking mutable operations while tracing feature flagged Enable testing default orig_tracer_mutable_flag = torch fx proxy TracerBase check_mutable_operations torch fx proxy TracerBase check_mutable_operations = True tearDown torch fx proxy TracerBase check_mutable_operations = orig_tracer_mutable_flag PROXY_ITERATED = TraceError r Proxy object cannot iterated INCONSISTENT_TYPE = RuntimeError r Return value annotated having type __torch__ torchvision models \w + actually type Tensor UNTRACEABLE_MODELS = fasterrcnn_resnet _fpn PROXY_ITERATED fasterrcnn_resnet _fpn_v PROXY_ITERATED fasterrcnn_mobilenet_v _large_ _fpn PROXY_ITERATED fasterrcnn_mobilenet_v _large_fpn PROXY_ITERATED maskrcnn_resnet _fpn PROXY_ITERATED maskrcnn_resnet _fpn_v PROXY_ITERATED keypointrcnn_resnet _fpn PROXY_ITERATED retinanet_resnet _fpn PROXY_ITERATED retinanet_resnet _fpn_v PROXY_ITERATED ssd _vgg PROXY_ITERATED fcos_resnet _fpn PROXY_ITERATED ssdlite _mobilenet_v _large PROXY_ITERATED UNSCRIPTABLE_MODELS = googlenet INCONSISTENT_TYPE inception_v INCONSISTENT_TYPE output_transform = fcn_resnet lambda x x out fcn_resnet lambda x x out deeplabv _resnet lambda x x out deeplabv _resnet lambda x x out deeplabv _mobilenet_v _large lambda x x out lraspp_mobilenet_v _large lambda x x out fasterrcnn_resnet _fpn lambda x x fasterrcnn_mobilenet_v _large_fpn lambda x x fasterrcnn_mobilenet_v _large_ _fpn lambda x x maskrcnn_resnet _fpn lambda x x keypointrcnn_resnet _fpn lambda x x retinanet_resnet _fpn lambda x x classmethod generate_test_fn cls name x kwargs run_test model = torchvision_models get_model name kwargs model = model eval name UNTRACEABLE_MODELS err exc = UNTRACEABLE_MODELS name assertRaisesRegex err exc graph = symbolic_trace model out_transform = output_transform get name lambda x x graph torch fx GraphModule = symbolic_trace model = out_transform model x b = out_transform graph x assertEqual b name UNSCRIPTABLE_MODELS err exc = UNSCRIPTABLE_MODELS name assertRaisesRegex err exc script = torch jit script graph script = torch jit script graph c = out_transform script x assertEqual c run_test classmethod generate_classification_tests cls k torchvision_models list_models module=torchvision_models test_name = test_torchvision_models_ + k x = torch rand k inception_v torch rand kwargs = dict num_classes= model_test = cls generate_test_fn k x kwargs setattr cls test_name model_test classmethod generate_segmentation_tests cls k torchvision_models list_models module=torchvision_models segmentation test_name = test_torchvision_models_segmentation_ + k x = torch rand kwargs = dict num_classes= pretrained_backbone=False model_test = cls generate_test_fn k x kwargs setattr cls test_name model_test classmethod generate_detection_tests cls k torchvision_models list_models module=torchvision_models detection test_name = test_torchvision_models_detection_ + k x = torch rand kwargs = dict num_classes= pretrained_backbone=False model_test = cls generate_test_fn k x kwargs setattr cls test_name model_test classmethod generate_video_tests cls k torchvision_models list_models module=torchvision_models video test_name = test_torchvision_models_video_ + k x = torch rand k mvit_v _b mvit_v _s s d torch rand kwargs = dict num_classes= model_test = cls generate_test_fn k x kwargs setattr cls test_name model_test classmethod generate_tests cls cls generate_classification_tests cls generate_detection_tests cls generate_segmentation_tests cls generate_video_tests HAS_TORCHVISION TestVisionTracing generate_tests __name__ == __main__ run_tests