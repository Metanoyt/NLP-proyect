mypy allow-untyped-defs functools itertools torch _dynamo utils counters config pattern_matcher Arg CallFunction KeywordArg freezing_patterns register_binary_folding_pattern aten = torch ops aten prims = torch ops prims mark_mixed_dtype computation_node computation_node_dtype = computation_node meta val dtype computation_node_dtype torch float torch bfloat len computation_node users = computation_node_user = next iter computation_node users keys isinstance computation_node_user meta val torch Tensor computation_node_user meta val dtype = torch float while computation_node_user target _binary_ops len computation_node_user users = computation_node_user = next iter computation_node_user users keys computation_node_user target = prims convert_element_type default computation_node meta _allow_mixed_dtype_folding = computation_node_dtype mark_mixed_dtype_allowed_computation_ops gm Mark convolutions linear which we will binary fold even mixed precision constants We constant fold higher precision better accuracy then recover original precision after target aten convolution default aten addmm default aten mm default node gm graph find_nodes op= call_function target=target mark_mixed_dtype node recover_original_precision_folded_computation_ops gm After binary folding conv linear weights biases higher dtype recover original precision they graph = gm graph target idx aten convolution default aten addmm default aten mm default node graph find_nodes op= call_function target=target orig_dtype = node meta get _allow_mixed_dtype_folding None orig_dtype None continue graph inserting_before node i idx old_input = node args i old_input None continue new_input = graph create_node call_function prims convert_element_type default old_input orig_dtype node replace_input_with old_input new_input _binary_ops = aten add Tensor aten sub Tensor aten mul Tensor aten div Tensor functools cache binary_folding_init _conv_args = Arg _ range _addmm_args = Arg _ range _mm_args = Arg _ range _computation_ops = aten convolution default aten addmm default aten mm default _computation_calls = CallFunction aten convolution default _conv_args _users= CallFunction aten addmm default _addmm_args _users= CallFunction aten reshape default CallFunction aten addmm default _addmm_args _users= Arg _users= CallFunction aten mm default _mm_args _users= CallFunction aten reshape default CallFunction aten mm default _mm_args _users= Arg _users= In order fuse add sub mul div conv linear dimensions its constant tensor must satisfy following - resizing broadcast w weight bias tensor shape - broadcast conv linear output shape It needs have shape can resize weight bias tensor shape because we need run op conv linear weights bias without changing their sizes It needs broadcast conv linear output shape so we do accidentally change shape op output pre-fusing compared eager The only dimension value shared weight bias conv linear output they all contain dim value = channels-out In conv linear output tensor second dimension so pointwise op tensor may have second dimension value == channels-out all other dimensions have _op_not_broadcasting_with_conv weight_tensor other_tensor According opDoesNotBroadCastWithConv frozen_conv_folding cpp weight_shape = weight_tensor shape other_shape = other_tensor shape len weight_shape len other_shape False len weight_shape == len other_shape + weight shape o i other_shape o i reversed range len other_shape i == weight_shape == other_shape i continue other_shape i = False weight shape o i other_shape i i reversed range len other_shape i == weight_shape == other_shape i continue other_shape i = False True _op_not_broadcasting_with_linear weight_tensor other_tensor has_reshape weight_shape = weight_tensor shape other_shape = other_tensor shape other_shapes = torch Size weight_shape torch Size weight_shape torch Size torch Size has_reshape other_shapes extend torch Size weight_shape torch Size other_shape other_shapes _check_conv_and_broadcast_op conv_node other According checkConvAndBroadcastingOpPreConditions frozen_conv_folding cpp conv weight conv_node args op = get_attr False conv bias conv_node args None conv_node args op = get_attr False isinstance other int isinstance other float other op = get_attr False len conv_node args users = False weight_meta_value = conv_node args meta get val weight_meta_value None False Avoid fusing op causes type promotion restricting float avoids int float difficulties scalar overload weight_meta_value is_floating_point False isinstance other torch fx Node other op == get_attr other_meta_value = other meta get val other_meta_value is_floating_point type ignore union-attr False torch promote_types other_meta_value dtype weight_meta_value dtype type ignore union-attr = weight_meta_value dtype conv_node meta get _allow_mixed_dtype_folding False False other_meta_value dtype = torch float type ignore union-attr weight_meta_value dtype torch float torch bfloat False _op_not_broadcasting_with_conv weight_meta_value other_meta_value False isinstance other float False True _check_linear_and_broadcast_op linear_node other has_reshape weight_node = linear_node args linear_node target aten addmm default linear_node args bias_node = linear_node args linear_node target aten addmm default None weight_node op = get_attr False bias_node None bias_node op = get_attr False isinstance other int isinstance other float other op = get_attr False len weight_node users = False weight_meta_value = weight_node meta get val weight_meta_value None False Avoid fusing op causes type promotion restricting float avoids int float difficulties scalar overload weight_meta_value is_floating_point False isinstance other torch fx Node other op == get_attr other_meta_value = other meta get val other_meta_value is_floating_point type ignore union-attr False torch promote_types other_meta_value dtype weight_meta_value dtype type ignore union-attr = weight_meta_value dtype linear_node meta get _allow_mixed_dtype_folding False False other_meta_value dtype = torch float type ignore union-attr weight_meta_value dtype torch float torch bfloat False _op_not_broadcasting_with_linear weight_meta_value other_meta_value has_reshape False isinstance other float False True _is_foldable_pattern match binary_node = match output_node has_reshape = False binary_node args target _computation_ops computation_node = binary_node args other = binary_node args binary_node args target aten reshape default computation_node = binary_node args args other = binary_node args has_reshape = True binary_node args target _computation_ops computation_node = binary_node args other = binary_node args computation_node = binary_node args args other = binary_node args has_reshape = False computation_node target aten convolution default _check_conv_and_broadcast_op computation_node other computation_node target aten addmm default aten mm default config enable_linear_binary_folding _check_linear_and_broadcast_op computation_node other has_reshape False resize_scalar_or_tensor_to_shape graph other shape weight isinstance other float torch utils _python_dispatch _disable_current_modes other_tensor = torch tensor other dtype=weight dtype device=weight device graph owning_module register_buffer other_tensor other_tensor res = graph create_node get_attr other_tensor res = graph create_node call_function aten reshape default res res = graph create_node call_function aten expand default res shape other meta get val numel == expand errors shape input has less dims than tensor input res = graph create_node call_function aten reshape default other res = graph create_node call_function aten expand default res shape res = graph create_node call_function aten reshape default other shape res _create_new_conv_node graph conv_node binary_node other assert conv_node target aten convolution default conv_args = list conv_node args weight_meta_value = conv_node args meta get val bias = conv_args binary_node target aten add Tensor aten sub Tensor other_reshape = resize_scalar_or_tensor_to_shape graph other weight_meta_value size weight_meta_value new_bias = graph create_node call_function binary_node target bias None bias other_reshape conv_args = new_bias assert binary_node target aten mul Tensor aten div Tensor weight_broadcast_shape = _ range len weight_meta_value shape weight_broadcast_shape = weight_meta_value size other_reshape = resize_scalar_or_tensor_to_shape graph other tuple weight_broadcast_shape weight_meta_value new_weight = graph create_node call_function binary_node target conv_args other_reshape new_weight meta update conv_args meta conv_args = new_weight bias None other_reshape = resize_scalar_or_tensor_to_shape graph other weight_meta_value size weight_meta_value new_bias = graph create_node call_function binary_node target bias other_reshape new_bias meta update bias meta conv_args = new_bias graph create_node call_function conv_node target tuple conv_args _create_new_linear_node graph linear_node binary_node other assert linear_node target aten addmm default aten mm default input_node = linear_node args linear_node target aten addmm default linear_node args weight_node = linear_node args linear_node target aten addmm default linear_node args bias_node = linear_node args linear_node target aten addmm default None weight_meta_value = weight_node meta get val binary_node target aten add Tensor aten sub Tensor other_reshape = resize_scalar_or_tensor_to_shape graph other weight_meta_value size weight_meta_value new_bias_node = graph create_node call_function binary_node target bias_node None bias_node other_reshape graph create_node call_function aten addmm default new_bias_node input_node weight_node assert binary_node target aten mul Tensor aten div Tensor weight_broadcast_shape = weight_meta_value size other_reshape = resize_scalar_or_tensor_to_shape graph other tuple weight_broadcast_shape weight_meta_value new_weight_node = graph create_node call_function binary_node target weight_node other_reshape new_weight_node meta update weight_node meta bias_node None other_reshape = resize_scalar_or_tensor_to_shape graph other weight_meta_value size weight_meta_value new_bias_node = graph create_node call_function binary_node target bias_node other_reshape new_bias_node meta update bias_node meta graph create_node call_function linear_node target new_bias_node input_node new_weight_node graph create_node call_function linear_node target input_node new_weight_node _computation_call binary_op itertools product _computation_calls _binary_ops register_binary_folding_pattern CallFunction binary_op _computation_call KeywordArg other extra_check=_is_foldable_pattern folded_op match args kwargs counters inductor binary_folding += other = kwargs get other binary_node = match output_node reshape_node = None binary_node args target _computation_ops computation_node = binary_node args binary_node args target aten reshape default computation_node = binary_node args args reshape_node = binary_node args binary_node args target _computation_ops computation_node = binary_node args computation_node = binary_node args args reshape_node = binary_node args graph = match graph graph inserting_before reshape_node reshape_node binary_node assert computation_node target _computation_ops computation_node target aten convolution default counters inductor binary_folding_conv += new_computation_node = _create_new_conv_node graph computation_node binary_node other new_computation_node = _create_new_linear_node graph computation_node binary_node other new_computation_node meta update computation_node meta reshape_node assert reshape_node target aten reshape default computation_node replace_all_uses_with new_computation_node binary_node replace_all_uses_with reshape_node binary_node replace_all_uses_with new_computation_node graph erase_node binary_node graph erase_node computation_node