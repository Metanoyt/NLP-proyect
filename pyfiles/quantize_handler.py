mypy allow-untyped-defs abc ABC collections abc Callable typing Optional torch torch ao quantization backend_config BackendConfig DTypeConfig ObservationType torch ao quantization utils NodePattern Pattern QuantizerCls torch fx graph Node utils all_node_args_have_no_tensors __all__ = QuantizeHandler BinaryOpQuantizeHandler CatQuantizeHandler ConvReluQuantizeHandler LinearReLUQuantizeHandler BatchNormQuantizeHandler EmbeddingQuantizeHandler RNNDynamicQuantizeHandler DefaultNodeQuantizeHandler FixedQParamsOpQuantizeHandler CopyNodeQuantizeHandler GeneralTensorShapeOpQuantizeHandler CustomModuleQuantizeHandler StandaloneModuleQuantizeHandler _default_root_node_getter node_pattern node_pattern None node_pattern while isinstance node_pattern Node node_pattern = node_pattern - node_pattern Base Pattern Handler QuantizeHandler ABC noqa B Base handler quantizer patterns __init__ node_pattern NodePattern modules dict str torch nn Module root_node_getter Optional Callable = None is_custom_module=False is_standalone_module=False Records pattern information __init__ which will used convert node_pattern = node_pattern modules = modules root_node_getter None root_node_getter = _default_root_node_getter root_node = root_node_getter node_pattern is_custom_module_ = is_custom_module is_standalone_module_ = is_standalone_module num_tensor_args = determine how many first two args Tensors versus scalars distinguishes things like x + y x + + x isinstance root_node Node cache_for_no_tensor_check dict Node bool = arg_idx range len root_node args arg = root_node args arg_idx isinstance arg Node all_node_args_have_no_tensors arg modules cache_for_no_tensor_check num_tensor_args += is_general_tensor_value_op - bool Returns True operator works both floating point quantized input does some computation based input Tensor ops only re-arranges Tensor values query some metadata about Tensor so we need insert observer fake_quant output operator same observer instance input since distribution values different input output Tensors HistogramObserver while they share same quantization parameters Example operator avgpool d reshape transpose maxpool d Example observed operator observer_ - avgpool d - observer_ same observer instance input False is_custom_module is_custom_module_ is_standalone_module is_standalone_module_ _get_quantize_handler_cls observation_type ObservationType dtype_configs list DTypeConfig num_tensor_args_to_observation_type dict int ObservationType - type QuantizeHandler Return configurable QuantizeHandler matches given specifications backend ConfigurableQuantizeHandler QuantizeHandler __init__ node_pattern NodePattern modules dict str torch nn Module root_node_getter Optional Callable = None super __init__ node_pattern modules root_node_getter num_tensor_args_to_observation_type num_tensor_args num_tensor_args_to_observation_type raise AssertionError f Must provide observation_type config tensor number num_tensor_args f num_tensor_args_to_observation_type node_pattern observation_type = num_tensor_args_to_observation_type num_tensor_args observation_type = observation_type dtype_configs = dtype_configs is_general_tensor_value_op - bool observation_type == ObservationType OUTPUT_SHARE_OBSERVER_WITH_INPUT ConfigurableQuantizeHandler _get_pattern_to_quantize_handlers backend_config BackendConfig - dict Pattern QuantizerCls Note Quantize handler just holder some check methods like should_insert_observer_for_output maybe can enum well we can refactor after we convert path fbgemm qnnpack fully new path exposed backend developers pattern_to_quantize_handlers = pattern config backend_config _pattern_complex_format_to_config items observation_type = config observation_type dtype_configs = config dtype_configs num_tensor_args_to_observation_type = config _num_tensor_args_to_observation_type pattern_to_quantize_handlers pattern = _get_quantize_handler_cls observation_type dtype_configs num_tensor_args_to_observation_type pattern_to_quantize_handlers TODO remove still exposed torch ao quantization we should able break bc BinaryOpQuantizeHandler QuantizeHandler pass CatQuantizeHandler QuantizeHandler pass TODO remove ConvReluQuantizeHandler QuantizeHandler pass TODO remove LinearReLUQuantizeHandler QuantizeHandler pass TODO remove BatchNormQuantizeHandler QuantizeHandler pass TODO remove EmbeddingQuantizeHandler QuantizeHandler pass TODO remove RNNDynamicQuantizeHandler QuantizeHandler pass TODO remove DefaultNodeQuantizeHandler QuantizeHandler Common quantized op first input first output will quantized TODO remove FixedQParamsOpQuantizeHandler QuantizeHandler pass TODO remove CopyNodeQuantizeHandler QuantizeHandler pass TODO remove GeneralTensorShapeOpQuantizeHandler QuantizeHandler pass TODO used can removed after torch ao quantization namespace deprecated CustomModuleQuantizeHandler QuantizeHandler pass TODO used can removed after torch ao quantization namespace deprecated StandaloneModuleQuantizeHandler QuantizeHandler pass